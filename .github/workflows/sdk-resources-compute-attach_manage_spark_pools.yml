# This code is autogenerated.
# Code is generated by running custom script: python3 readme.py
# Any manual changes to this file may cause incorrect behavior.
# Any manual changes will be overwritten if the code is regenerated.

name: sdk-resources-compute-attach_manage_spark_pools
# This file is created by sdk/python/readme.py.
# Please do not edit directly.
on:
  workflow_dispatch:
  schedule:
    - cron: "27 3/12 * * *"
  pull_request:
    branches:
      - main
    paths:
      - sdk/python/resources/compute/**
      - .github/workflows/sdk-resources-compute-attach_manage_spark_pools.yml
      - sdk/python/dev-requirements.txt
      - infra/bootstrapping/**
      - sdk/python/setup.sh
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - name: check out repo
      uses: actions/checkout@v2
    - name: setup python
      uses: actions/setup-python@v2
      with:
        python-version: "3.8"
    - name: pip install notebook reqs
      run: pip install -r sdk/python/dev-requirements.txt
    - name: azure login
      uses: azure/login@v1
      with:
        creds: ${{secrets.AZUREML_CREDENTIALS}}
    - name: bootstrap resources
      run: |
          echo '${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}';
          bash bootstrap.sh
      working-directory: infra/bootstrapping
      continue-on-error: false
    - name: setup SDK
      run: |
          source "${{ github.workspace }}/infra/bootstrapping/sdk_helpers.sh";
          source "${{ github.workspace }}/infra/bootstrapping/init_environment.sh";
          bash setup.sh
      working-directory: sdk/python
      continue-on-error: true
    - name: setup-cli
      run: |
          source "${{ github.workspace }}/infra/bootstrapping/sdk_helpers.sh";
          source "${{ github.workspace }}/infra/bootstrapping/init_environment.sh";
          bash setup.sh
      working-directory: cli
      continue-on-error: true
    - name: setup spark resources
      run: |
          bash -x jobs/spark/setup_spark.sh jobs/spark/ resources/compute/attach_manage_spark_pools.ipynb
      working-directory: sdk/python
      continue-on-error: true
    - name: run resources/compute/attach_manage_spark_pools.ipynb
      run: |
          source "${{ github.workspace }}/infra/bootstrapping/sdk_helpers.sh";
          source "${{ github.workspace }}/infra/bootstrapping/init_environment.sh";
          bash "${{ github.workspace }}/infra/bootstrapping/sdk_helpers.sh" generate_workspace_config "../../.azureml/config.json";
          bash "${{ github.workspace }}/infra/bootstrapping/sdk_helpers.sh" replace_template_values "attach_manage_spark_pools.ipynb";
          [ -f "../../.azureml/config" ] && cat "../../.azureml/config";
          papermill -k python attach_manage_spark_pools.ipynb attach_manage_spark_pools.output.ipynb > sample_log.txt 2>&1
          cat sample_log.txt
      working-directory: sdk/python/resources/compute
    - name: Determine Failure Reason
      run: |
          failure_reason="N/A"
          if [ "${{ job.status }}" == "failure" ]; then
            if grep -q "ResourceNotReady" sample_log.txt; then
              failure_reason = "ResourceNotReady"
            elif grep -q "quota" sample_log.txt; then
              failure_reason="QuotaIssue"
            elif grep -q "ParentResourceNotFound" sample_log.txt; then
              failure_reason="ParentResourceNotFound"
            else
              failure_reason="UncategorizedFailure"
            fi
          fi
          echo "FAILURE_REASON=$failure_reason" >> $GITHUB_ENV
      working-directory: sdk/python/resources/compute
      if: ${{ always() }}
      continue-on-error: true
    - name: Log Job Results to Application Insights
      uses: syedhassaanahmed/app-insights-event-action@main
      with:
          instrumentation-key: "${{secrets.APP_INSIGHTS_INSTRUMENTATION_KEY}}"
          event-name: "${{ job.status }}_${{ env.FAILURE_REASON }}"
      if: ${{ always() }}
    - name: upload notebook's working folder as an artifact
      if: ${{ always() }}
      uses: actions/upload-artifact@v2
      with:
        name: attach_manage_spark_pools
        path: sdk/python/resources/compute
