# This code is autogenerated.
# Code is generated by running custom script: python3 readme.py
# Any manual changes to this file may cause incorrect behavior.
# Any manual changes will be overwritten if the code is regenerated.

name: cli-jobs-spark-attached-spark-pipeline-user-identity
on:
  workflow_dispatch:
  schedule:
    - cron: "15 4/12 * * *"
  pull_request:
    branches:
      - main
    paths:
      - cli/jobs/spark/**
      - infra/bootstrapping/**
      - .github/workflows/cli-jobs-spark-attached-spark-pipeline-user-identity.yml
      - cli/jobs/spark/data/titanic.csv
      - cli/setup.sh
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - name: check out repo
      uses: actions/checkout@v2
    - name: azure login
      uses: azure/login@v1
      with:
        creds: ${{secrets.AZUREML_CREDENTIALS}}
    - name: bootstrap resources
      run: |
          echo '${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}';
          bash bootstrap.sh
      working-directory: infra/bootstrapping
      continue-on-error: false
    - name: setup-cli
      run: |
          source "${{ github.workspace }}/infra/bootstrapping/sdk_helpers.sh";
          source "${{ github.workspace }}/infra/bootstrapping/init_environment.sh";
          bash setup.sh
      working-directory: cli
      continue-on-error: true
    - name: upload data
      run: |
          bash -x upload-data-to-blob.sh jobs/spark/
      working-directory: cli
    - name: setup attached spark
      working-directory: cli
      continue-on-error: true
      run: |
          bash -x jobs/spark/setup-attached-resources.sh resources/compute/attached-spark-user-identity.yml jobs/spark/attached-spark-pipeline-user-identity.yml
    - name: run job
      run: |
          source "${{ github.workspace }}/infra/bootstrapping/sdk_helpers.sh";
          source "${{ github.workspace }}/infra/bootstrapping/init_environment.sh";
          bash -x ../../run-job.sh attached-spark-pipeline-user-identity.yml > sample_log.txt 2>&1
      working-directory: cli/jobs/spark
    - name: Determine Failure Reason
      run: |
          cat sample_log.txt
          failure_reason="N/A"
          if [ "${{ job.status }}" == "failure" ]; then
            if grep -q "ResourceNotReady" sample_log.txt; then
              failure_reason = "ResourceNotReady"
            elif grep -q "quota" sample_log.txt; then
              failure_reason="QuotaIssue"
            elif grep -q "ParentResourceNotFound" sample_log.txt; then
              failure_reason="ParentResourceNotFound"
            elif grep -q "already exists" sample_log.txt; then
              failure_reason="ResourceAlreadyExists"
            elif grep -q "StorageAccountTypeConversionNotAllowed" sample_log.txt; then
              failure_reason="InvalidStorageAccount"
            else
              failure_reason="UncategorizedFailure"
            fi
          fi
          echo "FAILURE_REASON=$failure_reason" >> $GITHUB_ENV
      working-directory: cli/jobs/spark
      if: ${{ always() }}
      continue-on-error: true
    - name: Log Job Results to Application Insights
      uses: syedhassaanahmed/app-insights-event-action@v0.1-alpha
      with:
          instrumentation-key: "${{ secrets.APP_INSIGHTS_INSTRUMENTATION_KEY }}"
          event-name: "${{ job.status }}_${{ env.FAILURE_REASON }}_${{ github.ref_name }}"
      if: always()
