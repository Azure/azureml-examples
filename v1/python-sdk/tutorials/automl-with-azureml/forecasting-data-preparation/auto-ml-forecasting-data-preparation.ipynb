{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "**Data Preparation for Demand Forecasting Notebooks**\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data Work](#DataWork)\n",
    "1. [Data Partitioning](#DataPartition)\n",
    "1. [Data Upload](#DataUpload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The objective of this notebook is to illustrate how to pre-process the raw data and register partitioned datasets to be used in demand forecasting notebooks: (i) Demand Forecastsing Using TCN ([link placeholder]()) and (ii) Demand Forecasting Using Many Models ([link placeholder]()).\n",
    "\n",
    "For illustration purposes we use the UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)) which contains electricity consumption data for 370 consumers measured at 15 minute intervals. In this notebook we will show how to ingest the data from the original source, aggregate to an hourly frequency, select a subsample of unique time series, determine the approriate way to partition the data, and, finally, register the datasets to be used on the aforementoned notebooks.\n",
    "\n",
    "Make sure you have executed the [configuration](https://github.com/Azure/azureml-examples/blob/b13d6f9c5cf5c042bcc265fdbc4ed741c4527ded/v1/python-sdk/configuration.ipynb) before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import azureml.core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the Azure ML workspace requires authentication with Azure.\n",
    "\n",
    "The default authentication is interactive authentication using the default tenant. Executing the ws = Workspace.from_config() line in the cell below will prompt for authentication the first time that it is run.\n",
    "\n",
    "If you have multiple Azure tenants, you can specify the tenant by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
    "```\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "auth = InteractiveLoginAuthentication(tenant_id = 'mytenantid')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "If you need to run in an environment where interactive login is not possible, you can use Service Principal authentication by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
    "```\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "auth = ServicePrincipalAuthentication('mytenantid', 'myappid', 'mypassword')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "For more details, see aka.ms/aml-notebook-auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "output = {}\n",
    "output[\"Subscription ID\"] = ws.subscription_id\n",
    "output[\"Workspace\"] = ws.name\n",
    "output[\"Resource Group\"] = ws.resource_group\n",
    "output[\"Location\"] = ws.location\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "outputDf = pd.DataFrame(data=output, index=[\"\"])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data work\n",
    "We start with declaring some of the parameters that will be used in this notebook.\n",
    "\n",
    "- `IS_ORIGINAL_DATASET_DOWNLOADED` is a flag for wether we want to doanload the original data from the source. The flag is here to reduce the download time since the original dataset is larger than 1 GB.\n",
    "- `IS_MODIFIED_DATASET_UPLOADED` is a flag for wether the datasets are uploaded to the Datastore. We use it to prevent unintentional uploads of the same datasets.\n",
    "- `DOES_PARTITION_INCLUDE_VALIDATION_SET` is a placeholder for determining whether the partitioned data should include the validation set. The value True/False will be determined later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_ORIGINAL_DATASET_DOWNLOADED = False\n",
    "IS_MODIFIED_DATASET_UPLOADED = False\n",
    "DOES_PARTITION_INCLUDE_VALIDATION_SET = (\n",
    "    None  # place holder for the parameter value we will determine later.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify parameters specific to the data we will work with.\n",
    "\n",
    "- **Target column** is what we want to forecast. In our case it is electricity consumption per customer measured in kilowatt hours (kWh).\n",
    "- **Time column** is the time axis along which to predict.\n",
    "- **Time series identifier columns** are identified by values of the columns listed `time_series_id_column_names`. In our case all unique time series are identified by a single column `customer_id`. However, it is quite common to have multiple columns identifying unique time series. See the link for a more detailed explanation on this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = \"usage\"\n",
    "time_column_name = \"date\"\n",
    "GRAIN_COL = \"customer_id\"\n",
    "time_series_id_column_names = [GRAIN_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the followng block of code we will download the data from the original source to the `data` folder, load the data and print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_ORIGINAL_DATASET_DOWNLOADED:\n",
    "    print(\"Downloading data from the source ...\\n---\")\n",
    "    # Download original data\n",
    "    from io import BytesIO\n",
    "    from urllib.request import urlopen\n",
    "    from zipfile import ZipFile\n",
    "\n",
    "    zipurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\"\n",
    "\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            zfile.extractall(\"data\")\n",
    "    IS_ORIGINAL_DATASET_DOWNLOADED = True\n",
    "\n",
    "DATA_LOCATION = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing the first few rows of the downloaded data ...\\n---\")\n",
    "raw_df = pd.read_table(\"data/LD2011_2014.txt\", sep=\";\", low_memory=False)\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded data is in what is known as a \"wide\" format, meaning each column name that starts with \"MT_xxx\" represents one unique time series. The first columnn \"Unnamed: 0\" is actually a time stamp at which the obervation for every time series takes place. Let's raname this column to something more meaningful. We will call it `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.rename(columns={raw_df.columns[0]: \"date\"}, inplace=True)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"wide\" data format is not usable in AutoML, which is deisgned to accept the so-called \"long\" format data. To tranform the data from wide to long format, we will take the each uniqe time seres (date, MT_xxx) and stack them vertically. The end result will be a data frame with 3 columns: (i) the `date` column, (ii) `custmer_id` column which is the identifier of the time series and is derived from the column name in the wide format, and (iii) `usage` which is the target varaible we are trying to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting data from wide to long format. This may take a few minutes ...\\n---\")\n",
    "raw_df = pd.melt(raw_df, id_vars=\"date\", var_name=\"customer_id\", value_name=\"usage\")\n",
    "raw_df[time_column_name] = pd.to_datetime(raw_df[time_column_name])\n",
    "raw_df.to_csv(\"data/LD2011_2014_long_format.csv\", index=False)\n",
    "\n",
    "print(\"The first few rows of the data in the long format ...\\n---\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nseries = raw_df.groupby(time_series_id_column_names).ngroups\n",
    "print(\"Data contains {0} individual time-series.\".format(nseries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data tracks customers' electricity consumption every 15 minutes and is measured in kilowatt (kW) consumed. Say, the business requirement is to generate 24 hour forecast in kilowatt hours (kWh). Such forecast at 15 minute frequency results in the forecast horizon of 96 steps ahead (there 96 15-minute intervals in a 24-hour period). Moreover, if the requirement is to generate 24-hour ahead forecast, it makes sence to aggregate data measured at 15-minute intervals to an hourly frequency. This will reduce the forecast horizon by a factor of 4. The shorter the forecast horizon usually results in higher probability of achieving better forecast accuracy. \n",
    "\n",
    "In the next block of code we will create a `datetime` column which will identify the date and the hour of the day each observation belogs to. We also convert the target variable from kW to kWh, where $kWh = \\frac{1}{4} \\times kW $. After the conversion is complete, the hourly data will be stored in the `raw_hourly_df` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the previous command shows that the target varaible `usage` is an object. We need to transform it into a float in order to convert kW to kWh. The next command does exactly this. Because the original data contains European style format with decimals separated by commmas, we replase commas with periods before declaring target variaible as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[target_column_name] = (\n",
    "    raw_df[target_column_name].astype(str).apply(lambda x: float(x.replace(\",\", \".\")))\n",
    ")\n",
    "raw_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data to hourly. Here, the hourly column is called \"datetime\"\n",
    "new_time_column_name = \"datetime\"\n",
    "\n",
    "raw_df[new_time_column_name] = raw_df[time_column_name].dt.to_period(\"H\")\n",
    "raw_df[target_column_name] = raw_df[target_column_name] / 4  # convert to kWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to hourly consumption by adding kWh for every 15 min interval\n",
    "raw_hourly_series = raw_df.groupby([GRAIN_COL, new_time_column_name])[\n",
    "    target_column_name\n",
    "].sum()\n",
    "raw_hourly_df = pd.DataFrame(raw_hourly_series)\n",
    "raw_hourly_df.reset_index(drop=False, inplace=True)\n",
    "print(raw_hourly_df.head())\n",
    "\n",
    "del raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to the datetime format\n",
    "raw_hourly_df[new_time_column_name] = pd.to_datetime(\n",
    "    raw_hourly_df[new_time_column_name].astype(str)\n",
    ")\n",
    "raw_hourly_df.to_csv(\n",
    "    os.path.join(DATA_LOCATION, \"hourly_data_long_format.csv\"), index=False\n",
    ")\n",
    "raw_hourly_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize a sample of 50 randomly selected series. The plots will be stored in the `output_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grains = list(\n",
    "    pd.unique(raw_hourly_df[time_series_id_column_names].values.ravel(\"K\"))\n",
    ")\n",
    "grains_to_plot = random.sample(all_grains, k=50)\n",
    "print(f\"The following grains will be selected for plotting: \\n{grains_to_plot}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = raw_hourly_df[raw_hourly_df[GRAIN_COL].isin(grains_to_plot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an output folder\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"output_folder\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper_scripts import _draw_one_plot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plot_filename = \"raw_ts_plots.pdf\"\n",
    "\n",
    "pdf = PdfPages(os.path.join(OUTPUT_DIR, plot_filename))\n",
    "for grain, one_forecast in data_subset.groupby(GRAIN_COL):\n",
    "    one_forecast[new_time_column_name] = pd.to_datetime(\n",
    "        one_forecast[new_time_column_name]\n",
    "    )\n",
    "    one_forecast.sort_values(new_time_column_name, inplace=True)\n",
    "    _draw_one_plot(\n",
    "        one_forecast,\n",
    "        new_time_column_name,\n",
    "        target_column_name,\n",
    "        time_series_id_column_names,\n",
    "        pdf,\n",
    "        plot_predictions=False,\n",
    "    )\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"./output_folder/raw_ts_plots.pdf\", width=800, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{os.path.join(OUTPUT_DIR, \"raw_ts_plots.pdf\")}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close examination of the consumption plots per customer shows there are quite a few customers that have no usage data prior to January 1, 2012. Some customers do not have the data until January of 2013 or 2014. It is reasonable at this point to drop all observations prior to January 1, 2012. We will call this object `clean_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop grains that have no usage as of Jan 1, 2012\n",
    "tmp_df = raw_hourly_df[raw_hourly_df[new_time_column_name] == \"2012-01-01 01:00:00\"]\n",
    "grains_to_drop = list(tmp_df[tmp_df[target_column_name] == 0][GRAIN_COL])\n",
    "print(f\"Number of grains to be dropped: {len(grains_to_drop)}\")\n",
    "del tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = raw_hourly_df[~raw_hourly_df[GRAIN_COL].isin(grains_to_drop)]\n",
    "\n",
    "# drop observations prior to 1/1/2012 since they are zero for all grains\n",
    "clean_df = clean_df[clean_df[new_time_column_name] > \"2011-12-31 23:00:00\"]\n",
    "\n",
    "del raw_hourly_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time illustrating demand forecasting notebooks, we will use a small subset of 10 unique time series from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grains = list(pd.unique(clean_df[time_series_id_column_names].values.ravel(\"K\")))\n",
    "selected_grains = random.sample(all_grains, k=10)\n",
    "print(f\"The following grains will be selected:  {selected_grains}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = clean_df[clean_df[GRAIN_COL].isin(selected_grains)]\n",
    "nseries = data_subset.groupby(time_series_id_column_names).ngroups\n",
    "print(\"Data subset contains {0} individual time-series.\\n---\".format(nseries))\n",
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_grains, selected_grains, clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full dataset\\n---\")\n",
    "for grain, tmp_df in data_subset.groupby(time_series_id_column_names):\n",
    "    print(\n",
    "        \"Grain:{}.\\\n",
    "    Min date: {}\\\n",
    "    Max date: {}\\\n",
    "    N: {}\".format(\n",
    "            grain, tmp_df[\"datetime\"].min(), tmp_df[\"datetime\"].max(), tmp_df.shape[0]\n",
    "        )\n",
    "    )\n",
    "# del tmp_df, grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.to_csv(os.path.join(OUTPUT_DIR, \"small_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Partitioning\n",
    "The objective of this section is to determine where you want to estimate low capacity models such as many models (see [link](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-many-models)) or a deep learning model (see [link](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-github-dau) for details). Low capacity models, which include all models with the exception of TCN, require frequent retraining to maintain desired forecast accuracy. It best not to provide validation set data in such scenario. TCN, on the other hand, is a high capacity model and requires infrequent retraining. Hence, validation data is imperative here. As a result, we need to partition the data in such a way as to reflect the modelling choice.\n",
    "\n",
    "Before deciding on the modelling framework, let's visualize the small subset of data we selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filename = \"ts_plots_small_data.pdf\"\n",
    "\n",
    "pdf = PdfPages(os.path.join(OUTPUT_DIR, plot_filename))\n",
    "for grain, one_forecast in data_subset.groupby(GRAIN_COL):\n",
    "    one_forecast[new_time_column_name] = pd.to_datetime(\n",
    "        one_forecast[new_time_column_name]\n",
    "    )\n",
    "    one_forecast.sort_values(new_time_column_name, inplace=True)\n",
    "    _draw_one_plot(\n",
    "        one_forecast,\n",
    "        new_time_column_name,\n",
    "        target_column_name,\n",
    "        time_series_id_column_names,\n",
    "        pdf,\n",
    "        plot_predictions=False,\n",
    "    )\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"./output_folder/ts_plots_small_data.pdf\", width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can make a determination of the modelling framework based on the visual examination of the raw time series plots. If all time series exhibit similar behavior patterns, a deep learning model can be an excellent choice in such scenario. If, on the other hand, if individual time series show heterogeneous behavior, one may be advised to run a many models accelerator which estimates one model per time seires as oppsed to one model for all time series.\n",
    "\n",
    "In our case, one can make an argument that a deep learning model can be a good modeeling choice. As a result, we set the `DOES_PARTITION_INCLUDE_VALIDATION_SET` parameter to True. Please note, to explore the best option, you can still partition the data in such a way as to run the low capacity models. To do so, set this parametr to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOES_PARTITION_INCLUDE_VALIDATION_SET = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate train/valid/test/inference sets\n",
    "\n",
    "Since deep learning models are considered \"high capacity\" models, they generally do not require frequent re-training. As a result, we use 2 months of data for validation and test set, respectively. The choice of 2 months is fairly arbitrary and can be modified to suit your needs. We use the 2 month of validation and test set data to reflect the infrequent re-traiing of the model given that the data frequency is hourly. Thus, there will be more than 1200 observations in the vlaidation and test sets per time series. This will give us enough data points to generate conclusions about model's performance.\n",
    "\n",
    "This is in  constrast to the ML models that require frequent retraining and as a result require much shorter test sets to have a reasonable understanding of the model accuracy.\n",
    "\n",
    "**Note:** Once the backtesting functionality is available, replace the statement regarding the shorter test set with the need to backtesting given a relatively frequent need to re-train the models compared to the DNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_periods = 60 * 24\n",
    "n_valid_periods = 60 * 24  # applicable only to single/TCN model\n",
    "n_inference_periods = 24\n",
    "\n",
    "\n",
    "def split_last_n_by_series_id(df, n, time_column_name):\n",
    "    \"\"\"Group df by series identifiers and split on last n rows for each group.\"\"\"\n",
    "    df_grouped = df.sort_values(time_column_name).groupby(  # Sort by ascending time\n",
    "        time_series_id_column_names, group_keys=False\n",
    "    )\n",
    "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
    "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
    "    return df_head, df_tail\n",
    "\n",
    "\n",
    "train_valid_test, inference = split_last_n_by_series_id(\n",
    "    data_subset, n_inference_periods, time_column_name=new_time_column_name\n",
    ")\n",
    "\n",
    "if DOES_PARTITION_INCLUDE_VALIDATION_SET:\n",
    "    train_valid, test = split_last_n_by_series_id(\n",
    "        train_valid_test, n_test_periods, new_time_column_name\n",
    "    )\n",
    "    train, valid = split_last_n_by_series_id(\n",
    "        train_valid, n_valid_periods, new_time_column_name\n",
    "    )\n",
    "else:\n",
    "    train, test = split_last_n_by_series_id(\n",
    "        train_valid_test, n_test_periods, new_time_column_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the target column from the inference dataset to reflect the fact that the future is unknown and the forecast is our best guess about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.drop(columns=[target_column_name], inplace=True)\n",
    "inference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will examine the start- and end dates as well as the number of observations per time series in each of the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full dataset\\n---\")\n",
    "for grain, tmp_df in data_subset.groupby(time_series_id_column_names):\n",
    "    print(\n",
    "        \"Grain:{}.\\\n",
    "    Min date: {}\\\n",
    "    Max date: {}\\\n",
    "    N: {}\".format(\n",
    "            grain, tmp_df[\"datetime\"].min(), tmp_df[\"datetime\"].max(), tmp_df.shape[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset\\n---\")\n",
    "for grain, tmp_df in train.groupby(time_series_id_column_names):\n",
    "    print(\n",
    "        \"Grain:{}.\\\n",
    "    Min date: {}\\\n",
    "    Max date: {}\\\n",
    "    N: {}\".format(\n",
    "            grain, tmp_df[\"datetime\"].min(), tmp_df[\"datetime\"].max(), tmp_df.shape[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOES_PARTITION_INCLUDE_VALIDATION_SET:\n",
    "    print(\"Valid dataset\\n---\")\n",
    "    for grain, tmp_df in valid.groupby(time_series_id_column_names):\n",
    "        print(\n",
    "            \"Grain:{}.\\\n",
    "        Min date: {}\\\n",
    "        Max date: {}\\\n",
    "        N: {}\".format(\n",
    "                grain,\n",
    "                tmp_df[\"datetime\"].min(),\n",
    "                tmp_df[\"datetime\"].max(),\n",
    "                tmp_df.shape[0],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Test dataset\\n---\")\n",
    "for grain, tmp_df in test.groupby(time_series_id_column_names):\n",
    "    print(\n",
    "        \"Grain:{}.\\\n",
    "    Min date: {}\\\n",
    "    Max date: {}\\\n",
    "    N: {}\".format(\n",
    "            grain, tmp_df[\"datetime\"].min(), tmp_df[\"datetime\"].max(), tmp_df.shape[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inference dataset\\n---\")\n",
    "for grain, tmp_df in inference.groupby(time_series_id_column_names):\n",
    "    print(\n",
    "        \"Grain:{}.\\\n",
    "    Min date: {}\\\n",
    "    Max date: {}\\\n",
    "    N: {}\".format(\n",
    "            grain, tmp_df[\"datetime\"].min(), tmp_df[\"datetime\"].max(), tmp_df.shape[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Upload data to datastore\n",
    "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace), is paired with the storage account, which contains the default data store. We will use it to upload the train and test data and create [tabular datasets](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py) for training and testing. A tabular dataset defines a series of lazily-evaluated, immutable operations to load data from the data source into tabular representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PREFFIX_NAME = (\n",
    "    \"uci_electro_small_tcn\"\n",
    "    if DOES_PARTITION_INCLUDE_VALIDATION_SET\n",
    "    else \"uci_electro_small\"\n",
    ")\n",
    "print(f\"Dataset preffix name: {DATASET_PREFFIX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_MODIFIED_DATASET_UPLOADED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "if not IS_MODIFIED_DATASET_UPLOADED:\n",
    "    print(\"---\\nUploading data ...\\n---\")\n",
    "\n",
    "    train_dataset = TabularDatasetFactory.register_pandas_dataframe(\n",
    "        train, target=(datastore, \"dataset/\"), name=f\"{DATASET_PREFFIX_NAME}_train\"\n",
    "    )\n",
    "\n",
    "    if DOES_PARTITION_INCLUDE_VALIDATION_SET:\n",
    "        valid_dataset = TabularDatasetFactory.register_pandas_dataframe(\n",
    "            valid, target=(datastore, \"dataset/\"), name=f\"{DATASET_PREFFIX_NAME}_valid\"\n",
    "        )\n",
    "\n",
    "    test_dataset = TabularDatasetFactory.register_pandas_dataframe(\n",
    "        test, target=(datastore, \"dataset/\"), name=f\"{DATASET_PREFFIX_NAME}_test\"\n",
    "    )\n",
    "\n",
    "    inference_dataset = TabularDatasetFactory.register_pandas_dataframe(\n",
    "        inference,\n",
    "        target=(datastore, \"dataset/\"),\n",
    "        name=f\"{DATASET_PREFFIX_NAME}_inference\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Using uploaded data ...\\n---\")\n",
    "\n",
    "    target_path_train = f\"{DATASET_PREFFIX_NAME}_train\"\n",
    "    target_path_valid = f\"{DATASET_PREFFIX_NAME}_valid\"\n",
    "    target_path_test = f\"{DATASET_PREFFIX_NAME}_test\"\n",
    "    target_path_inference = f\"{DATASET_PREFFIX_NAME}_test\"\n",
    "\n",
    "    train_dataset = Dataset.get_by_name(ws, name=target_path_train)\n",
    "    if DOES_PARTITION_INCLUDE_VALIDATION_SET:\n",
    "        valid_dataset = Dataset.get_by_name(ws, name=target_path_valid)\n",
    "    test_dataset = Dataset.get_by_name(ws, name=target_path_test)\n",
    "    inference_dataset = Dataset.get_by_name(ws, name=target_path_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete downladed data files to save space\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "jialiu"
   }
  ],
  "category": "tutorial",
  "celltoolbar": "Raw Cell Format",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Orange Juice Sales"
  ],
  "deployment": [
   "Azure Container Instance"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML AutoML"
  ],
  "friendly_name": "Forecasting orange juice sales with deployment",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "tags": [
   "None"
  ],
  "task": "Forecasting",
  "vscode": {
   "interpreter": {
    "hash": "6bd77c88278e012ef31757c15997a7bea8c943977c43d6909403c00ae11d43ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
