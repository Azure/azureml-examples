{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "**Demand Forecasting Using TCN**\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train TCN](#TrainTCN)\n",
    "1. [Train Baseline](#TrainBaseline)\n",
    "1. [Test Set Inference](#TestSetInference)\n",
    "1. [Test Set Evaluation](#TestSetEvaluation)\n",
    "1. [Generate Forecast](#GenerateForecast)\n",
    "1. [Schedule Inference Pipelines](#ScheduleInference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The objective of this notebook is to illustrate how to use the AutoML deep learning model, temporal convolutional network (TCN), for demand forecasting tasks. It walks you through all stages of model evaluation and production process starting with data ingestion and concluding with scheduling inference runs. For more information on the TCN model in AutoML refer to this [publication](https://learn.microsoft.com/en-us/azure/machine-learning/concept-automl-forecasting-deep-learning).\n",
    "\n",
    "We use a subset of UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)) with the objective of predicting electricity demand per consumer 24 hours ahead. The data was preprocessed using the [data prep notebook link placeholder]() notebook. Please refer to it for illustration on how to download the data from the source, aggregate to an hourly frequency, convert from wide to long format and upload to the Datastore. Here, we will work with the already uploaded data. \n",
    "\n",
    "Having a problem description such as to generate accurate forecasts 24 hours ahead sounds like a relatively straight forward task. However, there are quite a few steps a user needs to take before the model is put in production. A user needs to prepare the data, partition it into appropriate sets, select the best model, evaluate it against a baseline, and monitor the model in real life to collect enough observations on how it would perform had it been put in production. Some of these steps are time consuming, some require certain expertise in writing code. The steps shown in this notebook follow a typical thought process one follows before the model is put in production.\n",
    "\n",
    "Make sure you have executed the [configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Both training and inferencing is done using pipelines.\n",
    "\n",
    "Relies on the following scripts:\n",
    "\n",
    "- `helper_scripts.py`: Inference script that is executed on the remote compute\n",
    "- `register_model.py`: needed for the training stage. Registers model in the workspace.\n",
    "- `inference_script_naive.py`: Inference script for the Naive model\n",
    "- `inference_script_tcn.py`: Inference script for the TCN model\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This notebook was created using version 1.47.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the Azure ML workspace requires authentication with Azure.\n",
    "\n",
    "The default authentication is an interactive authentication using the default tenant. Executing the ws = Workspace.from_config() line in the cell below will prompt for authentication the first time it is run.\n",
    "\n",
    "If you have multiple Azure tenants, you can specify the tenant by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
    "```\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "auth = InteractiveLoginAuthentication(tenant_id = 'mytenantid')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "If you need to run in an environment where interactive login is not possible, you can use Service Principal authentication by replacing the ws = Workspace.from_config() line in the cell below with the following:\n",
    "```\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "auth = ServicePrincipalAuthentication('mytenantid', 'myappid', 'mypassword')\n",
    "ws = Workspace.from_config(auth = auth)\n",
    "```\n",
    "For more details, see [this link](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = \"forecasting-pipeline-tcn-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y%m%d\"\n",
    ")\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Subscription ID\"] = ws.subscription_id\n",
    "output[\"Workspace\"] = ws.name\n",
    "output[\"Resource Group\"] = ws.resource_group\n",
    "output[\"Location\"] = ws.location\n",
    "output[\"Run History Name\"] = experiment_name\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "outputDf = pd.DataFrame(data=output, index=[\"\"])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Compute \n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "\n",
    "You will need to create a compute target for your AutoML run. In this tutorial, you will create AmlCompute as your training compute resource.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "\n",
    "To run deep learning models we recommend to use GPU compute. Here, we use a 12 node cluster of the `Standard_NC8as_T4_v3` [series](https://learn.microsoft.com/en-us/azure/virtual-machines/nct4-v3-series) for illustration purposes. You will need to adjust the compute type and the number of nodes based on your needs which can be driven by the speed needed for model seelction, data size, etc. \n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "amlcompute_cluster_name = \"demand-fcst-gpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print(\"Found existing cluster, use it.\")\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_NC8as_T4_v3\", max_nodes=12, vm_priority=\"lowpriority\"\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data\n",
    "If you ran the data preparation notebook [link placeholder]() and want to use the registered data, skip section 3.1 and, instead, uncomment and execute the code in section 3.2. If, on the other hand, you did not run the notebook and want to use the data that we pre-processed and saved in the public blob, execute the code in section 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading and registering the data from public blob store\n",
    "\n",
    "Run the code in this section only if you want to use the data that is already available in the blobstore. If you want to use your own data that is already registered in your workspace, skip this section and procceed to run the commented out code in section 3.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code registers a datastore `autom_fcst_tcn` in your workspace and links the data from the container `automl-sample-notebook-data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "# Please change the following to point to your own blob container and pass in account_key\n",
    "blob_datastore_name = \"automl_fcst_tcn\"\n",
    "container_name = \"automl-sample-notebook-data\"\n",
    "account_name = \"automlsamplenotebookdata\"\n",
    "\n",
    "print(f'Creating datastore \"{blob_datastore_name}\" in your workspace ...\\n---')\n",
    "demand_tcn_datastore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws,\n",
    "    datastore_name=blob_datastore_name,\n",
    "    container_name=container_name,\n",
    "    account_name=account_name,\n",
    "    create_if_not_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code registers datasets from the `automl-sample-notebook-data` container in the datastore we just created. Once the datasets are registered, we will be able to use them in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "print(\"Registering datasets in your workspace ...\\n---\")\n",
    "\n",
    "FOLDER_PREFIX_NAME = \"uci_electro_small_public_tcn\"\n",
    "\n",
    "target_path_train = f\"{FOLDER_PREFIX_NAME}_train\"\n",
    "target_path_valid = f\"{FOLDER_PREFIX_NAME}_valid\"\n",
    "target_path_test = f\"{FOLDER_PREFIX_NAME}_test\"\n",
    "target_path_inference = f\"{FOLDER_PREFIX_NAME}_infer\"\n",
    "\n",
    "train_dataset = Dataset.Tabular.from_delimited_files(\n",
    "    path=demand_tcn_datastore.path(target_path_train + \"/\"),\n",
    "    validate=False,\n",
    "    infer_column_types=True,\n",
    ").register(workspace=ws, name=target_path_train, create_new_version=True)\n",
    "\n",
    "valid_dataset = Dataset.Tabular.from_delimited_files(\n",
    "    path=demand_tcn_datastore.path(target_path_valid + \"/\"),\n",
    "    validate=False,\n",
    "    infer_column_types=True,\n",
    ").register(workspace=ws, name=target_path_valid, create_new_version=True)\n",
    "\n",
    "test_dataset = Dataset.Tabular.from_delimited_files(\n",
    "    path=demand_tcn_datastore.path(target_path_test + \"/\"),\n",
    "    validate=False,\n",
    "    infer_column_types=True,\n",
    ").register(workspace=ws, name=target_path_test, create_new_version=True)\n",
    "\n",
    "inference_dataset = Dataset.Tabular.from_delimited_files(\n",
    "    path=demand_tcn_datastore.path(target_path_inference + \"/\"),\n",
    "    validate=False,\n",
    "    infer_column_types=True,\n",
    ").register(workspace=ws, name=target_path_inference, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using data that is registered in your workspace\n",
    "\n",
    "If you ran the [data prep notebook link placeholder]() notebook, the partitioned data is already uploaded and registered in your workspace. Uncomment the following code and change the `DATASET_PREFIX_NAME`, to match the value in the data preparation notebook, and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "# from azureml.core.dataset import Dataset\n",
    "\n",
    "# DATASET_PREFIX_NAME = \"uci_electro_small_tcn\"\n",
    "# print(f'Dataset prefix name: {DATASET_PREFIX_NAME}\\n---\\nLoading train, validation, test and inference sets ...\\n---')\n",
    "\n",
    "# target_path_train = f\"{DATASET_PREFIX_NAME}_train\"\n",
    "# target_path_valid = f\"{DATASET_PREFIX_NAME}_valid\"\n",
    "# target_path_test = f\"{DATASET_PREFIX_NAME}_test\"\n",
    "# target_path_inference = f\"{DATASET_PREFIX_NAME}_inference\"\n",
    "\n",
    "# train_dataset = Dataset.get_by_name(ws, name=target_path_train)\n",
    "# valid_dataset = Dataset.get_by_name(ws, name=target_path_valid)\n",
    "# test_dataset = Dataset.get_by_name(ws, name=target_path_test)\n",
    "# inference_dataset = Dataset.get_by_name(ws, name=target_path_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test and inference sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have *test* and *inference* sets. The difference between the two is the presence of the target column. The test set contains the target column and is used to evaluate model performance using [rolling forecast](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-auto-train-forecast-v1#evaluating-model-accuracy-with-a-rolling-forecast). On the other hand, the target column is not present in the inference set to illustrate how to generate an actual forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first few rows of the test set ...\\n---\")\n",
    "print(test_dataset.to_pandas_dataframe().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first few rows of the inference set ...\\n---\")\n",
    "print(inference_dataset.to_pandas_dataframe().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up what we know about the dataset.\n",
    "\n",
    "- **Target column** is what we want to forecast. In our case it is electricity consumption per customer measured in kilowatt hours (kWh).\n",
    "- **Time column** is the time axis along which to predict.\n",
    "- **Time series identifier columns** are identified by values of the columns listed `time_series_id_column_names`. In our case all unique time series are identified by a single column `customer_id`. However, it is quite common to have multiple columns identifying unique time series. See the [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast#configuration-settings) for a more detailed explanation on this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = \"usage\"\n",
    "time_column_name = \"datetime\"\n",
    "GRAIN_COL = \"customer_id\"\n",
    "time_series_id_column_names = [GRAIN_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download training data from the Datastore to make sure it looks as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_pandas_dataframe()\n",
    "\n",
    "nseries = train_df.groupby(time_series_id_column_names).ngroups\n",
    "print(\n",
    "    f\"Data contains {nseries} individual time-series:\\n{list(train_df[GRAIN_COL].unique())}\\n---\"\n",
    ")\n",
    "print(\"Printing the first few rows of the training data ...\\n---\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train TCN\n",
    "\n",
    "In this section we will train and select the best TCN model as well as the baseline model. The baseline model will be used as a reference point to understand TCN's accuracy performance. The goal of forecasting is to have the most accurate predictions measured by some accuracy metric. What is considered an accurate prediction is fairly subjective. Take, for example, the MAPE (mean absolute percentage error) metric. A perfect forecast will result in the MAPE value of zero, which is not achievable using business data. For this reason it is imperative to have a baseline model to compare TCN results against. Doing this adds objectivity to the model acceptance criteria. \n",
    "\n",
    "The baseline model can be the model that is currently in production. Oftentimes, the baseline is set to be a Naive forecast, which we will use in this notebook. The choice of the baseline is also specific to the data. For example, if there is a clear trend in the data one may not want to use a Naive model.  Instead, one can use an ARIMA model. Please see this [document](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-configure-auto-train-v1#supported-models) for a list of AutoML models one can chose from to use as a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 2 parameters allow us to re-use training runs for the TCN and baseline models, respectively. This can be helpful it you need to experiment with the post model training steps thus avoiding the need to train a new model which can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TCN_MODEL_TRAINED = False\n",
    "IS_BASE_MODEL_TRAINED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train AutoML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Set up training parameters\n",
    "We need to provide the `ForecastingParameters` and `AutoMLConfig` objects. For the forecasting task we also need to define several settings including the name of the time column, the maximum forecast horizon, and the partition column name(s) definition.\n",
    "\n",
    "#### Forecasting Parameters\n",
    "To define forecasting parameters for your experiment training, you can leverage the `ForecastingParameters` class. The table below details the forecasting parameters we will be passing into our experiment.\n",
    "\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of the time column in the data.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
    "|**time_series_id_column_names**|The column names used to uniquely identify the time series in data that has multiple rows with the same timestamp. If the time series identifiers are not defined, the data set is assumed to be one time series.|\n",
    "|**freq**|Forecast frequency. This optional parameter represents the period for which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information.\n",
    "\n",
    "\n",
    "#### AutoMLConfig arguments\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **task**                           | forecasting |\n",
    "| **primary_metric**                 | This is the metric that you want to optimize. Forecasting supports the following primary metrics<ul><li>`normalized_root_mean_squared_error`</li><li>`normalized_mean_absolute_error`</li><li>`spearman_correlation`</li><li>`r2_score`</li></ul> We recommend using either the normalized root mean squared error or normalized mean absolute erorr as a primary metric because they measure forecast accuracy. See the [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-automl-forecasting-faq#how-do-i-choose-the-primary-metric) for a more detailed discussion on this topic. |\n",
    "| **experiment_timeout_hours**       | Maximum amount of time in hours that each experiment can take before it terminates. This is optional but provides customers with greater control on exit criteria. When setting this criteria we advise to take into account the number of desired iterations parameter and set experiment timeout setting such that the desired number of iterations will be completed.|\n",
    "| **iterations**                     | Number of models to train. This is optional but provides customers with greater control on exit criteria. For TCN models we recommend to have at least 50 iterations to choose the best architecture. For our experiment we will set the number of iterations to 100, however, due to the experiment timeout settings being 1 hour on the specified compute cluster, we will not obtain 100 completions. |\n",
    "| **label_column_name**              | The name of the target column we are trying to predict. |\n",
    "| **enable_early_stopping**          | Flag to enable early termination if the primary metric is no longer improving. |\n",
    "| **max_concurrent_iterations**      | Number of TCN models that are estimated simultaneously. This number should be set to the number of nodes in your cluster. In our case, we have a 12 node cluster and set this value to 12. |\n",
    "| **enable_dnn**                     | Enable Forecasting DNNs. The default value is `False`. |\n",
    "| **allowed_models**                 | List of models we want to consider. Since we are only interested in the deep learning models, we list only the `TCNForecaster`.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "\n",
    "forecast_horizon = 24\n",
    "\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name=time_column_name,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    time_series_id_column_names=time_series_id_column_names,\n",
    "    freq=\"H\",\n",
    ")\n",
    "\n",
    "automl_config = AutoMLConfig(\n",
    "    task=\"forecasting\",\n",
    "    debug_log=\"tcn_main.log\",\n",
    "    primary_metric=\"normalized_root_mean_squared_error\",\n",
    "    experiment_timeout_hours=1,\n",
    "    iterations=100,\n",
    "    training_data=train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    label_column_name=target_column_name,\n",
    "    compute_target=compute_target,\n",
    "    enable_early_stopping=True,\n",
    "    verbosity=logging.INFO,\n",
    "    max_concurrent_iterations=12,\n",
    "    max_cores_per_iteration=-1,\n",
    "    enable_dnn=True,\n",
    "    allowed_models=[\"TCNForecaster\"],\n",
    "    forecasting_parameters=forecasting_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Construct pipeline steps\n",
    "\n",
    "The objective of the next block of code is to create an AzureML pipeline step that encapsulates the AutoML run. For more details see this [link](https://learn.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.automlstep?view=azure-ml-py). You do not have to change anything here, so run it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
    "from azureml.pipeline.steps import AutoMLStep, PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
    "\n",
    "metrics_output_name = \"metrics_output\"\n",
    "best_model_output_name = \"best_model_output\"\n",
    "model_file_name = \"model_file\"\n",
    "metrics_data_name = \"metrics_data\"\n",
    "\n",
    "metrics_data = PipelineData(\n",
    "    name=metrics_data_name,\n",
    "    datastore=datastore,\n",
    "    pipeline_output_name=metrics_output_name,\n",
    "    training_output=TrainingOutput(type=\"Metrics\"),\n",
    ")\n",
    "model_data = PipelineData(\n",
    "    name=model_file_name,\n",
    "    datastore=datastore,\n",
    "    pipeline_output_name=best_model_output_name,\n",
    "    training_output=TrainingOutput(type=\"Model\"),\n",
    ")\n",
    "\n",
    "automl_step = AutoMLStep(\n",
    "    name=\"automl_module\",\n",
    "    automl_config=automl_config,\n",
    "    outputs=[metrics_data, model_data],\n",
    "    allow_reuse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Register model step\n",
    "\n",
    "#### 4.3.1 Run Configuration and Environment\n",
    "To have a pipeline step run, we first need an environment to run the jobs. The environment can be built using the following code and you do not have to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "\n",
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "\n",
    "conda_run_config.docker.use_docker = True\n",
    "\n",
    "cd = CondaDependencies.create(\n",
    "    pip_packages=[\n",
    "        \"azureml-sdk[automl]\",\n",
    "        \"applicationinsights\",\n",
    "        \"azureml-opendatasets\",\n",
    "        \"azureml-defaults\",\n",
    "    ],\n",
    "    conda_packages=[\"numpy==1.19.5\"],\n",
    "    pin_sdk_version=False,\n",
    ")\n",
    "conda_run_config.environment.python.conda_dependencies = cd\n",
    "\n",
    "print(\"run config is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Step to register the model.\n",
    "The following code generates a step to register the model in your workspace from the previous step. Once you have the environment created, you are ready to define your pipeline's steps. There are many built-in steps available via the Azure Machine Learning SDK, as you can see on the [reference documentation](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py) for the `azureml.pipeline.steps` package. The most flexible class is *PythonScriptStep*, which runs a Python script.\n",
    "\n",
    "The arguments values specify the inputs and outputs of the step. In the example below, the data is the `uci_electro_small_train` dataset. The script `register_model.py` registers the best model from the training run in the workspace. This step will run on the machine defined by the `compute_target` parameter using the configuration `conda_run_config`.\n",
    "\n",
    "Please note that in order to use the already completed training pipeline for the TCN to generate predictions, we need to know the model name under which the best model is registered in the workspace. To avoid confusion and registering a different model with the same name, we give the model a unique name that is based on date and time of the day. As a result, you will need to record the model name that is printed in the console when the `IS_TCN_MODEL_TRAINED` parameter is `False` in order to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model name with which to register the trained model in the workspace.\n",
    "if not IS_TCN_MODEL_TRAINED:\n",
    "    model_name_str = \"uci-small-tcn-model-\" + datetime.datetime.now().strftime(\n",
    "        \"%Y%m%d%H%M\"\n",
    "    )\n",
    "    model_name = PipelineParameter(\"model_name\", default_value=model_name_str)\n",
    "    print(\n",
    "        f\"A new model will be registered under the name: {model_name_str}\\n\\\n",
    "    Please record this in order to use in the subsequent inference runs.\\n---\"\n",
    "    )\n",
    "else:\n",
    "    model_name_str = (\n",
    "        \"<Replace with the output of model_name_str during the model training stage.>\"\n",
    "    )\n",
    "    model_name = PipelineParameter(\"model_name\", default_value=model_name_str)\n",
    "    print(f\"Using trained model name: {model_name_str}\\n---\")\n",
    "\n",
    "register_model_step = PythonScriptStep(\n",
    "    script_name=\"register_model.py\",\n",
    "    name=\"register_model\",\n",
    "    source_directory=\"scripts\",\n",
    "    allow_reuse=False,\n",
    "    arguments=[\n",
    "        \"--model_name\",\n",
    "        model_name,\n",
    "        \"--model_path\",\n",
    "        model_data,\n",
    "        \"--ds_name\",\n",
    "        target_path_train,\n",
    "    ],\n",
    "    inputs=[model_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=conda_run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model name: {model_name_str}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Build pipeline\n",
    "\n",
    "Next, we build the pipeline and kick off a run which will select the best TCN model and register it in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = Pipeline(\n",
    "    description=\"training_pipeline_tcn\",\n",
    "    workspace=ws,\n",
    "    steps=[automl_step, register_model_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is the TCN model trained? {IS_TCN_MODEL_TRAINED}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IS_TCN_MODEL_TRAINED:\n",
    "    print(\"Training new AutoML model ...\\n---\")\n",
    "    training_pipelin_run = experiment.submit(training_pipeline)\n",
    "    training_pipelin_run.wait_for_completion(show_output=False)\n",
    "    IS_TCN_MODEL_TRAINED = True\n",
    "else:\n",
    "    from azureml.train.automl.run import AutoMLRun\n",
    "    from azureml.pipeline.core.run import PipelineRun\n",
    "\n",
    "    PIPELINE_RUN_ID = (\n",
    "        \"<Replace with the PipelineRunId used for training the best TCN model>\"\n",
    "    )\n",
    "    training_pipelin_run = PipelineRun(experiment=experiment, run_id=PIPELINE_RUN_ID)\n",
    "    print(f\"Using previously trained model. Pipeline run ID: {PIPELINE_RUN_ID}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Naive model as our baseline. To train it, we kick off another automl experiment with the following settings. Please note that we disabled DNN (`enable_dnn=False`), added the Naive model to the allowed models list, and set the number of iterations to 1 since we only interested in training one specific model. To reduce the training time for non-deep learning models, we set the number of cross validations to 2. Read the following [document](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-auto-train-forecast-v1#training-and-validation-data) for more information on this topic. Note that we did not use the cross validations settings for the TCN model training because such models typically require validation set that are substantially longer than a small multiple of a forecast horizon.\n",
    "\n",
    "The only `AutoMLConfig` settings you might consider changing are the `experiment_timeout_hours` and `allowed_models`. You might want to increase the experiment timeout if your data has lots of unique time series. The allowed model list can be modified to refect a different choice of the baseline model and can be selected from the supported [forecasting models](https://learn.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting) and [regression models](https://learn.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(\n",
    "    task=\"forecasting\",\n",
    "    debug_log=\"baseline.log\",\n",
    "    primary_metric=\"normalized_root_mean_squared_error\",\n",
    "    experiment_timeout_hours=1,\n",
    "    iterations=1,\n",
    "    training_data=train_dataset,\n",
    "    label_column_name=target_column_name,\n",
    "    compute_target=compute_target,\n",
    "    enable_early_stopping=True,\n",
    "    n_cross_validations=2,\n",
    "    verbosity=logging.INFO,\n",
    "    max_cores_per_iteration=-1,\n",
    "    enable_dnn=False,\n",
    "    allowed_models=[\"Naive\"],\n",
    "    forecasting_parameters=forecasting_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IS_BASE_MODEL_TRAINED:\n",
    "    remote_base_run = experiment.submit(automl_config, show_output=False)\n",
    "    remote_base_run.wait_for_completion(show_output=False)\n",
    "    IS_BASE_MODEL_TRAINED = True\n",
    "else:\n",
    "    BASE_RUN_ID = \"<Replace with the run ID used for training the baseline model>\"\n",
    "    # during the initial training run copy-paste the run id to be utilized later if nedded.\n",
    "    remote_base_run = AutoMLRun(experiment=experiment, run_id=BASE_RUN_ID)\n",
    "    print(f\"Using previously trained model. Run ID: {BASE_RUN_ID}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test set inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Inference the best TCN model\n",
    "\n",
    "We create an output folder which will be used to save the output of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an output folder\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"forecast_output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Get Inference Pipeline Environment\n",
    "To trigger an inference pipeline run, we first need a running environment that contains all the appropriate packages for the model unpickling. This environment can be either assessed from the training run or using the `yml` file that comes with the model. In this section we use the environment from the training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.register_model import get_best_automl_run\n",
    "\n",
    "best_run = get_best_automl_run(training_pipelin_run)\n",
    "inference_env = best_run.get_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = inference_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Build and submit the inference pipeline\n",
    "\n",
    "The inference pipeline will create two different output formats, 1) a tabular dataset that contains the prediction and 2) an `OutputFileDatasetConfig` that can be used for the sequential pipeline steps. The `inference_script_tcn.py` performs a rolling evaluation on the test set with the step size of 24 hours. This generates the same results as if the model was put in production and generated a 24-hour forecast once a day using the most recent data context. This is an efficient way of testing historical model performance at a tiny fraction of the compute and time commitment costs. For a more detailed information on rolling evaluation see the this [document](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-auto-train-forecast-v1#evaluating-model-accuracy-with-a-rolling-forecast) for a more detailed explanation.\n",
    "\n",
    "<!-- The test_step uses the following arguments:<ul><li>`spearman_correlation`</li><li>`normalized_root_mean_squared_error`</li><li>`r2_score`</li><li>`normalized_mean_absolute_error`</li></ul> -->\n",
    "\n",
    "The `PythonScriptStep` uses the following arguments:<ul>\n",
    "    <li> `model_name`: the name under which the model was registered in the workspace. </li>\n",
    "    <li> `ouput_dataset_name`: name of the dataset under which the test set predictions will registered in the datastore</li>\n",
    "    <li> `test_dataset_name`: name of the test dataset </li>\n",
    "    <li> `target_column_name`: target column name</li>\n",
    "    <li> `time_column_name`: time column name</li>\n",
    "    <li> `output_path`: output data that is obtained from prediction step. </li>\n",
    "    <li> `run_rolling_evaluation`: Boolean parameter. Set to True to run rolling evaluation. Otherwise, runs a recursive forecast for the entire test set. If the length of the test set is larger than the forecast horizon the model was trained for, the recursive forecast method recursively applies the regular forecaster to generate context so that we can forecast further into the future. For more information, see the \"Recursive forecasting\" section in [this notebook](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-forecast-function/auto-ml-forecasting-function.ipynb).</li>\n",
    "    <li> `rolling_evaluation_step_size`: Optional parameter that instructs the rolling forecaster how many periods to step forward. See this [document](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-auto-train-forecast-v1#evaluating-model-accuracy-with-a-rolling-forecast) for a more detailed explanation. </li></ul>\n",
    "    \n",
    "The only parameters you may consider changing in this sections are the `run_rolling_evaluation` and the `rolling_evaluation_step_size`. The rest should be left as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "output_data = OutputFileDatasetConfig(name=\"prediction_result\")\n",
    "\n",
    "output_ds_name = \"uci-tcn-test-output\"\n",
    "\n",
    "test_step = PythonScriptStep(\n",
    "    name=\"infer-results\",\n",
    "    source_directory=\"scripts\",\n",
    "    script_name=\"inference_script_tcn.py\",\n",
    "    arguments=[\n",
    "        \"--model_name\",\n",
    "        model_name_str,\n",
    "        \"--ouput_dataset_name\",\n",
    "        output_ds_name,\n",
    "        \"--test_dataset_name\",\n",
    "        test_dataset.name,\n",
    "        \"--target_column_name\",\n",
    "        target_column_name,\n",
    "        \"--time_column_name\",\n",
    "        time_column_name,\n",
    "        \"--output_path\",\n",
    "        output_data,\n",
    "        \"--run_rolling_evaluation\",\n",
    "        True,\n",
    "        \"--rolling_evaluation_step_size\",\n",
    "        forecast_horizon,\n",
    "    ],\n",
    "    compute_target=compute_target,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = Pipeline(ws, [test_step])\n",
    "test_run = experiment.submit(test_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Get the predicted data\n",
    "\n",
    "We download the test set predictions and actuals, and store them in the `backtest_tcn` object, which in turn is saved to the output folder. We chose the name *backtest* to reflect the fact that the rolling evaluation on the test set is equivalent to monitoring model's performance in real life had it been put in production for the duration of the test set and generated forecast every 24 hours using the most recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "backtest_tcn = Dataset.get_by_name(ws, output_ds_name)\n",
    "backtest_tcn = backtest_tcn.to_pandas_dataframe()\n",
    "backtest_tcn.to_csv(os.path.join(OUTPUT_DIR, \"predictions-tcn.csv\"), index=False)\n",
    "backtest_tcn.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Inference the baseline model\n",
    "\n",
    "Next, we perform a rolling evaluation on the test set for the baseline model. To do this, we use the `run_remote_inference` method which downloads the pickle file of the model into the temporary folder `forecast_naive` and copies the `inference_script_naive.py` file to it. This folder is then uploaded on the compute cluster where inference is performed. The `inference_script_naive.py` script performs a rolling evaluation on the test set, similarly to what we have done for the TCN model. Upon completion of this step, we delete the newly created `forecast_naive` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_experiment_base = Experiment(ws, experiment_name + \"_inference_base\")\n",
    "\n",
    "baseline_run = remote_base_run.get_best_child()\n",
    "baseline_model_name = baseline_run.properties[\"model_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from scripts.helper_scripts import run_remote_inference\n",
    "\n",
    "if True:\n",
    "    remote_base_run_test = run_remote_inference(\n",
    "        test_experiment=test_experiment_base,\n",
    "        compute_target=compute_target,\n",
    "        train_run=baseline_run,\n",
    "        test_dataset=test_dataset,\n",
    "        target_column_name=target_column_name,\n",
    "        rolling_evaluation_step_size=forecast_horizon,\n",
    "        inference_folder=\"./forecast_naive\",\n",
    "    )\n",
    "    remote_base_run_test.wait_for_completion(show_output=False)\n",
    "\n",
    "    # download the forecast file to the local machine\n",
    "    remote_base_run_test.download_file(\n",
    "        \"outputs/predictions.csv\", os.path.join(OUTPUT_DIR, \"predictions-base.csv\")\n",
    "    )\n",
    "\n",
    "    # delete downloaded scripts\n",
    "    shutil.rmtree(\"./forecast_naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test set model evaluation\n",
    "\n",
    "In this section we will evaluate the test set performance for the best TCN model and compare it with the baseline. We will generate time series plots for forecasts and actuals, calculate accuracy metrics and plot the evolution of metrics for each model over time. All output from this section will be stored in the `forecast_output` folder and can be referenced any time you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load test set results\n",
    "\n",
    "Here, we will import test set results for the TCN and the baseline experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_tcn = pd.read_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"predictions-tcn.csv\"), parse_dates=[time_column_name]\n",
    ")\n",
    "backtest_base = pd.read_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"predictions-base.csv\"), parse_dates=[time_column_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine outputs into a single dataframe which will be used for plotting and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest = backtest_tcn.merge(\n",
    "    backtest_base.drop(target_column_name, axis=1),\n",
    "    on=[\"customer_id\", \"datetime\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=[\"\", \"_base\"],\n",
    ")\n",
    "backtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"N model: {backtest_tcn.shape[0]}. N baseline: {backtest_base.shape[0]}. N merged: {backtest.shape[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are working with has an hourly frequency and we plan to generate the forecasts every 24 hours. If the model were to be put in production such that the forecasts are generated and model's performance is monitored every 24 hours, we will mimic the scoring process on the test set by generating daily accuracy metrics. To do this, we create a date column (\"ymd\"). If you want to score the output at any other frequency, say, weekly, just change the frequency parameter to the desired frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD_COLUMN = \"ymd\"\n",
    "\n",
    "backtest[PERIOD_COLUMN] = backtest[time_column_name].dt.to_period(\n",
    "    \"D\"\n",
    ")  # year-month-day to be used for daily metrics computation\n",
    "backtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Generate time series plots\n",
    "\n",
    "Here, we generate forecast versus actuals plot for the test set for both the best TCN model and the baseline. Since we use rolling evaluation with the step size of 24 hours, this mimics the behavior of putting both models in production and monitoring their behavior for the duration of the test set. This step allows users to make informed decisons about model performance and saves numerous costs associated with productionalizing the model and monitoring its performance in real life. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper_scripts import _draw_one_plot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plot_filename = \"forecast_vs_actual.pdf\"\n",
    "\n",
    "pdf = PdfPages(os.path.join(os.getcwd(), OUTPUT_DIR, plot_filename))\n",
    "for _, one_forecast in backtest.groupby(GRAIN_COL):\n",
    "    one_forecast[time_column_name] = pd.to_datetime(one_forecast[time_column_name])\n",
    "    one_forecast.sort_values(time_column_name, inplace=True)\n",
    "    _draw_one_plot(\n",
    "        one_forecast,\n",
    "        time_column_name,\n",
    "        target_column_name,\n",
    "        [GRAIN_COL],\n",
    "        [target_column_name, \"predicted\", \"predicted_base\"],\n",
    "        pdf,\n",
    "        plot_predictions=True,\n",
    "    )\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Calculate metrics\n",
    "Here, we will calculate the metric of interest for each day. For illustration purposes we use root mean squared error as the metric of choice. However, the `compute_all_metrics` method calculated all primary and secondary metrics for AutoML runs. Please refer to this <u>*Regression/forecasting metrics*</u> section in this [document](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml#regressionforecasting-metrics) for the list of available metrics. We will calculate the distribution of this metrics for each time series in our dataset. Looking at the descrptive stats of such metrics can be more informative than calculating a single metric such as the mean for each time series. As an example, we are looking at the RMSE (root mean squared error) metric, but you can choose any other metric computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper_scripts import compute_all_metrics\n",
    "\n",
    "DESIRED_METRIC_NAME = \"root_mean_squared_error\"\n",
    "\n",
    "metrics_per_grain_day = compute_all_metrics(\n",
    "    fcst_df=backtest,\n",
    "    actual_col=target_column_name,\n",
    "    fcst_col=\"predicted\",\n",
    "    ts_id_colnames=[GRAIN_COL, PERIOD_COLUMN],\n",
    ")\n",
    "metrics_per_grain_day = metrics_per_grain_day.query(\n",
    "    f'metric_name == \"{DESIRED_METRIC_NAME}\"'\n",
    ")\n",
    "metrics_per_grain_day[[GRAIN_COL, PERIOD_COLUMN]] = metrics_per_grain_day[\n",
    "    \"time_series_id\"\n",
    "].str.split(\"|\", 1, expand=True)\n",
    "metrics_per_grain_day.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"metrics-automl.csv\"), index=False\n",
    ")\n",
    "metrics_per_grain_day.groupby(GRAIN_COL)[\"metric\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to see all metrics we computed\n",
    "# print(f'List of available metrics: {metrics_per_grain_day[\"metric_name\"].unique()}\\n---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline metrics\n",
    "metrics_per_grain_day_base = compute_all_metrics(\n",
    "    fcst_df=backtest,\n",
    "    actual_col=target_column_name,\n",
    "    fcst_col=\"predicted_base\",\n",
    "    ts_id_colnames=[GRAIN_COL, PERIOD_COLUMN],\n",
    ")\n",
    "metrics_per_grain_day_base = metrics_per_grain_day_base.query(\n",
    "    f'metric_name == \"{DESIRED_METRIC_NAME}\"'\n",
    ")\n",
    "metrics_per_grain_day_base[[GRAIN_COL, PERIOD_COLUMN]] = metrics_per_grain_day[\n",
    "    \"time_series_id\"\n",
    "].str.split(\"|\", 1, expand=True)\n",
    "metrics_per_grain_day_base.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"metrics-base.csv\"), index=False\n",
    ")\n",
    "metrics_per_grain_day_base.groupby(GRAIN_COL)[\"metric\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4  Visualize metrics\n",
    "\n",
    "In this section we plot metric evolution over time for the TCN and the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_per_grain_day.drop(\"time_series_id\", axis=1).merge(\n",
    "    metrics_per_grain_day_base.drop(\"time_series_id\", axis=1),\n",
    "    on=[\"metric_name\", GRAIN_COL, PERIOD_COLUMN],\n",
    "    how=\"inner\",\n",
    "    suffixes=[\"\", \"_base\"],\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain = [GRAIN_COL]\n",
    "plot_filename = \"metrics_plot.pdf\"\n",
    "\n",
    "pdf = PdfPages(os.path.join(os.getcwd(), OUTPUT_DIR, plot_filename))\n",
    "for _, one_forecast in metrics_df.groupby(grain):\n",
    "    one_forecast[PERIOD_COLUMN] = pd.to_datetime(one_forecast[PERIOD_COLUMN])\n",
    "    one_forecast.sort_values(PERIOD_COLUMN, inplace=True)\n",
    "    _draw_one_plot(\n",
    "        one_forecast,\n",
    "        PERIOD_COLUMN,\n",
    "        target_column_name,\n",
    "        grain,\n",
    "        [\"metric\", \"metric_base\"],\n",
    "        pdf,\n",
    "        plot_predictions=True,\n",
    "    )\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(os.path.join(\"./forecast_output/metrics_plot.pdf\"), width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we generate an actual forecast by providing an inference set that does not contain actual values. This illustrates how to generate production forecasts in real life. The code in this section is pretty much identical to the one in section 6.1 with one exception, we set the `run_rolling_evaluation` argument to `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Build and submit the inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = inference_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = OutputFileDatasetConfig(name=\"prediction_result\")\n",
    "\n",
    "output_ds_name = \"uci-tcn-inference-output\"\n",
    "\n",
    "inference_step = PythonScriptStep(\n",
    "    name=\"infer-results\",\n",
    "    source_directory=\"scripts\",\n",
    "    script_name=\"inference_script_tcn.py\",\n",
    "    arguments=[\n",
    "        \"--model_name\",\n",
    "        model_name_str,\n",
    "        \"--ouput_dataset_name\",\n",
    "        output_ds_name,\n",
    "        \"--test_dataset_name\",\n",
    "        inference_dataset.name,\n",
    "        \"--target_column_name\",\n",
    "        target_column_name,\n",
    "        \"--time_column_name\",\n",
    "        time_column_name,\n",
    "        \"--output_path\",\n",
    "        output_data,\n",
    "        \"--run_rolling_evaluation\",\n",
    "        False,\n",
    "    ],\n",
    "    compute_target=compute_target,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline(ws, [inference_step])\n",
    "inference_run = experiment.submit(inference_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Get the predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "inference_ds = Dataset.get_by_name(ws, output_ds_name)\n",
    "inference_df = inference_ds.to_pandas_dataframe()\n",
    "inference_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Schedule Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is about how to schedule a pipeline for periodically predictions. For more info about pipeline schedule and pipeline endpoint, please follow this [notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-setup-schedule-for-a-published-pipeline.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_published_pipeline = inference_pipeline.publish(\n",
    "    name=\"UCI Inference\", description=\"UCI Inference\"\n",
    ")\n",
    "print(\"Newly published pipeline id: {}\".format(inference_published_pipeline.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `inference_dataset` is going to refresh every 24 hours and we want to predict every 24 hours (forecast_horizon), we can schedule our pipeline to run every day at 11 pm to get daily inference results. You can refresh your test dataset (a newer version will be created) periodically when new data is available (i.e. target column in test dataset would have values in the beginning as context data, and followed by NaNs to be predicted). The inference pipeline will pick up context to further improve the forecast accuracy. See the <u><i>Forecasting away from training data</i></u> in this [notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/forecasting-forecast-function/auto-ml-forecasting-function.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "recurrence = ScheduleRecurrence(\n",
    "    frequency=\"Day\", interval=1, hours=[23], minutes=[00]  # Runs every day at 11:00 pm\n",
    ")\n",
    "\n",
    "schedule = Schedule.create(\n",
    "    workspace=ws,\n",
    "    name=\"tcn_inference_schedule\",\n",
    "    pipeline_id=inference_published_pipeline.id,\n",
    "    experiment_name=\"schedule-run-tcn-uci-electro\",\n",
    "    recurrence=recurrence,\n",
    "    wait_for_provisioning=True,\n",
    "    description=\"Schedule Run\",\n",
    ")\n",
    "\n",
    "# You may want to make sure that the schedule is provisioned properly\n",
    "# before making any further changes to the schedule\n",
    "\n",
    "print(\"Created schedule with id: {}\\n---\".format(schedule.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 [Optional] Disable schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "jialiu"
   }
  ],
  "category": "tutorial",
  "celltoolbar": "Raw Cell Format",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Orange Juice Sales"
  ],
  "deployment": [
   "Azure Container Instance"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML AutoML"
  ],
  "friendly_name": "Forecasting orange juice sales with deployment",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "tags": [
   "None"
  ],
  "task": "Forecasting",
  "vscode": {
   "interpreter": {
    "hash": "6bd77c88278e012ef31757c15997a7bea8c943977c43d6909403c00ae11d43ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
