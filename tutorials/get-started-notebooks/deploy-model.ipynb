{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Internal notes\n",
    "\n",
    "- Code has not been tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Deploy a model as an online endpoint\n",
    "\n",
    "Learn to deploy a model, using Azure Machine Learning (AzureML) Python SDK v2. This tutorial will present an overview of how AzureML handles deployment and walk you through the steps to deploy your model to an online endpoint. \n",
    "\n",
    "To set the stage for deployment, you'll first configure a workspace, compute resources, and a job environment necessary for deployment. Then you'll \n",
    "\n",
    "You'll learn to register a model, create an endpoint and create a first deployment. Then, you'll test the deployment with data and retrieve the deployment details. You'll also create a second deployment and allocate some percentage of production traffic to it. After you obtain logs from the second deployment and are satisfied with its performance, you'll eventually send all the production traffic to your second deployment and delete the first one.\n",
    "\n",
    "The steps you'll take are:\n",
    "\n",
    "> * [DELETE] Connect to your AzureML workspace\n",
    "> * [DELETE] Create your compute resource and job environment\n",
    "> * [DELETE] Configure authorization\n",
    "> * Ensure your model is registered\n",
    "> * Configure environments\n",
    "> * Import libraries\n",
    "> * Verify environment\n",
    "> * Create an endpoint and a first deployment\n",
    "> * Deploy a trial run\n",
    "> * Manually send test data to the deployment\n",
    "> * Get details of the deployment\n",
    "> * Create a second deployment\n",
    "> * Manually scale the second deployment\n",
    "> * Update allocation of production traffic between both deployments\n",
    "> * Get details of the second deployment\n",
    "> * Roll out the new deployment and delete the first deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Overview\n",
    "You can deploy your machine learning model as an [`online endpoint`](https://docs.microsoft.com/azure/machine-learning/concept-endpoints), that is, as a web service in the Azure cloud, so that so you don't have to create and manage the underlying infrastructure. For this deployment, you typically need:\n",
    "\n",
    "* The model assets (file and metadata) that you want to deploy.\n",
    "* A scoring script, that is, some code to run as a service. The code executes the model on a given input request. This entry script receives data submitted to a deployed web service and passes it to the model, then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns. With an MLflow model, as in this tutorial, this script is automatically created for you. Samples of scoring scripts can be found [here](https://github.com/Azure/azureml-examples/tree/sdk-preview/sdk/endpoints/online)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prerequisites\n",
    "If you already completed the earlier Day 1 tutorials, _train model responsibly_ or _create reusable pipeline_, you have everything you need to continue with the tutorial.\n",
    "\n",
    "If you’ve not completed the earlier tutorials, be sure to do the following: \n",
    "\n",
    "- Access an Azure account with an active subscription. If you don't have an Azure subscription, [create a free account](https://azure.microsoft.com/free/) to begin.\n",
    "- Create an AzureML workspace and a compute instance if you don't have them already. The [Quickstart: Create workspace resources](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources) provides steps that you can follow. Be sure to have enough quota for the compute resources.\n",
    "    - <mark>**(TODO: specify what \"enough quota\" means)**</mark>\n",
    "- Configure the environment: \n",
    "    - <mark>#TODO: We should specify what additional environment configuration is needed for inferencing. Dev environment would be set up in article 2.)** ~~Skip if you have already done it. If you haven’t done it, go read tutorial 2. ~~</mark>\n",
    "- Retrieve the metadata and files for the model you'll deploy \n",
    "    - <mark>**(TODO: add links to the files here)**</mark>\n",
    "    - <mark> MOVE FROM HERE: You'll be working with an MLflow model. AzureML supports no-code deployment of models created and logged with MLflow. This means that you don't have to provide a scoring script or an environment. Note that if you were using a custom model, you'd have to specify the environment and scoring script. See [Customizing MLflow model deployments with scoring script](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?tabs=cli#customizing-mlflow-model-deployments) to learn how to use your scoring script. **(TODO: put a link to how this can be done)**</mark>\n",
    "> [!IMPORTANT]\n",
    "> If you typically deploying models using scoring scripts and custom environments and want to achieve the same functionality using MLflow models, we recommend reading [Using MLflow models for no-code deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-mlflow-models?tabs=wrapper).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Connect to the workspace\n",
    "Before you dive in the code, you'll need to connect to your AzureML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section, we'll connect to the workspace in which you'll perform deployment tasks.\n",
    "\n",
    "We're using `DefaultAzureCredential` to get access to the workspace. \n",
    "`DefaultAzureCredential` is used to handle most Azure SDK authentication scenarios. \n",
    "\n",
    "If `DefaultAzureCredential` doesn't work for you, you can access other available credentials by checking these references: [configure credential example](../../configuration.ipynb) and [azure-identity reference doc](https://docs.microsoft.com/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you want to use a browser to login and authenticate, you can uncomment the following code to use the `InteractiveBrowserCredential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle to the workspace\n",
    "# from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "# from azure.identity import InteractiveBrowserCredential\n",
    "# credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to a workspace, you need identifier parameters—a Subscription ID, Resource Group name and Workspace name. You'll use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required AzureML workspace. This example uses the [default Azure authentication](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python).\n",
    "\n",
    "In the next cell, replace the placeholder text `<RESOURCE_GROUP>` with your resource group name, `<SUBSCRIPTION_ID>` with your subscription ID, and `<AZUREML_WORKSPACE_NAME>` with your workspace name. To find these values:\n",
    "\n",
    "1. In the top right of the Azure Machine Learning studio toolbar, select your workspace name.\n",
    "2. Copy the value for workspace, resource group and subscription ID into the code.\n",
    "3. You'll need to copy one value at a time, close the area, paste, then come back for the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your AzureML workspace\n",
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace = \"<AZUREML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "     DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a handle to the workspace that you'll use to manage other resources and jobs.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Creating `MLClient` will not connect to the workspace. The client initialization is lazy and will wait for the first time it needs to make a call. In this notebook, this call will happen during compute creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "\n",
    "If you didn't complete the the earlier Day 1 tutorials, _train model responsibly_ or _create reusable pipeline_, you'll need to register the model. <mark>Deployment of unregistered models is not supported in Azure Machine Learning.</mark> If you already completed either of those tutorials, you can skip to the next section.\n",
    "\n",
    "In this example, we specify the `path` (where to upload files from) inline. Here, you'll upload the model from your local computer. The SDK automatically uploads the files and registers the model ~~and environment~~. As a best practice for production, you should register the model ~~and environment~~ and specify the registered name and version separately in the code. \n",
    "\n",
    "For more information on registering your model as an asset, see [Register your model as an asset in Machine Learning by using the SDK](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models?tabs=use-local%2Ccli#register-your-model-as-an-asset-in-machine-learning-by-using-the-sdk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "file_model = Model(\n",
    "    path=\"mlflow-model/model.pkl\",\n",
    "    type=AssetType.MLFLOW_MODEL,\n",
    "    name=\"local-file-example\",\n",
    "    description=\"Model created from local file.\",\n",
    ")\n",
    "ml_client.models.create_or_update(file_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm that the model is registered\n",
    "If you already completed tutorial 4 or 5, you registered an MLflow model as part of the training script. <mark>Deployment of unregistered models is not supported in Azure Machine Learning.</mark> ~~We recommend registering your model as a best practice for production.~~\n",
    "\n",
    "You can check the **Models** page in AzureML studio to identify the latest version of your registered model. Alternatively, the code below will retrieve the latest version number for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"credit_defaults_model\"\n",
    "\n",
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key aspects of the model\n",
    "\n",
    " <mark> MOVE FROM HERE: You'll be working with an MLflow model. AzureML supports no-code deployment of models created and logged with MLflow. This means that you don't have to provide a scoring script or an environment. Note that if you were using a custom model, you'd have to specify the environment and scoring script. See [Customizing MLflow model deployments with scoring script](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?tabs=cli#customizing-mlflow-model-deployments) to learn how to use your scoring script. **(TODO: put a link to how this can be done)**</mark>\n",
    "> [!IMPORTANT]\n",
    "> If you typically deploying models using scoring scripts and custom environments and want to achieve the same functionality using MLflow models, we recommend reading [Using MLflow models for no-code deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-mlflow-models?tabs=wrapper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a registered model, you can continue to create an endpoint and deployment. In the next section, we'll briefly cover some key details about these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoints and deployments\n",
    "\n",
    "After you train a machine learning model, you need to deploy the model so that others can use it to do inferencing. In Azure Machine Learning, you can use **endpoints** and **deployments** to do so.\n",
    "\n",
    "An **endpoint** is an interface, based on the HTTPS protocol, that clients can call to receive the inferencing (scoring) output of a trained model. It provides: \n",
    "- Authentication using \"key & token\" based auth \n",
    "- SSL termination \n",
    "- A stable scoring URI (endpoint-name.region.inference.ml.azure.com)\n",
    "\n",
    "\n",
    "A **deployment** is a set of resources required for hosting the model that does the actual inferencing. \n",
    "\n",
    "A single endpoint can contain multiple deployments. Endpoints and deployments are independent Azure Resource Manager resources that appear in the Azure portal.\n",
    "\n",
    "Azure Machine Learning uses the concept of endpoints and deployments to implement different types of endpoints: [online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints#what-are-online-endpoints) and [batch endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints#what-are-batch-endpoints). In this tutorial, we'll walk you through the steps of implementing an online endpoint—that is, an endpoint used for receiving data from clients and sending back responses in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create an online endpoint\n",
    "\n",
    "Now that you have a registered model ~~and an inference script (auto-generated this time)~~, it's time to create your online endpoint. The endpoint name needs to be unique in the entire Azure region. For this tutorial, you'll create a unique name using a universally unique identifier [`UUID`](https://en.wikipedia.org/wiki/Universally_unique_identifier#:~:text=A%20universally%20unique%20identifier%20(UUID,%2C%20for%20practical%20purposes%2C%20unique). For more information on the endpoint naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n",
    "\n",
    "> [!NOTE]\n",
    "> Expect the endpoint creation to take approximately 6 to 8 minutes.\n",
    "\n",
    "> [!TIP]\n",
    "> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](https://learn.microsoft.com/azure/machine-learning/how-to-authenticate-online-endpoint).\n",
    "> * Optionally, you can add a description and tags to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Create a unique name for the endpoint\n",
    "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create the endpoint using the `ManagedOnlineEndpoint` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
    "\n",
    "# define an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is an online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"credit_defaults\",\n",
    "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `MLClient` created earlier, we'll now create the endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the online endpoint\n",
    "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Once you've created an endpoint, you can retrieve it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(\n",
    "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding online deployments\n",
    "\n",
    "The example contains all the files needed to deploy a model to an online endpoint. To deploy a model, you must have:\n",
    "\n",
    "- Model files (or the name and version of a model that's already registered in your workspace).\n",
    "- A scoring script, that is, code that takes in user input and returns the scored result.\n",
    "- An environment in which the model runs. As you'll see, the environment might be a Docker image with Conda dependencies, or it might be a Dockerfile.\n",
    "- Settings to specify the instance type and scaling capacity\n",
    "\n",
    " **Key aspects of a deployment**\n",
    "    * `name` - Name of the deployment.\n",
    "    * `endpoint_name` - Name of the endpoint that will contain the deployment.\n",
    "    * `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
    "    * `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification. For more information on creating an environment, see \n",
    "[Manage Azure Machine Learning environments with the CLI & SDK (v2)](how-to-manage-environments-v2.md#create-an-environment)\n",
    "    * `code_configuration` - the configuration for the source code and scoring script.\n",
    "        * `path`- Path to the source code directory for scoring the model.\n",
    "        * `scoring_script` - Relative path to the scoring file in the source code directory\n",
    "    * `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](reference-managed-online-endpoints-vm-sku-list.md).\n",
    "    * `instance_count` - The number of instances to use for the deployment.\n",
    "    \n",
    "### Deployment using an MLflow model\n",
    "\n",
    "The registered model is an MLflow model. AzureML supports no-code deployment of models created and logged with MLflow. This means that you don't have to provide a scoring script or an environment during model deployment, as the scoring script and environment are automatically generated for MLflow models. However, if you were using a custom model, you'd have to specify the environment and scoring script. See [Customizing MLflow model deployments with scoring script](https://learn.microsoft.com/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints#customizing-mlflow-model-deployments) to learn how to use a scoring script.\n",
    "> [!IMPORTANT]\n",
    "> If you typically deploy models using scoring scripts and custom environments and want to achieve the same functionality using MLflow models, we recommend reading [Using MLflow models for no-code deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-mlflow-models?tabs=wrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy the model to the endpoint\n",
    "\n",
    "You'll begin by creating a single deployment that handles 100% of the incoming traffic. We've chosen an arbitrary color name (*blue*) for the deployment. To create the deployment for our endpoint, we'll use the `ManagedOnlineDeployment` class.\n",
    "\n",
    "> [!NOTE]\n",
    "> No need to specify an environment because we are using an MLflow model. MLflow automatically generates the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineDeployment\n",
    "\n",
    "# Choosing the latest version of our registered model for deployment\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "# define an online deployment\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `MLClient` created earlier, we'll now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the online deployment\n",
    "blue_deployment = ml_client.online_deployments.begin_create_or_update(blue_deployment).result()\n",
    "\n",
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the status of the endpoint\n",
    "You can check the status of the endpoint to see whether the model was deployed without error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "print(f\"Name: {endpoint.name}\\nStatus: {endpoint.provisioning_state}\\nDescription: {endpoint.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint with sample data\n",
    "\n",
    "Using the `MLClient` created earlier, we'll get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n",
    "\n",
    "* `endpoint_name` - Name of the endpoint\n",
    "* `request_file` - File with request data\n",
    "* `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We'll send a sample request using a [json](https://github.com/Azure/azureml-examples/tree/main/sdk/python/endpoints/online/model-1/sample-request.json) file. <mark>UPDATE Json file and add it to the repo</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get logs of the deployment\n",
    "Check the logs to see whether the endpoint/deployment were invoked successfuly\n",
    "If you face errors, see [Troubleshooting online endpoints deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints?tabs=cli)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = ml_client.online_deployments.get_logs(name=\"blue\", endpoint_name=online_endpoint_name, lines=50)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a second deployment \n",
    "Deploy the model as a second deployment called `green`. For the purpose of our example, you'll deploy the same model. In a real use case, you'd want to deploy a new version of the model or a new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the model to deploy. Here we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "# define an online deployment\n",
    "green_deployment = ManagedOnlineDeployment(\n",
    "    name=\"green\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "# create the online deployment\n",
    "green_deployment = ml_client.online_deployments.begin_create_or_update(green_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale deployment to handle more traffic\n",
    "\n",
    "Using the `MLClient` created earlier, we'll get a handle to the `green` deployment. The deployment can be scaled by increasing or decreasing the `instance_count`.\n",
    "\n",
    "In the following code, you'll increase the VM instance manually. However, note that it is also possible to autoscale online endpoints. Autoscale automatically runs the right amount of resources to handle the load on your application. Managed online endpoints support autoscaling through integration with the Azure monitor autoscale feature. To configure autoscaling, see [autoscale online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-autoscale-endpoints?tabs=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update definition of the deployment\n",
    "green_deployment.instance_count = 2\n",
    "\n",
    "# update the deployment\n",
    "ml_client.online_deployments.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update traffic allocation for deployments\n",
    "You can split production traffic between deployments. You may first want to test the `green` deployment with sample data, just like you did for the `blue` deployment. Once you've tested your green deployment, allocate a small percentage of traffic to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 90, \"green\": 10}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test traffic allocation by invoking the endpoint several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can invoke the endpoint several times\n",
    "for i in range(20):\n",
    "    ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        request_file=\"./deploy/sample-request.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show logs from the `green` deployment to check that there were incoming requests and the model was scored successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(name=\"green\", endpoint_name=online_endpoint_name, lines=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send all traffic to the new deployment\n",
    "Once you're fully satisfied with your `green` deployment, switch all traffic to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 0, \"green\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the old deployment\n",
    "Remove the old (blue) deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_delete(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint and deployment\n",
    "\n",
    "If you aren't going use the deployment, you should delete it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Mirror traffic\n",
    "- Batch deployment articles\n",
    "- autoscaling deployment\n",
    "- understanding the cost of running endpoints\n",
    "- getting user logs and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections below could be developed or removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> @shohei<mark>\n",
    "### GET DETAILS (@shohei can you add code for these?)\n",
    "- every detail/status of the endpoint/deployment. This includes checking  \n",
    "(1) metadata of endpoint/deployment (deployment, container, access, etc), tracking  \n",
    "(2) operational metrics (operational metrics e.g. latency/throughput/utilization/memory/etc via AZmon) \n",
    "(3) system logs (to a level to debug which instance did what) (Log Analytics / Azure Monitor) \n",
    "(4) user logs (AppInsight) \n",
    "\n",
    "### REQUEST QUOTA  \n",
    "add reference documentation to MLOps docs: \n",
    "- workspace-level quotas\n",
    "- VMquota\n",
    "- #cores\n",
    "- #endpoint quota incraeases (endpoint/deployment)\n",
    "- #connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what you need for model deployment\n",
    "### (*@shohei, we can delete this section, right?, the information is already in the \"prerequisites\"*)\n",
    "Again, these resources are needed to be specifed in the definition of Online deployment. \n",
    "\n",
    "- Model: \n",
    "    - Registered\n",
    "- Environment (specific to model):\n",
    "    - conda.yaml and docker image (you don’t need to install docker; you need a link to the docker image)\n",
    "    - Automatically generated for MLflow models. You don't need to specify this time. If you were using a custom model, you'd have to specify the environment.(TODO: put a link to how this can be done?)\n",
    "- Compute resource\n",
    "    - VM SkU is to be specified \n",
    "- Scoring script\n",
    "    - The scoring script which takes input requests and returns the scored results.\n",
    "    - Automatically generated for MLflow models. You don't need to specify this time.\n",
    "\n",
    "## Understand the scoring script (DELETE section)\n",
    "\n",
    "[Customizing MLflow model deployments with scoring script](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?tabs=cli#customizing-mlflow-model-deployments)\n",
    "\n",
    "Skip explaining scoring script, we can mention we support bringing own scoring script for advanced scenarios and link to [this article](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?tabs=cli#customizing-mlflow-model-deployments) for learners to see how to use a scoring scipt\n",
    "\n",
    "### TODO:\n",
    "- Can we show the content of the scoring script (autogenerated by MLflow) and explain it\n",
    "    - mention that for custom models (not mlflow), you'd need to provide a scoring script and validate it.\n",
    "    - TODO: Link to an article that shows how to do this\n",
    "\n",
    "\n",
    "> [!TIP]\n",
    "> The format of the scoring script for online endpoints is the same format that's used in the preceding version of the CLI and in the Python SDK.\n",
    "\n",
    "As noted earlier, the script specified in `CodeConfiguration(scoring_script=\"score.py\")` must have an `init()` function and a `run()` function. \n",
    "\n",
    "\n",
    "This example uses the [score.py file](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/model-1/onlinescoring/score.py):\n",
    "__score.py__\n",
    ":::code language=\"python\" source=\"~/azureml-examples-main/cli/endpoints/online/model-1/onlinescoring/score.py\" :::\n",
    "\n",
    "The `init()` function is called when the container is initialized or started. Initialization typically occurs shortly after the deployment is created or updated. Write logic here for global initialization operations like caching the model in memory (as we do in this example). The `run()` function is called for every invocation of the endpoint and should do the actual scoring and prediction. In the example, we extract the data from the JSON input, call the scikit-learn model's `predict()` method, and then return the result."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
