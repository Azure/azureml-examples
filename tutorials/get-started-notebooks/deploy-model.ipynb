{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Internal notes\n",
    "- descriptions should be refined a lot\n",
    "- not tested for work\n",
    "\n",
    "# Open Questions\n",
    "- Do we customize the deployment with a scoring script and explain the scoring script in this tutorial, or should we simply deploy using the automatically generated scoring script and link to [this article](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?tabs=cli#customizing-mlflow-model-deployments) for learners to see how to use a scoring scipt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Deploy the model as an online endpoint\n",
    "\n",
    "## Working Outline\n",
    "Now deploy your machine learning model as a web service in the Azure cloud, an [`online endpoint`](https://docs.microsoft.com/azure/machine-learning/concept-endpoints).\n",
    "\n",
    "To deploy a machine learning service, you usually need:\n",
    "\n",
    "* The model assets (file, metadata) that you want to deploy. You've already registered these assets in your training job.\n",
    "* Some code to run as a service. The code executes the model on a given input request. This entry script receives data submitted to a deployed web service and passes it to the model, then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns. With an MLFlow model, as in this tutorial, this script is automatically created for you. Samples of scoring scripts can be found [here](https://github.com/Azure/azureml-examples/tree/sdk-preview/sdk/endpoints/online).\n",
    "\n",
    "## Prerequisites\n",
    "If you already completed tutorial 4 or tutorial 5, you have everything you need.\n",
    "\n",
    "If you’re starting from here, you need to do the following: \n",
    "\n",
    "- Create a workspace, create compute resource,  \n",
    "- Configure the environment: Dev environment would be set up in article 2. Skip if you have already done it. If you haven’t done it, go read tutorial 2. We probably want to make it explicit what additional environment configuration is needed for inferencing. \n",
    "- We’ll provide the model files ~~(they’ll register later)~~. \n",
    "- Make sure to have enough quota for the compute resources (VM SKUs) \n",
    "\n",
    "## Connect to the workspace (configure auth)\n",
    "Before you dive in the code, you'll need to connect to your Azure ML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
    "\n",
    "We're using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` is used to handle most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it doesn't work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"f66b853e-91bc-4852-9a4e-506ac873520a\",\n",
    "    resource_group_name=\"rg-azureml\",\n",
    "    workspace_name=\"ml-lab\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required AzureML libraries for model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what you need\n",
    "Again, these resources are needed to be specifed in the definition of Online deployment. \n",
    "\n",
    "- Model: \n",
    "    - Registered\n",
    "- Environment (specific to model):\n",
    "    - conda.yaml and docker image (you don’t need to install docker; you need a link to the docker image)\n",
    "    - Automatically generated for MLflow models. You don't need to specify this time. If you were using a custom model, you'd have to specify the environment.(TODO: put a link to how this can be done?)\n",
    "- Compute resource\n",
    "    - VM SkU is to be specified \n",
    "- Scoring script\n",
    "    - The scoring script which takes input requests and returns the scored results.\n",
    "    - Automatically generated for MLflow models. You don't need to specify this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the model is registered\n",
    "If you already completed tutorial 4 or 5, your model was registered in the training script.\n",
    "We recommend registering model as a best practice.\n",
    "\n",
    "You can check the **Models** page on Azure ML studio, to identify the latest version of your registered model. Alternatively, the code below will retrieve the latest version number for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"credit_defaults_model\"\n",
    "\n",
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the scoring script (MAYBE?)\n",
    "\n",
    "[Customizing MLflow model deployments with scoring script](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?tabs=cli#customizing-mlflow-model-deployments)\n",
    "\n",
    "### TODO:\n",
    "- Can we show the content of the scoring script (autogenerated by MLflow) and explain it\n",
    "    - mention that for custom models (not mlflow), you'd need to provide a scoring script and validate it.\n",
    "    - TODO: Link to an article that shows how to do this\n",
    "\n",
    "\n",
    "> [!TIP]\n",
    "> The format of the scoring script for online endpoints is the same format that's used in the preceding version of the CLI and in the Python SDK.\n",
    "\n",
    "As noted earlier, the script specified in `CodeConfiguration(scoring_script=\"score.py\")` must have an `init()` function and a `run()` function. \n",
    "\n",
    "\n",
    "This example uses the [score.py file](https://github.com/Azure/azureml-examples/blob/main/sdk/python/endpoints/online/model-1/onlinescoring/score.py):\n",
    "__score.py__\n",
    ":::code language=\"python\" source=\"~/azureml-examples-main/cli/endpoints/online/model-1/onlinescoring/score.py\" :::\n",
    "\n",
    "The `init()` function is called when the container is initialized or started. Initialization typically occurs shortly after the deployment is created or updated. Write logic here for global initialization operations like caching the model in memory (as we do in this example). The `run()` function is called for every invocation of the endpoint and should do the actual scoring and prediction. In the example, we extract the data from the JSON input, call the scikit-learn model's `predict()` method, and then return the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoints and deployments\n",
    "\n",
    "After you train a machine learning model, you need to deploy the model so that others can use it to do inferencing. In Azure Machine Learning, you can use **endpoints** and **deployments** to do so.\n",
    "\n",
    "An **endpoint** is an interface, based on the HTTPS protocol, that clients can call to receive the inferencing (scoring) output of a trained model. It provides: \n",
    "- Authentication using \"key & token\" based auth \n",
    "- SSL termination \n",
    "- A stable scoring URI (endpoint-name.region.inference.ml.azure.com)\n",
    "\n",
    "\n",
    "A **deployment** is a set of resources required for hosting the model that does the actual inferencing. \n",
    "\n",
    "A single endpoint can contain multiple deployments. Endpoints and deployments are independent Azure Resource Manager resources that appear in the Azure portal.\n",
    "\n",
    "Azure Machine Learning uses the concept of endpoints and deployments to implement different types of endpoints: [online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints#what-are-online-endpoints) and [batch endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints#what-are-batch-endpoints). In this tutorial, we'll walk you through the steps of implementing an online endpoint—that is, an endpoint used for receiving data from clients and sending back responses in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create an online endpoint\n",
    "\n",
    "Now that you have a registered model and an inference script (auto-generated this time), it's time to create your online endpoint. The endpoint name needs to be unique in the entire Azure region. For this tutorial, you'll create a unique name using a universally unique identifier [`UUID`](https://en.wikipedia.org/wiki/Universally_unique_identifier#:~:text=A%20universally%20unique%20identifier%20(UUID,%2C%20for%20practical%20purposes%2C%20unique). For more information on the endpoint naming rules, see [managed online endpoint limits](how-to-manage-quotas.md#azure-machine-learning-managed-online-endpoints).\n",
    "\n",
    "> [!NOTE]\n",
    "> Expect the endpoint creation to take approximately 6 to 8 minutes.\n",
    "\n",
    "> [!TIP]\n",
    "> * `auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` doesn't expire, but `aml_token` does expire. For more information on authenticating, see [Authenticate to an online endpoint](how-to-authenticate-online-endpoint.md).\n",
    "> * Optionally, you can add a description and tags to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Create a unique name for the endpoint\n",
    "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create the endpoint using the `ManagedOnlineEndpoint` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# define an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is an online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"credit_defaults\",\n",
    "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `MLClient` created earlier, we'll now create the endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the online endpoint\n",
    "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Once you've created an endpoint, you can retrieve it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(\n",
    "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy the model to the endpoint\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. Once the endpoint is created, deploy the model with the entry script. Each endpoint can have multiple deployments. Direct traffic to these deployments can be specified using rules. Here, you'll create a single deployment that handles 100% of the incoming traffic. We've chosen an arbitrary color name (*blue*) for the deployment. We'll create the deployment for our endpoint using the `ManagedOnlineDeployment` class.\n",
    "\n",
    "Note: no need to specify an environment because we are using an MLflow model. MLflow automatically generates the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# choosing the model to deploy. Here, we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "# define an online deployment\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1,\n",
    "    environment = env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `MLClient` created earlier, we'll now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the online deployment\n",
    "blue_deployment = ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the status of the endpoint\n",
    "You can check the status of the endpoint to see whether the model was deployed without error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "print(f\"Name: {endpoint.name}\\nStatus: {endpoint.provisioning_state}\\nDescription: {endpoint.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a sample query (Manually send test data)\n",
    "\n",
    "Now that the model is deployed to the endpoint, you can run inference with it.\n",
    "\n",
    "Create a sample request file following the design expected in the run method in the score script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store a sample request file\n",
    "import os\n",
    "deploy_dir = \"./deploy\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sample request file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\n",
    "  \"input_data\": {\n",
    "    \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
    "    \"index\": [0, 1],\n",
    "    \"data\": [\n",
    "            [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
    "            [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
    "        ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    "    deployment_name=\"blue\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get logs of the deployment\n",
    "Check the logs to see whether the endpoint/deployment were invoked successfuly\n",
    "If you face errors, see [Troubleshooting online endpoints deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints?tabs=cli)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = ml_client.online_deployments.get_logs(name=\"blue\", endpoint_name=online_endpoint_name, lines=50)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a second deployment \n",
    "Deploy the model as a new deployment `green`\n",
    "This time we'll deploy the same model as an example but you can deploy new model for real use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the model to deploy. Here we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "# define an online deployment\n",
    "green_deployment = ManagedOnlineDeployment(\n",
    "    name=\"green\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "# create the online deployment\n",
    "green_deployment = ml_client.online_deployments.begin_create_or_update(green_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling online endpoints\n",
    "(Show manual, mention (link) auto-scale. Do this for model #2 only. )\n",
    "This time we'll increase the VM instance manually. \n",
    "You can setup autoscaling: [Autoscale online endpoints - Azure Machine Learning | Microsoft Learn](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-autoscale-endpoints?tabs=python)\n",
    "You can update deployment settings like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update definition of the deployment\n",
    "green_deployment.instance_count = 2\n",
    "# update the deployment\n",
    "ml_client.online_deployments.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update traffic allocation for deployments\n",
    "You can split production traffic between deployments. Traffic allocation settings should be done at the endpoint settings.\n",
    "Once you've tested your green deployment, allocate a small percentage of traffic to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 90, \"green\": 10}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test traffic allocation by invoking the endpoint several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ml_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# You can invoke the endpoint several times\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mml_client\u001b[49m\u001b[38;5;241m.\u001b[39monline_endpoints\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      4\u001b[0m         endpoint_name\u001b[38;5;241m=\u001b[39monline_endpoint_name,\n\u001b[1;32m      5\u001b[0m         request_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./deploy/sample-request.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ml_client' is not defined"
     ]
    }
   ],
   "source": [
    "# You can invoke the endpoint several times\n",
    "for i in range(20):\n",
    "    ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        request_file=\"./deploy/sample-request.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show logs from green deployment to check there were incoming requests and the model was scored successfully "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(name=\"green\", endpoint_name=online_endpoint_name, lines=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll out the new deployment and delete the old deployment\n",
    "Once you're fully satisfied with your green deployment, switch all traffic to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 0, \"green\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the old (blue) deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_delete(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Mirror traffic\n",
    "- Batch deployment articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
