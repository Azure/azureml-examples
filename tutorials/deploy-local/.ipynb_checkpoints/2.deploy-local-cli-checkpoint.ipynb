{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd969e25",
   "metadata": {},
   "source": [
    "# Deploy machine learning models to Azure\n",
    "\n",
    "description: (preview) deploy your machine learning or deep learning model as a web service in the Azure cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996082fd",
   "metadata": {},
   "source": [
    "# Connect to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9f14ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('chrjia-rg', '13e50845-67bc-4ac5-94db-48d493a6d9e8')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "#get workspace configurations\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#get subscription and resourcegroup from config\n",
    "SUBSCRIPTION_ID = ws.subscription_id\n",
    "RESOURCE_GROUP = ws.resource_group\n",
    "\n",
    "RESOURCE_GROUP, SUBSCRIPTION_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63df057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-canadacentral\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-ncus\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjiaws-uksouth\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-wus\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-cus\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-eus\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-neurope\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-eus2\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-wus2\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-wcus\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-weurope\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-scus\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-eastus2euap2\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"mlnotebooksexp\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-cuseuap3\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"resourceGroup\": \"chrjia-rg\",\r\n",
      "    \"subscriptionId\": \"13e50845-67bc-4ac5-94db-48d493a6d9e8\",\r\n",
      "    \"workspaceName\": \"chrjia-cuseuap2\"\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!az account set -s $SUBSCRIPTION_ID\n",
    "!az ml workspace list --resource-group=$RESOURCE_GROUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057aaf6",
   "metadata": {},
   "source": [
    "# Register your model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3cd0e",
   "metadata": {},
   "source": [
    "## Register a model from a local file\n",
    "\n",
    "You can register a model by providing the local path of the model. You can provide the path of either a folder or a single file on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf6e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model bidaf_onnx\n",
      "{\n",
      "  \"cpu\": \"\",\n",
      "  \"createdTime\": \"2021-05-05T06:37:31.597001+00:00\",\n",
      "  \"description\": \"\",\n",
      "  \"experimentName\": \"\",\n",
      "  \"framework\": \"Custom\",\n",
      "  \"frameworkVersion\": null,\n",
      "  \"gpu\": \"\",\n",
      "  \"id\": \"bidaf_onnx:25\",\n",
      "  \"memoryInGB\": \"\",\n",
      "  \"name\": \"bidaf_onnx\",\n",
      "  \"properties\": \"\",\n",
      "  \"runId\": \"\",\n",
      "  \"sampleInputDatasetId\": \"\",\n",
      "  \"sampleOutputDatasetId\": \"\",\n",
      "  \"tags\": \"\",\n",
      "  \"version\": 25\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!wget https://aka.ms/bidaf-9-model -o model.onnx\n",
    "!az ml model register -n bidaf_onnx -p ./model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d34207",
   "metadata": {},
   "source": [
    "## Deploy your machine learning model\n",
    "\n",
    "Replace bidaf_onnx:1 with the name of your model and its version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9762e02b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model bidaf_onnx:1 to /tmp/azureml_xswhj173/bidaf_onnx/1\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry chrjiaeastusc6ba5291.azurecr.io\n",
      "Logging into Docker registry chrjiaeastusc6ba5291.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM chrjiaeastusc6ba5291.azurecr.io/azureml/azureml_7610012a4379937742d36a55b47b6388\n",
      " ---> 79bfd0273b68\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> 7acb3557c595\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjEzZTUwODQ1LTY3YmMtNGFjNS05NGRiLTQ4ZDQ5M2E2ZDllOCIsInJlc291cmNlR3JvdXBOYW1lIjoiY2hyamlhLXJnIiwiYWNjb3VudE5hbWUiOiJjaHJqaWEtZWFzdHVzMmV1YXAyIiwid29ya3NwYWNlSWQiOiI1OTkzZGVjMS01OTczLTQ4NTktOGJlMS03MDczMzJjNTQ5NWQifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in 0ac193f12b61\n",
      " ---> f9deb295ee83\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmpf483ft4y.py' /var/azureml-app/main.py\n",
      " ---> Running in e693cfee00ca\n",
      " ---> 9333af3cfd3e\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 5805470302f6\n",
      " ---> 88ba78acaf65\n",
      "Successfully built 88ba78acaf65\n",
      "Successfully tagged myservice:latest\n",
      "Container (name:distracted_proskuriakova, id:c3b062783db0338a71c05921ff5a7ed061d3a64fa2bf6d809fefcb4044369cdd) cannot be killed.\n",
      "Container has been successfully cleaned up.\n",
      "Image sha256:116dc62679cdb637314f1e2246a677072383551b59b8584c753eff4c3238245d successfully removed.\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:32267\n",
      "{\n",
      "  \"computeType\": \"Local\",\n",
      "  \"environmentDetails\": null,\n",
      "  \"imageId\": null,\n",
      "  \"name\": \"myservice\",\n",
      "  \"properties\": \"\",\n",
      "  \"scoringUri\": \"http://localhost:32267/score\",\n",
      "  \"state\": \"running\",\n",
      "  \"tags\": \"\",\n",
      "  \"updatedAt\": \"2021-05-05T12:07:56.770909\"\n",
      "}\n",
      "\u001b[91m{'Azure-cli-ml Version': '1.27.0', 'Error': TypeError(\"get_logs() got an unexpected keyword argument 'init'\",)}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!az ml model deploy -n myservice -m bidaf_onnx:1 --overwrite --ic dummyinferenceconfig.json --dc deploymentconfig.json\n",
    "!az ml service get-logs -n myservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d47b4",
   "metadata": {},
   "source": [
    "## Call into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e92e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:32267...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 32267 (#0)\n",
      "> GET / HTTP/1.1\n",
      "> Host: localhost:32267\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Server: nginx/1.10.3 (Ubuntu)\n",
      "< Date: Wed, 05 May 2021 06:44:10 GMT\n",
      "< Content-Type: text/html; charset=utf-8\n",
      "< Content-Length: 7\n",
      "< Connection: keep-alive\n",
      "< x-ms-request-id: 60fd3a45-682c-48f1-8104-1160305cc129\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "HealthyNote: Unnecessary use of -X or --request, POST is already inferred.\n",
      "*   Trying 127.0.0.1:32267...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 32267 (#0)\n",
      "> POST /score HTTP/1.1\n",
      "> Host: localhost:32267\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> content-type:application/json\n",
      "> Content-Length: 94\n",
      "> \n",
      "* upload completely sent off: 94 out of 94 bytes\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Server: nginx/1.10.3 (Ubuntu)\n",
      "< Date: Wed, 05 May 2021 06:44:10 GMT\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 104\n",
      "< Connection: keep-alive\n",
      "< x-ms-run-function-failed: False\n",
      "< x-ms-request-id: 16f47088-0219-42a2-8716-c4cd668e1e82\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n",
      "\"test is {'query': 'What color is the fox', 'context': 'The quick brown fox jumped over the lazy dog.'}\""
     ]
    }
   ],
   "source": [
    "!curl -v http://localhost:32267\n",
    "!curl -v -X POST -H \"content-type:application/json\" -d '{\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}' http://localhost:32267/score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9e82c",
   "metadata": {},
   "source": [
    "Notice the use of the AZUREML_MODEL_DIR environment variable to locate your registered model. Now that you've added some pip packages, you also need to update your inference configuration with [new configurations](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=azcli#tabpanel_7_azcli) to add in those additional packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0c387",
   "metadata": {},
   "source": [
    "## Deploy again and call your service\n",
    "\n",
    "Replace `bidaf_onnx:1` with the name of your model and its version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd084966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model bidaf_onnx:1 to /tmp/azureml_u3bd2n0j/bidaf_onnx/1\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry chrjiaeastusc6ba5291.azurecr.io\n",
      "Logging into Docker registry chrjiaeastusc6ba5291.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM chrjiaeastusc6ba5291.azurecr.io/azureml/azureml_444d498040a1ecec3683d0ba62162807\n",
      " ---> f31b09e72b32\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> b56af3a88f05\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjEzZTUwODQ1LTY3YmMtNGFjNS05NGRiLTQ4ZDQ5M2E2ZDllOCIsInJlc291cmNlR3JvdXBOYW1lIjoiY2hyamlhLXJnIiwiYWNjb3VudE5hbWUiOiJjaHJqaWEtZWFzdHVzMmV1YXAyIiwid29ya3NwYWNlSWQiOiI1OTkzZGVjMS01OTczLTQ4NTktOGJlMS03MDczMzJjNTQ5NWQifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in 7edb0e5f7963\n",
      " ---> 032d0fb62d37\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmpq4sn3whp.py' /var/azureml-app/main.py\n",
      " ---> Running in e1eee6912b8f\n",
      " ---> c69370229fde\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 91caf6502132\n",
      " ---> 34380aabec6d\n",
      "Successfully built 34380aabec6d\n",
      "Successfully tagged myservice:latest\n",
      "Container has been successfully cleaned up.\n",
      "Image sha256:88ba78acaf65529adba7bee4e85fefe09bad853e59f7eb32d970c026dd0b4314 successfully removed.\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:32267\n",
      "{\n",
      "  \"computeType\": \"Local\",\n",
      "  \"environmentDetails\": null,\n",
      "  \"imageId\": null,\n",
      "  \"name\": \"myservice\",\n",
      "  \"properties\": \"\",\n",
      "  \"scoringUri\": \"http://localhost:32267/score\",\n",
      "  \"state\": \"running\",\n",
      "  \"tags\": \"\",\n",
      "  \"updatedAt\": \"2021-05-05T12:21:14.486962\"\n",
      "}\n",
      "\u001b[91m{'Azure-cli-ml Version': '1.27.0', 'Error': TypeError(\"get_logs() got an unexpected keyword argument 'init'\",)}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!az ml model deploy -n myservice -m bidaf_onnx:1 --overwrite --ic inferenceconfig.json --dc deploymentconfig.json\n",
    "!az ml service get-logs -n myservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861af8f",
   "metadata": {},
   "source": [
    "Then ensure you can send a post request to the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dac0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Unnecessary use of -X or --request, POST is already inferred.\r\n",
      "*   Trying 127.0.0.1:32267...\r\n",
      "* TCP_NODELAY set\r\n",
      "* Connected to localhost (127.0.0.1) port 32267 (#0)\r\n",
      "> POST /score HTTP/1.1\r",
      "\r\n",
      "> Host: localhost:32267\r",
      "\r\n",
      "> User-Agent: curl/7.68.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> content-type:application/json\r",
      "\r\n",
      "> Content-Length: 94\r",
      "\r\n",
      "> \r",
      "\r\n",
      "* upload completely sent off: 94 out of 94 bytes\r\n",
      "* Mark bundle as not supporting multiuse\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Server: nginx/1.10.3 (Ubuntu)\r",
      "\r\n",
      "< Date: Wed, 05 May 2021 07:01:19 GMT\r",
      "\r\n",
      "< Content-Type: application/json\r",
      "\r\n",
      "< Content-Length: 9\r",
      "\r\n",
      "< Connection: keep-alive\r",
      "\r\n",
      "< x-ms-run-function-failed: False\r",
      "\r\n",
      "< x-ms-request-id: 85469a97-5cbd-4693-8270-fdfa14e0a79e\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host localhost left intact\r\n",
      "[\"brown\"]"
     ]
    }
   ],
   "source": [
    "!curl -v -X POST -H \"content-type:application/json\" -d '{\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}' http://localhost:32267/score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9aab6b",
   "metadata": {},
   "source": [
    "## Re-deploy to cloud\n",
    "\n",
    "Once you've confirmed your service works locally and chosen a remote compute target, you are ready to deploy to the cloud.\n",
    "Change your re-deploy configuration to correspond to the compute target you've chosen, in this case Azure Container Instances.\n",
    "\n",
    "Deploy your service again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51029f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "{\n",
      "  \"computeType\": \"ACI\",\n",
      "  \"environmentDetails\": {\n",
      "    \"databricks\": {\n",
      "      \"eggLibraries\": [],\n",
      "      \"jarLibraries\": [],\n",
      "      \"mavenLibraries\": [],\n",
      "      \"pypiLibraries\": [],\n",
      "      \"rcranLibraries\": []\n",
      "    },\n",
      "    \"docker\": {\n",
      "      \"arguments\": [],\n",
      "      \"baseDockerfile\": null,\n",
      "      \"baseImage\": \"mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\",\n",
      "      \"baseImageRegistry\": {\n",
      "        \"address\": null,\n",
      "        \"password\": null,\n",
      "        \"registryIdentity\": null,\n",
      "        \"username\": null\n",
      "      },\n",
      "      \"enabled\": false,\n",
      "      \"platform\": {\n",
      "        \"architecture\": \"amd64\",\n",
      "        \"os\": \"Linux\"\n",
      "      },\n",
      "      \"sharedVolumes\": true,\n",
      "      \"shmSize\": null\n",
      "    },\n",
      "    \"environmentVariables\": {\n",
      "      \"AZUREML_ENTRY_SCRIPT\": \"source_dir/score.py\",\n",
      "      \"AZUREML_SOURCE_DIRECTORY\": \"source_dir\",\n",
      "      \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
      "    },\n",
      "    \"inferencingStackVersion\": null,\n",
      "    \"name\": \"my-deploy-env\",\n",
      "    \"python\": {\n",
      "      \"baseCondaEnvironment\": null,\n",
      "      \"condaDependencies\": {\n",
      "        \"channels\": [],\n",
      "        \"dependencies\": [\n",
      "          \"python=3.6.2\",\n",
      "          {\n",
      "            \"pip\": [\n",
      "              \"azureml-defaults\",\n",
      "              \"nltk\",\n",
      "              \"numpy\",\n",
      "              \"onnxruntime\"\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"name\": \"azureml_d1b0e61be4880220e8632d706cd2f520\"\n",
      "      },\n",
      "      \"condaDependenciesFile\": null,\n",
      "      \"interpreterPath\": \"python\",\n",
      "      \"userManagedDependencies\": false\n",
      "    },\n",
      "    \"r\": null,\n",
      "    \"spark\": {\n",
      "      \"packages\": [],\n",
      "      \"precachePackages\": true,\n",
      "      \"repositories\": []\n",
      "    },\n",
      "    \"version\": \"4\"\n",
      "  },\n",
      "  \"imageId\": null,\n",
      "  \"name\": \"myaciservice\",\n",
      "  \"properties\": {\n",
      "    \"azureml.git.branch\": \"deployLocalSamples\",\n",
      "    \"azureml.git.commit\": \"5e3a69633f9c162a39a6013bda32660cfb6dbe1e\",\n",
      "    \"azureml.git.dirty\": \"True\",\n",
      "    \"azureml.git.repository_uri\": \"https://github.com/HiteshTetarwal/azureml-examples.git\",\n",
      "    \"hasHttps\": \"False\",\n",
      "    \"hasInferenceSchema\": \"False\",\n",
      "    \"mlflow.source.git.branch\": \"deployLocalSamples\",\n",
      "    \"mlflow.source.git.commit\": \"5e3a69633f9c162a39a6013bda32660cfb6dbe1e\",\n",
      "    \"mlflow.source.git.repoURL\": \"https://github.com/HiteshTetarwal/azureml-examples.git\"\n",
      "  },\n",
      "  \"scoringUri\": \"http://8d8eafe9-3f04-4ff5-9925-80acbd6bbfd9.eastus2.azurecontainer.io/score\",\n",
      "  \"state\": \"Healthy\",\n",
      "  \"tags\": \"\",\n",
      "  \"updatedAt\": \"2021-05-05T10:20:10.245431+00:00\"\n",
      "}\n",
      "[\n",
      "  \"2021-05-05T10:23:41,088369600+00:00 - gunicorn/run \\n2021-05-05T10:23:41,104440400+00:00 - iot-server/run \\n2021-05-05T10:23:41,091679500+00:00 - rsyslog/run \\n2021-05-05T10:23:41,144813800+00:00 - nginx/run \\n/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\n/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\\n2021-05-05T10:23:41,746166900+00:00 - iot-server/finish 1 0\\n2021-05-05T10:23:41,754253000+00:00 - Exit code 1 is normal. Not restarting iot-server.\\nStarting gunicorn 19.9.0\\nListening at: http://127.0.0.1:31311 (70)\\nUsing worker: sync\\nworker timeout is set to 300\\nBooting worker with pid: 95\\nInitialized PySpark session.\\nInitializing logger\\nStarting up app insights client\\nStarting up request id generator\\nStarting up app insight hooks\\nInvoking user's init function\\n2021-05-05 10:23:44.235170800 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Word_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\\n2021-05-05 10:23:44.235321400 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Char_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\\n2021-05-05 10:23:44.235354900 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __OneFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\\n2021-05-05 10:23:44.235424300 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\\n2021-05-05 10:23:44.235471400 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\\n2021-05-05 10:23:44.235499700 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __NegINF_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\\n2021-05-05 10:23:44.236178800 [W:onnxruntime:, graph.cc:1074 Graph] Initializer _Const_0 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\\n[nltk_data] Downloading package punkt to /root/nltk_data...\\n[nltk_data]   Unzipping tokenizers/punkt.zip.\\nUsers's init has completed successfully\\nScoring timeout is found from os.environ: 60000 ms\\nSwagger file not present\\n404\\n127.0.0.1 - - [05/May/2021:10:24:31 +0000] \\\"GET /swagger.json HTTP/1.0\\\" 404 19 \\\"-\\\" \\\"Go-http-client/1.1\\\"\\nSwagger file not present\\n404\\n127.0.0.1 - - [05/May/2021:10:24:36 +0000] \\\"GET /swagger.json HTTP/1.0\\\" 404 19 \\\"-\\\" \\\"Go-http-client/1.1\\\"\\nSwagger file not present\\n404\\n127.0.0.1 - - [05/May/2021:10:24:42 +0000] \\\"GET /swagger.json HTTP/1.0\\\" 404 19 \\\"-\\\" \\\"Go-http-client/1.1\\\"\\n\",\n",
      "  null\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!az ml model deploy -n myaciservice -m bidaf_onnx:1 --overwrite --ic inferenceconfig.json --dc re-deploymentconfig.json\n",
    "!az ml service get-logs -n myaciservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5448133",
   "metadata": {},
   "source": [
    "## Call your remote webservice\n",
    "\n",
    "When you deploy remotely, you may have key authentication enabled. The example below shows how to get your service key with Python in order to make an inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd48987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"brown\"]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from azureml.core import Webservice, Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "service = Webservice(workspace=ws, name=\"myaciservice\")\n",
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "# If the service is authenticated, set the key or token\n",
    "key, _ = service.get_keys()\n",
    "\n",
    "# Set the appropriate headers\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "headers[\"Authorization\"] = f\"Bearer {key}\"\n",
    "\n",
    "# Make the request and display the response and logs\n",
    "data = {\n",
    "    \"query\": \"What color is the fox\",\n",
    "    \"context\": \"The quick brown fox jumped over the lazy dog.\",\n",
    "}\n",
    "data = json.dumps(data)\n",
    "resp = requests.post(scoring_uri, data=data, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51a6b357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-05T10:23:41,088369600+00:00 - gunicorn/run \n",
      "2021-05-05T10:23:41,104440400+00:00 - iot-server/run \n",
      "2021-05-05T10:23:41,091679500+00:00 - rsyslog/run \n",
      "2021-05-05T10:23:41,144813800+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d1b0e61be4880220e8632d706cd2f520/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-05-05T10:23:41,746166900+00:00 - iot-server/finish 1 0\n",
      "2021-05-05T10:23:41,754253000+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (70)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 95\n",
      "Initialized PySpark session.\n",
      "Initializing logger\n",
      "Starting up app insights client\n",
      "Starting up request id generator\n",
      "Starting up app insight hooks\n",
      "Invoking user's init function\n",
      "2021-05-05 10:23:44.235170800 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Word_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-05-05 10:23:44.235321400 [W:onnxruntime:, graph.cc:1074 Graph] Initializer Char_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-05-05 10:23:44.235354900 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __OneFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-05-05 10:23:44.235424300 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-05-05 10:23:44.235471400 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __ZeroFloat_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-05-05 10:23:44.235499700 [W:onnxruntime:, graph.cc:1074 Graph] Initializer __NegINF_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-05-05 10:23:44.236178800 [W:onnxruntime:, graph.cc:1074 Graph] Initializer _Const_0 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Users's init has completed successfully\n",
      "Scoring timeout is found from os.environ: 60000 ms\n",
      "Swagger file not present\n",
      "404\n",
      "127.0.0.1 - - [05/May/2021:10:24:31 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "Swagger file not present\n",
      "404\n",
      "127.0.0.1 - - [05/May/2021:10:24:36 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "Swagger file not present\n",
      "404\n",
      "127.0.0.1 - - [05/May/2021:10:24:42 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "Swagger file not present\n",
      "404\n",
      "127.0.0.1 - - [05/May/2021:10:26:06 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "Validation Request Content-Type\n",
      "Received input: {\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}\n",
      "Headers passed in (total 12):\n",
      "\tHost: localhost:5001\n",
      "\tX-Real-Ip: 127.0.0.1\n",
      "\tX-Forwarded-For: 127.0.0.1\n",
      "\tX-Forwarded-Proto: http\n",
      "\tConnection: close\n",
      "\tContent-Length: 94\n",
      "\tUser-Agent: python-requests/2.25.1\n",
      "\tAccept: */*\n",
      "\tAccept-Encoding: gzip, deflate\n",
      "\tAuthorization: Bearer mq2qAJ2XKrnAhM0OPz0N4otldjTFGdfL\n",
      "\tContent-Type: application/json\n",
      "\tX-Ms-Request-Id: 23189e1c-6866-4917-bdf1-e64589325db9\n",
      "Scoring Timer is set to 60.0 seconds\n",
      "{\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}\n",
      "['brown']\n",
      "200\n",
      "127.0.0.1 - - [05/May/2021:10:26:09 +0000] \"POST /score HTTP/1.0\" 200 9 \"-\" \"python-requests/2.25.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8ef4c",
   "metadata": {},
   "source": [
    "# Delete resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ffb1039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bidaf_onnx:25'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the current model id\n",
    "import os\n",
    "\n",
    "stream = os.popen(\n",
    "    'az ml model list --model-name=bidaf_onnx --latest --query \"[0].id\" -o tsv'\n",
    ")\n",
    "MODEL_ID = stream.read()[0:-1]\n",
    "MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1b69c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"myaciservice\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"bidaf_onnx:25\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az ml service delete -n myservice\n",
    "!az ml service delete -n myaciservice\n",
    "!az ml model delete --model-id=$MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3af1a4",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f530f08",
   "metadata": {},
   "source": [
    "Try reading [our documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
