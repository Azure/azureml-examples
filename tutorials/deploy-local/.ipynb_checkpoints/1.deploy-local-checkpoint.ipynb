{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ccea8c",
   "metadata": {},
   "source": [
    "# Deploy machine learning models to Azure\n",
    "\n",
    "description: (preview) deploy your machine learning or deep learning model as a web service in the Azure cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83b2e5",
   "metadata": {},
   "source": [
    "# Connect to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace(subscription_id=\"<subscription_id>\",\n",
    "               resource_group=\"<resource_group>\",\n",
    "               workspace_name=\"<workspace_name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb89db4",
   "metadata": {},
   "source": [
    "# Register your model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74835c0d",
   "metadata": {},
   "source": [
    "## Register a model from a local file\n",
    "\n",
    "You can register a model by providing the local path of the model. You can provide the path of either a folder or a single file on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096264ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from azureml.core.model import Model\n",
    "# Download model\n",
    "urllib.request.urlretrieve(\"https://aka.ms/bidaf-9-model\", 'model.onnx')\n",
    "\n",
    "# Register model\n",
    "model = Model.register(ws, model_name='bidaf_onnx', model_path='./model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20f9cc",
   "metadata": {},
   "source": [
    "## Register a model from an Azure ML training run\n",
    "\n",
    "When you use the SDK to train a model, you can receive either a Run object or an AutoMLRun object, depending on how you trained the model. Each object can be used to register a model created by an experiment run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4dd84",
   "metadata": {},
   "source": [
    "- Register a model from an `azureml.core.Run` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='bidaf_onnx',\n",
    "                           tags={'area': 'qna'},\n",
    "                           model_path='outputs/model.onnx')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee15bc9",
   "metadata": {},
   "source": [
    "- Register a model from an `azureml.train.automl.run.AutoMLRun` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd13986",
   "metadata": {},
   "outputs": [],
   "source": [
    " description = 'My AutoML Model'\n",
    "    model = run.register_model(description = description,\n",
    "                               tags={'area': 'qna'})\n",
    "\n",
    "    print(run.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242cef7",
   "metadata": {},
   "source": [
    "# Define an inference configuration\n",
    "\n",
    "The inference configuration below specifies that the machine learning deployment will use the file echo_score.py in the ./source_dir directory to process incoming requests and that it will use the Docker image with the Python packages specified in the project_environment environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b864106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "env = Environment(name='project_environment')\n",
    "inf_config = InferenceConfig(environment=env, source_directory='./source_dir', entry_script='./echo_score.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a21e8",
   "metadata": {},
   "source": [
    "## Define a deployment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c037102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deploy_config = LocalWebservice.deploy_configuration(port=6789)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b897c16",
   "metadata": {},
   "source": [
    "## Deploy your machine learning model\n",
    "\n",
    "A deployment configuration specifies the amount of memory and cores to reserve for your webservice will require in order to run, as well as configuration details of the underlying webservice. For example, a deployment configuration lets you specify that your service needs 2 gigabytes of memory, 2 CPU cores, 1 GPU core, and that you want to enable autoscaling.\n",
    "\n",
    "The options available for a deployment configuration differ depending on the compute target you choose. In a local deployment, all you can specify is which port your webservice will be served on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e4e01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, \"myservice\", [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d801ba7",
   "metadata": {},
   "source": [
    "## Call into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "uri = service.scoring_uri\n",
    "requests.get('http://localhost:6789')\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}\n",
    "data = json.dumps(data)\n",
    "response = requests.post(uri, data=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af65179",
   "metadata": {},
   "source": [
    "Notice the use of the AZUREML_MODEL_DIR environment variable to locate your registered model. Now that you've added some pip packages, you also need to update your inference configuration to add in those additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(name='myenv')\n",
    "python_packages = ['nltk', 'numpy', 'onnxruntime']\n",
    "for package in python_packages:\n",
    "    env.python.conda_dependencies.add_pip_package(package)\n",
    "\n",
    "inf_config = InferenceConfig(environment=env, source_directory='./source_dir', entry_script='./score.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f89234",
   "metadata": {},
   "source": [
    "## Deploy again and call your service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, \"myservice\", [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c923bc6",
   "metadata": {},
   "source": [
    "Then ensure you can send a post request to the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "uri = service.scoring_uri\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}\n",
    "data = json.dumps(data)\n",
    "response = requests.post(uri, data=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f082c",
   "metadata": {},
   "source": [
    "## Re-deploy to cloud\n",
    "\n",
    "Once you've confirmed your service works locally and chosen a remote compute target, you are ready to deploy to the cloud.\n",
    "\n",
    "Change your deploy configuration to correspond to the compute target you've chosen, in this case Azure Container Instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 0.5, memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46141ec",
   "metadata": {},
   "source": [
    "Deploy your service again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, \"myservice\", [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ae349",
   "metadata": {},
   "source": [
    "## Call your remote webservice\n",
    "\n",
    "When you deploy remotely, you may have key authentication enabled. The example below shows how to get your service key with Python in order to make an inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from azureml.core import Webservice\n",
    "\n",
    "service = Webservice(workspace=ws, name='myservice')\n",
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "# If the service is authenticated, set the key or token\n",
    "primary_key, _ = service.get_keys()\n",
    "\n",
    "# Set the appropriate headers\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response and logs\n",
    "data = {\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}\n",
    "data = json.dumps(data)\n",
    "resp = requests.post(scoring_uri, data=data, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84af09",
   "metadata": {},
   "source": [
    "## Delete resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773047d6",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee44a2",
   "metadata": {},
   "source": [
    "Try reading [our documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
