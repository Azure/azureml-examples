{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd969e25",
   "metadata": {},
   "source": [
    "# Deploy machine learning models to Azure\n",
    "\n",
    "description: (preview) deploy your machine learning or deep learning model as a web service in the Azure cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996082fd",
   "metadata": {},
   "source": [
    "# Connect to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login\n",
    "!az account set -s <my subscription>\n",
    "!az ml workspace list --resource-group=<my resource group>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057aaf6",
   "metadata": {},
   "source": [
    "# Register your model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3cd0e",
   "metadata": {},
   "source": [
    "## Register a model from a local file\n",
    "\n",
    "You can register a model by providing the local path of the model. You can provide the path of either a folder or a single file on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://aka.ms/bidaf-9-model -o model.onnx\n",
    "!az ml model register -n bidaf_onnx -p ./model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8fecd",
   "metadata": {},
   "source": [
    "## Register a model from an Azure ML training run\n",
    "\n",
    "When you use the SDK to train a model, you can receive either a Run object or an AutoMLRun object, depending on how you trained the model. Each object can be used to register a model created by an experiment run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f41c2",
   "metadata": {},
   "source": [
    "- Register a model from an `azureml.core.Run` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml model register -bidaf_onnx  --asset-path outputs/model.onnx  --experiment-name myexperiment --run-id myrunid --tag area=qna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432596fe",
   "metadata": {},
   "source": [
    "If you get an error message stating that the ml extension isn't installed, use the following command to install it: `az extension add -n azure-cli-ml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc200fe9",
   "metadata": {},
   "source": [
    "# Define an inference configuration\n",
    "\n",
    "The inference configuration below specifies that the machine learning deployment will use the file echo_score.py in the ./source_dir directory to process incoming requests and that it will use the Docker image with the Python packages specified in the project_environment environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "env = Environment(name='project_environment')\n",
    "inf_config = InferenceConfig(environment=env, source_directory='./source_dir', entry_script='./echo_score.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850e5f4",
   "metadata": {},
   "source": [
    "## Define a deployment configuration\n",
    "\n",
    "A deployment configuration specifies the amount of memory and cores to reserve for your webservice will require in order to run, as well as configuration details of the underlying webservice. For example, a deployment configuration lets you specify that your service needs 2 gigabytes of memory, 2 CPU cores, 1 GPU core, and that you want to enable autoscaling.\n",
    "\n",
    "The options available for a deployment configuration differ depending on the compute target you choose. In a local deployment, all you can specify is which port your webservice will be served on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deploy_config = LocalWebservice.deploy_configuration(port=6789)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d34207",
   "metadata": {},
   "source": [
    "## Deploy your machine learning model\n",
    "\n",
    "Replace bidaf_onnx:1 with the name of your model and its version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762e02b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!az ml model deploy -n myservice -m bidaf_onnx:1 --ic inferenceconfig.json --dc deploymentconfig.json\n",
    "!az ml service get-logs -n myservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d47b4",
   "metadata": {},
   "source": [
    "## Call into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e92e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v http://localhost:32267\n",
    "!curl -v -X POST -H \"content-type:application/json\" -d '{\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}' http://localhost:32267/score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9e82c",
   "metadata": {},
   "source": [
    "Notice the use of the AZUREML_MODEL_DIR environment variable to locate your registered model. Now that you've added some pip packages, you also need to update your inference configuration to add in those additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(name='myenv')\n",
    "python_packages = ['nltk', 'numpy', 'onnxruntime']\n",
    "for package in python_packages:\n",
    "    env.python.conda_dependencies.add_pip_package(package)\n",
    "\n",
    "inf_config = InferenceConfig(environment=env, source_directory='./source_dir', entry_script='./score.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0c387",
   "metadata": {},
   "source": [
    "## Deploy again and call your service\n",
    "\n",
    "Replace `bidaf_onnx:1` with the name of your model and its version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd084966",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml model deploy -n myservice -m bidaf_onnx:1 --ic inferenceconfig.json --dc deploymentconfig.json\n",
    "!az ml service get-logs -n myservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861af8f",
   "metadata": {},
   "source": [
    "Then ensure you can send a post request to the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dac0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v -X POST -H \"content-type:application/json\" -d '{\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}' http://localhost:32267/score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9aab6b",
   "metadata": {},
   "source": [
    "## Re-deploy to cloud\n",
    "\n",
    "Once you've confirmed your service works locally and chosen a remote compute target, you are ready to deploy to the cloud.\n",
    "Change your re-deploy configuration to correspond to the compute target you've chosen, in this case Azure Container Instances.\n",
    "\n",
    "Deploy your service again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51029f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml model deploy -n myservice -m bidaf_onnx:1 --ic inferenceconfig.json --dc re-deploymentconfig.json\n",
    "!az ml service get-logs -n myservice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5448133",
   "metadata": {},
   "source": [
    "## Call your remote webservice\n",
    "\n",
    "When you deploy remotely, you may have key authentication enabled. The example below shows how to get your service key with Python in order to make an inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from azureml.core import Webservice\n",
    "\n",
    "service = Webservice(workspace=ws, name='myservice')\n",
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "# If the service is authenticated, set the key or token\n",
    "primary_key, _ = service.get_keys()\n",
    "\n",
    "# Set the appropriate headers\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response and logs\n",
    "data = {\"query\": \"What color is the fox\", \"context\": \"The quick brown fox jumped over the lazy dog.\"}\n",
    "data = json.dumps(data)\n",
    "resp = requests.post(scoring_uri, data=data, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ca0cb",
   "metadata": {},
   "source": [
    "## Delete resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml service delete myservice\n",
    "!az ml model delete bidaf_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3af1a4",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f530f08",
   "metadata": {},
   "source": [
    "Try reading [our documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
