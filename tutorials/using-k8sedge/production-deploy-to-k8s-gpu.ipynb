{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a ML model as web service on Kubernetes\n",
    "This notebook shows the steps to : registering a model, creating an image, creating Kubernetes Deployment for a service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1601051195792
    }
   },
   "outputs": [],
   "source": [
    "#upgrade to latest versionof sdk \n",
    "!pip install --upgrade azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599847277289
    }
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get workspace\n",
    "Please create a azure Machine learnign workspace on portal.azure.com before runing this notebook, once created download config.json from your workspace,  please place config.json file from portal to same folder as notebook ![Capture_withoverlay.JPG](pics/conf_file_download.JPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599847578183
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path=\"ml_workspace_config.json\")\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model\n",
    "\n",
    "Prior to registering the model, you should have a TensorFlow [Saved Model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md) in the `resnet50` directory. This cell will download a [pretrained resnet50](http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NCHW_jpg.tar.gz) and unpack it to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599847838909
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "model_url = \"http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NCHW_jpg.tar.gz\"\n",
    "\n",
    "archive_prefix = \"./resnet_v1_fp32_savedmodel_NCHW_jpg/1538686758/\"\n",
    "target_folder = \"resnet50\"\n",
    "\n",
    "if not os.path.exists(target_folder):\n",
    "    print(f\"target_folder \\\"{target_folder}\\\" does not exist.\")\n",
    "    print(f\"Downloading it from \\\"{model_url}\\\"...\")\n",
    "    response = requests.get(model_url)\n",
    "    archive = tarfile.open(fileobj=BytesIO(response.content))\n",
    "    with tempfile.TemporaryDirectory() as temp_folder:\n",
    "        print(f\"extracting in \\\"{temp_folder}\\\"...\")\n",
    "        archive.extractall(temp_folder)\n",
    "        print(f\"copyint to \\\"{target_folder}\\\"...\")\n",
    "        shutil.copytree(os.path.join(temp_folder, archive_prefix), target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model\n",
    "Register an existing trained model, add description and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599847916495
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path=\"resnet50\", # This points to the local directory to upload.\n",
    "                       model_name=\"resnet50\", # This is the name the model is registered as.\n",
    "                       tags={'area': \"Image classification\", 'type': \"classification\"},\n",
    "                       description=\"Image classification trained on Imagenet Dataset\",\n",
    "                       workspace=ws)\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model as a web service\n",
    "\n",
    "We begin by writing a score.py file that will be invoked by the web service call. The init() function is called once when the container is started so we load the model using the Tensorflow session. The run() function is called when the webservice is invoked for inferencing. After running the code below you should see a score.py file in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from azureml.contrib.services.aml_request import AMLRequest, rawhttp\n",
    "from azureml.contrib.services.aml_response import AMLResponse\n",
    "\n",
    "def init():\n",
    "    global session\n",
    "    global input_name\n",
    "    global output_name\n",
    "    \n",
    "    print(f\"getting tf.Session()...\")\n",
    "    session = tf.Session()\n",
    "    print(f\"got tf.Session()\")\n",
    "    \n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'resnet50')\n",
    "    print(f\"model_path: \\\"{model_path}\\\"\")\n",
    "    \n",
    "    model = tf.saved_model.loader.load(session, ['serve'], model_path)\n",
    "    if len(model.signature_def['serving_default'].inputs) > 1:\n",
    "        raise ValueError(\"This score.py only supports one input\")\n",
    "    input_name = [tensor.name for tensor in model.signature_def['serving_default'].inputs.values()][0]\n",
    "    output_name = [tensor.name for tensor in model.signature_def['serving_default'].outputs.values()]\n",
    "    \n",
    "\n",
    "@rawhttp\n",
    "def run(request):\n",
    "    if request.method == 'POST':\n",
    "        reqBody = request.get_data(False)\n",
    "        resp = score(reqBody)\n",
    "        return AMLResponse(resp, 200)\n",
    "    if request.method == 'GET':\n",
    "        respBody = str.encode(\"GET is not supported\")\n",
    "        return AMLResponse(respBody, 405)\n",
    "    return AMLResponse(\"bad request\", 500)\n",
    "\n",
    "def score(data):\n",
    "    print(f\"doing score()...\")\n",
    "    result = session.run(output_name, {input_name: [data]})\n",
    "    return json.dumps(result[1].tolist())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    with open(\"test_image.jpg\", 'rb') as f:\n",
    "        content = f.read()\n",
    "        print(score(content))\n",
    "    print(f\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the deployment configuration objects. We mention `score.py`, which we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599848048510
    }
   },
   "outputs": [],
   "source": [
    "# Set the web service configuration (using default here)\n",
    "from azureml.core.model import InferenceConfig\n",
    "#from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\n",
    "\n",
    "env = Environment('deploytoedgeenv')\n",
    "# Please see [Azure ML Containers repository](https://github.com/Azure/AzureML-Containers#featured-tags)\n",
    "# for open-sourced GPU base images.\n",
    "env.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "env.python.conda_dependencies = CondaDependencies.create(conda_packages=['tensorflow-gpu==1.12.0','numpy'],\n",
    "                                 pip_packages=['azureml-contrib-services', 'azureml-defaults'])\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create container image in Azure ML\n",
    "Use Azure ML to create the container image. This step will likely take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599849666607
    }
   },
   "outputs": [],
   "source": [
    "# provide name of azure contaienr image and tag \n",
    "imagename= \"tfgpuk8s\"\n",
    "imagelabel=\"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599849926532
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Builds an image in ACR.\n",
    "\n",
    "package = Model.package(ws, [model], inference_config=inference_config,image_name=imagename, image_label=imagelabel)\n",
    "package.wait_for_creation(show_output=True)\n",
    "\n",
    "print(\"ACR:\", package.get_container_registry)\n",
    "print(\"Image:\", package.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy container to Azure Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "acr_name = package.location.split(\"/\")[0]\n",
    "reg_name = acr_name.split(\".\")[0]\n",
    "subscription_id = ws.subscription_id\n",
    "\n",
    "print('acr_name: {}'.format(acr_name))\n",
    "print('subscription_id: {}'.format(subscription_id))\n",
    "\n",
    "# TODO: Derive image_location through code.\n",
    "image_location = acr_name + \"/\" + imagename + \":\" + imagelabel\n",
    "\n",
    "print('image_location: {}'.format(image_location))\n",
    "\n",
    "# Fetch username, password of ACR.\n",
    "from azure.mgmt.containerregistry import ContainerRegistryManagementClient\n",
    "from azure.mgmt import containerregistry\n",
    "\n",
    "client = ContainerRegistryManagementClient(ws._auth,subscription_id)\n",
    "result= client.registries.list_credentials(ws.resource_group, reg_name, custom_headers=None, raw=False)\n",
    "\n",
    "myusername = result.username\n",
    "mypassword = result.passwords[0].value\n",
    "\n",
    "# Do not commit your credentials to this notebook's source repository.\n",
    "# print(\"using username \\\"\" + myusername + \"\\\"\")\n",
    "# print(\"using password \\\"\" + mypassword + \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Deployment file for AS Edge or another Kubernetes cluster.\n",
    "\n",
    "Below is the template for our Kubernetes deployments. We create it within this notebook for visibility, for more complex deployments you can make it part of source control repository.\n",
    "\n",
    "We will apply `image_location` that we composed to __REGISTRY_IMAGE_LOCATION.\n",
    "\n",
    "The value for the new marco-definition, __REGISTRY_SECRET_NAME, is the result of the authentication process you need to do to be able to connect to a private container registry(docker image repository)\n",
    "\n",
    "We will do steps described in https://github.com/Azure-Samples/azure-intelligent-edge-patterns/tree/master/Research/deploying-model-on-k8s , and you are welcome to familiarize yourself with the subject at https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating kubectl on AS Edge\n",
    "\n",
    "If you created your cluster on Azure Stack Edge using aks-engine, you will have `kubectl`. You can check the version:\n",
    "\n",
    "    $ kubectl version --client\n",
    "    Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.2\",\n",
    "    GitCommit:\"f5743093fd1c663cb0cbc89748f730662345d44d\", GitTreeState:\"clean\",\n",
    "    BuildDate:\"2020-09-16T13:41:02Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n",
    "    \n",
    "For more information, visit https://docs.microsoft.com/en-us/azure/databox-online/azure-stack-edge-j-series-deploy-stateless-application-kubernetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating kubectl on a generic Kubernetes cluster\n",
    "\n",
    "You also need `kubectl`. If the following command fails, see https://kubernetes.io/docs/tasks/tools/install-kubectl/\n",
    "\n",
    "    $ kubectl version --client\n",
    "    Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.2\",\n",
    "    GitCommit:\"f5743093fd1c663cb0cbc89748f730662345d44d\", GitTreeState:\"clean\",\n",
    "    BuildDate:\"2020-09-16T13:41:02Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n",
    "\n",
    "A simple way to install it, is using `snap`:\n",
    "\n",
    "    $ snap install kubectl --classic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if you have Kubectl in your environment:\n",
    "!kubectl version --client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT You need your `kubectl` be able to access the cluster. Cluster is usually defined by a `kubeconfig` file, which contains the authentication tokens, usually located in `~/.kube/config` on the master node of your Kubernetes cluster. You need to copy its content to the machine where you installed kubectl.**\n",
    "\n",
    "Here is how you can look at your Kubernetes cluster nodes (once you copied your kubeconfig file):\n",
    "\n",
    "```\n",
    "$ kubectl get nodes\n",
    "NAME             STATUS   ROLES    AGE    VERSION\n",
    "docker-desktop   Ready    master   120d   v1.16.6-beta.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create your private registry access secret, we will call it `secret4acr2infer`.\n",
    "\n",
    "The simpliest way is to create it passing credentials in the command line like so:\n",
    "\n",
    "```\n",
    "$ kubectl create secret docker-registry secret4acr2infer --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>\n",
    "```\n",
    "\n",
    "An alternative to that, is to login using Docker and create a secret based on the `~/.docker/config.json`:\n",
    "\n",
    "```\n",
    "$ docker login -u <user id we had before> -p <what we had before> <your account it>.azurecr.io\n",
    "Login Succeeded\n",
    "$ kubectl create secret generic secret4acr2infer \\\n",
    "    --from-file=.dockerconfigjson=/home/azureuser/.docker/config.json \\\n",
    "    --type=kubernetes.io/dockerconfigjson\n",
    "secret/secret4acr2infer created\n",
    "```    \n",
    "    \n",
    "For more information, please see [Pull an Image from a Private Registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can define your other settings to normalize the parameters across the notebook\n",
    "secret4acr2infer = \"secret4acr2infer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create secret docker-registry $secret4acr2infer --docker-server=$acr_name \\\n",
    "--docker-username=$myusername --docker-password=$mypassword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deployment-k8s-template-gpu.yaml\n",
    "#\n",
    "# You can deploy this Deployment like so:\n",
    "#\n",
    "# $ kubectl create -f deployment-k8s-gpu.yaml\n",
    "#\n",
    "# \n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: my-infer\n",
    "  labels:\n",
    "    app: my-infer\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: my-infer\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: my-infer\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: my-infer\n",
    "        image: __REGISTRY_IMAGE_LOCATION\n",
    "        ports:\n",
    "        # we use only 5001, but the container exposes  EXPOSE 5001 8883 8888\n",
    "        - containerPort: 5001\n",
    "        - containerPort: 8883\n",
    "        - containerPort: 8888\n",
    "        resources:\n",
    "          limits:\n",
    "            # if you know your models minimal requirements, you can control\n",
    "            # the resource usage here. Some models may not work unless they\n",
    "            # have enough.\n",
    "            #\n",
    "            # memory: \"128Mi\" #128 MB\n",
    "            # cpu: \"200m\" # 200 millicpu (0.2 or 20% of the cpu)\n",
    "            nvidia.com/gpu:  1\n",
    "      imagePullSecrets:\n",
    "        - name: __REGISTRY_SECRET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployment_gpu.yaml file using the template and the settings earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('deployment-k8s-template-gpu.yaml')\n",
    "\n",
    "contents = file.read()\n",
    "contents = contents.replace('__REGISTRY_IMAGE_LOCATION', image_location)\n",
    "contents = contents.replace('__REGISTRY_SECRET_NAME', secret4acr2infer)\n",
    "\n",
    "with open('./deployment_gpu.yaml', 'wt', encoding='utf-8') as output_file:\n",
    "    output_file.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying .yaml to your Kubernetes cluster\n",
    "Then copy your deployment.yaml to the control box of your Kubernetes cluster(using `scp` or any ftp utility you like).\n",
    "\n",
    "At that machine, you apply the deployment to your cluster, and expose the service like so:\n",
    "    \n",
    "```    \n",
    "    $ kubectl create -f deployment_gpu.yaml\n",
    "    deployment.apps/my-infer created\n",
    "    \n",
    "    $ kubectl expose deployment my-infer --type=LoadBalancer --name=my-service-infer\n",
    "    service/my-service-infer exposed\n",
    "\n",
    "    $ kubectl get services\n",
    "    NAME               TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                        AGE\n",
    "    kubernetes         ClusterIP      10.152.183.1    <none>        443/TCP                                        7d19h\n",
    "    my-service-infer   LoadBalancer   10.152.183.61   <your ip>     5001:30056/TCP,8883:31448/TCP,8888:31236/TCP   4h28m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f deployment_gpu.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl expose deployment my-infer --type=LoadBalancer --name=my-service-infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the web service\n",
    "We test the web sevice by passing the test images content.\n",
    "\n",
    "**You need the `EXTERNAL-IP` value to change from `pending` to an actual ip address before you start using it.**\n",
    "\n",
    "If you are using your Cluster-IP, you need to be at that cluster. SSH to your master node, for example, and un the following as a stand-alone script there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "\n",
    "#downloading labels for imagenet that resnet model was trained on\n",
    "classes_entries = requests.get(\"https://raw.githubusercontent.com/Lasagne/Recipes/master/examples/resnet50/imagenet_classes.txt\").text.splitlines()\n",
    "\n",
    "test_sample = open('snowleopardgaze.jpg', 'rb').read()\n",
    "print(\"test_sample size is {}\".format(len(test_sample)))\n",
    "\n",
    "try:\n",
    "    #eg http://51.141.178.47:5001/score\n",
    "    scoring_uri = 'http://<replace with yout edge device ip address>:5001/score'\n",
    "    print(\"scoring_uri is {}\".format(scoring_uri))\n",
    "\n",
    "    # Set the content type\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    # Make the request\n",
    "    resp = requests.post(scoring_uri, test_sample, headers=headers)  \n",
    "    \n",
    "    print(\"Found a :: \" + classes_entries[int(resp.text.strip(\"[]\")) - 1] )\n",
    "except KeyError as e:\n",
    "    print(str(e))"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "vaidyas"
   }
  ],
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
