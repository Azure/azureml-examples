{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbce700",
   "metadata": {},
   "source": [
    "# Using Azure ML Pipelines to Train and Use HuggingFace models for Text Summarization Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe4d4d",
   "metadata": {},
   "source": [
    "**Learning Objectives** \n",
    "By the end of this two part tutorial, you should be able to use Azure Machine Learning (AML) to finetune Hugging Face NLP models.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00c3c3",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "In order to benefit from this tutorial, you need to have:\n",
    "- basic understanding of Machine Learning projects workflow\n",
    "- an Azure subscription. If you don't have an Azure subscription, [create a free account](https://aka.ms/AMLFree) before you begin.\n",
    "- a working AML workspace. A workspace can be created via Azure Portal, Azure CLI, or Python SDK. [Read more](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\n",
    "- a Python environmnet\n",
    "- [installed Azure Machine Learning Python SDK v2](https://github.com/Azure/azureml-examples/blob/sdk-preview/sdk/setup.sh)\n",
    "- familiarity with Hugging Face framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113b218",
   "metadata": {},
   "source": [
    "**Motivations** \n",
    "In this tutorial, we will create an AML pipeline to finetune a huggingface model in AML. Specifically, we train a light bert model on CNN News dataset to perform text summarization . We evaluate the trained model's performance on a specific domain (here Medical), then finetune the model on the medical domain, and compare the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b672aeb",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Text summarizarion is a task that tries to condense a text, usually an article, into a few sentences. The idea is to keep the key informatioanl elements of the context. Since the outout of this task is a short paragrah, evaluation should be done using text comparison methods. One widely used scoring method is [ROUGE](https://aclanthology.org/W04-1013/) score, which uses the count of common unigrams, bigrams, and subsequences between the candidate the reference texts, to evaluate the quality of the generated summary.\n",
    "\n",
    "Text summarization metrics are not standardized, therefore it is a good practice if we create a baseline to compare the results we get from our transformer models. A simple method can be using the first three sentences of the text as the summary.\n",
    "\n",
    "![](media/baseline_train_evaluate.png)\n",
    "\n",
    "In this work, we first run the baseline evaluator to calculate some reference summarization scores, then we prepare our data for finetuning, finetune a model, and evaluate the model performance against the prelabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fe006",
   "metadata": {},
   "source": [
    "### Connect to AzureML\n",
    "\n",
    "Before we dive in the code, we'll need to create an instance of MLClient to connect to Azure ML. Please provide the references to your workspace below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e510cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398f7b3",
   "metadata": {},
   "source": [
    "### Provision the required resources for this notebook\n",
    "We'll need a compute clusters for this notebook, you can use a CPU cluster or a GPU cluster. First, let's create a minimal clusters for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9856f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your prefered compute type here\n",
    "USE_GPU = True\n",
    "\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# Let's create the AML compute object with the intended parameters\n",
    "cluster_basic = AmlCompute(\n",
    "    # Name assigned to the compute cluster\n",
    "    name=\"gpu-cluster\" if USE_GPU else \"cpu-cluster\",\n",
    "    # AML Compte is AML's on-demand VM service\n",
    "    type=\"amlcompute\",\n",
    "    # VM Family: 1 x NVIDIA Tesla K80 or 14 GB RAM, 4 CPU VM\n",
    "    size=\"Standard_NC6\" if USE_GPU else \"Standard_DS3_v2\",\n",
    "    # Minimum running nodes when there is no job running\n",
    "    min_instances=0,\n",
    "    # nodes in cluster\n",
    "    max_instances=6,\n",
    "    # How many seconds will the node running after the job termination\n",
    "    idle_time_before_scale_down=600,\n",
    "    # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "    tier=\"Dedicated\",\n",
    ")\n",
    "\n",
    "# Now, we pass the object to clinet's create_or_update method\n",
    "cluster_basic = ml_client.begin_create_or_update(cluster_basic)\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {cluster_basic.name} is created, the compute size is {cluster_basic.size}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccf792",
   "metadata": {},
   "source": [
    "# 1. Preparing the Resources\n",
    "\n",
    "## 1.1. Create a Job Environment\n",
    "So far, in the requirements section, we have created a development environment on our development machine. AML needs to know what environment to use for each step of the pipeline. We can use any published docker image as is, or add or required dependencies to the image.In this example, we create a conda environment for our jobs, using a [conda yaml file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually) and add it to an Ubuntu image in Microsoft Container Registry. For more information on AML environments and Azure Container Registries, please check [AML documentaiton](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02858031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "custom_env_name = \"transformers-gpu\" if USE_GPU else \"transformers-cpu\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for transformer model training\",\n",
    "    tags={\"transformers\": \">4.11.0\"},\n",
    "    conda_file=os.path.join(\"dependencies\", \"transformers_conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04\"\n",
    "    if USE_GPU\n",
    "    else \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220218.v1\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is created, the version is {pipeline_job_env.version}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e89b69",
   "metadata": {},
   "source": [
    "## 1.2. Create or Load Components\n",
    "Now that we have our workspace, compute and input data ready, let's work on the individual steps of our pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd41a3",
   "metadata": {},
   "source": [
    "### 1.2.1 Baseline Evaluation Component\n",
    "We have implemented a baseline component below. It treats the first three sentences of each article as the summary and evaulate that agains the provided labels using ROUGE scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf83950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the CommandComponent Package\n",
    "from azure.ai.ml.entities import CommandComponent\n",
    "\n",
    "src_dir = \"src/summarization/\"\n",
    "\n",
    "\n",
    "baseline_component = CommandComponent(\n",
    "    # Name of the component\n",
    "    name=\"baseline_summarization\",\n",
    "    # Component Version, no Version and the component will be automatically versioned\n",
    "    # version=\"26\",\n",
    "    # The dictionary of the inputs. Each item is a dictionary itself.\n",
    "    inputs=dict(\n",
    "        dataset_name=dict(type=\"string\"),\n",
    "        dataset_config=dict(type=\"string\"),\n",
    "        text_column=dict(type=\"string\"),\n",
    "        summary_column=dict(type=\"string\"),\n",
    "        max_samples=dict(type=\"integer\", default=-1),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    # The environment the component job will be using\n",
    "    environment=f\"{pipeline_job_env.name}@latest\",\n",
    "    # The command that will be run in the component\n",
    "    command=\"python baseline_evaluation.py --dataset_name ${{inputs.dataset_name}} --dataset_config ${{inputs.dataset_config}} \"\n",
    "    \"--text_column ${{inputs.text_column}} --summary_column ${{inputs.summary_column}} \"\n",
    "    \"--max_samples ${{inputs.max_samples}}\",\n",
    ")\n",
    "\n",
    "baseline_component = ml_client.create_or_update(baseline_component)\n",
    "\n",
    "print(\n",
    "    f\"Component {baseline_component.name} with Version {baseline_component.version} is registered\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb5547",
   "metadata": {},
   "source": [
    "### 1.2.1 Data Preparation Component\n",
    "We can use a preprocessing step to save time on dataset download and tokenization. This is especially useful in scenarios involving Hyper Parameter Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1417909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the CommandComponent Package\n",
    "from azure.ai.ml.entities import CommandComponent\n",
    "\n",
    "src_dir = \"src/summarization/\"\n",
    "\n",
    "\n",
    "data_prep_component = CommandComponent(\n",
    "    # Name of the component\n",
    "    name=\"data_prep_summarization\",\n",
    "    # Component Version, no Version and the component will be automatically versioned\n",
    "    # version=\"26\",\n",
    "    # The dictionary of the inputs. Each item is a dictionary itself.\n",
    "    inputs=dict(\n",
    "        dataset_name=dict(type=\"string\"),\n",
    "        dataset_config=dict(type=\"string\"),\n",
    "        text_column=dict(type=\"string\"),\n",
    "        summary_column=dict(type=\"string\"),\n",
    "        max_samples=dict(type=\"integer\", default=-1),\n",
    "        max_input_length=dict(type=\"integer\", optional=True, default=512),\n",
    "        max_target_length=dict(type=\"integer\", optional=True, default=40),\n",
    "        padding=dict(type=\"string\", optional=True, default=\"max_length\"),\n",
    "        model_checkpoint=dict(type=\"string\"),\n",
    "        source_prefix=dict(type=\"string\", optional=True, default=None),\n",
    "    ),\n",
    "    # The dictionary of the outputs. Each item is a dictionary itself.\n",
    "    outputs=dict(\n",
    "        encodings=dict(type=\"path\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    # The environment the component job will be using\n",
    "    environment=f\"{pipeline_job_env.name}@latest\",\n",
    "    # The command that will be run in the component\n",
    "    command=\"python data_prep.py --dataset_name ${{inputs.dataset_name}} --dataset_config ${{inputs.dataset_config}} \"\n",
    "    \"--text_column ${{inputs.text_column}} --summary_column ${{inputs.summary_column}} \"\n",
    "    \"--max_samples ${{inputs.max_samples}} --model_checkpoint ${{inputs.model_checkpoint}} \"\n",
    "    \"[--max_input_length ${{inputs.max_input_length}}]  [--max_target_length ${{inputs.max_target_length}}] \"\n",
    "    \"[--padding ${{inputs.padding}}] [--source_prefix ${{inputs.source_prefix}}] \"\n",
    "    \"--encodings ${{outputs.encodings}}\",\n",
    ")\n",
    "\n",
    "data_prep_component = ml_client.create_or_update(data_prep_component)\n",
    "\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f2625",
   "metadata": {},
   "source": [
    "### 1.2.3 Train Component\n",
    "We have created a generic summarization script that can handle training and evaluation tasks with the right argument. The code is based on [`run_summarization.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py) in huggingface example repository. We use the same script to create two components, one for training and one for evaluation. The design choice was arbitrary as a single component with different inputs could be used for both tasks. \n",
    "\n",
    "Here both data related and training related arguments are exposed. We envoke the training action by adding `--do_train` input argument to the script. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the CommandComponent Package\n",
    "from azure.ai.ml.entities import CommandComponent\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "src_dir = \"src/summarization/\"\n",
    "\n",
    "\n",
    "train_component = CommandComponent(\n",
    "    # Name of the component\n",
    "    name=\"train_summarization\",\n",
    "    # Component Version, no Version and the component will be automatically versioned\n",
    "    # version=\"26\",\n",
    "    # The dictionary of the inputs. Each item is a dictionary itself.\n",
    "    inputs=dict(\n",
    "        dataset_name=dict(type=\"string\", optional=True),\n",
    "        dataset_config=dict(type=\"string\", optional=True),\n",
    "        text_column=dict(type=\"string\", optional=True),\n",
    "        summary_column=dict(type=\"string\", optional=True),\n",
    "        preprocessed_datasets=dict(type=\"path\", optional=True, default=None),\n",
    "        model_name=dict(type=\"string\", optional=True, default=None),\n",
    "        registered_model_name=dict(type=\"string\", optional=True, default=None),\n",
    "        max_samples=dict(type=\"integer\", default=-1),\n",
    "        model_path=dict(type=\"path\", optional=True, default=None),\n",
    "        learning_rate=dict(type=\"number\", default=2e-5),\n",
    "        num_train_epochs=dict(type=\"integer\", default=5),\n",
    "        per_device_train_batch_size=dict(type=\"integer\", default=16),\n",
    "        per_device_eval_batch_size=dict(type=\"integer\", default=16),\n",
    "        source_prefix=dict(type=\"string\", optional=True, default=None),\n",
    "    ),\n",
    "    # The dictionary of the outputs. Each item is a dictionary itself.\n",
    "    outputs=dict(\n",
    "        trained_model_path=dict(type=\"path\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    # The environment the component job will be using\n",
    "    environment=f\"{pipeline_job_env.name}@latest\",\n",
    "    distribution=dict(type=\"pytorch\", process_count_per_instance=1),  # number of gpus\n",
    "    resources=dict(instance_count=1),  # number of nodes\n",
    "    # The command that will be run in the component\n",
    "    command=\"python run_summarization.py [--dataset_name ${{inputs.dataset_name}}] [--dataset_config ${{inputs.dataset_config}}] \"\n",
    "    \"[--text_column ${{inputs.text_column}}] [--summary_column ${{inputs.summary_column}}] [--preprocessed_datasets ${{inputs.preprocessed_datasets}}] \"\n",
    "    \"--learning_rate ${{inputs.learning_rate}} --per_device_train_batch_size ${{inputs.per_device_train_batch_size}} \"\n",
    "    \"--per_device_eval_batch_size ${{inputs.per_device_eval_batch_size}} --max_samples ${{inputs.max_samples}} \"\n",
    "    \"[--model_name ${{inputs.model_name}}] [--registered_model_name ${{inputs.registered_model_name}}] --output_dir outputs \"\n",
    "    \"--num_train_epochs ${{inputs.num_train_epochs}}  --trained_model_path ${{outputs.trained_model_path}} \"\n",
    "    \"--disable_tqdm True --do_train --do_eval [--source_prefix ${{inputs.source_prefix}}] [--model_path ${{inputs.model_path}}]\",\n",
    ")\n",
    "\n",
    "train_component = ml_client.create_or_update(train_component)\n",
    "\n",
    "print(\n",
    "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefc74f",
   "metadata": {},
   "source": [
    "### 1.2.3 Evaluate Component\n",
    "for the evaluate component, only the data arguments are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba350e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the CommandComponent Package\n",
    "from azure.ai.ml.entities import CommandComponent\n",
    "\n",
    "src_dir = \"src/summarization/\"\n",
    "\n",
    "\n",
    "evaluate_component = CommandComponent(\n",
    "    # Name of the component\n",
    "    name=\"evaluate_summarization\",\n",
    "    # Component Version, no Version and the component will be automatically versioned\n",
    "    # version=\"26\",\n",
    "    # The dictionary of the inputs. Each item is a dictionary itself.\n",
    "    inputs=dict(\n",
    "        dataset_name=dict(type=\"string\", optional=True),\n",
    "        dataset_config=dict(type=\"string\", optional=True),\n",
    "        text_column=dict(type=\"string\", optional=True),\n",
    "        summary_column=dict(type=\"string\", optional=True),\n",
    "        preprocessed_datasets=dict(type=\"path\", optional=True, default=None),\n",
    "        model_name=dict(type=\"string\", optional=True, default=None),\n",
    "        max_samples=dict(type=\"integer\", default=-1),\n",
    "        model_path=dict(type=\"path\", optional=True, default=None),\n",
    "    ),\n",
    "    # The dictionary of the outputs. Each item is a dictionary itself.\n",
    "    outputs=dict(\n",
    "        trained_model_path=dict(type=\"path\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    # The environment the component job will be using\n",
    "    environment=f\"{pipeline_job_env.name}@latest\",\n",
    "    # The command that will be run in the component\n",
    "    command=\"python run_summarization.py [--dataset_name ${{inputs.dataset_name}}] [--dataset_config ${{inputs.dataset_config}}] \"\n",
    "    \"[--text_column ${{inputs.text_column}}] [--summary_column ${{inputs.summary_column}}] [--preprocessed_datasets ${{inputs.preprocessed_datasets}}] \"\n",
    "    \"[--model_name ${{inputs.model_name}}] --max_samples ${{inputs.max_samples}} --output_dir outputs \"\n",
    "    \"[--model_path ${{inputs.model_path}}] --trained_model_path ${{outputs.trained_model_path}} --do_eval\",\n",
    ")\n",
    "\n",
    "evaluate_component = ml_client.create_or_update(evaluate_component)\n",
    "\n",
    "print(\n",
    "    f\"Component {evaluate_component.name} with Version {evaluate_component.version} is registered\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586ede9",
   "metadata": {},
   "source": [
    "# 2. Train and Evaluate Hugging Face Model in AML\n",
    "\n",
    "## 2.1 Creating Azure ML Pipeline\n",
    "The created component can be used in a pipeline to be connected to other steps if required. For training, we use the [CNN Dailymail](https://huggingface.co/datasets/cnn_dailymail) for text summarization. To mimic the scenario we explained in the first section, we use [PubMed dataset](https://huggingface.co/datasets/ccdv/pubmed-summarization) for evaulation to estimate the performance of a generically trained model on medial domain task.\n",
    "\n",
    "Our training component supports distribution training, the settings should be set during the build time. Based on the compute we are using, we might be able to increase the number of GPUs per node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1216c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of nodes and number of GPUs per node\n",
    "num_nodes = 4\n",
    "num_gpus_per_node = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"summarization-example_pipeline\",\n",
    "    compute=\"gpu-cluster\" if USE_GPU else \"cpu-cluster\",\n",
    "    description=\"Text Summarization pipeline\",\n",
    ")\n",
    "def txt_summarization_pipeline(\n",
    "    model_name, max_samples=-1, num_train_epochs=1, batch_size=8, learning_rate=5e-5\n",
    "):\n",
    "    baseline_step = baseline_component(\n",
    "        dataset_name=\"ccdv/pubmed-summarization\",\n",
    "        dataset_config=\"section\",\n",
    "        text_column=\"article\",\n",
    "        summary_column=\"abstract\",\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "    baseline_step.compute = \"cpu-cluster\"\n",
    "\n",
    "    data_prep_for_training_step = data_prep_component(\n",
    "        dataset_name=\"cnn_dailymail\",\n",
    "        dataset_config=\"3.0.0\",\n",
    "        text_column=\"article\",\n",
    "        summary_column=\"highlights\",\n",
    "        max_samples=max_samples,\n",
    "        max_input_length=512,\n",
    "        max_target_length=40,\n",
    "        padding=\"max_length\",\n",
    "        model_checkpoint=model_name,\n",
    "        source_prefix=\"summarize: \",\n",
    "    )\n",
    "\n",
    "    train_step = train_component(\n",
    "        preprocessed_datasets=data_prep_for_training_step.outputs.encodings,\n",
    "        model_name=model_name,\n",
    "        max_samples=max_samples,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        source_prefix=\"summarize: \",\n",
    "    )\n",
    "\n",
    "    train_step.distribution = dict(\n",
    "        type=\"pytorch\", process_count_per_instance=num_gpus_per_node\n",
    "    )  # number of gpus\n",
    "    train_step.resources = dict(instance_count=num_nodes)  # number of nodes\n",
    "\n",
    "    data_prep_for_evaluation_step = data_prep_component(\n",
    "        dataset_name=\"ccdv/pubmed-summarization\",\n",
    "        dataset_config=\"section\",\n",
    "        text_column=\"article\",\n",
    "        summary_column=\"abstract\",\n",
    "        max_samples=max_samples,\n",
    "        max_input_length=512,\n",
    "        max_target_length=40,\n",
    "        padding=\"max_length\",\n",
    "        model_checkpoint=model_name,\n",
    "        source_prefix=\"summarize: \",\n",
    "    )\n",
    "\n",
    "    evaluate_step = evaluate_component(\n",
    "        preprocessed_datasets=data_prep_for_evaluation_step.outputs.encodings,\n",
    "        model_name=\"\",\n",
    "        model_path=train_step.outputs.trained_model_path,\n",
    "        max_samples=max_samples,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5e0fc",
   "metadata": {},
   "source": [
    "Let's now use our pipeline definition to instantiate a pipeline with the parameters we choose for our run. Let's have a quick run with a `max_samples=1000` and `num_train_epochs=2` and to validate the whole pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e766b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "model_name = \"t5-small\"\n",
    "max_samples = 1000\n",
    "num_train_epochs = 1\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "\n",
    "\n",
    "pipeline = txt_summarization_pipeline(\n",
    "    model_name=model_name,\n",
    "    max_samples=max_samples,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192bb5b",
   "metadata": {},
   "source": [
    "## 2.2. Submitting a Job to AML Workspace\n",
    "It is now time to submit the job for running in AML. This time we use `create_or_update`  on `ml_client.jobs`. Here we also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in AML studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"text-summarization-example\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    "    tags={\n",
    "        \"model_name\": model_name,\n",
    "        \"max_samples\": max_samples,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_nodes\": num_nodes,\n",
    "        \"num_gpus_per_node\":num_gpus_per_node \n",
    "    },\n",
    ")\n",
    "# get a URL for the status of the job\n",
    "webbrowser.open(returned_job.services[\"Studio\"].endpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb992690",
   "metadata": {},
   "source": [
    "Wait until the pipeline run ends, now it is time call the pipeline with more data points and a more effective number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "max_samples = 10000\n",
    "num_train_epochs = 5\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "\n",
    "\n",
    "pipeline = txt_summarization_pipeline(\n",
    "    model_name=model_name,\n",
    "    max_samples=max_samples,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"text-summarization-example\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    "    tags={\n",
    "        \"model_name\": model_name,\n",
    "        \"max_samples\": max_samples,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_nodes\": num_nodes,\n",
    "        \"num_gpus_per_node\":num_gpus_per_node \n",
    "    },\n",
    ")\n",
    "# get a URL for the status of the job\n",
    "webbrowser.open(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10d189",
   "metadata": {},
   "source": [
    "Checking the result and compare with the baseline, it seems that there is room for finetuning the model to be more domain aware. Here we use the PubMed dataset to finetune the model we trained in the previous stage, to see the effect on the evaluation metrics.\n",
    "\n",
    "![](media/baseline_train_finetune_evaluate.png)\n",
    "\n",
    "Let's use the same components but expand the pipeline to include the extra finetunig step. There will be no cost for rerunnig the previous steps, as the system will reuse the results if no change is made to the inputs. \n",
    "\n",
    "| Method  |eval_rouge1|eval_rouge2|eval_rougeL|eval_rougeLsum |\n",
    "|---------|-----------|-----------|-----------|---------------|\n",
    "|baseline | 26.99    |9.16     | 16.95    |        24.38 |\n",
    "|t5-small trained on CNN news         | 25.963    |  10.028  |  20.131   |   21.801      |\n",
    "|t5-small finetuned on CNN PubMed         | 28.399    |  11.477  |  22.814   |   24.292      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of nodes and number of GPUs per node\n",
    "num_nodes = 4\n",
    "num_gpus_per_node = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"summarization-example_pipeline\",\n",
    "    compute=\"gpu-cluster\" if USE_GPU else \"cpu-cluster\",\n",
    "    description=\"Text Summarization pipeline\",\n",
    ")\n",
    "def txt_summarization_pipeline(\n",
    "    model_name, max_samples=-1, num_train_epochs=1, batch_size=8, learning_rate=5e-5\n",
    "):\n",
    "    baseline_step = baseline_component(\n",
    "        dataset_name=\"ccdv/pubmed-summarization\",\n",
    "        dataset_config=\"section\",\n",
    "        text_column=\"article\",\n",
    "        summary_column=\"abstract\",\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "    baseline_step.compute = \"cpu-cluster\"\n",
    "\n",
    "    data_prep_for_training_step = data_prep_component(\n",
    "        dataset_name=\"cnn_dailymail\",\n",
    "        dataset_config=\"3.0.0\",\n",
    "        text_column=\"article\",\n",
    "        summary_column=\"highlights\",\n",
    "        max_samples=max_samples,\n",
    "        max_input_length=512,\n",
    "        max_target_length=40,\n",
    "        padding=\"max_length\",\n",
    "        model_checkpoint=model_name,\n",
    "        source_prefix=\"summarize: \",\n",
    "    )\n",
    "\n",
    "    train_step = train_component(\n",
    "        preprocessed_datasets=data_prep_for_training_step.outputs.encodings,\n",
    "        model_name=model_name,\n",
    "        max_samples=max_samples,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        source_prefix=\"summarize: \",\n",
    "    )\n",
    "\n",
    "    train_step.distribution= dict(type=\"pytorch\",\n",
    "            process_count_per_instance=num_gpus_per_node) # number of gpus\n",
    "    train_step.resources=dict(instance_count=num_nodes)  # number of nodes\n",
    "\n",
    "    data_prep_for_evaluation_step = data_prep_component(\n",
    "        dataset_name=\"ccdv/pubmed-summarization\",\n",
    "        dataset_config=\"section\",\n",
    "        text_column=\"article\",\n",
    "        summary_column=\"abstract\",\n",
    "        max_samples=max_samples,\n",
    "        max_input_length=512,\n",
    "        max_target_length=40,\n",
    "        padding=\"max_length\",\n",
    "        model_checkpoint=model_name,\n",
    "        source_prefix=\"summarize: \",\n",
    "    )\n",
    "\n",
    "    evaluate_step = evaluate_component(\n",
    "        preprocessed_datasets=data_prep_for_evaluation_step.outputs.encodings,\n",
    "        model_name=\"\",\n",
    "        model_path=train_step.outputs.trained_model_path,\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "\n",
    "    train_step_2 = train_component(\n",
    "        preprocessed_datasets=data_prep_for_evaluation_step.outputs.encodings,\n",
    "        registered_model_name=\"t5-small-cnn-pubmed\",\n",
    "        model_path=train_step.outputs.trained_model_path,\n",
    "        max_samples=max_samples,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        source_prefix=\"summarize: \",\n",
    "    )\n",
    "    train_step_2.distribution= dict(type=\"pytorch\",\n",
    "                process_count_per_instance=num_gpus_per_node) # number of gpus\n",
    "    train_step_2.resources=dict(instance_count=num_nodes)  # number of nodes\n",
    "    \n",
    "    # This is not effective yet\n",
    "    train_step_2.comment = \"fine tuning on medical data\"\n",
    "\n",
    "    evaluate_step_2 = evaluate_component(\n",
    "        preprocessed_datasets=data_prep_for_evaluation_step.outputs.encodings,\n",
    "        model_name=\"\",\n",
    "        model_path=train_step_2.outputs.trained_model_path,\n",
    "        max_samples=max_samples,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4513f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "max_samples = 10000\n",
    "num_train_epochs = 5\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "\n",
    "\n",
    "pipeline = txt_summarization_pipeline(\n",
    "    model_name=model_name,\n",
    "    max_samples=max_samples,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"text-summarization-example\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    "    tags={\n",
    "        \"model_name\": model_name,\n",
    "        \"max_samples\": max_samples,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_nodes\": num_nodes,\n",
    "        \"num_gpus_per_node\":num_gpus_per_node \n",
    "    },\n",
    ")\n",
    "# get a URL for the status of the job\n",
    "webbrowser.open(returned_job.services[\"Studio\"].endpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc36d22",
   "metadata": {},
   "source": [
    "# 4. Deploy the Model as an Online Endpoint\n",
    "Let's learn how to deploy your machine learning model as a web service in the Azure cloud [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=azcli). \n",
    "A typical situation for a deployed machine learning service is that you need the following resources:\n",
    "\n",
    " - The model assets (filed, metadata) that you want deployed. We have already registered these in our training component.\n",
    " - Some code to run as a service. It executes the model on a given input request. This entry script receives data submitted to a deployed web service and passes it to the model. It then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns.\n",
    "\n",
    "The two things you need to accomplish in your entry script are:\n",
    "\n",
    "- Loading your model (using a function called `init()`)\n",
    "- Running your model on input data (using a function called `run()`)\n",
    "\n",
    "such entry script is located under *./src/deployment/*\n",
    "In this implementation the `init()` function loads the model, and the run function expects the data in `json` format with the input data stored under `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfd568",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_dir = \"./src/deployment/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627be2fe",
   "metadata": {},
   "source": [
    "## 4.1 Create an Inference Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "custom_env_name = \"transformer_inference\"\n",
    "\n",
    "endpoint_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for transformer endpoints\",\n",
    "    tags={\"transformers\": \"4.17.0\", \"azureml-defaults\": \"1.39.0\"},\n",
    "    conda_file=os.path.join(\"dependencies\", \"transformers_inference_conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04\",\n",
    ")\n",
    "endpoint_env = ml_client.environments.create_or_update(endpoint_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {endpoint_env.name} is created, the version is {endpoint_env.version}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c5b09",
   "metadata": {},
   "source": [
    "## 4.2. Create a New Online Endpoint\n",
    "It is now straight forward to create an online endpoint. First, we create an endpoint by providing its description. The deployment name needs to be unique in the entire azure region, therefore, for this tutorial, we create a unique name using [`UUID`](https://en.wikipedia.org/wiki/Universally_unique_identifier#:~:text=A%20universally%20unique%20identifier%20(UUID,%2C%20for%20practical%20purposes%2C%20unique.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Creating a unique name for the endpoint\n",
    "online_endpoint_name = \"summarization-endpoint\" + str(uuid.uuid4())[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is an online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"cnn_news\",\n",
    "        \"model_type\": \"t5-small\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint)\n",
    "\n",
    "print(f\"Endpint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab33fb4",
   "metadata": {},
   "source": [
    "If you have previously created an endpoint, you can retrieve it as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(\n",
    "    f'Endpint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377c297",
   "metadata": {},
   "source": [
    "## 4.3. Deploy the Model to the Endpoint\n",
    "\n",
    "Once the endpoint is created, we deploy the model with the entry script. Each endpoint can have multiple deployments and direct traffic to these deployments can be specified using rules. Here we create a single deployment that handles 100% of the incoming traffic. We have chosen a color name for our deployment, e.g. *blue*, *green*, *red* deployments, which is totally arbitrary.\n",
    "\n",
    "You can check the *Models* page on the Azure ML Studio, to identify the latest version of your registered model. Alternatively, the code below can surface the latest version, if integer numbers are used for versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"t5-small-cnn-pubmed\"\n",
    "\n",
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")\n",
    "latest_model_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the model to deploy. Here we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "\n",
    "# create an online deployment.\n",
    "cpu_deployment = ManagedOnlineDeployment(\n",
    "    name=\"cpu\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=f\"{endpoint_env.name}:{endpoint_env.version}\",\n",
    "    code_path=deploy_dir,\n",
    "    scoring_script=\"score.py\",\n",
    "    instance_type=\"STANDARD_DS5_V2\",  #'Standard_NC12s_v3',\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "cpu_deployment = ml_client.begin_create_or_update(cpu_deployment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1a025",
   "metadata": {},
   "source": [
    "## 4.4. Test with a Sample Query\n",
    "\n",
    "With the endpoint already published, we can run inference with it.\n",
    "\n",
    "Let's create a sample request file following the design expected in the run method in the score script. The json file should have valid json format, with required string escaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e056b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\n",
    "   \"data\":\"Los Angeles (CNN) -- A medical doctor in Vancouver, British Columbia, said Thursday that California arson suspect Harry Burkhart suffered from severe mental illness in 2010, when she examined him as part of a team of doctors. Dr. Blaga Stancheva, a family physician and specialist in obstetrics, said both Burkhart and his mother, Dorothee, were her patients in Vancouver while both were applying for refugee status in Canada. \\\\\\\"I was asked to diagnose and treat Harry to support a claim explaining why he was unable to show up in a small-claims court case,\\\\\\\" Stancheva told CNN in a phone interview. She declined to cite the case or Burkhart\\\\'s role in it. Stancheva said she and other doctors including a psychiatrist diagnosed Burkhart with \\\\\\\"autism, severe anxiety, post-traumatic stress disorder and depression.\\\\\\\" The diagnosis was spelled out in a letter she wrote for the small-claims court case, Stancheva said. Stancheva, citing doctor-patient confidentiality, would not elaborate further, nor would she identify the psychiatrist involved in the diagnosis. Burkhart, a 24-year-old German national, has been charged with 37 counts of arson following a string of 52 fires in Los Angeles. The charges are in connection with arson fires at 12 locations scattered through Hollywood, West Hollywood and Sherman Oaks, according to authorities. Stancheva said the refugee applications by Burkhart and his mother were denied by the Canadian government, and she has not seen Burkhart since early March of 2010. \\\\\\\"I was shocked and dismayed at what happened in Los Angeles, and it appears he was not being treated for his depression,\\\\\\\" she said. Burkhart was in court on Wednesday for a preliminary hearing. Prosecutors said his \\\\\\\"rage against Americans,\\\\\\\" triggered by his mother\\\\'s arrest last week, motivated his \\\\\\\"campaign of terror\\\\\\\" with dozens of fires in Hollywood and nearby communities. Burkhart kept his eyes closed and remained limp during most of his hearing, requiring sheriff\\\\'s deputies to hold him up. The district attorney called his courtroom behavior \\\\\\\"very bizarre.\\\\\\\" \\\\\\\"This defendant has engaged in a protracted campaign in which he has set, the people believe, upwards of 52 arson fires in what essentially amounts to a campaign of terror against this community,\\\\\\\" Los Angeles County Deputy District Attorney Sean Carney said. \\\\\\\"The people believe he has engaged in this conduct because he has a hatred for Americans.\\\\\\\" Carney told the court Burkhart would flee the country if he was allowed out of jail on bond, but Los Angeles Superior Court Judge Upinder Kalra said he had no choice but to set bail. To go free while awaiting trial, Burkhart must post a $2.85 million bond and surrender his German passport. It was revealed that Burkhart is also under investigation for arson and fraud in relation to a fire in Neukirchen, near Frankfurt, Germany. The worst arson sprees in the city\\\\'s history began last Friday morning with a car fire in Hollywood that spread to apartments above a garage, but no new fires have happened since Burkhart was arrested Monday, Los Angeles District Attorney Steve Cooley said. No one was hurt in the fires, but property damage costs are likely to reach $3 million, authorities said. Cooley called it \\\\\\\"almost attempted murder,\\\\\\\" because people were sleeping in apartments above where Burkhart allegedly set cars on fire with incendiary devices placed under their engines. The criminal complaint filed Wednesday also alleged that the fires were \\\\\\\"caused by use of a device designed to accelerate the fire,\\\\\\\" Cooley said. \\\\\\\"If found true, the allegation could mean additional custody time for the defendant.\\\\\\\" \\\\\\\"In numerous instances, the cars were parked in carports, resulting in the fires spreading to the adjacent occupied apartment buildings,\\\\\\\" a sworn affidavit from a Los Angeles arson investigator said. \\\\\\\"The vast majority of these fires occurred late at night when the occupants of the apartment buildings were asleep.\\\\\\\" Investigator Edward Nordskog\\\\'s affidavit detailed Burkhart\\\\'s behavior a day before the fires began, when he was in a federal courtroom during extradition proceedings for his mother. \\\\\\\"While in the audience, the defendant (Burkhart) began yelling in an angry manner, \\\\'F--k all Americans.\\\\' The defendant also attempted to communicate with his mother who was in custody. Shortly thereafter, the defendant was ejected from the courtroom by Deputy U.S. Marshals,\\\\\\\" Nordskog wrote. Dorothee Burkhart was arrested a day before on an international arrest warrant issued by a district court in Frankfurt, Germany, said federal court spokesman Gunther Meilinger. The 53-year-old German woman is wanted on 16 counts of fraud and three counts of embezzlement, he said. The charges include an allegation that she failed to pay for a breast enhancement operation performed on her in 2004, Meilinger said. Most of the German charges, however, stem from phony real estate deals that Dorothee Burkhart allegedly conducted between 2000 and 2006. \\\\\\\"It is my opinion that the defendant\\\\'s criminal spree was motivated by his rage against Americans and that by setting these fires the defendant intended to harm and terrorize as many residents of the city and county of Los Angeles as possible,\\\\\\\" Nordskog wrote. A search of Burkhart\\\\'s Hollywood apartment found newspaper clippings about the Los Angeles fires and articles from Germany reporting similar car fires in Frankfurt, Germany in September, 2011, the investigator said. \\\\\\\"It is my opinion based on my experience that it is highly likely the defendant has a history of setting arson fires in Germany before he came to the United States,\\\\\\\" Nordskog wrote. Burkhart\\\\'s mother is scheduled for another extradition hearing Friday, while he is due back in court for arraignment on January 24. Meanwhile, both Burkharts are housed in a Los Angeles jail.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=f\"{deploy_dir}/sample-request.json\",\n",
    "    deployment_name=\"cpu\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb734cb",
   "metadata": {},
   "source": [
    "It is also possible to use consume the Rest endpoint directly, or test it with a UI is Azure Machine Learning Studio.\n",
    "\n",
    "![](media/endpoint.png)\n",
    "\n",
    "\n",
    "## 4.5. Delete the Online Endpoint to Release Resources\n",
    "\n",
    "The online endpoint consumes resources while running, in order to release the allocated resources, we should delete the deployment if not planning to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.delete(name=\"cpu\", endpoint_name=online_endpoint_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dpv2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b797ecb8b79f4a87fc43ac2e25b3bc6fe8aa5f3a0a3df12e3163ad31b5e9f88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
