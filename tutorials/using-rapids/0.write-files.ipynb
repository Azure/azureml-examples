{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rapids.dockerfile\n",
    "FROM rapidsai/rapidsai:0.16-cuda10.2-runtime-ubuntu18.04-py3.7\n",
    "RUN apt-get update && \\\n",
    "apt-get install -y fuse && \\\n",
    "source activate rapids && \\\n",
    "pip install azureml-mlflow && \\\n",
    "pip install azureml-dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "#\n",
    "# Copyright (c) 2019-2020, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cuml\n",
    "import mlflow\n",
    "\n",
    "from cuml import RandomForestClassifier as cuRF\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from cuml.metrics.accuracy import accuracy_score\n",
    "\n",
    "from rapids_csp_azure import RapidsCloudML, PerfTimer\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--data_dir\", type=str, help=\"location of data\")\n",
    "    parser.add_argument(\n",
    "        \"--n_estimators\", type=int, default=100, help=\"Number of trees in RF\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_depth\", type=int, default=16, help=\"Max depth of each tree\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_bins\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of bins used in split point calculation\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_features\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Number of features for best split\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--compute\",\n",
    "        type=str,\n",
    "        default=\"single-GPU\",\n",
    "        help=\"set to multi-GPU for algorithms via dask\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cv-folds\", type=int, default=5, help=\"Number of CV fold splits\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    data_dir = args.data_dir\n",
    "\n",
    "    n_estimators = args.n_estimators\n",
    "    mlflow.log_param(\"n_estimators\", np.int(args.n_estimators))\n",
    "    max_depth = args.max_depth\n",
    "    mlflow.log_param(\"max_depth\", np.int(args.max_depth))\n",
    "    n_bins = args.n_bins\n",
    "    mlflow.log_param(\"n_bins\", np.int(args.n_bins))\n",
    "    max_features = args.max_features\n",
    "    mlflow.log_param(\"max_features\", np.str(args.max_features))\n",
    "\n",
    "    print(\"\\n---->>>> cuDF version <<<<----\\n\", cudf.__version__)\n",
    "    print(\"\\n---->>>> cuML version <<<<----\\n\", cuml.__version__)\n",
    "\n",
    "    azure_ml = RapidsCloudML(\n",
    "        cloud_type=\"Azure\",\n",
    "        model_type=\"RandomForest\",\n",
    "        data_type=\"Parquet\",\n",
    "        compute_type=args.compute,\n",
    "    )\n",
    "    print(args.compute)\n",
    "\n",
    "    if args.compute == \"single-GPU\":\n",
    "        dataset, _, y_label, _ = azure_ml.load_data(\n",
    "            filename=os.path.join(data_dir, \"airline_20m.parquet\")\n",
    "        )\n",
    "    else:\n",
    "        # use parquet files from 'https://airlinedataset.blob.core.windows.net/airline-10years' for multi-GPU training\n",
    "        dataset, _, y_label, _ = azure_ml.load_data(\n",
    "            filename=os.path.join(data_dir, \"part*.parquet\"),\n",
    "            col_labels=[\n",
    "                \"Flight_Number_Reporting_Airline\",\n",
    "                \"Year\",\n",
    "                \"Quarter\",\n",
    "                \"Month\",\n",
    "                \"DayOfWeek\",\n",
    "                \"DOT_ID_Reporting_Airline\",\n",
    "                \"OriginCityMarketID\",\n",
    "                \"DestCityMarketID\",\n",
    "                \"DepTime\",\n",
    "                \"DepDelay\",\n",
    "                \"DepDel15\",\n",
    "                \"ArrDel15\",\n",
    "                \"ArrDelay\",\n",
    "                \"AirTime\",\n",
    "                \"Distance\",\n",
    "            ],\n",
    "            y_label=\"ArrDel15\",\n",
    "        )\n",
    "\n",
    "    X = dataset[dataset.columns.difference([\"ArrDelay\", y_label])]\n",
    "    y = dataset[y_label]\n",
    "    del dataset\n",
    "\n",
    "    print(\"\\n---->>>> Training using GPUs <<<<----\\n\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------\n",
    "    # cross-validation folds\n",
    "    # ----------------------------------------------------------------------------------------------------\n",
    "    accuracy_per_fold = []\n",
    "    train_time_per_fold = []\n",
    "    infer_time_per_fold = []\n",
    "    trained_model = []\n",
    "    global_best_test_accuracy = 0\n",
    "\n",
    "    model_params = {\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"max_features\": max_features,\n",
    "        \"n_bins\": n_bins,\n",
    "    }\n",
    "\n",
    "    # optional cross-validation w/ model_params['n_train_folds'] > 1\n",
    "    for i_train_fold in range(args.cv_folds):\n",
    "        print(f\"\\n CV fold {i_train_fold} of {args.cv_folds}\\n\")\n",
    "\n",
    "        # split data\n",
    "        X_train, X_test, y_train, y_test, _ = azure_ml.split_data(\n",
    "            X, y, random_state=i_train_fold\n",
    "        )\n",
    "        # train model\n",
    "        trained_model, training_time = azure_ml.train_model(\n",
    "            X_train, y_train, model_params\n",
    "        )\n",
    "\n",
    "        train_time_per_fold += [round(training_time, 4)]\n",
    "\n",
    "        # evaluate perf\n",
    "        test_accuracy, infer_time = azure_ml.evaluate_test_perf(\n",
    "            trained_model, X_test, y_test\n",
    "        )\n",
    "        accuracy_per_fold += [round(test_accuracy, 4)]\n",
    "        infer_time_per_fold += [round(infer_time, 4)]\n",
    "\n",
    "        # update best model [ assumes maximization of perf metric ]\n",
    "        if test_accuracy > global_best_test_accuracy:\n",
    "            global_best_test_accuracy = test_accuracy\n",
    "\n",
    "    mlflow.log_metric(\n",
    "        \"Total training inference time\", np.float(training_time + infer_time)\n",
    "    )\n",
    "    mlflow.log_metric(\"Accuracy\", np.float(global_best_test_accuracy))\n",
    "    print(\"\\n Accuracy             :\", global_best_test_accuracy)\n",
    "    print(\"\\n accuracy per fold    :\", accuracy_per_fold)\n",
    "    print(\"\\n train-time per fold  :\", train_time_per_fold)\n",
    "    print(\"\\n train-time all folds  :\", sum(train_time_per_fold))\n",
    "    print(\"\\n infer-time per fold  :\", infer_time_per_fold)\n",
    "    print(\"\\n infer-time all folds  :\", sum(infer_time_per_fold))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with PerfTimer() as total_script_time:\n",
    "        main()\n",
    "    print(\"Total runtime: {:.2f}\".format(total_script_time.duration))\n",
    "    mlflow.log_metric(\"Total runtime\", np.float(total_script_time.duration))\n",
    "    print(\"\\n Exiting script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rapids_csp_azure.py\n",
    "#\n",
    "# Copyright (c) 2019-2020, NVIDIA CORPORATION.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import sklearn\n",
    "from dask.distributed import Client, wait\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split as sklearn_train_test_split,\n",
    ")\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "import dask_cudf\n",
    "import pynvml\n",
    "import xgboost\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestClassifier as cumlDaskRF\n",
    "from cuml.metrics.accuracy import accuracy_score\n",
    "from cuml.preprocessing.model_selection import (\n",
    "    train_test_split as cuml_train_test_split,\n",
    ")\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask_ml.model_selection import train_test_split as dask_train_test_split\n",
    "\n",
    "default_azureml_paths = {\n",
    "    \"train_script\": \"./train_script\",\n",
    "    \"train_data\": \"./data_airline\",\n",
    "    \"output\": \"./output\",\n",
    "}\n",
    "\n",
    "\n",
    "class RapidsCloudML(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cloud_type=\"Azure\",\n",
    "        model_type=\"RandomForest\",\n",
    "        data_type=\"Parquet\",\n",
    "        compute_type=\"single-GPU\",\n",
    "        verbose_estimator=False,\n",
    "        CSP_paths=default_azureml_paths,\n",
    "    ):\n",
    "\n",
    "        self.CSP_paths = CSP_paths\n",
    "        self.cloud_type = cloud_type\n",
    "        self.model_type = model_type\n",
    "        self.data_type = data_type\n",
    "        self.compute_type = compute_type\n",
    "        self.verbose_estimator = verbose_estimator\n",
    "        self.log_to_file(\n",
    "            f\"\\n> RapidsCloudML\\n\\tCompute, Data , Model, Cloud types {self.compute_type, self.data_type, self.model_type, self.cloud_type}\"\n",
    "        )\n",
    "\n",
    "        # Setting up client for multi-GPU option\n",
    "        if \"multi\" in self.compute_type:\n",
    "            self.log_to_file(\"\\n\\tMulti-GPU selected\")\n",
    "            # This will use all GPUs on the local host by default\n",
    "            cluster = LocalCUDACluster(threads_per_worker=1)\n",
    "            self.client = Client(cluster)\n",
    "\n",
    "            # Query the client for all connected workers\n",
    "            self.workers = self.client.has_what().keys()\n",
    "            self.n_workers = len(self.workers)\n",
    "            self.log_to_file(f\"\\n\\tClient information {self.client}\")\n",
    "\n",
    "    def load_hyperparams(self, model_name=\"XGBoost\"):\n",
    "        \"\"\"\n",
    "        Selecting model paramters based on the model we select for execution.\n",
    "        Checks if there is a config file present in the path self.CSP_paths['hyperparams'] with\n",
    "        the parameters for the experiment. If not present, it returns the default parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name : string\n",
    "                     Selects which model to set the parameters for. Takes either 'XGBoost' or 'RandomForest'.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        model_params : dict\n",
    "                       Loaded model parameters (dict)\n",
    "        \"\"\"\n",
    "\n",
    "        self.log_to_file(\"\\n> Loading Hyperparameters\")\n",
    "\n",
    "        # Default parameters of the models\n",
    "        if self.model_type == \"XGBoost\":\n",
    "            # https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            model_params = {\n",
    "                \"max_depth\": 6,\n",
    "                \"num_boost_round\": 100,\n",
    "                \"learning_rate\": 0.3,\n",
    "                \"gamma\": 0.0,\n",
    "                \"lambda\": 1.0,\n",
    "                \"alpha\": 0.0,\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"random_state\": 0,\n",
    "            }\n",
    "\n",
    "        elif self.model_type == \"RandomForest\":\n",
    "            # https://docs.rapids.ai/api/cuml/stable/  -> cuml.ensemble.RandomForestClassifier\n",
    "            model_params = {\n",
    "                \"n_estimators\": 10,\n",
    "                \"max_depth\": 10,\n",
    "                \"n_bins\": 16,\n",
    "                \"max_features\": 1.0,\n",
    "                \"seed\": 0,\n",
    "            }\n",
    "\n",
    "        hyperparameters = {}\n",
    "        try:\n",
    "            with open(self.CSP_paths[\"hyperparams\"], \"r\") as file_handle:\n",
    "                hyperparameters = json.load(file_handle)\n",
    "                for key, value in hyperparameters.items():\n",
    "                    model_params[key] = value\n",
    "                pprint.pprint(model_params)\n",
    "                return model_params\n",
    "\n",
    "        except Exception as error:\n",
    "            self.log_to_file(str(error))\n",
    "            return\n",
    "\n",
    "    def load_data(\n",
    "        self, filename=\"dataset.orc\", col_labels=None, y_label=\"ArrDelayBinary\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loading the data into the object from the filename and based on the columns that we are\n",
    "        interested in. Also, generates y_label from 'ArrDelay' column to convert this into a binary\n",
    "        classification problem.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : string\n",
    "                   the path of the dataset to be loaded\n",
    "\n",
    "        col_labels : list of strings\n",
    "                     The input columns that we are interested in. None selects all the columns\n",
    "\n",
    "        y_label : string\n",
    "                  The column to perform the prediction task in.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dataset : dataframe (Pandas, cudf or dask-cudf)\n",
    "                  Ingested dataset in the format of a dataframe\n",
    "\n",
    "        col_labels : list of strings\n",
    "                     The input columns selected\n",
    "\n",
    "        y_label : string\n",
    "                  The generated y_label name for binary classification\n",
    "\n",
    "        duration : float\n",
    "                   The time it took to execute the function\n",
    "        \"\"\"\n",
    "        target_filename = filename\n",
    "        self.log_to_file(f\"\\n> Loading dataset from {target_filename}\")\n",
    "\n",
    "        with PerfTimer() as ingestion_timer:\n",
    "            if \"CPU\" in self.compute_type:\n",
    "                # CPU Reading options\n",
    "                self.log_to_file(f\"\\n\\tCPU read\")\n",
    "\n",
    "                if self.data_type == \"ORC\":\n",
    "                    with open(target_filename, mode=\"rb\") as file:\n",
    "                        dataset = pyarrow_orc.ORCFile(file).read().to_pandas()\n",
    "                elif self.data_type == \"CSV\":\n",
    "                    dataset = pd.read_csv(target_filename, names=col_labels)\n",
    "\n",
    "                elif self.data_type == \"Parquet\":\n",
    "\n",
    "                    if \"single\" in self.compute_type:\n",
    "                        dataset = pd.read_parquet(target_filename)\n",
    "\n",
    "                    elif \"multi\" in self.compute_type:\n",
    "                        self.log_to_file(f\"\\n\\tReading using dask dataframe\")\n",
    "                        dataset = dask.dataframe.read_parquet(\n",
    "                            target_filename, columns=columns\n",
    "                        )\n",
    "\n",
    "            elif \"GPU\" in self.compute_type:\n",
    "                # GPU Reading Option\n",
    "\n",
    "                self.log_to_file(f\"\\n\\tGPU read\")\n",
    "                if self.data_type == \"ORC\":\n",
    "                    dataset = cudf.read_orc(target_filename)\n",
    "\n",
    "                elif self.data_type == \"CSV\":\n",
    "                    dataset = cudf.read_csv(target_filename, names=col_labels)\n",
    "\n",
    "                elif self.data_type == \"Parquet\":\n",
    "\n",
    "                    if \"single\" in self.compute_type:\n",
    "                        dataset = cudf.read_parquet(target_filename)\n",
    "\n",
    "                    elif \"multi\" in self.compute_type:\n",
    "                        self.log_to_file(f\"\\n\\tReading using dask_cudf\")\n",
    "                        dataset = dask_cudf.read_parquet(\n",
    "                            target_filename, columns=col_labels\n",
    "                        )\n",
    "\n",
    "        # cast all columns to float32\n",
    "        for col in dataset.columns:\n",
    "            dataset[col] = dataset[col].astype(\n",
    "                np.float32\n",
    "            )  # needed for random forest\n",
    "\n",
    "        # Adding y_label column if it is not present\n",
    "        if y_label not in dataset.columns:\n",
    "            dataset[y_label] = 1.0 * (dataset[\"ArrDelay\"] > 10)\n",
    "\n",
    "        dataset[y_label] = dataset[y_label].astype(\n",
    "            np.int32\n",
    "        )  # Needed for cuml RF\n",
    "\n",
    "        dataset = dataset.fillna(\n",
    "            0.0\n",
    "        )  # Filling the null values. Needed for dask-cudf\n",
    "\n",
    "        self.log_to_file(\n",
    "            f\"\\n\\tIngestion completed in {ingestion_timer.duration}\"\n",
    "        )\n",
    "        self.log_to_file(\n",
    "            f\"\\n\\tDataset descriptors: {dataset.shape}\\n\\t{dataset.dtypes}\"\n",
    "        )\n",
    "        return dataset, col_labels, y_label, ingestion_timer.duration\n",
    "\n",
    "    def split_data(\n",
    "        self, dataset, y_label, train_size=0.8, random_state=0, shuffle=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Splitting data into train and test split, has appropriate imports for different compute modes.\n",
    "        CPU compute - Uses sklearn, we manually filter y_label column in the split call\n",
    "        GPU Compute - Single GPU uses cuml and multi GPU uses dask, both split y_label internally.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : dataframe\n",
    "                  The dataframe on which we wish to perform the split\n",
    "        y_label : string\n",
    "                  The name of the column (not the series itself)\n",
    "        train_size : float\n",
    "                     The size for the split. Takes values between 0 to 1.\n",
    "        random_state : int\n",
    "                       Useful for running reproducible splits.\n",
    "        shuffle : binary\n",
    "                  Specifies if the data must be shuffled before splitting.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        X_train : dataframe\n",
    "                  The data to be used for training. Has same type as input dataset.\n",
    "        X_test : dataframe\n",
    "                  The data to be used for testing. Has same type as input dataset.\n",
    "        y_train : dataframe\n",
    "                  The label to be used for training. Has same type as input dataset.\n",
    "        y_test : dataframe\n",
    "                  The label to be used for testing. Has same type as input dataset.\n",
    "        duration : float\n",
    "                   The time it took to perform the split\n",
    "        \"\"\"\n",
    "        self.log_to_file(\"\\n> Splitting train and test data\")\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        with PerfTimer() as split_timer:\n",
    "            if \"CPU\" in self.compute_type:\n",
    "                X_train, X_test, y_train, y_test = sklearn_train_test_split(\n",
    "                    dataset.loc[:, dataset.columns != y_label],\n",
    "                    dataset[y_label],\n",
    "                    train_size=train_size,\n",
    "                    shuffle=shuffle,\n",
    "                    random_state=random_state,\n",
    "                )\n",
    "\n",
    "            elif \"GPU\" in self.compute_type:\n",
    "                if \"single\" in self.compute_type:\n",
    "                    X_train, X_test, y_train, y_test = cuml_train_test_split(\n",
    "                        X=dataset,\n",
    "                        y=y_label,\n",
    "                        train_size=train_size,\n",
    "                        shuffle=shuffle,\n",
    "                        random_state=random_state,\n",
    "                    )\n",
    "                elif \"multi\" in self.compute_type:\n",
    "                    X_train, X_test, y_train, y_test = dask_train_test_split(\n",
    "                        dataset,\n",
    "                        y_label,\n",
    "                        train_size=train_size,\n",
    "                        shuffle=False,  # shuffle not available for dask_cudf yet\n",
    "                        random_state=random_state,\n",
    "                    )\n",
    "\n",
    "        self.log_to_file(\n",
    "            f\"\\n\\tX_train shape and type{X_train.shape} {type(X_train)}\"\n",
    "        )\n",
    "        self.log_to_file(f\"\\n\\tSplit completed in {split_timer.duration}\")\n",
    "        return X_train, X_test, y_train, y_test, split_timer.duration\n",
    "\n",
    "    def train_model(self, X_train, y_train, model_params):\n",
    "        \"\"\"\n",
    "        Trains a model with the model_params specified by calling fit_xgboost or\n",
    "        fit_random_forest depending on the model_type.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : dataframe\n",
    "                  The data for traning\n",
    "        y_train : dataframe\n",
    "                  The label to be used for training.\n",
    "        model_params : dict\n",
    "                       The model params to use for this training\n",
    "        Returns\n",
    "        ----------\n",
    "        trained_model : The object of the trained model either of XGBoost or RandomForest\n",
    "\n",
    "        training_time : float\n",
    "                        The time it took to train the model\n",
    "        \"\"\"\n",
    "        self.log_to_file(\n",
    "            f\"\\n> Training {self.model_type} estimator w/ hyper-params\"\n",
    "        )\n",
    "        training_time = 0\n",
    "\n",
    "        try:\n",
    "            if self.model_type == \"XGBoost\":\n",
    "                trained_model, training_time = self.fit_xgboost(\n",
    "                    X_train, y_train, model_params\n",
    "                )\n",
    "            elif self.model_type == \"RandomForest\":\n",
    "                trained_model, training_time = self.fit_random_forest(\n",
    "                    X_train, y_train, model_params\n",
    "                )\n",
    "        except Exception as error:\n",
    "            self.log_to_file(\"\\n\\n!error during model training: \" + str(error))\n",
    "        self.log_to_file(f\"\\n\\tFinished training in {training_time:.4f} s\")\n",
    "        return trained_model, training_time\n",
    "\n",
    "    def fit_xgboost(self, X_train, y_train, model_params):\n",
    "        \"\"\"\n",
    "        Trains a XGBoost model on X_train and y_train with model_params\n",
    "\n",
    "        Parameters and Objects returned are same as trained_model\n",
    "        \"\"\"\n",
    "        if \"GPU\" in self.compute_type:\n",
    "            model_params.update({\"tree_method\": \"gpu_hist\"})\n",
    "        else:\n",
    "            model_params.update({\"tree_method\": \"hist\"})\n",
    "\n",
    "        with PerfTimer() as train_timer:\n",
    "            if \"single\" in self.compute_type:\n",
    "                train_DMatrix = xgboost.DMatrix(data=X_train, label=y_train)\n",
    "                trained_model = xgboost.train(\n",
    "                    dtrain=train_DMatrix,\n",
    "                    params=model_params,\n",
    "                    num_boost_round=model_params[\"num_boost_round\"],\n",
    "                )\n",
    "            elif \"multi\" in self.compute_type:\n",
    "                self.log_to_file(\"\\n\\tTraining multi-GPU XGBoost\")\n",
    "                train_DMatrix = xgboost.dask.DaskDMatrix(\n",
    "                    self.client, data=X_train, label=y_train\n",
    "                )\n",
    "                trained_model = xgboost.dask.train(\n",
    "                    self.client,\n",
    "                    dtrain=train_DMatrix,\n",
    "                    params=model_params,\n",
    "                    num_boost_round=model_params[\"num_boost_round\"],\n",
    "                )\n",
    "        return trained_model, train_timer.duration\n",
    "\n",
    "    def fit_random_forest(self, X_train, y_train, model_params):\n",
    "        \"\"\"\n",
    "        Trains a RandomForest model on X_train and y_train with model_params.\n",
    "        Depending on compute_type, estimators from appropriate packages are used.\n",
    "        CPU - sklearn\n",
    "        Single-GPU - cuml\n",
    "        multi_gpu - cuml.dask\n",
    "\n",
    "        Parameters and Objects returned are same as trained_model\n",
    "        \"\"\"\n",
    "        if \"CPU\" in self.compute_type:\n",
    "            rf_model = sklearn.ensemble.RandomForestClassifier(\n",
    "                n_estimators=model_params[\"n_estimators\"],\n",
    "                max_depth=model_params[\"max_depth\"],\n",
    "                max_features=model_params[\"max_features\"],\n",
    "                n_jobs=int(self.n_workers),\n",
    "                verbose=self.verbose_estimator,\n",
    "            )\n",
    "        elif \"GPU\" in self.compute_type:\n",
    "            if \"single\" in self.compute_type:\n",
    "                rf_model = cuml.ensemble.RandomForestClassifier(\n",
    "                    n_estimators=model_params[\"n_estimators\"],\n",
    "                    max_depth=model_params[\"max_depth\"],\n",
    "                    n_bins=model_params[\"n_bins\"],\n",
    "                    max_features=model_params[\"max_features\"],\n",
    "                    verbose=self.verbose_estimator,\n",
    "                )\n",
    "            elif \"multi\" in self.compute_type:\n",
    "                self.log_to_file(\"\\n\\tFitting multi-GPU daskRF\")\n",
    "                X_train, y_train = dask_utils.persist_across_workers(\n",
    "                    self.client,\n",
    "                    [X_train.fillna(0.0), y_train.fillna(0.0)],\n",
    "                    workers=self.workers,\n",
    "                )\n",
    "                rf_model = cuml.dask.ensemble.RandomForestClassifier(\n",
    "                    n_estimators=model_params[\"n_estimators\"],\n",
    "                    max_depth=model_params[\"max_depth\"],\n",
    "                    n_bins=model_params[\"n_bins\"],\n",
    "                    max_features=model_params[\"max_features\"],\n",
    "                    verbose=self.verbose_estimator,\n",
    "                )\n",
    "        with PerfTimer() as train_timer:\n",
    "            try:\n",
    "                trained_model = rf_model.fit(X_train, y_train)\n",
    "            except Exception as error:\n",
    "                self.log_to_file(\"\\n\\n! Error during fit \" + str(error))\n",
    "        return trained_model, train_timer.duration\n",
    "\n",
    "    def evaluate_test_perf(self, trained_model, X_test, y_test, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Evaluates the model performance on the inference set. For XGBoost we need\n",
    "        to generate a DMatrix and then we can evaluate the model.\n",
    "        For Random Forest, in single GPU case, we can just call .score function.\n",
    "        And multi-GPU Random Forest needs to predict on the model and then compute\n",
    "        the accuracy score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trained_model : The object of the trained model either of XGBoost or RandomForest\n",
    "        X_test : dataframe\n",
    "                  The data for testing\n",
    "        y_test : dataframe\n",
    "                  The label to be used for testing.\n",
    "        Returns\n",
    "        ----------\n",
    "        test_accuracy : float\n",
    "                        The accuracy achieved on test set\n",
    "        duration : float\n",
    "                   The time it took to evaluate the model\n",
    "        \"\"\"\n",
    "        self.log_to_file(f\"\\n> Inferencing on test set\")\n",
    "        test_accuracy = None\n",
    "        with PerfTimer() as inference_timer:\n",
    "            try:\n",
    "                if self.model_type == \"XGBoost\":\n",
    "                    if \"multi\" in self.compute_type:\n",
    "                        test_DMatrix = xgboost.dask.DaskDMatrix(\n",
    "                            self.client, data=X_test, label=y_test\n",
    "                        )\n",
    "                        xgb_pred = xgboost.dask.predict(\n",
    "                            self.client, trained_model, test_DMatrix\n",
    "                        ).compute()\n",
    "                        xgb_pred = (xgb_pred > threshold) * 1.0\n",
    "                        test_accuracy = accuracy_score(\n",
    "                            y_test.compute(), xgb_pred\n",
    "                        )\n",
    "                    elif \"single\" in self.compute_type:\n",
    "                        test_DMatrix = xgboost.DMatrix(\n",
    "                            data=X_test, label=y_test\n",
    "                        )\n",
    "                        xgb_pred = trained_model.predict(test_DMatrix)\n",
    "                        xgb_pred = (xgb_pred > threshold) * 1.0\n",
    "                        test_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "\n",
    "                elif self.model_type == \"RandomForest\":\n",
    "                    if \"multi\" in self.compute_type:\n",
    "                        cuml_pred = trained_model.predict(X_test).compute()\n",
    "                        self.log_to_file(\"\\n\\tPrediction complete\")\n",
    "                        test_accuracy = accuracy_score(\n",
    "                            y_test.compute(), cuml_pred, convert_dtype=True\n",
    "                        )\n",
    "                    elif \"single\" in self.compute_type:\n",
    "                        test_accuracy = trained_model.score(\n",
    "                            X_test, y_test.astype(\"int32\")\n",
    "                        )\n",
    "\n",
    "            except Exception as error:\n",
    "                self.log_to_file(\"\\n\\n!error during inference: \" + str(error))\n",
    "\n",
    "        self.log_to_file(\n",
    "            f\"\\n\\tFinished inference in {inference_timer.duration:.4f} s\"\n",
    "        )\n",
    "        self.log_to_file(f\"\\n\\tTest-accuracy: {test_accuracy}\")\n",
    "        return test_accuracy, inference_timer.duration\n",
    "\n",
    "    def set_up_logging(self):\n",
    "        \"\"\"\n",
    "        Function to set up logging for the object.\n",
    "        \"\"\"\n",
    "        logging_path = self.CSP_paths[\"output\"] + \"/log.txt\"\n",
    "        logging.basicConfig(filename=logging_path, level=logging.INFO)\n",
    "\n",
    "    def log_to_file(self, text):\n",
    "        \"\"\"\n",
    "        Logs the text that comes in as input.\n",
    "        \"\"\"\n",
    "        logging.info(text)\n",
    "        print(text)\n",
    "\n",
    "\n",
    "# perf_counter = highest available timer resolution\n",
    "class PerfTimer:\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        self.duration = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.duration = time.perf_counter() - self.start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
