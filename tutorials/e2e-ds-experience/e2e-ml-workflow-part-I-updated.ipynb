{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial, you'll create an Azure ML pipeline to train a model for credit default prediction. The pipeline handles the data preparation, training and registering the trained model.  You'll then run the pipeline, deploy the model and use it.\n",
        "\n",
        "The image below shows the pipeline as you'll see it in the AzureML portal once submitted. It's a rather simple pipeline we'll use to walk you through the AzureML SDK v2.\n",
        "\n",
        "The two steps are first data preparation and second training. \n",
        "\n",
        "![Screenshot that shows the AML Pipeline](media/tutorial-pipeline-sdk/pipeline-overview.jpg \"Overview of the pipeline\")\n",
        "\n",
        "## Set up the pipeline resources\n",
        "\n",
        "The Azure ML framework can be used from CLI, Python SDK, or studio interface. In this example, you'll use the AzureML Python SDK v2 to create a pipeline. \n",
        "\n",
        "Before creating the pipeline, you'll set up the resources the pipeline will use:\n",
        "\n",
        "* The dataset for training\n",
        "* The software environment to run the pipeline\n",
        "* A compute resource to where the job will run\n",
        "\n",
        "## Connect to the workspace\n",
        "\n",
        "Before we dive in the code, you'll need to connect to your Azure ML workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# handle to the workspace\n",
        "from azure.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find your Subscription ID:\n",
        "1. In the upper right Azure Machine Learning Studio toolbar, select your workspace name.\n",
        "1. At the bottom, select **View all properties in Azure Portal**\n",
        "1. Copy the value from Azure Portal into the code.\n",
        "\n",
        ":::image type=\"content\" source=\"media/tutorial-pipeline-python-sdk/find-info.png\" alt-text=\"Screenshot shows how to find values needed for your code.\":::"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(),\n",
        "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
        "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
        "    workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is a handler to the workspace that you'll use to manage other resources and jobs.\n",
        "\n",
        "> [!IMPORTANT]\n",
        "> Creating MLClient will not connect to the workspace. The client initialization is lazy, it will wait for the first time it needs to make a call (in the notebook below, that will happen during dataset registration).\n",
        "\n",
        "## Register a dataset from an external url\n",
        "\n",
        "The data you use for training is usually in one of the locations below:\n",
        "\n",
        "* Local machine\n",
        "* Web\n",
        "* Big Data Storage services (for example, Azure Blob, Azure Data Lake Storage, SQL)\n",
        " \n",
        "Azure ML uses a `Dataset` object to register a reusable definition of data, and consume data within a pipeline. A `Dataset` object is a pointer to a data storage service and a path. In the section below, you'll consume some data from web url as one example. Datasets from other sources can be created as well."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ml.entities import Dataset\n",
        "\n",
        "web_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "\n",
        "credit_dataset = Dataset(\n",
        "    name=\"creditcard_defaults\",\n",
        "    paths=[dict(file=web_path)],\n",
        "    description=\"Dataset for credit card defaults\",\n",
        "    tags={\"source_type\": \"web\", \"source\": \"UCI ML Repo\"},\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code just created a Dataset object. The dataset is ready to be consumed as an input by the pipeline that you'll define in the next sections. In addition, you can register the dataset to your workspace so it becomes reusable across pipelines.\n",
        "\n",
        "Registering the dataset will enable you to:\n",
        "\n",
        "* Reuse and share the dataset in future pipelines\n",
        "* Use versions to track the modification to the dataset\n",
        "* Use the dataset from Azure ML designer, which is Azure ML's GUI for pipeline authoring\n",
        "\n",
        "Since this is the first time that you're making a call to the workspace, you may be asked to authenticate. Once the authentication is complete, you'll then see the dataset registration completion message."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "credit_dataset = ml_client.create_or_update(credit_dataset)\n",
        "print(\n",
        "    f\"Dataset with name {credit_dataset.name} was registered to workspace, the dataset version is {credit_dataset.version}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In future, you can fetch the same dataset from the workspace using `credit_dataset = ml_client.datasets.get(\"<DATASET NAME>\", version='<VERSION>')`.\n",
        "\n",
        "\n",
        "## Create a job environment for pipeline steps\n",
        "\n",
        "So far, you've created a development environment on the compute instance, your development machine. You'll also need an environment to use for each step of the pipeline. Each step can have its own environment, or you can use some common environments for multiple steps.\n",
        "\n",
        "In this example, you'll create a conda environment for your jobs, using a conda yaml file.\n",
        "First, create a directory to store the file in."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create the file in the dependencies directory."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - azureml-defaults==1.38.0\n",
        "    - azureml-mlflow==1.38.0\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - joblib==1.0.1\n",
        "    - xlrd==2.0.1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specification contains some usual packages, that you'll use in your pipeline (numpy, pip), together with some Azure ML specific packages (azureml-defaults, azureml-mlflow).\n",
        "\n",
        "The Azure ML packages aren't mandatory to run Azure ML jobs. However, adding these packages will let you interact with Azure ML for logging metrics and registering models, all inside the Azure ML job. You'll use them in the training script later in this tutorial.\n",
        "\n",
        "Use the *yaml* file to create and register this custom environment in your workspace:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"aml-scikit-learn\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\", \"azureml-defaults\": \"1.38.0\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the training pipeline\n",
        "\n",
        "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself, using the Azure ML Python SDK v2.\n",
        "\n",
        "Azure ML pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
        "\n",
        "* Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\n",
        "* Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n",
        "* Load that component from the pipeline code.\n",
        "* Implement the pipeline using this component inputs, outputs and parameters. @@ Not sure what this means?\n",
        "* Submit the pipeline.\n",
        "\n",
        "## Create component 1: data prep (using programmatic definition)\n",
        "\n",
        "Let's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prep.py* python file.\n",
        "\n",
        "First create a source folder for the data_prep component:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_prep_src_dir = \"./components/data_prep\"\n",
        "os.makedirs(data_prep_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script performs the simple task of splitting the data into train and test datasets. \n",
        "Azure ML mounts datasets as folders to the computes, therefore, we created an auxiliary `select_first_file` function to access the data file inside the mounted input folder.\n",
        "\n",
        "[MLFlow](https://mlflow.org/docs/latest/tracking.html) will be used to log the parameters and metrics during our pipeline run."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {data_prep_src_dir}/data_prep.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "from azureml.core import Run\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", select_first_file(args.data))\n",
        "\n",
        "    credit_df = pd.read_excel(select_first_file(args.data), header=1, index_col=0)\n",
        "\n",
        "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
        "\n",
        "    credit_train_df, credit_test_df = train_test_split(\n",
        "        credit_df,\n",
        "        test_size=args.test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "\n",
        "    credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have a script that can perform the desired task, create an Azure ML Component from it. \n",
        "\n",
        "You'll use the general purpose **CommandComponent** that can run command line actions. This command line action can directly call system commands or run a script. The inputs/outputs are specified on the command line via the `${{ ... }}` notation."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the CommandComponent Package\n",
        "from azure.ml.entities import CommandComponent\n",
        "\n",
        "# importing the CommandComponent Package\n",
        "from azure.ml.entities import Code\n",
        "\n",
        "data_prep_component = CommandComponent(\n",
        "    # Name of the component\n",
        "    name=\"Data_Preparation\",\n",
        "    # Component Version, no Version and the component will be automatically versioned\n",
        "    #     version=\"26\",\n",
        "    # The dictionary of the inputs. Each item is a dictionary itself.\n",
        "    inputs=dict(\n",
        "        data=dict(type=\"path\"),\n",
        "        test_train_ratio=dict(type=\"number\"),\n",
        "    ),\n",
        "    # The dictionary of the outputs. Each item is a dictionary itself.\n",
        "    outputs=dict(\n",
        "        train_data=dict(type=\"path\"),\n",
        "        test_data=dict(type=\"path\"),\n",
        "    ),\n",
        "    # The source folder of the component\n",
        "    code=Code(local_path=data_prep_src_dir),\n",
        "    # The environment the component job will be using\n",
        "    environment=ml_client.environments.get(\n",
        "        name=pipeline_job_env.name, version=pipeline_job_env.version\n",
        "    ),\n",
        "    # The command that will be run in the component, using ${{}} to create a command template\n",
        "    # the actual parameter values will be injected at runtime.\n",
        "    command=\"python data_prep.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \"\n",
        "    \"--train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \",\n",
        ")\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "data_prep_component = ml_client.create_or_update(data_prep_component)\n",
        "\n",
        "print(\n",
        "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create component 2: training (using yaml definition)\n",
        "\n",
        "The second component that you'll create will consume the training and test data, train a tree based model and return the output model. You'll use Azure ML logging capabilities to record and visualize the learning progress.\n",
        "\n",
        "You used the `CommandComponent` class to create your first component. This time you'll use the yaml definition to define the second component. Each method has its own advantages. A yaml definition can actually be checked-in along the code, and would provide a readable history tracking. The programmatic method using `CommandComponent` can be easier with built-in class documentation and code completion.\n",
        "\n",
        "\n",
        "Create the directory for this component:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "train_src_dir = \"./components/train\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training script in the directory:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "import argparse\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from azureml.core.model import Model\n",
        "from azureml.core import Run\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "# This line creates a handles to the current run. It is used for model registration\n",
        "run = Run.get_context()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_train = train_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = train_df.values\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    # Extracting the label column\n",
        "    y_test = test_df.pop(\"default payment next month\")\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_test = test_df.values\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    clf = GradientBoostingClassifier(\n",
        "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # setting the full path of the model file\n",
        "    model_file = os.path.join(args.model, \"model.pkl\")\n",
        "    with open(model_file, \"wb\") as mf:\n",
        "        joblib.dump(clf, mf)\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    model = Model.register(\n",
        "        run.experiment.workspace,\n",
        "        model_name=args.registered_model_name,\n",
        "        model_path=model_file,\n",
        "        tags={\"type\": \"sklearn.GradientBoostingClassifier\"},\n",
        "        description=\"Model created in Azure ML on credit card defaults dataset\",\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in this training script, once the model is trained, the model file is saved and registered to the workspace. Now you can use the registered model in inferencing endpoints.\n",
        "\n",
        "\n",
        "For the environment of this step, you'll use one of the built-in (curated) Azure ML environments. The tag `azureml`, tells the system to use look for the name in curated environments.\n",
        "\n",
        "First, create the *yaml* file describing the component:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "# <component>\n",
        "name: TrainCreditDefaultsModel\n",
        "display_name: Train Credit Defaults Model\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: path\n",
        "  test_data: \n",
        "    type: path\n",
        "  learning_rate:\n",
        "    type: number     \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: path\n",
        "code:\n",
        "  local_path: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curate environment\n",
        "  azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\n",
        "command: >-\n",
        "  python train.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --learning_rate ${{inputs.learning_rate}}\n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "# </component>\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create and register the component:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the Component Package\n",
        "from azure.ml.entities import Component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "train_component = Component.load(path=os.path.join(train_src_dir, \"train.yml\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we register the component to the workspace\n",
        "train_component = ml_client.create_or_update(train_component)\n",
        "\n",
        "# Create (register) the component in your workspace\n",
        "print(\n",
        "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the pipeline from components\n",
        "\n",
        "Now that both your components are defined and registered, you can start implementing the pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ml import dsl, MLClient\n",
        "from azure.ml.dsl import Pipeline\n",
        "from azure.ml.entities import Component as ComponentEntity, Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Let's load the data-prep registered component from the workspace\n",
        "data_prep_func = dsl.load_component(\n",
        "    client=ml_client,\n",
        "    name=data_prep_component.name,\n",
        "    version=data_prep_component.version,\n",
        ")\n",
        "\n",
        "# Let's load the train registered component from the workspace\n",
        "train_func = dsl.load_component(\n",
        "    client=ml_client,\n",
        "    name=train_component.name,\n",
        "    version=train_component.version,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you'll use *input data*, *split ratio* and *registered model name* as input variables. Then call the components and connect them via their inputs /outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\n",
        "\n",
        "In the code below, replace `<CPU-CLUSTER-NAME>` with the name you used when you created a compute cluster in the [Quickstart: Create workspace resources you need to get started with Azure Machine Learning](quickstart-create-resources.md)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "@dsl.pipeline(\n",
        "    compute=\"<CPU-CLUSTER-NAME>\",\n",
        "    description=\"E2E data_perp-train pipeline\",\n",
        ")\n",
        "def credit_defaults_pipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    # using data_prep_function like a python call with its own inputs\n",
        "    data_prep_job = data_prep_func(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # using train_func like a python call with its own inputs\n",
        "    train_job = train_func(\n",
        "        train_data=data_prep_job.outputs.train_data, # note: using outputs from previous step\n",
        "        test_data=data_prep_job.outputs.test_data, # note: using outputs from previous step\n",
        "        learning_rate=pipeline_job_learning_rate, # note: using a pipeline input as parameter\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dict of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use your pipeline definition to instantiate a pipeline with your dataset, split rate of choice and the name you picked for your model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"credit_defaults_model\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = credit_defaults_pipeline(\n",
        "    pipeline_job_data_input=credit_dataset,\n",
        "    pipeline_job_test_train_ratio=0.2,\n",
        "    pipeline_job_learning_rate=0.25,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit the job \n",
        "\n",
        "It's now time to submit the job to run in Azure ML. This time you'll use `create_or_update`  on `ml_client.jobs`.\n",
        "\n",
        "Here you'll also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure ML studio.\n",
        "\n",
        "Once completed, the pipeline will register a model in your workspace as a result of training."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    \n",
        "    # Project's name\n",
        "    experiment_name=\"e2e_registered_components\",\n",
        "    \n",
        "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
        "    continue_run_on_step_failure=True,\n",
        ")\n",
        "# get a URL for the status of the job\n",
        "returned_job.services[\"Studio\"].endpoint"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can track the progress of your pipeline, by using the link generated in the cell above.\n",
        "\n",
        "When you select on each component, you'll see more information about the results of that component. \n",
        "There are two important parts to look for at this stage:\n",
        "* `Outputs+logs` > `user_logs` > `std_log.txt`\n",
        "This section shows the script run sdtout."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        ":::image type=\"content\" source=\"media/tutorial-pipeline-python-sdk/user-logs.jpg\" alt-text=\"Screenshot of std_log.txt\":::\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `Outputs+logs` > `Metric`\n",
        "This section shows different logged metrics. In this example. mlflow `autologging`, has automatically logged the training metrics."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        ":::image type=\"content\" source=\"media/tutorial-pipeline-python-sdk/metrics.jpg\" alt-text=\"Screenshot shows logged metrics.txt\":::"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy the model as an online endpoint\n",
        "\n",
        "Now deploy your machine learning model as a web service in the Azure cloud.\n",
        "\n",
        "To deploy a machine learning service, you'll usually need:\n",
        "\n",
        "* The model assets (filed, metadata) that you want to deploy. You've already registered these assets in your training component.\n",
        "* Some code to run as a service. The code executes the model on a given input request. This entry script receives data submitted to a deployed web service and passes it to the model, then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns.\n",
        "\n",
        "## Create an inference script\n",
        "\n",
        "The two things you need to accomplish in your inference script are:\n",
        "\n",
        "* Load your model (using a function called `init()`)\n",
        "* Run your model on input data (using a function called `run()`)\n",
        "\n",
        "In the following implementation the `init()` function loads the model, and the run function expects the data in `json` format with the input data stored under `data`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "deploy_dir = \"./deploy\"\n",
        "os.makedirs(deploy_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {deploy_dir}/score.py\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "import numpy\n",
        "import joblib\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"model.pkl\")\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "    logging.info(\"Init complete\")\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
        "    In the example we extract the data from the json input and call the scikit-learn model's predict()\n",
        "    method and return the result back\n",
        "    \"\"\"\n",
        "    logging.info(\"Request received\")\n",
        "    data = json.loads(raw_data)[\"data\"]\n",
        "    data = numpy.array(data)\n",
        "    result = model.predict(data)\n",
        "    logging.info(\"Request processed\")\n",
        "    return result.tolist()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a new online endpoint\n",
        "\n",
        "Now that you have a registered model and an inference script, it's time to create your online endpoint. The endpoint name needs to be unique in the entire Azure region. For this tutorial, you'll create a unique name using [`UUID`](https://en.wikipedia.org/wiki/Universally_unique_identifier#:~:text=A%20universally%20unique%20identifier%20(UUID,%2C%20for%20practical%20purposes%2C%20unique.)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Creating a unique name for the endpoint\n",
        "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        ")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\n",
        "        \"training_dataset\": \"credit_defaults\",\n",
        "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
        "    },\n",
        ")\n",
        "\n",
        "endpoint = ml_client.begin_create_or_update(endpoint)\n",
        "\n",
        "print(f\"Endpint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've created an endpoint, you can retrieve it as below:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = ml_client.online_endpoints.get(name = online_endpoint_name)\n",
        "\n",
        "print(f\"Endpint \\\"{endpoint.name}\\\" with provisioning state \\\"{endpoint.provisioning_state}\\\" is retrieved\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy the model to the endpoint\n",
        "\n",
        "Once the endpoint is created, deploy the model with the entry script. Each endpoint can have multiple deployments and direct traffic to these deployments can be specified using rules. Here you'll create a single deployment that handles 100% of the incoming traffic. We have chosen a color name for the deployment, for example, *blue*, *green*, *red* deployments, which is arbitrary.\n",
        "\n",
        "You can check the *Models* page on the Azure ML studio, to identify the latest version of your registered model. Alternatively, the code below will retrieve the latest version number for you to use."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's pick the latest version of the model\n",
        "latest_model_version = max(\n",
        "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy the latest version of the model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# picking the model to deploy. Here we use the latest version of our registered model\n",
        "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
        "\n",
        "\n",
        "#create an online deployment.\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name='blue',\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\",\n",
        "    code_local_path=deploy_dir,\n",
        "    scoring_script=\"score.py\",\n",
        "    instance_type='Standard_DS3_v2',\n",
        "    instance_count=1)\n",
        "\n",
        "blue_deployment = ml_client.begin_create_or_update(blue_deployment)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with a sample query\n",
        "\n",
        "Now that the model is deployed to the endpoint, you can run inference with it.\n",
        "\n",
        "Create a sample request file following the design expected in the run method in the score script."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {deploy_dir}/sample-request.json\n",
        "{\"data\": [\n",
        "    [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0], \n",
        "    [10,9,8,7,6,5,4,3,2,1, 10,9,8,7,6,5,4,3,2,1,10,9,8]\n",
        "]}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the blue deployment with some sample data\n",
        "ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    request_file=\"./deploy/sample-request.json\",\n",
        "    deployment_name='blue'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "Python"
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean up resources\n",
        "\n",
        "If you're not going to use the endpoint, delete it to stop using the resource.  Make sure no other deployments are using an endpoint before you delete it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "python "
          ],
          "id": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "Learn more about [Azure ML logging](https://github.com/Azure/azureml-examples/blob/sdk-preview/notebooks/mlflow/mlflow-v1-comparison.ipynb)."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}