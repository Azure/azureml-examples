{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbce700",
   "metadata": {},
   "source": [
    "# Using Azure ML Pipelines to Productionize E2E ML Workflows: Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe4d4d",
   "metadata": {},
   "source": [
    "**Learning Objectives** - By the end of this two part tutorial, you should be able to use Azure Machine Learning (Azure ML) to productionize your ML project.\n",
    "\n",
    "This means you will be able to leverage the AzureML Python SDK to:\n",
    "\n",
    "- connect to your Azure ML workspace\n",
    "- create Azure ML Datasets\n",
    "- create reusable Azure ML Components\n",
    "- create, validate and run Azure ML pipelines\n",
    "- deploy the newly-trained model as an endpoint\n",
    "- call the Azure ML endpoint for inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113b218",
   "metadata": {},
   "source": [
    "**Motivations** - This tutorial is intended to introduce Azure ML to data scientists who want to scale up or publish their ML projects. By completing a familiar end-to-end project, which starts by loading the data and ends by creating and calling an online inference endpoint, the user should become familiar with the core concepts of Azure ML and their most common usage. Each step of this tutorial can be modified or performed in other ways that might have security or scalability advantages. We will cover some of those in the Part II of this tutorial, however, we suggest the reader use the provide links in each section to learn more on each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00c3c3",
   "metadata": {},
   "source": [
    "**Requirements** - In order to benefit from this tutorial, you need to have:\n",
    "- basic understanding of Machine Learning projects workflow\n",
    "- an Azure subscription. If you don't have an Azure subscription, [create a free account](https://aka.ms/AMLFree) before you begin.\n",
    "- a working Azure ML workspace. A workspace can be created via Azure Portal, Azure CLI, or Python SDK. [Read more](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\n",
    "- a Python environmnet\n",
    "- installed Azure Machine Learning Python SDK v2\n",
    "\n",
    "    ```python\n",
    "    pip install azure-ml==0.0.61212840 --extra-index-url https://azuremlsdktestpypi.azureedge.net/sdk-cli-v2\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28babe15",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this tutorial, we will create an Azure ML pipeline to train a model for credit default prediction. The pipeline handles the data preparation, training and registering the trained model.\n",
    "\n",
    "The image below shows the pipeline as you will see it in the AzureML portal once submitted. It is a rather simple pipeline we'll use to walk you through the new AzureML SDK.\n",
    "\n",
    "The two steps are first data preparation and second training. The pipeline will **register a model** so that we can deploy the model as an endpoint and evaluate the inferencing through that endpoint by invoking sample queries.\n",
    "\n",
    "<div>\n",
    "<img src=\"media/pipeline-overview.jpg\" width=\"400\"/img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab7fbf",
   "metadata": {},
   "source": [
    "# 2. Set Up the Pipeline Resources\n",
    "\n",
    "Azure ML Framework can be used from CLI, Python SDK, or GUI. In this example we will use the AzureML Python SDK to create a pipeline. This requires importing specific python `entities` (ex: dataset, component, pipeline) and assemble in a python script to build a full pipeline.\n",
    "\n",
    "Note: In this tutorial, we try to postpone the imports of the required packages to the sections directly using those packages. This way, you will better understand the role of each of the `entities` in the SDK. If you decide to run the cells out of the presented order, make sure you have imported the packages first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201c72d",
   "metadata": {},
   "source": [
    "## 2.1. Connect to the Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle to the workspace\n",
    "from azure.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e8f61",
   "metadata": {},
   "source": [
    "In the next cell, we enter the *Subscription ID*, *Resource Group* name and *Workspace* name. These parameters can be obtained during workspace creation, or by visiting the Azure ML Studio. The result is a handler to the workspace that we can use to manage other resources and jobs.\n",
    "\n",
    "Note: We use the default *interactive authentication* for this tutorial which will require you to manually confirm your connection in a browser. More advanced connection methods can be found here [sdkv1link](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.authentication?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e25c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    InteractiveBrowserCredential(),\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32afc68",
   "metadata": {},
   "source": [
    "**Important**: Creating `MLClient` will not connect to the workspace. The client initialization is _lazy_, it will wait for the first time we'll need to make a call (in the notebook below, that will happen during dataset registration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402c1c7",
   "metadata": {},
   "source": [
    "## 2.2. Register a dataset from an external url\n",
    "\n",
    "The data we use for our training is usually in one of the locations below:\n",
    "- Local Machine\n",
    "- Web\n",
    "- Big Data Storage services (e.g. Azure Blob, Azure Data Lake Storage, SQL... )\n",
    "\n",
    "Azure ML uses a Dataset object to register a reusable definition of data, and consume data within a pipeline. A Data asset object is a pointer to a data location. In the section below, we consume some data from web url as one example. Data Assets from other sources can be created as well. \n",
    "\n",
    "For more information on various sources of data and their consumption, please check [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Data\n",
    "from azure.ml._constants import AssetTypes\n",
    "web_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "\n",
    "credit_data = Data(\n",
    "    name=\"creditcard_defaults\",\n",
    "    path=web_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset for credit card defaults\",\n",
    "    tags={\"source_type\": \"web\", \"source\": \"UCI ML Repo\"},\n",
    "    version='1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e2d7a",
   "metadata": {},
   "source": [
    "We just created a `Data Asset`, it is ready to be consumed as an input by the pipeline that we will define in the next sections. In addition, we can also register the dataset to our workspace so it becomes reusable across pipelines.\n",
    "\n",
    "This will enable us to:\n",
    "- reuse and share the dataset in future pipelines\n",
    "- use versions to track the modification to the dataset\n",
    "- use the dataset from Azure ML designer which is Azure ML's GUI for pipeline authoring\n",
    "\n",
    "Since this is the first time in this tutorial that we are making a call to the workspace, running the next cell should direct you to Azure ML's web authentication page. Please login with your Azure credentials. Once you are logged in, you should receive a message that informs you that the authentication is complete and you can close the authentication window. You should then see the dataset registration completion below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = ml_client.data.create_or_update(credit_data)\n",
    "print(\n",
    "    f\"Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b4cb1",
   "metadata": {},
   "source": [
    "In future, you can fetch the same dataset from the workspace using:\n",
    "\n",
    "```python\n",
    "credit_dataset = ml_client.data.get(\"<DATA ASSET NAME>\", version='<VERSION>')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323154d5",
   "metadata": {},
   "source": [
    "## 2.3. Create a Compute Resource to run our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b6b5a",
   "metadata": {},
   "source": [
    "Each step of an Azure ML pipelines can use a different compute resource for running the specific job of that step. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark on HDInsight. \n",
    "\n",
    "In this section, we provision a Linux compute cluster. You can check [here](https://azure.microsoft.com/en-ca/pricing/details/machine-learning/) for a full list on VM sizes and prices.\n",
    "\n",
    "For this tutorial we only need a basic cluster, let's pick a *Standard_DS3_v2* model with 2 vCPU cores, 7 GB RAM and create an Azure ML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "# Let's create the Azure ML compute object with the intended parameters\n",
    "cluster_basic = AmlCompute(\n",
    "    # Name assigned to the compute cluster\n",
    "    name=\"cpu-cluster\",\n",
    "    # Azure ML Compute is the on-demand VM service\n",
    "    type=\"amlcompute\",\n",
    "    # VM Family\n",
    "    size=\"Standard_DS3_v2\",\n",
    "    # Minimum running nodes when there is no job running\n",
    "    min_instances=0,\n",
    "    # nodes in cluster\n",
    "    max_instances=2,\n",
    "    # How many seconds will the node running after the job termination\n",
    "    idle_time_before_scale_down=180,\n",
    "    # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "    tier=\"Dedicated\",\n",
    ")\n",
    "\n",
    "# # Now, we pass the object to clinet's create_or_update method\n",
    "cluster_basic = ml_client.begin_create_or_update(cluster_basic)\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {cluster_basic.name} is created, the compute size is {cluster_basic.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccf792",
   "metadata": {},
   "source": [
    "## 2.4. Create a Job Environment for pipeline steps\n",
    "\n",
    "So far, in the requirements section, we have created a development environment on our development machine. Azure ML needs to know what environment to use for each step of the pipeline. Each step can have its own environment, or you can use some common environments for multiple steps.\n",
    "\n",
    "An environment will be built using any published docker image as-is, or a Dockerfile, and add required dependencies no top of it.\n",
    "\n",
    "In our example, we create a conda environment for our jobs, using a [conda yaml file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually) and add it to an Ubuntu image in Microsoft Container Registry. For more information on Azure ML environments and Azure Container Registries, please check [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8892ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d12731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - azureml-defaults==1.38.0\n",
    "    - azureml-mlflow==1.38.0\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - joblib==1.0.1\n",
    "    - xlrd==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2959d0",
   "metadata": {},
   "source": [
    "Here we added some usual packages we use in our pipeline (numpy, pip) together with some Azure ML specific packages (`azureml-defaults`, `azureml-mlflow`).\n",
    "    \n",
    "These Azure ML packages are not mandatory to run Azure ML jobs. However, adding those will let us interact with Azure ML for logging metrics and registering models, all inside the Azure ML job (see training script for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ed22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\", \"azureml-defaults\": \"1.38.0\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec44c39",
   "metadata": {},
   "source": [
    "# 3. Create the Training Pipeline\n",
    "\n",
    "Now that we've provisioned the assets required to run our pipeline, we'll build the pipeline itself using the Azure ML Python SDK.\n",
    "\n",
    "Azure ML pipelines are reusable ML workflows that usually consist of several components. Azure ML defines these components in yaml files. The typical life of a component will consist in:\n",
    "- writing directly the yaml specification of the component or create it programmatically using `ComponentMethod`,\n",
    "- optionally register this component with a name and version in your workspace to make it reusable and shareable,\n",
    "- load that component from the pipeline code\n",
    "- implement the pipeline using this component inputs, outputs and parameters\n",
    "- submit the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e89b69",
   "metadata": {},
   "source": [
    "## 3.1. Create or Load Components\n",
    "\n",
    "### 3.1.1 Data Preparation Component (using programmatic definition)\n",
    "\n",
    "Let's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prep.py* python file.\n",
    "\n",
    "Let's first create a source folder for the data_prep component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1cb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_prep_src_dir = \"./components/data_prep\"\n",
    "os.makedirs(data_prep_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e39f3",
   "metadata": {},
   "source": [
    "This script performs the simple task of splitting the data into train and test datasets. \n",
    "\n",
    "[MLFlow](https://mlflow.org/docs/latest/tracking.html) can be used to log the parameters and metrics during our pipeline run. A detailed guide on Azure ML logging is available [here](https://github.com/Azure/azureml-examples/blob/sdk-preview/notebooks/mlflow/mlflow-v1-comparison.ipynb). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e23e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {data_prep_src_dir}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "\n",
    "    credit_df = pd.read_excel(args.data, header=1, index_col=0)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
    "\n",
    "    credit_train_df, credit_test_df = train_test_split(\n",
    "        credit_df,\n",
    "        test_size=args.test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
    "    credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
    "\n",
    "    credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137cba9",
   "metadata": {},
   "source": [
    "Now that we have a script that can perform the desired task, we can create an Azure ML Component from it. Azure ML support various types of components for performing ML tasks, such as running scripts, data transfer, etc.\n",
    "\n",
    "Here we use the general purpose **CommandComponent** that can run command line actions. This command line action can be directly calling system commands or running a script. The inputs/outputs are accessible in the command via the `${{ ... }}` notation.\n",
    "\n",
    "A component can be created by calling the component instantiators, or directly writing the defining yaml file. For this tutorial we will use `yaml` definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {data_prep_src_dir}/data_prep.yml\n",
    "# <component>\n",
    "name: data_prep_credit_defaults\n",
    "display_name: Data prep for credit defaults Model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  data: \n",
    "    type: uri_folder\n",
    "  test_train_ratio:\n",
    "    type: number     \n",
    "outputs:\n",
    "  train_data:\n",
    "    type: uri_folder\n",
    "  test_data:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:aml-scikit-learn:1.0.0\n",
    "command: >-\n",
    "  python data_prep.py \n",
    "  --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}}\n",
    "  --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}}\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483cd784",
   "metadata": {},
   "source": [
    "Once the `yaml` file and the script are ready, we can create our component using `load_component()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Component Package\n",
    "from azure.ml.entities import load_component\n",
    "\n",
    "# Loading the component from the yml file\n",
    "data_prep_component = load_component(yaml_file=os.path.join(data_prep_src_dir, \"data_prep.yml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7328d67",
   "metadata": {},
   "source": [
    "We optionally register the component in the workspace for future re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_component = ml_client.create_or_update(data_prep_component)\n",
    "\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc8d31",
   "metadata": {},
   "source": [
    "### 3.1.2. Training Component\n",
    "\n",
    "The second component that we will create will consume the training and test data, train a tree based model and then returns the output model. We use Azure ML logging capabilities to record and visualize the learning progress.\n",
    "\n",
    "Once the model is trained, the model file is saved and registered to the workspace. This will allow us to use the registered model in inferencing endpoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_src_dir = \"./components/train\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c84dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {train_src_dir}/train.py\n",
    "import argparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Run\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# Start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# This line creates a handles to the current run. It is used for model registration\n",
    "run = Run.get_context()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
    "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_train = train_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_train = train_df.values\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_test = test_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_test = test_df.values\n",
    "\n",
    "    print(f\"Training with data of shape {X_train.shape}\")\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # setting the full path of the model file\n",
    "    model_file = os.path.join(args.model, \"model.pkl\")\n",
    "    with open(model_file, \"wb\") as mf:\n",
    "        joblib.dump(clf, mf)\n",
    "\n",
    "    # Registering the model to the workspace\n",
    "    model = Model.register(\n",
    "        run.experiment.workspace,\n",
    "        model_name=args.registered_model_name,\n",
    "        model_path=model_file,\n",
    "        tags={\"type\": \"sklearn.GradientBoostingClassifier\"},\n",
    "        description=\"Model created in Azure ML on credit card defaults dataset\",\n",
    "    )\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca90aa",
   "metadata": {},
   "source": [
    "For the environment of this step, we can use one of the built-in Azure ML environments. The tag `azureml`, tells the system to use look for the name in the previously built environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28074d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {train_src_dir}/train.yml\n",
    "# <component>\n",
    "name: train_credit_defaults_model\n",
    "display_name: Train Credit Defaults Model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "  test_data: \n",
    "    type: uri_folder\n",
    "  learning_rate:\n",
    "    type: number     \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data}} \n",
    "  --test_data ${{inputs.test_data}} \n",
    "  --learning_rate ${{inputs.learning_rate}}\n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Component Package\n",
    "from azure.ml.entities import load_component\n",
    "\n",
    "# Loading the component from the yml file\n",
    "train_component = load_component(yaml_file=os.path.join(train_src_dir, \"train.yml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece603a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we register the component to the workspace\n",
    "train_component = ml_client.create_or_update(train_component)\n",
    "\n",
    "print(\n",
    "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b490a70",
   "metadata": {},
   "source": [
    "## 3.2. Creating the Pipeline from registered Components\n",
    "\n",
    "Now that both our components are defined and registered, we can start implementing the pipeline. This consists in using a specific python syntax based on our  *dsl functions*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a785f",
   "metadata": {},
   "source": [
    "The python functions returned by `load_component()` work as any regular python function that we'll use within a pipeline to call each step.\n",
    "\n",
    "To code the pipeline, we use a specific `@dsl.pipeline` decorator that identifies the Azure ML pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a python function, pipelines can have inputs, you can then create multiple instances of a single pipeline with different inputs.\n",
    "\n",
    "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs /outputs identifiers. The outputs of each step can be accessed via the `.outputs` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e808dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl, Input, Output\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "@dsl.pipeline(\n",
    "    compute=\"cpu-cluster\",\n",
    "    description=\"E2E data_perp-train pipeline\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_on_step_failure=True,\n",
    ")\n",
    "def credit_defaults_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_test_train_ratio,\n",
    "    pipeline_job_learning_rate,\n",
    "    pipeline_job_registered_model_name,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(\n",
    "        data=pipeline_job_data_input,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # using train_func like a python call with its own inputs\n",
    "    train_job = train_component(\n",
    "        train_data=data_prep_job.outputs.train_data, # note: using outputs from previous step\n",
    "        test_data=data_prep_job.outputs.test_data, # note: using outputs from previous step\n",
    "        learning_rate=pipeline_job_learning_rate, # note: using a pipeline input as parameter\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dict of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003bcf8",
   "metadata": {},
   "source": [
    "Let's now use our pipeline definition to instantiate a pipeline with our dataset, split rate of choice and the name we picked for our model.\n",
    "\n",
    "Currently, `dsl` pipelines do not supprt `Data Assets` as input, so, we temporarily use the path to data, instead of the the registered data. This will be upgraded once the issue is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"credit_defaults_model\"\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = credit_defaults_pipeline(\n",
    "    # pipeline_job_data_input=credit_data,\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=web_path),\n",
    "    pipeline_job_test_train_ratio=0.2,\n",
    "    pipeline_job_learning_rate=0.25,\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192bb5b",
   "metadata": {},
   "source": [
    "## 3.3. Submitting a Job to Azure ML Workspace\n",
    "It is now time to submit the job for running in Azure ML. This time we use `create_or_update`  on `ml_client.jobs`.\n",
    "\n",
    "Here we also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure ML studio.\n",
    "\n",
    "Once completed, the pipeline will have registered a model in your workspace as a result of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    \n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_registered_components\",\n",
    ")\n",
    "# get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa31f1",
   "metadata": {},
   "source": [
    "You can track the progress of your pipeline, by using the link generated in the cell above.\n",
    "Clicking on each component, will reveal more information on that one. \n",
    "There are two important parts to look for at this stage:\n",
    "- `Outputs+logs` > `user_logs` > `std_log.txt`\n",
    "This section shows the script run sdtout\n",
    "<div>\n",
    "<img src=\"media/user-logs.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "- `Outputs+logs` > `Metric`\n",
    "This section shows different logged metrics. In this example. mlflow `autologging`, has automatically logged the training metrics.\n",
    "\n",
    "<div>\n",
    "<img src=\"media/metrics.jpg\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8b036",
   "metadata": {},
   "source": [
    "# 4. Deploy the Model as an Online Endpoint\n",
    "Let's learn how to deploy your machine learning model as a web service in the Azure cloud [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=azcli). \n",
    "A typical situation for a deployed machine learning service is that you need the following resources:\n",
    "\n",
    " - The model assets (filed, metadata) that you want deployed. We have already registered these in our training component.\n",
    " - Some code to run as a service. It executes the model on a given input request. This entry script receives data submitted to a deployed web service and passes it to the model. It then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns.\n",
    "\n",
    "The two things you need to accomplish in your entry script are:\n",
    "\n",
    "- Loading your model (using a function called `init()`)\n",
    "- Running your model on input data (using a function called `run()`)\n",
    "\n",
    "## 4.1. Creating an Inference Script\n",
    "\n",
    "In the following implementation the `init()` function loads the model, and the run function expects the data in `json` format with the input data stored under `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_dir = \"./deploy\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/score.py\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy\n",
    "import joblib\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
    "    You can write the logic here to perform init operations like caching the model in memory\n",
    "    \"\"\"\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"model.pkl\")\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "    logging.info(\"Init complete\")\n",
    "\n",
    "\n",
    "def run(raw_data):\n",
    "    \"\"\"\n",
    "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
    "    In the example we extract the data from the json input and call the scikit-learn model's predict()\n",
    "    method and return the result back\n",
    "    \"\"\"\n",
    "    logging.info(\"Request received\")\n",
    "    data = json.loads(raw_data)[\"data\"]\n",
    "    data = numpy.array(data)\n",
    "    result = model.predict(data)\n",
    "    logging.info(\"Request processed\")\n",
    "    return result.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95611fb7",
   "metadata": {},
   "source": [
    "## 4.2. Create a New Online Endpoint\n",
    "It is now straight forward to create an online endpoint. First, we create an endpoint by providing its description. The deployment name needs to be unique in the entire azure region, therefore, for this tutorial, we create a unique name using [`UUID`](https://en.wikipedia.org/wiki/Universally_unique_identifier#:~:text=A%20universally%20unique%20identifier%20(UUID,%2C%20for%20practical%20purposes%2C%20unique.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Creating a unique name for the endpoint\n",
    "online_endpoint_name = \"credit-endpoint-\" + str(uuid.uuid4())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4926162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    CodeConfiguration,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is an online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"credit_defaults\",\n",
    "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint)\n",
    "\n",
    "print(f\"Endpint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13838967",
   "metadata": {},
   "source": [
    "If you have previously created an endpoint, you can retrieve it as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7df777",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name = online_endpoint_name)\n",
    "\n",
    "print(f\"Endpint \\\"{endpoint.name}\\\" with provisioning state \\\"{endpoint.provisioning_state}\\\" is retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd203d3",
   "metadata": {},
   "source": [
    "## 4.3. Deploy the Model to the Endpoint\n",
    "\n",
    "Once the endpoint is created, we deploy the model with the entry script. Each endpoint can have multiple deployments and direct traffic to these deployments can be specified using rules. Here we create a single deployment that handles 100% of the incoming traffic. We have chosen a color name for our deployment, e.g. *blue*, *green*, *red* deployments, which is totally arbitrary.\n",
    "\n",
    "You can check the *Models* page on the Azure ML Studio, to identify the latest version of your registered model. Alternatively, the code below can surface the latest version, if integer numbers are used for versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the model to deploy. Here we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "\n",
    "#create an online deployment.\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name='blue',\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\",\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=deploy_dir,\n",
    "        scoring_script=\"score.py\"),\n",
    "    instance_type='Standard_DS3_v2',\n",
    "    instance_count=1)\n",
    "\n",
    "blue_deployment = ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea1417",
   "metadata": {},
   "source": [
    "## 4.4. Test with a sample query\n",
    "\n",
    "With the endpoint already published, we can run inference with it.\n",
    "\n",
    "Let's create a sample request file following the design expected in the run method in the score script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c300b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\"data\": [\n",
    "    [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0], \n",
    "    [10,9,8,7,6,5,4,3,2,1, 10,9,8,7,6,5,4,3,2,1,10,9,8]\n",
    "]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    "    deployment_name='blue'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c35946",
   "metadata": {},
   "source": [
    "It is also possible to use consume the Rest endpoint directly [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python).\n",
    "\n",
    "The online endpoint consumes resources while running, in order to release the allocated resources, we should delete the deployment if not planning to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.delete(name='blue', endpoint_name=online_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7dea2",
   "metadata": {},
   "source": [
    "The endpoint itself, can also be removed if no other deployment exists:\n",
    "```python \n",
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c0536",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Coming Soon - We are working on an expansion of this tutorial using Sweep Component (work in progress), and also using multi-node training and performing Hyper Parameter Optimization."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e2b6a01f5dc9f68a48ed46bbfb0327231c221438b68bf46ed53c80b6f1cdd38"
  },
  "kernelspec": {
   "display_name": "sdk2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
