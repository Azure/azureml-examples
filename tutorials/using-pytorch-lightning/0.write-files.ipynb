{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytorch-lightning.yml\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.7\n",
    "  - pip\n",
    "  - pip:\n",
    "    - azureml-defaults\n",
    "    - torch==1.6.0\n",
    "    - torchvision==0.7.0\n",
    "    - pytorch-lightning==1.0.4\n",
    "    - mlflow\n",
    "    - azureml-mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile azureml_env_adapter.py\n",
    "import os\n",
    "\n",
    "\n",
    "def set_environment_variables(single_node=False, master_port=6105):\n",
    "    # os.environ[\"RANK\"] = os.environ[\"OMPI_COMM_WORLD_RANK\"]\n",
    "    # os.environ[\"WORLD_SIZE\"] = os.environ[\"OMPI_COMM_WORLD_SIZE\"]\n",
    "\n",
    "    if not single_node:\n",
    "        master_node_params = os.environ[\"AZ_BATCH_MASTER_NODE\"].split(\":\")\n",
    "        os.environ[\"MASTER_ADDR\"] = master_node_params[0]\n",
    "\n",
    "        # Do not overwrite master port with that defined in AZ_BATCH_MASTER_NODE\n",
    "        if \"MASTER_PORT\" not in os.environ:\n",
    "            os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "    else:\n",
    "        os.environ[\"MASTER_ADDR\"] = os.environ[\"AZ_BATCHAI_MPI_MASTER_NODE\"]\n",
    "        os.environ[\"MASTER_PORT\"] = \"54965\"\n",
    "    print(\n",
    "        \"NCCL_SOCKET_IFNAME original value = {}\".format(\n",
    "            os.environ[\"NCCL_SOCKET_IFNAME\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    os.environ[\"NCCL_SOCKET_IFNAME\"] = \"^docker0,lo\"\n",
    "    os.environ[\"NODE_RANK\"] = os.environ[\n",
    "        \"OMPI_COMM_WORLD_RANK\"\n",
    "    ]  # node rank is the world_rank from mpi run\n",
    "    # print(\"RANK = {}\".format(os.environ[\"RANK\"]))\n",
    "    # print(\"WORLD_SIZE = {}\".format(os.environ[\"WORLD_SIZE\"]))\n",
    "    print(\"MASTER_ADDR = {}\".format(os.environ[\"MASTER_ADDR\"]))\n",
    "    print(\"MASTER_PORT = {}\".format(os.environ[\"MASTER_PORT\"]))\n",
    "    print(\"NODE_RANK = {}\".format(os.environ[\"NODE_RANK\"]))\n",
    "    print(\n",
    "        \"NCCL_SOCKET_IFNAME new value = {}\".format(\n",
    "            os.environ[\"NCCL_SOCKET_IFNAME\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Script from: https://github.com/PyTorchLightning/pytorch-lightning/blob/1.0.0rc2/pl_examples/basic_examples/autoencoder.py\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "try:\n",
    "    from torchvision.datasets.mnist import MNIST\n",
    "    from torchvision import transforms\n",
    "except ModuleNotFoundError:\n",
    "    from tests.base.datasets import MNIST\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def cli_main():\n",
    "    pl.seed_everything(1234)\n",
    "\n",
    "    # ------------\n",
    "    # args\n",
    "    # ------------\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ------------\n",
    "    # data\n",
    "    # ------------\n",
    "    dataset = MNIST(\n",
    "        \"\", train=True, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_test = MNIST(\n",
    "        \"\", train=False, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=args.batch_size)\n",
    "    val_loader = DataLoader(mnist_val, batch_size=args.batch_size)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=args.batch_size)\n",
    "\n",
    "    # ------------\n",
    "    # model\n",
    "    # ------------\n",
    "    model = LitAutoEncoder()\n",
    "\n",
    "    # ------------\n",
    "    # training\n",
    "    # ------------\n",
    "    trainer = pl.Trainer.from_argparse_args(args)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # ------------\n",
    "    # testing\n",
    "    # ------------\n",
    "    result = trainer.test(test_dataloaders=test_loader)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cli_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train-multi-node.py\n",
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Script from: https://github.com/PyTorchLightning/pytorch-lightning/blob/1.0.0rc2/pl_examples/basic_examples/autoencoder.py\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "from azureml_env_adapter import set_environment_variables\n",
    "\n",
    "try:\n",
    "    from torchvision.datasets.mnist import MNIST\n",
    "    from torchvision import transforms\n",
    "except ModuleNotFoundError:\n",
    "    from tests.base.datasets import MNIST\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def cli_main():\n",
    "    pl.seed_everything(1234)\n",
    "\n",
    "    # ------------\n",
    "    # args\n",
    "    # ------------\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # set azureml env vars for multi-node ddp\n",
    "    set_environment_variables(single_node=int(args.num_nodes) > 1)\n",
    "\n",
    "    # ------------\n",
    "    # data\n",
    "    # ------------\n",
    "    dataset = MNIST(\n",
    "        \"\", train=True, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_test = MNIST(\n",
    "        \"\", train=False, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=args.batch_size)\n",
    "    val_loader = DataLoader(mnist_val, batch_size=args.batch_size)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=args.batch_size)\n",
    "\n",
    "    # ------------\n",
    "    # model\n",
    "    # ------------\n",
    "    model = LitAutoEncoder()\n",
    "\n",
    "    # ------------\n",
    "    # training\n",
    "    # ------------\n",
    "    trainer = pl.Trainer.from_argparse_args(args)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # ------------\n",
    "    # testing\n",
    "    # ------------\n",
    "    result = trainer.test(test_dataloaders=test_loader)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cli_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train-with-mlflow-logging.py\n",
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Script from: https://github.com/PyTorchLightning/pytorch-lightning/blob/1.0.0rc2/pl_examples/basic_examples/autoencoder.py\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "import mlflow\n",
    "from azureml.core import Run\n",
    "\n",
    "try:\n",
    "    from torchvision.datasets.mnist import MNIST\n",
    "    from torchvision import transforms\n",
    "except ModuleNotFoundError:\n",
    "    from tests.base.datasets import MNIST\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"loss\", loss, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def cli_main():\n",
    "    pl.seed_everything(1234)\n",
    "\n",
    "    # ------------\n",
    "    # args\n",
    "    # ------------\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ------------\n",
    "    # data\n",
    "    # ------------\n",
    "    dataset = MNIST(\n",
    "        \"\", train=True, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_test = MNIST(\n",
    "        \"\", train=False, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=args.batch_size)\n",
    "    val_loader = DataLoader(mnist_val, batch_size=args.batch_size)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=args.batch_size)\n",
    "\n",
    "    # ------------\n",
    "    # model\n",
    "    # ------------\n",
    "    model = LitAutoEncoder()\n",
    "\n",
    "    # ------------\n",
    "    # logging\n",
    "    # ------------\n",
    "    # get azureml run object\n",
    "    run = Run.get_context()\n",
    "    # get the tracking uri for the azureml workspace\n",
    "    mlflow_uri = run.experiment.workspace.get_mlflow_tracking_uri()\n",
    "    # get the azureml experiment name\n",
    "    exp_name = run.experiment.name\n",
    "\n",
    "    mlf_logger = MLFlowLogger(\n",
    "        experiment_name=exp_name, tracking_uri=mlflow_uri\n",
    "    )\n",
    "    # link the mlflowlogger run ID to the azureml run ID\n",
    "    mlf_logger._run_id = run.id\n",
    "\n",
    "    # ------------\n",
    "    # training\n",
    "    # ------------\n",
    "    trainer = pl.Trainer.from_argparse_args(args, logger=mlf_logger)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # ------------\n",
    "    # testing\n",
    "    # ------------\n",
    "    result = trainer.test(test_dataloaders=test_loader)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cli_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train-with-tensorboard-logging.py\n",
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Script from: https://github.com/PyTorchLightning/pytorch-lightning/blob/1.0.0rc2/pl_examples/basic_examples/autoencoder.py\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "try:\n",
    "    from torchvision.datasets.mnist import MNIST\n",
    "    from torchvision import transforms\n",
    "except ModuleNotFoundError:\n",
    "    from tests.base.datasets import MNIST\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def cli_main():\n",
    "    pl.seed_everything(1234)\n",
    "\n",
    "    # ------------\n",
    "    # args\n",
    "    # ------------\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int)\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"./logs\")\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ------------\n",
    "    # data\n",
    "    # ------------\n",
    "    dataset = MNIST(\n",
    "        \"\", train=True, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_test = MNIST(\n",
    "        \"\", train=False, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=args.batch_size)\n",
    "    val_loader = DataLoader(mnist_val, batch_size=args.batch_size)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=args.batch_size)\n",
    "\n",
    "    # ------------\n",
    "    # model\n",
    "    # ------------\n",
    "    model = LitAutoEncoder()\n",
    "\n",
    "    # ------------\n",
    "    # logging\n",
    "    # ------------\n",
    "    tb_logger = TensorBoardLogger(args.logdir)\n",
    "\n",
    "    # ------------\n",
    "    # training\n",
    "    # ------------\n",
    "    trainer = pl.Trainer.from_argparse_args(args, logger=tb_logger)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # ------------\n",
    "    # testing\n",
    "    # ------------\n",
    "    result = trainer.test(test_dataloaders=test_loader)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cli_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
