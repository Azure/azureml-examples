{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c649f8",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ebb1e2",
   "metadata": {},
   "source": [
    "# AOAI Finetuning and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540bf73",
   "metadata": {},
   "source": [
    "The objective of this notebook is to illustrate how to use the component-based approach to evaluate a fine-tuned AOAI model on a user-provided dataset. It walks you through all stages of the process starting with model fine-tuning and concluding with metrics calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f9d0c",
   "metadata": {},
   "source": [
    "## 1. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00d009",
   "metadata": {},
   "source": [
    "### 1.1. Compute with Managed Indentity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b35b6f",
   "metadata": {},
   "source": [
    "The AOAI Fine-tuning component requires specific permissions to access various resources, which must be granted prior to job submission. For authentication, permissions will be added to the User Managed Identity (UMI) and attached to the compute instance where the component will run. Detailed instructions for creating the managed identity can be found[here](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azp). The instruction on how to assign managed identity to the compute can be found [here](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-identity-based-service-authentication?view=azureml-api-2&tabs=cli#user-assigned-managed-identity). See the following [link](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control) for more details on the role-based access control for Azure OpenAI Service.\n",
    "\n",
    "The Following are the minimum permissions that need to be attached to the User Managed Identity (UMI):\n",
    "- `Cognitive service contributor` role over `Azure OpenAI resource`\n",
    "- `Cognitive service user` role over `Azure OpenAI resource`\n",
    "\n",
    "\n",
    "Furthermore, users can provide dataset URIs as inputs for training and validation data. If users intend to provide non-public data URIs, they must first store them in the workspace's associated Key Vault and then pass the Key Vault key as an input. In this case, the managed identity will need permissions to access both the workspace and the Key Vault:\n",
    "- `Reader` role over AML workspace\n",
    "- `Get Secret` permission over workspace' associated key vault\n",
    "\n",
    "Key Vault access configuration can be of two types: RBAC and Vault access policy. If the Key Vault supports RBAC authorization, assign the `Key Vault Secrets User` role to the UMI over the Key Vault's scope. Otherwise, navigate to the Access Policies tab in the Key Vault resource in the Azure portal and create a new access policy to grant the \"Get Secret\" permission to the UMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87891d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_identity_resource_id = \"subscriptions/72c03bf3-4e69-41af-9532-dfcdc3eefef4/resourceGroups/aml-benchmarking/providers/Microsoft.ManagedIdentity/userAssignedIdentities/finetuning-umi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbc206",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7acf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Output\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dfcc61",
   "metadata": {},
   "source": [
    "### 2.1. Configure workspace details and get a handle to the workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4171c3",
   "metadata": {},
   "source": [
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa0f14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "subscription_id = \"72c03bf3-4e69-41af-9532-dfcdc3eefef4\"\n",
    "resource_group = \"aml-benchmarking\"\n",
    "workspace_name = \"chirag-ws\"\n",
    "\n",
    "ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ed47a",
   "metadata": {},
   "source": [
    "### 2.2 Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff6df7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Workspace</th>\n",
       "      <td>chirag-ws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subscription ID</th>\n",
       "      <td>72c03bf3-4e69-41af-9532-dfcdc3eefef4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource Group</th>\n",
       "      <td>aml-benchmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>eastus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     \n",
       "Workspace                                   chirag-ws\n",
       "Subscription ID  72c03bf3-4e69-41af-9532-dfcdc3eefef4\n",
       "Resource Group                       aml-benchmarking\n",
       "Location                                       eastus"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.connections._subscription_id\n",
    "output[\"Resource Group\"] = ws.resource_group\n",
    "output[\"Location\"] = ws.location\n",
    "pd.DataFrame(data=output, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f39d63",
   "metadata": {},
   "source": [
    "## 3. Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a7d45",
   "metadata": {},
   "source": [
    "### Create or Attach existing AmlCompute with managed identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59fd9e",
   "metadata": {},
   "source": [
    "You will need to create a compute target for your pipeline run. In this tutorial, you will create AmlCompute as your compute resource with managed identity.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.**\n",
    "\n",
    "If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f87564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable_node_public_ip: true\n",
      "id: /subscriptions/72c03bf3-4e69-41af-9532-dfcdc3eefef4/resourceGroups/aml-benchmarking/providers/Microsoft.MachineLearningServices/workspaces/chirag-ws/computes/aoai-compute\n",
      "identity:\n",
      "  tenant_id: 72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "  type: user_assigned\n",
      "  user_assigned_identities:\n",
      "  - client_id: db93f625-6a19-4d4e-add2-d33fedc3addf\n",
      "    principal_id: 5d818a76-543c-40b6-94e2-0c35fdc24791\n",
      "    resource_id: /subscriptions/72c03bf3-4e69-41af-9532-dfcdc3eefef4/resourcegroups/aml-benchmarking/providers/Microsoft.ManagedIdentity/userAssignedIdentities/finetuning-umi\n",
      "  - client_id: 2ea8d340-9292-4985-9db8-0769ae8d5c91\n",
      "    principal_id: 849a91ea-9b48-46f0-849d-de0f6874b370\n",
      "    resource_id: /subscriptions/ed2cab61-14cc-4fb3-ac23-d72609214cfd/resourcegroups/openai-finetune-corp-rg/providers/Microsoft.ManagedIdentity/userAssignedIdentities/openai-finetune-corp-mi\n",
      "idle_time_before_scale_down: 120\n",
      "location: eastus\n",
      "max_instances: 4\n",
      "min_instances: 2\n",
      "name: aoai-compute\n",
      "provisioning_state: Succeeded\n",
      "size: Standard_D4s_v3\n",
      "ssh_public_access_enabled: false\n",
      "tier: dedicated\n",
      "type: amlcompute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedIdentityConfiguration,\n",
    "    IdentityConfiguration,\n",
    "    AmlCompute,\n",
    ")\n",
    "from azure.ai.ml.constants import ManagedServiceIdentityType\n",
    "\n",
    "# Create an identity configuration from the user-assigned managed identity\n",
    "managed_identity = ManagedIdentityConfiguration(\n",
    "    resource_id=managed_identity_resource_id\n",
    ")\n",
    "identity_config = IdentityConfiguration(\n",
    "    type=ManagedServiceIdentityType.USER_ASSIGNED,\n",
    "    user_assigned_identities=[managed_identity],\n",
    ")\n",
    "\n",
    "# specify aml compute name.\n",
    "cpu_compute_target = \"aoai-compute\"\n",
    "\n",
    "try:\n",
    "    compute = ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    # Pass the identity configuration\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        size=\"STANDARD_DS3_V2\",\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        identity=identity_config,\n",
    "    )\n",
    "    poller = ml_client.compute.begin_create_or_update(compute)\n",
    "    poller.wait()\n",
    "print(compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd69ac0",
   "metadata": {},
   "source": [
    "## 4. Import Components From Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e49aa7",
   "metadata": {},
   "source": [
    "An Azure Machine Learning component is a self-contained piece of code that does one step in a machine learning pipeline. A component is analogous to a function - it has a name, inputs, outputs, and a body. Components are the building blocks of the Azure Machine Learning pipelines. It's a good engineering practice to build a machine learning pipeline where each step has well-defined inputs and outputs. In Azure Machine Learning, a component represents one reusable step in a pipeline. Components are designed to help improve the productivity of pipeline building. Specifically, components offer:\n",
    "\n",
    "- Well-defined interface: Components require a well-defined interface (input and output). The interface allows the user to build steps and connect steps easily. The interface also hides the complex logic of a step and removes the burden of understanding how the step is implemented.\n",
    "\n",
    "- Share and reuse: As the building blocks of a pipeline, components can be easily shared and reused across pipelines, workspaces, and subscriptions. Components built by one team can be discovered and used by another team.\n",
    "\n",
    "- Version control: Components are versioned. The component producers can keep improving components and publish new versions. Consumers can use specific component versions in their pipelines. This gives them compatibility and reproducibility.\n",
    "\n",
    "For a more detailed information on this subject, refer to the this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2).\n",
    "\n",
    "To import components,  we need to get the registry. The following command obtains the public regsitry from which we will import components for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed9a670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x000001A085C46FA0>,\n",
      "         subscription_id=d4d34678-c0d7-4d69-a257-366e3cb4a7d8,\n",
      "         resource_group_name=registry-builtin-1p-preview-eastus,\n",
      "         workspace_name=None)\n"
     ]
    }
   ],
   "source": [
    "azureml_preview_registry = MLClient(credential=credential, registry_name=\"azureml-1p-preview\")\n",
    "print(azureml_preview_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49ad4f",
   "metadata": {},
   "source": [
    "Next, we pull specific components from the corresponding registires and use them to build a pipeline of steps. For the illustration of the evaluation workflow we will use the following components:\n",
    "\n",
    "| Component name | Description  | Registry name |\n",
    "|:---|:---|:---|\n",
    "| **aoai_finetuning**  | Upload dataset to Azure OpenAI, perform finetuning and delete dataset from Azure OpenAI. | _azureml-1p-preview_ |\n",
    "| **batch_benchmark_inference**  | Inference the input dataset. | _azureml-1--preview_ |\n",
    "| **batch_resource_manager** | Dual purpose: (i) deploy fine-tuned model, and (ii) delete deployment. | _azureml-1p-preview_ |\n",
    "| **benchmark_result_aggregator** | Aggregate results of perfromance and quality metrics components output. | _azureml-1p-preview_ |\n",
    "| **compute_metrics** | Compute quality metrics such as accuracy scores. | _azureml-1p-preview_ |\n",
    "| **compute_performance_metrics** | Compute performance metrics such as runtimes. | _azureml-1p-preview_ |\n",
    "| **inference_postprocessor** | Process output of the inderence step to be used by the compute metrics components.| _azureml-1p-preview_ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05765b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Upload component version: 0.0.7\n",
      "---\n",
      "Resource manager component version: 0.0.5\n",
      "---\n",
      "Batch inference component version: 0.0.9\n",
      "---\n",
      "Compute quality metrics component version: 0.0.26\n",
      "---\n",
      "Compute performance metrics component version: 0.0.6\n",
      "---\n",
      "Postprocessor component version: 0.0.7\n",
      "---\n",
      "Results aggregator component version: 0.0.8\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "finetuning_component = azureml_preview_registry.components.get(name=\"aoai_finetuning\")\n",
    "print(f\"Data Upload component version: {finetuning_component.version}\\n---\")\n",
    "\n",
    "resource_manager_component = azureml_preview_registry.components.get( \"batch_resource_manager\")\n",
    "print(f\"Resource manager component version: {resource_manager_component.version}\\n---\")\n",
    "\n",
    "batch_component = azureml_preview_registry.components.get(\"batch_benchmark_inference\")\n",
    "print(f\"Batch inference component version: {batch_component.version}\\n---\")\n",
    "\n",
    "compute_metrics_component = azureml_preview_registry.components.get(\"compute_metrics\")\n",
    "print(f\"Compute quality metrics component version: {compute_metrics_component.version}\\n---\")\n",
    "\n",
    "compute_perf_metrics_component = azureml_preview_registry.components.get(\"compute_performance_metrics\")\n",
    "print(f\"Compute performance metrics component version: {compute_perf_metrics_component.version}\\n---\")\n",
    "\n",
    "postprocessor_component = azureml_preview_registry.components.get(\"inference_postprocessor\")\n",
    "print(f\"Postprocessor component version: {postprocessor_component.version}\\n---\")\n",
    "\n",
    "result_aggregator_component = azureml_preview_registry.components.get(\"benchmark_result_aggregator\")\n",
    "print(f\"Results aggregator component version: {result_aggregator_component.version}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1cad43",
   "metadata": {},
   "source": [
    "## 4. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bace997",
   "metadata": {},
   "source": [
    "The component supports three methods of providing training and validation data input:\n",
    "1. Direct Dataset Provisioning: Users can input data assets directly via the training_file_path and validation_file_path ports. The component will then load the data and upload it to the AOAI resource.\n",
    "2. Dataset URI Provisioning:  Alternatively, users can provide dataset URIs via the `training_import_path` and `validation_import_path` ports. For this method to work, data should be accesible via GET request to the uri without requiring additional permissions. If the dataset uri is public and does not contains any credentials, user can pass it against the `data_uri` key in the training_import_path/validation_import_path json. However if the uri should not be exposed, users must first upload the data uri to the user workspaces' associated keyvault and then pass key vault key against `keyvault_key_for_data_uri` key in training_import_path/validation_import_path json\n",
    "\n",
    "Note that exactly one of either `training_file_path` or `training_import_path` must be provided. Providing validation dataset is optional.\n",
    "\n",
    "Along with `training_file_path` user can provide `validation_file_path`. If the latter is not provided, the training dataset will be automatically split in an 80:20 ratio to create validation data.\n",
    "\n",
    "Along with `training_import_path`, user can provide `validation_import_path`. In the import_path json exactly one of the fields `data_uri` or `keyvault_key_for_data_uri` must be present. Since data is not loaded in component in case uri is provided, training data will not be split in the absence of validation_import_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72764ded",
   "metadata": {},
   "source": [
    "In the next cell we define train and validation data which will be used to fine-tune an AOAI model. We also define an inference dataset to be used to test the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af929da",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = ml_client.data.get(name=\"aoai_finetune_train\", version=\"1\")\n",
    "validation_file_path = ml_client.data.get(name=\"aoai_finetune_validation\", version=\"1\")\n",
    "inference_file_path = ml_client.data.get(name=\"aoai_finetune_test\", version=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b7e7a-b8b2-41af-8869-54065a2564ba",
   "metadata": {},
   "source": [
    "Alternatively we can also define training_import_path and validation_import_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "170ee239-63f5-4033-9f4c-c1473aa0ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_import_path = ml_client.data.get(name=\"training_import_path\", version=\"1\")\n",
    "validation_import_path = ml_client.data.get(name=\"training_import_path\", version=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46671bbb",
   "metadata": {},
   "source": [
    "\n",
    "The input data has `prompt` column which will be sent to the endpoint for batch inference and `completion` column which the ground truth.\n",
    "\n",
    "Next, we need to set the input pattern for the batch inference step. This pattern is a string that contains the following fields:\n",
    "\n",
    "|Parameter|Description|\n",
    "|-|-|\n",
    "| **messages**| This is the input prompt or prompts that are fed into the language model for generating text. The prompt can be a single sentence or multiple sentences, and it provides the initial context or topic for the generated text. It is a placeholder that indicates where the prompt should be inserted in the batch input pattern.  |\n",
    "| **temperature** | this parameter controls the degree of randomness and creativity in the generated text. We set this value of 0 for reproducibility of the results. Otherwise, there will be differences in scores from run to run on the same model depending on how the data is sampled. It selects the most likely tokens until the cumulative probability exceeds a certain threshold, and then samples from those tokens based on their probabilities. |\n",
    "| **max_tokens** | specifies the maximum number of tokens (words or subwords) that the model can generate in response to a given prompt. |\n",
    "| **top_p** | This is another parameter used in the generation of text by language models like ChatGPT. It stands for \"top probability\" and controls the diversity of the generated text. This parameter is useful for controlling the length of the generated text and preventing the model from generating excessively long or irrelevant responses. |\n",
    "| **frequency_penalty** | This parameter is used in some language models to discourage the repetition of the same tokens in the generated text. Specifically, it penalizes the probability of selecting a token that has already been used in the generated text. A higher frequency penalty value will result in less repetition in the generated text, while a lower value will allow for more repetition. |\n",
    "| **presence_penalty** | Similar to the `frequency_penalty`, this is another parameter used in some language models to encourage the generation of new and diverse tokens (words or subwords) in the generated text. Specifically, it penalizes the probability of selecting a token that has already been used in the prompt or in the generated text. A higher presence penalty value will result in more unique and diverse responses, while a lower value will allow for more repetition. |\n",
    "| **stop** |  The value of this parameter is used to specify a sequence of tokens that should be used to signal the end of the generated text. When the language model generates text, it will continue generating tokens (words or subwords) until it encounters one of the tokens specified in the \"stop\" parameter. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffd2df38-411c-4023-9e44-c0933e70735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_pattern = '{\"messages\": ###<prompt>, \"temperature\": 0.0, \"max_tokens\": 20, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"stop\": null}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea67b3-4a23-4935-b29d-a69621d2cea2",
   "metadata": {},
   "source": [
    "Next, we specify two additional parameters `label_column_name` and `find_first`. The first parameter is just the name of the ground truth field. The `find_first` parameter is used by the post-processor component, which processes the output of the model inference step.  When specified, it must be a list of strings to search for in the inference results. The first occurrence of each string will be extracted and the occurrence with minimum index will be returned. In our dataset the objective is to select the best of the proposed choices \"A,B,C,D,E\". Hence, we set this value accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c648b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column_name = \"completion\"\n",
    "find_first = \"A,B,C,D,E\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc4bae",
   "metadata": {},
   "source": [
    "## 6. Build a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b815d7",
   "metadata": {},
   "source": [
    "Next, we build a pipeline from the imported components. Since this notebook is designed to illustrate the evaluation flow, we will string these components in the following fashion. First, we finetune and deploy an AOAI model. Next, we inference it and delete the deployment . After that, we postprocess  inference output and compute metrics. You do not have to modify anything in the next cell if you want to an exisiting postprocessing and metric computation components. \n",
    "\n",
    "<font color='blue' size=3 face='Verdana'>To calulate custom metrics, wrap your logic in a component, import it and modify the pipeline in the following cell. In this cenario, steps 5-8 may not be needed.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f6a4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(description=\"aoai_deployment\")\n",
    "def aoai_deployment_with_metrics(\n",
    "    training_file_path,\n",
    "    validation_file_path,\n",
    "    training_import_path,\n",
    "    validation_import_path,\n",
    "    inference_file_path,\n",
    "    batch_input_pattern,\n",
    "    compute_name,\n",
    "    model_version=1,\n",
    "    label_column_name=None,\n",
    "    n_samples=10,\n",
    "    handle_response_failure=\"neglect\",\n",
    "    fallback_value=None,\n",
    "    additional_headers=None,\n",
    "    ensure_ascii=False,\n",
    "    endpoint_subscription=None,\n",
    "    endpoint_resource_group=None,\n",
    "    endpoint_region=None,\n",
    "    deployment_sku=None,\n",
    "    endpoint_name=None,\n",
    "    deployment_name=None,\n",
    "    max_retry_time_interval=10,\n",
    "    initial_worker_count=2,\n",
    "    max_worker_count=5,\n",
    "    instance_count=1,\n",
    "    max_concurrency_per_instance=1,\n",
    "    debug_mode=False,\n",
    "    do_quota_validation=False,\n",
    "    use_max_quota=True,\n",
    "    is_finetuned_model=False,\n",
    "    finetuned_subscription_id=None,\n",
    "    finetuned_resource_group=None,\n",
    "    finetuned_workspace=None,\n",
    "    delete_managed_deployment=True,\n",
    "    remove_prefixes=None,\n",
    "    separator=None,\n",
    "    find_first=None,\n",
    "    extract_number=None,\n",
    "    regex_expr=None,\n",
    "    strip_characters=None,\n",
    "    label_map=None,\n",
    "    template=None,\n",
    "    evaluation_config_params=None,\n",
    "    openai_config_params=None,\n",
    "    connections_name=None,\n",
    "    model=\"gpt-35-turbo\",\n",
    "    task_type=\"chat\",\n",
    "    n_epochs=1,\n",
    "    batch_size=8,\n",
    "    learning_rate_multiplier=1,\n",
    "    suffix = None,       \n",
    "    n_ctx=4096,\n",
    "    lora_dim=1,\n",
    "    weight_decay_multiplier=0.001,\n",
    "):\n",
    "    # Step 1 : Finetune OAI model\n",
    "    finetune_step = finetuning_component(\n",
    "        training_file_path = training_file_path,\n",
    "        validation_file_path = validation_file_path,\n",
    "        training_import_path = training_import_path,\n",
    "        validation_import_path = validation_import_path,\n",
    "        endpoint_name=endpoint_name,\n",
    "        endpoint_subscription = endpoint_subscription,\n",
    "        endpoint_resource_group = endpoint_resource_group,\n",
    "        task_type=task_type,\n",
    "        model=model,\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate_multiplier=learning_rate_multiplier,\n",
    "        suffix = suffix,\n",
    "        n_ctx=n_ctx,\n",
    "        lora_dim=lora_dim,\n",
    "        weight_decay_multiplier=weight_decay_multiplier\n",
    "    )\n",
    "    \n",
    "    finetune_step.compute = compute_name\n",
    "\n",
    "    # 2. Fine-tuned model deployment\n",
    "    deployment_step = resource_manager_component(\n",
    "        delete_managed_deployment=delete_managed_deployment,\n",
    "        endpoint_subscription_id=endpoint_subscription,\n",
    "        endpoint_resource_group=endpoint_resource_group,\n",
    "        endpoint_location=endpoint_region,\n",
    "        deployment_sku=deployment_sku,\n",
    "        do_quota_validation=do_quota_validation,\n",
    "        use_max_quota=use_max_quota,\n",
    "        is_finetuned_model=is_finetuned_model,\n",
    "        finetuned_subscription_id=finetuned_subscription_id,\n",
    "        finetuned_resource_group=finetuned_resource_group,\n",
    "        finetuned_workspace=finetuned_workspace,\n",
    "        model=model,\n",
    "        model_version=model_version,\n",
    "        model_type=\"oai\",\n",
    "        deletion_model=False,\n",
    "        deployment_name=deployment_name,\n",
    "        connections_name=connections_name,\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait_finetuned_step = False,\n",
    "        finetuned_model_metadata = finetune_step.outputs.aoai_finetuning_output\n",
    "    )\n",
    "\n",
    "    deployment_step.compute = compute_name\n",
    "\n",
    "    # 3. Batch inference. Pipeline component which deploys the model before inference.\n",
    "    batch_step = batch_component(\n",
    "        input_dataset=inference_file_path,\n",
    "        batch_input_pattern=batch_input_pattern,\n",
    "        label_column_name=label_column_name,\n",
    "        deployment_name=deployment_name,\n",
    "        endpoint_url=\"dummy\",\n",
    "        model_type=\"oai\",\n",
    "        n_samples=n_samples,\n",
    "        handle_response_failure=handle_response_failure,\n",
    "        fallback_value=fallback_value,\n",
    "        additional_headers=additional_headers,\n",
    "        ensure_ascii=ensure_ascii,\n",
    "        max_retry_time_interval=max_retry_time_interval,\n",
    "        instance_count=instance_count,\n",
    "        initial_worker_count=initial_worker_count,\n",
    "        max_worker_count=max_worker_count,\n",
    "        max_concurrency_per_instance=1,\n",
    "        authentication_type = \"managed_identity\",\n",
    "        debug_mode=debug_mode,\n",
    "        endpoint_config_file=deployment_step.outputs.output_metadata,\n",
    "        connections_name=\"dummy\"\n",
    "    )\n",
    "    batch_step.compute = compute_name\n",
    "    batch_step.outputs.predictions = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=\"azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.jsonl\",\n",
    "    )\n",
    "    batch_step.outputs.performance_metadata = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=\"azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.jsonl\",\n",
    "    )\n",
    "    batch_step.outputs.ground_truth = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=\"azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.jsonl\",\n",
    "    )\n",
    "\n",
    "    # 4. Delete deployment\n",
    "    delete_step = resource_manager_component(\n",
    "        deployment_metadata=deployment_step.outputs.output_metadata,\n",
    "        wait_input=batch_step.outputs.predictions,\n",
    "        delete_managed_deployment=delete_managed_deployment,\n",
    "    )\n",
    "    delete_step.compute = compute_name\n",
    "\n",
    "    # 5. Comute performance metrics\n",
    "    compute_perf_metrics_step = compute_perf_metrics_component(\n",
    "        percentiles=\"50,90,99\",\n",
    "        batch_size_column_name=\"batch_size\",\n",
    "        start_time_column_name=\"start_time_iso\",\n",
    "        end_time_column_name=\"end_time_iso\",\n",
    "        input_token_count_column_name=\"input_token_count\",\n",
    "        output_token_count_column_name=\"output_token_count\",\n",
    "        performance_data=batch_step.outputs.performance_metadata,\n",
    "    )\n",
    "    compute_perf_metrics_step.compute = compute_name\n",
    "    compute_perf_metrics_step.outputs.performance_result = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=\"azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.json\",\n",
    "    )\n",
    "\n",
    "    # 6. Process inference output\n",
    "    postprocessor_step = postprocessor_component(\n",
    "        prediction_column_name=\"prediction\",\n",
    "        ground_truth_column_name=label_column_name,\n",
    "        remove_prefixes=remove_prefixes,\n",
    "        separator=separator,\n",
    "        find_first=find_first,\n",
    "        extract_number=extract_number,\n",
    "        regex_expr=regex_expr,\n",
    "        strip_characters=strip_characters,\n",
    "        label_map=label_map,\n",
    "        template=template,\n",
    "        prediction_dataset=batch_step.outputs.predictions,\n",
    "        ground_truth_dataset=batch_step.outputs.ground_truth,\n",
    "    )\n",
    "    postprocessor_step.compute = compute_name\n",
    "    postprocessor_step.outputs.output_dataset_result = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=\"azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.jsonl\",\n",
    "    )\n",
    "\n",
    "    # 7. Compute accuracy metrics\n",
    "    compute_metrics_step = compute_metrics_component(\n",
    "        task=\"question-answering\",\n",
    "        prediction_column_name=\"prediction\",\n",
    "        ground_truth_column_name=label_column_name,\n",
    "        evaluation_config_params=evaluation_config_params,\n",
    "        openai_config_params=openai_config_params,\n",
    "        prediction=postprocessor_step.outputs.output_dataset_result,\n",
    "        ground_truth=postprocessor_step.outputs.output_dataset_result,\n",
    "    )\n",
    "    compute_metrics_step.compute = compute_name\n",
    "    compute_metrics_step.outputs.evaluation_result = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=\"azureml://datastores/${{default_datastore}}/paths/azureml/${{name}}/${{output_name}}.json\",\n",
    "    )\n",
    "\n",
    "    # 8. Aggregate metrics\n",
    "    aggregate_step = result_aggregator_component(\n",
    "        performance_metrics=compute_perf_metrics_step.outputs.performance_result,\n",
    "        quality_metrics=compute_metrics_step.outputs.evaluation_result,\n",
    "    )\n",
    "    aggregate_step.compute = compute_name\n",
    "\n",
    "    return {\"benchmark_result\": aggregate_step.outputs.benchmark_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040fd94",
   "metadata": {},
   "source": [
    "## 7. Kick Off Pipeline Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3cff4",
   "metadata": {},
   "source": [
    "### 7.1. Pipeline Inputs\n",
    "In the previous sections, we have defined the `input_pattern`, `label_column_name` and `inference_data` parameters. We also need to define `model_name`, `model_version` and `task` parameters.\n",
    "\n",
    "The `task` parameter governs the task your model performs. Currently, we support `question-answering` and `chat`. In our example we ask the model to answer the questions. You may need to change this for your use case.\n",
    "\n",
    "The `model_name` parameter specifies the name under which the fine-tuned model will be registered. The `model_version` specifies the version if more than one models will be registered under the same name. If not specified, the latest version will be used. If you specify the version, make sure it is a string. For example, model version 5 should be entered as `\"5\"` and not `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a837d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name=\"aoai-proxy\"\n",
    "endpoint_subscription=\"72c03bf3-4e69-41af-9532-dfcdc3eefef4\"\n",
    "endpoint_resource_group=\"aml-benchmarking\"\n",
    "deployment_name= \"deployment_1\"\n",
    "task = \"chat\"  # \"question-answering\"\n",
    "model_name = \"gpt-35-turbo-0613\"\n",
    "suffix = \"testing\"\n",
    "model_name = \"gpt-35-turbo-0613\"\n",
    "suffix = \"testing\"\n",
    "n_epochs=1\n",
    "batch_size=8\n",
    "learning_rate_multiplier=1\n",
    "n_ctx=4096\n",
    "lora_dim=1\n",
    "weight_decay_multiplier=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac00c3",
   "metadata": {},
   "source": [
    "<!--- Another important input in the evaluation pipeline is the `endpoint_region`. It accepts a string of regions and iterates over them until it finds a region that has quota to run the pipeline. For illustration purposes we use the following regions. Your use case may be different and you will need to modify this list accordingly.\n",
    "\n",
    "# regions = \"eastus,westus,eastus2,southcentralus,centralus,northcentralus,australiaeast,canadaeast,francecentral,japaneast,swedencentral,uksouth\"\n",
    "# regions = None\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ce1b5",
   "metadata": {},
   "source": [
    "### 7.2. Kick Off the Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a2b7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: bright_floor_wshg70jhgg\n",
      "Web View: https://ml.azure.com/runs/bright_floor_wshg70jhgg?wsid=/subscriptions/72c03bf3-4e69-41af-9532-dfcdc3eefef4/resourcegroups/aml-benchmarking/workspaces/chirag-ws\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-05-16 11:31:09Z] Submitting 2 runs, first five are: 616794ef:7ef478b6-47ca-4ccc-bee6-5100a2d71df5,64829cec:b6fbbecd-3e0d-4db9-9c63-04c4b6d65fa6\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:233\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    232\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m--> 233\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_wait_before_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpoll_start_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m _current_details: RunDetails \u001b[38;5;241m=\u001b[39m run_operations\u001b[38;5;241m.\u001b[39mget_run_details(job_name)  \u001b[38;5;66;03m# TODO use FileWatcher\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 36\u001b[0m\n\u001b[0;32m     29\u001b[0m aoai_pipeline\u001b[38;5;241m.\u001b[39mtags \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     30\u001b[0m pipeline_submitted_job_base \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(\n\u001b[0;32m     31\u001b[0m     aoai_pipeline,\n\u001b[0;32m     32\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maoai-finetuning-with-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     skip_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     34\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcpu_compute_target,\n\u001b[0;32m     35\u001b[0m )\n\u001b[1;32m---> 36\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_submitted_job_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:645\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m--> 645\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:311\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails for canceling the run can be found here: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[0;32m    312\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[0;32m    313\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[0;32m    314\u001b[0m         no_personal_data_message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[0;32m    315\u001b[0m         error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[0;32m    316\u001b[0m     )\n",
      "\u001b[1;31mJobException\u001b[0m: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run"
     ]
    }
   ],
   "source": [
    "\"\"\"Uploading data with training_file_path\"\"\"\n",
    "aoai_pipeline = aoai_deployment_with_metrics(\n",
    "    training_file_path=training_file_path,\n",
    "    validation_file_path=validation_file_path,\n",
    "    inference_file_path=inference_file_path,\n",
    "    batch_input_pattern=batch_input_pattern,\n",
    "    model=model_name,\n",
    "    suffix=suffix,\n",
    "    deployment_name= deployment_name,\n",
    "    compute_name=cpu_compute_target,\n",
    "    label_column_name=label_column_name,\n",
    "    is_finetuned_model=True,\n",
    "    delete_managed_deployment=True,\n",
    "    find_first=find_first,\n",
    "    task_type=task,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate_multiplier=learning_rate_multiplier,       \n",
    "    n_ctx=n_ctx,\n",
    "    lora_dim=lora_dim,\n",
    "    weight_decay_multiplier=weight_decay_multiplier,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_subscription=endpoint_subscription,\n",
    "    endpoint_resource_group=endpoint_resource_group\n",
    ")\n",
    "\n",
    "aoai_pipeline.display_name = \"aoai-finetuning-with-data-asset\"\n",
    "aoai_pipeline.settings.default_compute = cpu_compute_target\n",
    "aoai_pipeline.tags = {}\n",
    "pipeline_submitted_job_base = ml_client.jobs.create_or_update(\n",
    "    aoai_pipeline,\n",
    "    experiment_name=\"aoai-finetuning-with-data\",\n",
    "    skip_validation=True,\n",
    "    compute=cpu_compute_target,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job_base.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "531e105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: nice_piano_jbfj9hf8r9\n",
      "Web View: https://ml.azure.com/runs/nice_piano_jbfj9hf8r9?wsid=/subscriptions/72c03bf3-4e69-41af-9532-dfcdc3eefef4/resourcegroups/aml-benchmarking/workspaces/chirag-ws\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-05-16 11:32:06Z] Submitting 2 runs, first five are: 6fd10535:fb60107e-0599-4158-b3c1-ea5147ae885d,cdec53d5:5cf4c50e-b94d-4ae9-9c59-7bad9108da5f\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:233\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    232\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m--> 233\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_wait_before_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpoll_start_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m _current_details: RunDetails \u001b[38;5;241m=\u001b[39m run_operations\u001b[38;5;241m.\u001b[39mget_run_details(job_name)  \u001b[38;5;66;03m# TODO use FileWatcher\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 36\u001b[0m\n\u001b[0;32m     29\u001b[0m aoai_pipeline\u001b[38;5;241m.\u001b[39mtags \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     30\u001b[0m pipeline_submitted_job_base \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(\n\u001b[0;32m     31\u001b[0m     aoai_pipeline,\n\u001b[0;32m     32\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maoai-finetuning-training-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     skip_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     34\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcpu_compute_target,\n\u001b[0;32m     35\u001b[0m )\n\u001b[1;32m---> 36\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_submitted_job_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:645\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m--> 645\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chiragbhatt\\Miniconda3\\envs\\notebook\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:311\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails for canceling the run can be found here: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[0;32m    312\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[0;32m    313\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[0;32m    314\u001b[0m         no_personal_data_message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[0;32m    315\u001b[0m         error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[0;32m    316\u001b[0m     )\n",
      "\u001b[1;31mJobException\u001b[0m: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run"
     ]
    }
   ],
   "source": [
    "\"\"\"Uploading data with training_file_key_uri\"\"\"\n",
    "aoai_pipeline = aoai_deployment_with_metrics(\n",
    "    training_import_path=training_import_path,\n",
    "    validation_import_path=validation_import_path,\n",
    "    inference_file_path=inference_file_path,\n",
    "    batch_input_pattern=batch_input_pattern,\n",
    "    model=model_name,\n",
    "    suffix=suffix,\n",
    "    deployment_name= deployment_name,\n",
    "    compute_name=cpu_compute_target,\n",
    "    label_column_name=label_column_name,\n",
    "    is_finetuned_model=True,\n",
    "    delete_managed_deployment=True,\n",
    "    find_first=find_first,\n",
    "    task_type=task,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate_multiplier=learning_rate_multiplier,       \n",
    "    n_ctx=n_ctx,\n",
    "    lora_dim=lora_dim,\n",
    "    weight_decay_multiplier=weight_decay_multiplier,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_subscription=endpoint_subscription,\n",
    "    endpoint_resource_group=endpoint_resource_group\n",
    ")\n",
    "\n",
    "aoai_pipeline.display_name = \"aoai-finetuning-with-data-uri\"\n",
    "aoai_pipeline.settings.default_compute = cpu_compute_target\n",
    "aoai_pipeline.tags = {}\n",
    "pipeline_submitted_job_base = ml_client.jobs.create_or_update(\n",
    "    aoai_pipeline,\n",
    "    experiment_name=\"aoai-finetuning-training-data\",\n",
    "    skip_validation=True,\n",
    "    compute=cpu_compute_target,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job_base.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(pipeline_submitted_job_base.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
