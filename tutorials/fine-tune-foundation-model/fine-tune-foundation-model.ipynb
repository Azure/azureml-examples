{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to fine tune a foundation model on Azure Machine Learning using SDK v2\n",
    "\n",
    "Fine tuning a [foundational model](https://learn.microsoft.com/en-us/azure/machine-learning/concept-foundation-models?view=azureml-api-2) has several advantages:\n",
    " \n",
    "* A foundation model may not be optimized for your specific use case, and fine tuning would allow you to customize it for your needs and better performance.\n",
    "* Fine tuning allows you to incorporate your own data into the model, resulting in better accuracy and more relevant results. \n",
    "* Training on your own data could also reduce bias and be more reflective of the unique characteristics of your domain. \n",
    "\n",
    "Ultimately, fine tuning gives you a competitive edge on your product. Customizing the model to your specific needs can make a big difference in your product experience. \n",
    "\n",
    "In this tutorial, you'll walk through the steps to fine tune a natural language processing (NLP) model to analyze sentiments expressed in single sentences written in English.  The tutorial uses the `emotion dataset` and `text-classification` components from the Azure Machine Learning system registry. \n",
    "\n",
    "By the end of this tutorial, you'll have the fine tuned model deployed to an online endpoint for real time inference, which can classify input texts into one of the six emotions: anger, fear, joy, love, sadness, and surprise.  Let's get started!  \n",
    "\n",
    "The steps are:\n",
    "\n",
    ">* Pick a model to fine tune\n",
    ">* Setup pre-requisites such as compute\n",
    ">* Pick and explore training data\n",
    ">* Configure & submit the fine tuning job\n",
    ">* Review training and evaluation metrics\n",
    ">* Register the fine tuned model\n",
    ">* Deploy the fine tuned model for real time inference\n",
    ">* Clean up resources\n",
    "\n",
    "**Training data**\n",
    "\n",
    "You'll use the [emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset. A copy of this dataset is available in the [emotion-dataset](./emotion-dataset/) folder. \n",
    "\n",
    "**Model**\n",
    "\n",
    "Models that can perform the `fill-mask` task are generally good foundation models to fine tune for `text-classification`. We will use the `bert-base-uncased` model in this notebook. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Open in studio and select a compute instance.\n",
    "    * If you opened this notebook from Azure Machine Learning studio, you need a compute instance to run the code. If you don't have a compute instance, select **Create compute** on the toolbar to first create one.  You can use all the default settings.  \n",
    "    \n",
    "    ![Screenshot shows how to create a compute instance.](../get-started-notebooks/media/create-compute.png)\n",
    "    \n",
    "    * If you're seeing this notebook elsewhere, complete [Create resources you need to get started](https://docs.microsoft.com/azure/machine-learning/quickstart-create-resources) to create an Azure Machine Learning workspace and a compute instance.\n",
    "    \n",
    "1. View your VM quota and ensure you have quota. In this tutorial, you will need at least some cores of a GPU compute, as well cores on a as Standard_DS3_v2 or higher. To view your VM quota usage and request quota increases, see [Manage resource quotas](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas#view-your-usage-and-quotas-in-the-azure-portal).  \n",
    "\n",
    "## Set your kernel\n",
    "\n",
    "* If your compute instance is stopped, start it now.  \n",
    "        \n",
    "    ![Screenshot shows where to start the compute instance.](../get-started-notebooks/media/start-compute.png)\n",
    "\n",
    "* Once your compute instance is running, make sure the that the kernel, found on the top right, is `Python 3.10 - SDK v2`.  If not, use the dropdown to select this kernel.\n",
    "\n",
    "    ![Screenshot shows setting the kernel.](../get-started-notebooks/media/set-kernel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a model to fine tune\n",
    "\n",
    "For `text-classification`, models that support `fill-mask` tasks are good candidates because they're pretrained language models that can understand the context of a given text and predict the missing words or tokens in it. This ability to understand the context of a text and predict missing words make `fill-mask` models highly effective in capturing the meaning of the text and identifying its underlying sentiment or emotion.\n",
    "\n",
    "Let's select a model to fine tune.\n",
    "\n",
    "1. Sign into [Azure Machine Learning studio](ml.azure.com)\n",
    "2. Select `model catalog` on the left navigation bar\n",
    "3. Search for `bert-base-uncased` on the model catalog\n",
    "4. Select the `bert-base-uncased` model to see the model card \n",
    "\n",
    "![Screenshot of the model catalog.](./media/model_catalog.png)\n",
    "\n",
    "On the model card, you can find the model name `bert-base-uncased`. This is the only reference you need in order to fine tune the model in your notebook using SDK v2. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your workstation for fine tuning\n",
    "\n",
    "Set up your workstation so you can use `Azure Machine Learning SDK v2` to fine tune the model. Follow these steps: \n",
    "\n",
    "### Install dependencies.\n",
    "\n",
    "Install dependencies by running the next cell. If running on a compute other than a compute instance, uncomment the commented installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets==2.9.0\n",
    "# when running this notebook on somewhere other than a compute instance, uncomment the following lines:\n",
    "# %pip install azure-ai-ml\n",
    "# %pip install azure-identity\n",
    "# %pip install mlflow\n",
    "# %pip install azureml-mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create handle to workspace\n",
    "Before we dive in the code, you need a way to reference your workspace. Create `ml_client` for a handle to the workspace. Then use `ml_client` to manage resources and jobs.\n",
    "\n",
    "In the next cell, enter your `Subscription ID`, `Resource Group` name and `Workspace` name. To find these values:\n",
    "\n",
    "- In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
    "- Copy the value for workspace, resource group and subscription ID into the code.\n",
    "- You'll need to copy one value, close the area and paste, then come back for the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    "    ClientSecretCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check or create a compute cluster\n",
    "\n",
    "For fine tuning tasks, you need a GPU compute cluster for the best results. The duration of the fine tuning depends on the capacity of the GPU SKU you choose. That is because a single GPU node can have multiple GPU cards. \n",
    "\n",
    "For example, in one node of `Standard_ND40rs_v2` there are eight NVIDIA GPUs. Meanwhile in `Standard_NC12s_v2` there are two NVIDIA V100 GPUs. When all GPUs in the node get utilized (by configuring the parameter in `gpus_per_node`), you get the most efficient fine tune run. You can read more about Azure's [GPU optimized VM offerings](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) and the recommended compute SKUs ([ncv3-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series), [ndv2-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series)).\n",
    "\n",
    "In this tutorial, you'll use `Standard_NC24rs_v3`.  The fine tuning job will take about 15-20 minutes to complete using this compute type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "compute_cluster = \"gpu-cluster-big\"\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "except Exception as ex:\n",
    "    compute = AmlCompute(\n",
    "        name=compute_cluster,\n",
    "        size=\"Standard_NC24rs_v3\",\n",
    "        max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "    )\n",
    "    workspace_ml_client.compute.begin_create_or_update(compute).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will find the the number of GPU's per node to use, based on your quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute.\n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpu_count_found = False\n",
    "workspace_compute_sku_list = workspace_ml_client.compute.list_sizes()\n",
    "available_sku_sizes = []\n",
    "for compute_sku in workspace_compute_sku_list:\n",
    "    available_sku_sizes.append(compute_sku.name)\n",
    "    if compute_sku.name.lower() == compute.size.lower():\n",
    "        gpus_per_node = compute_sku.gpus\n",
    "        gpu_count_found = True\n",
    "# if gpu_count_found not found, then print an error\n",
    "if gpu_count_found:\n",
    "    print(f\"Number of GPU's in compute {compute.size}: {gpus_per_node}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Number of GPU's in compute {compute.size} not found. Available skus are: {available_sku_sizes}.\"\n",
    "        f\"This should not happen. Please check the selected compute cluster: {compute_cluster} and try again.\"\n",
    "    )\n",
    "# CPU based finetune works only for single-node single-process\n",
    "if gpus_per_node == 0:\n",
    "    print(\n",
    "        \"WARNING! Selected compute doesn't have GPU. CPU based finetune is experimental and works on a single process in a single node\"\n",
    "    )\n",
    "    gpus_per_node = 1\n",
    "\n",
    "# genrating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to `azureml` system registry & import the model\n",
    "\n",
    "In order to access the preregistered foundation models hosted on the model catalog, you need to connect to `azureml` registry. Run the next cell to connect to the system registry and import the `bert-base-uncased` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "foundation_model = registry_ml_client.models.get(\n",
    "    model_name, label=\"latest\"\n",
    ")  # If you want to use a specific version of the model, use version=\"<version>\".\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set an optional experiment name\n",
    "This step is optional but useful if you want to find this fine tuning job easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"text-classification-emotion-detection\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset for fine-tuning the model\n",
    "\n",
    "There are two options to prepare the dataset for fine tuning. The first option is to choose the fine tune option on the model catalog where you found ` bert-base-uncased` model earlier. The second option is to prepare a dataset that matches your use case for fine tuning. This tutorial focuses on the second option.  \n",
    "\n",
    "You're going to use the [emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset. You can find a copy of this dataset in the emotion-dataset folder that came with this notebook. \n",
    "\n",
    "### Start by downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset using the helper script. This needs datasets library: https://pypi.org/project/datasets/\n",
    "import os\n",
    "\n",
    "exit_status = os.system(\"python ./download-dataset.py --download_dir emotion-dataset\")\n",
    "if exit_status != 0:\n",
    "    raise Exception(\"Error downloading dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some data rows\n",
    "It's important to understand the data and its features. Let's start by taking a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ./emotion-dataset/train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(\"./emotion-dataset/train.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace numerical categories in data with the actual string labels\n",
    "\n",
    "This data set uses numerical categories. For example, 0 refers to `sadness`. To get string labels such as `anger`, `joy`, etc., replace the categories. Run the next cell to get the string labels.\n",
    "\n",
    "You can see the detailed mapping in the [./emotion-dataset/label.json](./emotion-dataset/label.json). If you skip this step, the model returns numerical categories such as 0, 1, 2, etc. and you have to map them to what the category represents yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the id2label json element of the ./emotion-dataset/label.json file into pandas table with keys as 'label' column of int64 type and values as 'label_string' column as string type\n",
    "import json\n",
    "\n",
    "with open(\"./emotion-dataset/label.json\") as f:\n",
    "    id2label = json.load(f)\n",
    "    id2label = id2label[\"id2label\"]\n",
    "    label_df = pd.DataFrame.from_dict(\n",
    "        id2label, orient=\"index\", columns=[\"label_string\"]\n",
    "    )\n",
    "    label_df[\"label\"] = label_df.index.astype(\"int64\")\n",
    "    label_df = label_df[[\"label\", \"label_string\"]]\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test.jsonl, train.jsonl and validation.jsonl form the ./emotion-dataset folder into pandas dataframes\n",
    "test_df = pd.read_json(\"./emotion-dataset/test.jsonl\", lines=True)\n",
    "train_df = pd.read_json(\"./emotion-dataset/train.jsonl\", lines=True)\n",
    "validation_df = pd.read_json(\"./emotion-dataset/validation.jsonl\", lines=True)\n",
    "# join the train, validation and test dataframes with the id2label dataframe to get the label_string column\n",
    "train_df = train_df.merge(label_df, on=\"label\", how=\"left\")\n",
    "validation_df = validation_df.merge(label_df, on=\"label\", how=\"left\")\n",
    "test_df = test_df.merge(label_df, on=\"label\", how=\"left\")\n",
    "# show the first 5 rows of the train dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data\n",
    "Now the string labels are applied, let's save the dataset.\n",
    "\n",
    "For the fine tuning tutorial demonstration purposes, you're going to save a smaller dataset containing 10% of the original dataset into `train`, `validation` and `test` files. **Keep in mind that the fine tuned model will have lower accuracy, hence it should not be put to real-world use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10% of the rows from the train, validation and test dataframes into files with small_ prefix in the ./emotion-dataset folder\n",
    "frac = 1\n",
    "train_df.sample(frac=frac).to_json(\n",
    "    \"./emotion-dataset/small_train.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "validation_df.sample(frac=frac).to_json(\n",
    "    \"./emotion-dataset/small_validation.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "test_df.sample(frac=frac).to_json(\n",
    "    \"./emotion-dataset/small_test.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and submit the fine tuning job using the model and data as inputs\n",
    "\n",
    "To submit a fine tuning job using a foundation model, you're going to build a pipeline. There are two reasons for using a pipeline. \n",
    "\n",
    "First, since you're fine tuning an existing foundation model, you may not have access to the training code. Azure Machine Learning can generate the training code, which is hosted in the `azureml` registry, which requires using a pipeline. Second, fine tuning job requires several steps, including tokenization, converting English text to numeric representation, passing tokenized data to fine tune, and evaluation. It would make sense to componentize these discrete steps, building a pipeline.\n",
    "\n",
    "You're going to create a job that uses the `text-classification` pipeline component. \n",
    "\n",
    "This tutorial is fine tuning a model from the `azureml` system registery.  If you instead want to fine tune a model that is available on HuggingFace, but not available in `azureml` system registry, you can either [import](https://github.com/Azure/azureml-examples) the model or use the `huggingface_id` parameter to instruct the components to pull the model directly from [HuggingFace](https://huggingface.co). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"text_classification_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "    text_classification_pipeline = pipeline_component_func(\n",
    "        # specify the foundation model available in the azureml system registry id identified in step #3\n",
    "        mlflow_model_path=foundation_model.id,\n",
    "        # huggingface_id = 'bert-base-uncased', # if you want to use a huggingface model, uncomment this line and comment the above line\n",
    "        compute_model_import=compute_cluster,\n",
    "        compute_preprocess=compute_cluster,\n",
    "        compute_finetune=compute_cluster,\n",
    "        compute_model_evaluation=compute_cluster,\n",
    "        # map the dataset splits to parameters\n",
    "        train_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./emotion-dataset/small_train.jsonl\"\n",
    "        ),\n",
    "        validation_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./emotion-dataset/small_validation.jsonl\"\n",
    "        ),\n",
    "        test_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./emotion-dataset/small_test.jsonl\"\n",
    "        ),\n",
    "        evaluation_config=Input(\n",
    "            type=\"uri_file\", path=\"./text-classification-config.json\"\n",
    "        ),\n",
    "        # The following parameters map to the dataset fields\n",
    "        sentence1_key=\"text\",\n",
    "        label_key=\"label_string\",\n",
    "        # Training settings\n",
    "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        learning_rate=2e-5,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": text_classification_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pipeline job is configured, submit the job.  This will take approximately 15-20 minutes to complete, or longer depending on how many cores your GPU has available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> [!IMPORTANT]\n",
    "> Wait until the job completes before continuing with the next steps. When you run the cell, the notebook output shows a link to the job's details page on Azure Studio. Follow that link to check on status of the job.  Alternatively, you can select Jobs on the left navigation menu. to find the job.  Make sure the job status is complete before continuing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review training and evaluation metrics\n",
    "\n",
    "Now the pipeline job is submitted, you can view the job in Azure Machine Learning studio to analyze logs, metrics, and outputs of jobs. This way, you can create custom charts and compare metrics across different fine tuning jobs. See [View jobs/runs information in the studio](https://learn.microsoft.com/azure/machine-learning/how-to-log-view-metrics?tabs=interactive#view-jobsruns-information-in-the-studio) to learn more about job metrics.\n",
    "\n",
    "You may also want to programmatically log the same information so that it can be used by other services. In that case, use the following MLflow code, which is the recommended client for logging and querying metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, json\n",
    "\n",
    "mlflow_tracking_uri = workspace_ml_client.workspaces.get(\n",
    "    workspace_ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "# concat 'tags.mlflow.rootRunId=' and pipeline_job.name in single quotes as filter variable\n",
    "filter = \"tags.mlflow.rootRunId='\" + pipeline_job.name + \"'\"\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name], filter_string=filter, output_format=\"list\"\n",
    ")\n",
    "training_run = None\n",
    "evaluation_run = None\n",
    "# get the training and evaluation runs.\n",
    "# using a hacky way till 'Bug 2320997: not able to show eval metrics in FT notebooks - mlflow client now showing display names' is fixed\n",
    "for run in runs:\n",
    "    # check if run.data.metrics.epoch exists\n",
    "    if \"epoch\" in run.data.metrics:\n",
    "        training_run = run\n",
    "    # else, check if run.data.metrics.accuracy exists\n",
    "    elif \"accuracy\" in run.data.metrics:\n",
    "        evaluation_run = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_run:\n",
    "    print(\"Training metrics:\\n\\n\")\n",
    "    print(json.dumps(training_run.data.metrics, indent=2))\n",
    "else:\n",
    "    print(\"No Training job found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_run:\n",
    "    print(\"Evaluation metrics:\\n\\n\")\n",
    "    print(json.dumps(evaluation_run.data.metrics, indent=2))\n",
    "else:\n",
    "    print(\"No Evaluation job found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the fine tuned model with the workspace\n",
    "\n",
    "Register the model from the output of the fine tuning job. There are several benefits to register a fine tuned model to the Azure Machine Learning platform.\n",
    " \n",
    "- **Versioning & Traceability**: Tracks lineage between the fine tuned model and the fine tuning job. The fine tuning job, further, tracks lineage to the foundation model, data and training code.\n",
    "\n",
    "- **Reusability**: Once a model is registered, it can be reused across different experiments, pipelines, and deployments. This eliminates the need to recreate the model each time and saves time and effort.\n",
    "\n",
    "- **Collaboration**: Registered models can be easily shared with other team members, making it easier to collaborate on machine learning projects. This enables team members to work together on the same model and share their insights and feedback. \n",
    "\n",
    "- **Deployment**: Registered models can be easily deployed to production environments, making it easier to integrate machine learning models into business applications. Azure Machine Learning provides several deployment options, including Azure Kubernetes Service, Azure Container Instances, and Azure Functions. \n",
    "\n",
    "- **Monitoring**: Registered models can be monitored and evaluated over time to ensure that they continue to perform well in production environments. This enables you to detect and address issues early on and maintain the performance of your machine learning models.\n",
    "\n",
    "Use the following code to register the fine tuned model. Once registered, you can find the model under the Models tab of Azure Machine Learning studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# check if the `trained_model` output is available\n",
    "print(\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job.name).outputs)\n",
    "\n",
    "# fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
    "    pipeline_job.name, \"trained_model\"\n",
    ")\n",
    "\n",
    "finetuned_model_name = model_name + \"-emotion-detection\"\n",
    "finetuned_model_name = finetuned_model_name.replace(\"/\", \"-\")\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=finetuned_model_name,\n",
    "    version=timestamp,  # use timestamp as version to avoid version conflict\n",
    "    description=model_name + \" fine tuned model for emotion detection\",\n",
    ")\n",
    "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
    "# register the model from pipeline job output\n",
    "registered_model = workspace_ml_client.models.create_or_update(\n",
    "    prepare_to_register_model\n",
    ")\n",
    "print(\"registered model: \\n\", registered_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the fine tuned model to an online endpoint\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model. In this tutorial, you're going to use Managed Online Endpoint API, which handles many backend configurations for you.\n",
    "\n",
    "Let's start by creating an online endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "\n",
    "online_endpoint_name = \"emotion-\" + timestamp\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + registered_model.name\n",
    "    + \", fine tuned model for emotion detection\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying a model requires a compute resource. In this tutorial, you're going to use `Standard_DS3_v2` which takes about 15 minutes to complete the deployment. \n",
    "\n",
    "You can also read about [the list of other SKUs supported for deployment](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployment\n",
    "from azure.ai.ml.entities import ProbeSettings\n",
    "\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    "    liveness_probe=ProbeSettings(\n",
    "        initial_delay=600\n",
    "    ),  # wait for 10 minutes before probing, this is a large model\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> [!IMPORTANT]\n",
    "> Wait until the deployment completes before continuing with the next steps. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint with sample data\n",
    "\n",
    "Now the fine tuned model is deployed, we need to test if the model is working properly. You'll first fetch some sample data from the test dataset, and save as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ./emotion-dataset/small_test.jsonl into a pandas dataframe\n",
    "test_df = pd.read_json(\"./emotion-dataset/small_test.jsonl\", lines=True)\n",
    "# take a random sample of a row from the test dataframe\n",
    "test_df = test_df.sample(n=1)\n",
    "# rebuild index\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "# rename the label_string column to ground_truth_label\n",
    "test_df = test_df.rename(columns={\"label_string\": \"ground_truth_label\"})\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a json object with the key as \"inputs\" and value as a list of values from the text column of the test dataframe\n",
    "test_df_copy = test_df[[\"text\"]]\n",
    "test_df_copy.rename(columns = {'text':'input_string'}, inplace=True)\n",
    "test_json = {\"input_data\": test_df_copy.to_dict(\"split\")}\n",
    "# save the json object to a file named sample_score.json in the ./emotion-dataset folder\n",
    "with open(\"./emotion-dataset/sample_score.json\", \"w\") as f:\n",
    "    json.dump(test_json, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a sample data, let's test the online endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"demo\",\n",
    "    request_file=\"./emotion-dataset/sample_score.json\",\n",
    ")\n",
    "print(\"raw response: \\n\", response, \"\\n\")\n",
    "# convert the response to a pandas dataframe and rename the label column as scored_label\n",
    "response_df = pd.read_json(response)\n",
    "response_df = response_df.rename(columns={0: \"scored_label\"})\n",
    "response_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the test dataframe and the response dataframe on the index\n",
    "merged_df = pd.merge(test_df, response_df, left_index=True, right_index=True)\n",
    "merged_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the online endpoint\n",
    "Congratulation! You have completed the foundational model fine tuning tutorial.\n",
    "\n",
    "Don't forget to delete the online endpoint, else you'll leave the billing meter running for the compute used by the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code deletes the endpoint\n",
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
