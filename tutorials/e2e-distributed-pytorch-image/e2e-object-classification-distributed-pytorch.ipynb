{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1aadfd2",
   "metadata": {},
   "source": [
    "# Distributed PyTorch Image Classification\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial you should be able to use Azure Machine Learning (AzureML) to:\n",
    "- quickly implement basic commands for data preparation\n",
    "- assemble a pipeline with custom data preparation (python) scripts\n",
    "- test and run a multi-node multi-gpu pytorch job\n",
    "- use mlflow to analyze your metrics\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you need:\n",
    "- to have provisioned an AzureML workspace\n",
    "- to have permissions to provision a minimal cpu and gpu cluster\n",
    "\n",
    "**Motivations** - Let's consider the following scenario: we want to explore training different image classifiers on distinct kinds of problems, based on a large public dataset that is available at a given url. This ML pipeline will be future-looking, in particular we want:\n",
    "- **genericity**: to be fairly independent from the data we're ingesting (so that we could switch to internal proprietary data in the future),\n",
    "- **configurability**: to run different versions of that training with simple configuration changes,\n",
    "- **scalability**: to iterate on the pipeline on small sample, then smoothly transition to running at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641f516",
   "metadata": {},
   "source": [
    "### Connect to AzureML\n",
    "\n",
    "Before we dive in the code, we'll need to create an instance of MLClient to connect to Azure ML. Please provide the references to your workspace below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle to the workspace\n",
    "from azure.ml import MLClient\n",
    "\n",
    "# authentication package\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    InteractiveBrowserCredential(),\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3ce5e",
   "metadata": {},
   "source": [
    "### Provision the required resources for this notebook\n",
    "\n",
    "We'll need 2 clusters for this notebook, a CPU cluster and a GPU cluster. First, let's create a minimal cpu cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "# Let's create the Azure ML compute object with the intended parameters\n",
    "cpu_cluster = AmlCompute(\n",
    "    # Name assigned to the compute cluster\n",
    "    name=\"cpu-cluster\",\n",
    "    # Azure ML Compute is the on-demand VM service\n",
    "    type=\"amlcompute\",\n",
    "    # VM Family\n",
    "    size=\"STANDARD_DS3_V2\",\n",
    "    # Minimum running nodes when there is no job running\n",
    "    min_instances=0,\n",
    "    # Nodes in cluster\n",
    "    max_instances=4,\n",
    "    # How many seconds will the node running after the job termination\n",
    "    idle_time_before_scale_down=180,\n",
    "    # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "    tier=\"Dedicated\",\n",
    ")\n",
    "\n",
    "# Now, we pass the object to MLClient's create_or_update method\n",
    "cpu_cluster = ml_client.begin_create_or_update(cpu_cluster)\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481d489",
   "metadata": {},
   "source": [
    "For GPUs, we're creating the cluster below with the smallest VM family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute\n",
    "\n",
    "gpu_cluster = AmlCompute(\n",
    "    name=\"gpu-cluster\",\n",
    "    type=\"amlcompute\",\n",
    "    size=\"STANDARD_NC6\",  # 1 x NVIDIA Tesla K80\n",
    "    min_instances=0,\n",
    "    max_instances=4,\n",
    "    idle_time_before_scale_down=180,\n",
    "    tier=\"Dedicated\",\n",
    ")\n",
    "\n",
    "gpu_cluster = ml_client.begin_create_or_update(gpu_cluster)\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {gpu_cluster.name} is created, the compute size is {gpu_cluster.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405eae5b",
   "metadata": {},
   "source": [
    "# 1. Implement a reusable data preparation pipeline\n",
    "\n",
    "To develop our data preparation pipeline, there are a couple constraints that we're setting for ourselves:\n",
    "- we want to minimize the effort to ingest public data as it is used only as a learning opportunity,\n",
    "- we do not want to manipulate large data locally (ex: download/upload that data could take multiple hours),\n",
    "\n",
    "In this section, we'll achieve just that, by implementing the following:\n",
    "- a data ingestion and processing pipeline with simple shell commands (wget, unzip) using minimal boilerplate code,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ab207",
   "metadata": {},
   "source": [
    "## 1.1. Unzip archives with a simple command (no code)\n",
    "\n",
    "To train our classifier, we'll consume the [Common Objects in COntext (COCO) dataset](https://cocodataset.org/). If we were to use this locally, the sequence would be very basic: download 3 zip files, unzip each of them in a distinct folder for train/val/test, use python to extract annotations into a format we can use. We'll do just that, but in the cloud, without too much pain.\n",
    "\n",
    "The Azure ML SDK provides `entities` to implement any step of a workflow. In the example below, we create a `CommandComponent` with just a shell command. We parameterize this command by using a string template syntax provided by the SDK:\n",
    "\n",
    "> ```\n",
    "> curl -o local_archive.zip ${{inputs.url}} && unzip local_archive.zip -d ${{outputs.extracted_data}}\n",
    "> ```\n",
    "\n",
    "Creating the component just consists in declaring the names of the inputs, outputs, and specifying an environment. For this simple job we'll use a curated environment from AzureML. After that, we'll be able to reuse that component multiple times in our pipeline design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88190d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import CommandComponent, JobInput, JobOutput\n",
    "\n",
    "download_unzip_component = CommandComponent(\n",
    "    name=\"download_and_unzip\",  # optional: this will show in the UI\n",
    "    # this component has no code, just a simple unzip command\n",
    "    command=\"curl -o local_archive.zip ${{inputs.url}} && unzip local_archive.zip -d ${{outputs.extracted_data}}\",\n",
    "    # I/O specifications, each using a specific key and type\n",
    "    inputs={\n",
    "        # 'url' is the key of this input string\n",
    "        \"url\": {\"type\": \"string\"}\n",
    "    },\n",
    "    outputs={\n",
    "        # 'extracted_data' will be the key to link this output to other steps in the pipeline\n",
    "        \"extracted_data\": {\"type\": \"path\"}\n",
    "    },\n",
    "    # we're using a curated environment\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00903f3a",
   "metadata": {},
   "source": [
    "The component class we just created can now be loaded as a function in python. This function will be used to write a reusable step in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c17abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# we'll package this unzip command as a component to use within a pipeline\n",
    "download_unzip_component_func = dsl.load_component(component=download_unzip_component)\n",
    "\n",
    "?download_unzip_component_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a44973",
   "metadata": {},
   "source": [
    "## 1.2. Add a python script\n",
    "\n",
    "Next step in our pipeline is to implement a simple script to extract the annotations and format them for us. We've written that script in this repository, and it can be loaded as a component from its yaml specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ac9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "parse_annotations_func = dsl.load_component(\n",
    "    yaml_file=\"./src/coco_extract_annotations/spec.yaml\"\n",
    ")\n",
    "\n",
    "?parse_annotations_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe45054",
   "metadata": {},
   "source": [
    "## 1.3. Write a reusable pipeline\n",
    "\n",
    "We use the decorator `@dsl.pipeline` to construct an AzureML pipeline assembling the two components above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    # this will be the default compute on which each step below will run\n",
    "    compute=\"cpu-cluster\",\n",
    "    # a readable description\n",
    "    description=\"e2e images preparation\",\n",
    ")\n",
    "def coco_preparation_pipeline(\n",
    "    train_archive_url,\n",
    "    valid_archive_url,\n",
    "    test_archive_url,\n",
    "    annotations_archive_url,\n",
    "    category_id,\n",
    "    category_name,\n",
    "):\n",
    "    # 1st instance using the command component above on the training data\n",
    "    train_unzip_step = download_unzip_component_func(url=train_archive_url)\n",
    "\n",
    "    # 2nd instance for validation data\n",
    "    valid_unzip_step = download_unzip_component_func(url=valid_archive_url)\n",
    "\n",
    "    # 3rd instance for testing data\n",
    "    test_unzip_step = download_unzip_component_func(url=test_archive_url)\n",
    "\n",
    "    # 4th instance for the annotations\n",
    "    annotations_unzip_step = download_unzip_component_func(url=annotations_archive_url)\n",
    "\n",
    "    # add the annotations processing after the unzip command\n",
    "    parse_annotations_step = parse_annotations_func(\n",
    "        # here we consume the output of the unzip step\n",
    "        annotations_dir=annotations_unzip_step.outputs.extracted_data,\n",
    "        # parameters for this step are given as pipeline parameters\n",
    "        # to allow for genericity (no hardcoded value)\n",
    "        category_id=category_id,\n",
    "        category_name=category_name,\n",
    "    )\n",
    "\n",
    "    # outputs of this pipeline are coded as a dictionary\n",
    "    # keys can be used to assemble and link this pipeline with other pipelines\n",
    "    return {\n",
    "        \"train_images\": train_unzip_step.outputs.extracted_data,\n",
    "        \"valid_images\": valid_unzip_step.outputs.extracted_data,\n",
    "        \"test_images\": test_unzip_step.outputs.extracted_data,\n",
    "        \"train_annotations\": parse_annotations_step.outputs.train_annotations,\n",
    "        \"valid_annotations\": parse_annotations_step.outputs.valid_annotations,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d7718",
   "metadata": {},
   "source": [
    "The pipeline we just created, decorated by `@dsl.pipeline` can also be called from python, as a sub-pipeline within another pipeline, creating more complex workflows (we'll see in next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "?coco_preparation_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40f0d1",
   "metadata": {},
   "source": [
    "## 1.4.. Run an instance of this pipeline in AzureML\n",
    "\n",
    "When calling the pipeline function decorated with `@dsl.pipeline`, we will create an instance of this pipeline with the given arguments. In this scenario, we just provide the urls to the zip files we want to process, and the category of the objects we plan to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = coco_preparation_pipeline(\n",
    "    train_archive_url=\"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    valid_archive_url=\"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    test_archive_url=\"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "    annotations_archive_url=\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    category_id=1,\n",
    "    category_name=\"contains_person\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232807f6",
   "metadata": {},
   "source": [
    "That instance can be submitted to AzureML and run there. Use the `MLClient` to create the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149a504",
   "metadata": {},
   "source": [
    "Considering the size of the dataset, this job will take a couple hours to complete. The validation and annotations dataset are smaller, and should take a couple minutes only to unzip. So while we wait for the training dataset (110k+ images) to finalize, you can already go into AzureML and register the outputs of the pipeline as datasets (see below).\n",
    "\n",
    "<span style=\"color:red\">IMPORTANT</span> - To move forward with the next section, we'll need you to:\n",
    "- register the output of \"train_unzip_step\" as dataset \"coco_train2017\"\n",
    "- register the output of \"valid_unzip_step\" as dataset \"coco_val2017\"\n",
    "- (when available) register the 1st output of \"Extract Coco Annotations\" as dataset \"coco_train2017_annotations\"\n",
    "- register the 2nd output of \"Extract Coco Annotations\" as dataset \"coco_val2017_annotations\"\n",
    "\n",
    "![](media/image-prep-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26664b",
   "metadata": {},
   "source": [
    "# 2. Training a distributed gpu job\n",
    "\n",
    "Implementing a distributed pytorch training is complex. Of course in this tutorial we've written one for you, but the point is: it takes time, it takes several iterations, each requiring you to try your code locally, then in the cloud, then try it at scale, until satisfied and then run a full blown production model training. This trial/error process can be made easier if we can create reusable code we can iterate on quickly, and that can be configured to run from small to large scale.\n",
    "\n",
    "So, to develop our training pipeline, we set a couple constraints for ourselves:\n",
    "- we want to minimize the effort to iterate on the pipeline code when porting it in the cloud,\n",
    "- we want to use the same code for small scale and large scale testing (so that we don't do not want to manipulate large data locally (ex: download/upload that data could take multiple hours),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16c184",
   "metadata": {},
   "source": [
    "## 2.1. Implement training and test on a sample\n",
    "\n",
    "We've implemented a distributed pytorch training script that we can load as a component using its yaml specification, like we did for other components before.\n",
    "\n",
    "Writing a pipeline to run it will be greatly simplified by the Azure ML SDK `@dsl.pipeline` decorator. For this, we've decided to parameterize this pipeline with relevant arguments (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# load the component from its yaml specifications\n",
    "training_func = dsl.load_component(yaml_file=\"./src/pytorch_dl_train/spec.yaml\")\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "@dsl.pipeline(\n",
    "    description=\"e2e images classification\",  # TODO: document\n",
    ")\n",
    "def coco_model_training(\n",
    "    train_images,  # dataset containing training images\n",
    "    valid_images,  # dataset containing validation images\n",
    "    train_annotations,  # annotations in CSV (see coco_extract_annotations/)\n",
    "    valid_annotations,  # annotations in CSV (see coco_extract_annotations/)\n",
    "    model_name,  # a name to register the model after training\n",
    "    epochs,  # the number of epochs\n",
    "    enable_profiling,  # bonus: we've implemented pytorch profiling in our script\n",
    "):\n",
    "    # the training step is calling our training component with the right arguments\n",
    "    training_step = training_func(\n",
    "        # inputs\n",
    "        train_images=train_images,\n",
    "        valid_images=valid_images,\n",
    "        train_annotations=train_annotations,\n",
    "        valid_annotations=valid_annotations,\n",
    "        # params (some hardcoded, some given by pipeline parameters)\n",
    "        num_epochs=epochs,\n",
    "        register_model_as=model_name,\n",
    "        num_workers=-1,  # use all cpus (see train.py)\n",
    "        enable_profiling=enable_profiling,  # turns on profiler (see train.py)\n",
    "    )\n",
    "    # we set the name of the compute target for this training job\n",
    "    training_step.compute = \"gpu-cluster\"\n",
    "\n",
    "    # use process_count_per_instance to parallelize on multiple gpus\n",
    "    training_step.distribution.process_count_per_instance = (\n",
    "        1  # set to number of gpus on instance\n",
    "    )\n",
    "\n",
    "    # use instance_count to increase the number of nodes (machines)\n",
    "    training_step.resources.instance_count = 1\n",
    "\n",
    "    # outputs of this pipeline are coded as a dictionary\n",
    "    # keys can be used to assemble and link this pipeline with other pipelines\n",
    "    return {\"model\": training_step.outputs.trained_model}\n",
    "\n",
    "\n",
    "# TODO: document\n",
    "?coco_model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e172ac",
   "metadata": {},
   "source": [
    "We can now test this code by running it on a smaller dataset in Azure ML. Here, we will use the validation set for training. Of course, the model will not be valid. But training will be short (8 mins on STANDARD_NC6 for 1 epoch) to allow us to iterate if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = coco_model_training(\n",
    "    # inputs: using validation set for training makes model invalid\n",
    "    train_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    valid_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    train_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=1),\n",
    "    valid_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=1),\n",
    "    # training parameters surfaced in the pipeline definition\n",
    "    epochs=1,  # 1 epoch only for testing, model isn't valid anyway\n",
    "    model_name=\"coco_model_person_dev\",\n",
    "    enable_profiling=False,  # turns on profiler (see train.py)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b9c5f",
   "metadata": {},
   "source": [
    "Once we create that pipeline instance, we submit it through `MLClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)\n",
    "# open the browser with this url\n",
    "webbrowser.open(returned_job.services[\"Studio\"].endpoint)\n",
    "\n",
    "# print the pipeline run id\n",
    "print(\n",
    "    f\"The pipeline details can be access programmatically using identifier: {returned_job.name}\"\n",
    ")\n",
    "# saving it for later in this notebook\n",
    "small_scale_pipeline_id = returned_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf19ded",
   "metadata": {},
   "source": [
    "You can iterate on this design as much as you'd like, updating the local code of the component and re-submit the pipeline. You would have to go back to the beginning of this section (from the `dsl.load_component()` call) in order to refresh the pipeline definition with a new version of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d8b0f",
   "metadata": {},
   "source": [
    "## 2.2. Reuse the same pipeline on full scale data\n",
    "\n",
    "Once the pipeline is satisfying, running it full scale is actually just adjusting the inputs and parameters. We're creating a new instance below where we use training images as training set (as we should). If running on a minimal STANDARD_NC6, it should take approx < 2h30 to complexe 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = coco_model_training(\n",
    "    train_images=ml_client.datasets.get(\"coco_train2017\", version=1),\n",
    "    valid_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    train_annotations=ml_client.datasets.get(\"coco_train2017_annotations\", version=1),\n",
    "    valid_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=1),\n",
    "    epochs=10,\n",
    "    model_name=\"coco_model_person_full\",\n",
    "    enable_profiling=False,  # turns off profiler (see train.py)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)\n",
    "# open the browser with this url\n",
    "webbrowser.open(returned_job.services[\"Studio\"].endpoint)\n",
    "\n",
    "# print the pipeline run id\n",
    "print(\n",
    "    f\"The pipeline details can be access programmatically using identifier: {returned_job.name}\"\n",
    ")\n",
    "# saving it for later in this notebook\n",
    "large_scale_pipeline_id = returned_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782c68e",
   "metadata": {},
   "source": [
    "# 3. Analyze experiments using MLFlow\n",
    "\n",
    "Azure ML natively integrates with MLFlow so that if your code already supports MLFlow logging, you will not have to modify it to report your metrics within Azure ML. The component above is using MLFlow internally to report relevant metrics, logs and artifacts. Look for `mlflow` calls within the script `train.py`.\n",
    "\n",
    "To access this data in the Azure ML Studio, click on the component in the pipeline to open the Details panel, then choose the **Metrics** panel.\n",
    "\n",
    "You can also access those metrics programmatically using mlflow. We'll demo a couple examples below.\n",
    "\n",
    "## 3.1. Connect to Azure ML using MLFlow client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af554a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlflow.set_tracking_uri(ml_client.workspaces.get().mlflow_tracking_uri)\n",
    "\n",
    "# search for the training step within the pipeline\n",
    "mlflow.set_experiment(\"e2e_image_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fc568",
   "metadata": {},
   "source": [
    "## 3.2. Analyze the metrics of a specific jog\n",
    "\n",
    "Using MLFlow, you can retrive all the metrics produces by a given run. You can then leverage any usual tool to draw the analysis that is relevant for you. In the example below, we're plotting accuracy per epoch.\n",
    "\n",
    "![plot training and validation accuracy over epochs](./media/pytorch_train_mlflow_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we're using the small scale training on validation data\n",
    "training_pipeline_id = small_scale_pipeline_id\n",
    "# feel free to adapt with a pipeline id of yours\n",
    "# training_pipeline_id = \"...\"\n",
    "\n",
    "# use this to get the id of the training step within the pipeline\n",
    "training_step_run_id = mlflow.search_runs(\n",
    "    filter_string=f\"tags.mlflow.parentRunId = '{training_pipeline_id}'\"\n",
    ")[\"run_id\"][0]\n",
    "\n",
    "# alternatively, you can directly use a known training step id\n",
    "# training_step_run_id = \"...\"\n",
    "\n",
    "print(f\"Withiin pipeline run id: {training_pipeline_id}\")\n",
    "print(f\"Training step has run id: {training_step_run_id}\")\n",
    "\n",
    "# open a client to get metric history\n",
    "client = MlflowClient()\n",
    "\n",
    "# create a plot\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"epoch\")\n",
    "\n",
    "for metric in [\"epoch_train_acc\", \"epoch_valid_acc\"]:\n",
    "    # get all values taken by the metric\n",
    "    metric_history = client.get_metric_history(training_step_run_id, metric)\n",
    "\n",
    "    epochs = [metric_entry.step for metric_entry in metric_history]\n",
    "    metric_array = [metric_entry.value for metric_entry in metric_history]\n",
    "    ax.plot(epochs, metric_array, label=metric)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb98d44",
   "metadata": {},
   "source": [
    "## 3.2. Retrieve artifacts for local analysis (ex: tensorboard)\n",
    "\n",
    "MLFlow also allows you to record artifacts during training. The script `train.py` leverages the [PyTorch profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) to produce logs for analyzing GPU performance. It uses mlflow to record those logs as artifacts.\n",
    "\n",
    "In the following, we'll download those locally to inspect with other tools such as tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2796344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we're using the small scale training on validation data\n",
    "training_pipeline_id = small_scale_pipeline_id\n",
    "# feel free to adapt with a pipeline id of yours\n",
    "# training_pipeline_id = \"...\"\n",
    "\n",
    "# use this to get the id of the training step within the pipeline\n",
    "training_step_run_id = mlflow.search_runs(\n",
    "    filter_string=f\"tags.mlflow.parentRunId = '{training_pipeline_id}'\"\n",
    ")[\"run_id\"][0]\n",
    "\n",
    "# alternatively, you can directly use a known training step id\n",
    "# training_step_run_id = \"...\"\n",
    "\n",
    "print(f\"Withiin pipeline run id: {training_pipeline_id}\")\n",
    "print(f\"Training step has run id: {training_step_run_id}\")\n",
    "\n",
    "# open a client to get run artifacts\n",
    "client = MlflowClient()\n",
    "\n",
    "# create local directory to store artefacts\n",
    "os.makedirs(\"./logs/\", exist_ok=True)\n",
    "\n",
    "for artifact in client.list_artifacts(training_step_run_id, path=\"profiler/markdown/\"):\n",
    "    print(f\"Downloading artifact {artifact.path}\")\n",
    "    client.download_artifacts(\n",
    "        training_step_run_id, path=artifact.path, dst_path=\"./logs\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"No artefacts were found for profiler/markdown/ in run id {training_step_run_id}\"\n",
    "    )\n",
    "\n",
    "for artifact in client.list_artifacts(\n",
    "    training_step_run_id, path=\"profiler/tensorboard_logs/\"\n",
    "):\n",
    "    print(f\"Downloading artifact {artifact.path}\")\n",
    "    client.download_artifacts(\n",
    "        training_step_run_id, path=artifact.path, dst_path=\"./logs\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"No artefacts were found for profiler/markdown/ in run id {training_step_run_id}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a3912",
   "metadata": {},
   "source": [
    "We can now run tensorboard locally with the downloaded artifacts to run some analysis of GPU performance (see example snapshot below).\n",
    "\n",
    "```\n",
    "tensorboard --logdir=\"./logs/profiler/tensorboard_logs/\"\n",
    "```\n",
    "\n",
    "![tensorboard logs generated by pytorch profiler](./media/pytorch_train_tensorboard_logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bea13",
   "metadata": {},
   "source": [
    "## 3.3. Analyze metrics accross multiple jobs\n",
    "\n",
    "You can also use mlflow to search all your runs, filter by some specific properties and get the results as a pandas dataframe. Once you get that dataframe, you can implement any analysis on top of it.\n",
    "\n",
    "Below, we're extracting all runs and show the effect of profiling on the epoch training time.\n",
    "\n",
    "![mlflow runs in a pandas dataframe](./media/pytorch_train_mlflow_runs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = mlflow.search_runs(\n",
    "    # we're using mlflow syntax to restrict to a specific parameter\n",
    "    filter_string=f\"params.model_arch = 'resnet18'\"\n",
    ")\n",
    "\n",
    "# we're keeping only some relevant columns\n",
    "columns = [\n",
    "    \"run_id\",\n",
    "    \"status\",\n",
    "    \"end_time\",\n",
    "    \"metrics.epoch_train_time\",\n",
    "    \"metrics.epoch_train_acc\",\n",
    "    \"metrics.epoch_valid_acc\",\n",
    "    \"params.enable_profiling\",\n",
    "]\n",
    "\n",
    "# showing the raw results in notebook\n",
    "runs[columns].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f658d73",
   "metadata": {},
   "source": [
    "![](media/mlflow_plot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
