{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deploymentconfig.json\n",
    "{\n",
    "    \"computeType\": \"aks\",\n",
    "    \"containerResourceRequirements\": {\"cpu\": 1, \"memoryInGB\": 4},\n",
    "    \"gpuCores\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bidaf_utils.py\n",
    "\"\"\"score_bidaf.py\n",
    "\n",
    "Scoring script for use with the Bi-directional Attention Flow model from the ONNX model zoo.\n",
    "https://github.com/onnx/models/tree/master/text/machine_comprehension/bidirectional_attention_flow\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from utils import get_model_info, parse_model_http, triton_init, triton_infer\n",
    "from tritonclientutils import triton_to_np_dtype\n",
    "\n",
    "\n",
    "def preprocess(text, dtype):\n",
    "    \"\"\"Tokenizes text for use in the bidirectional attention flow model\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    text : str\n",
    "        Text to be tokenized\n",
    "\n",
    "    dtype : numpy datatype\n",
    "        Datatype of the resulting array\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    (np.array(), np.array())\n",
    "        Tuple containing two numpy arrays with the tokenized\n",
    "        words and chars, respectively.\n",
    "\n",
    "    From: https://github.com/onnx/models/tree/master/text/machine_comprehension/bidirectional_attention_flow  # noqa\n",
    "    \"\"\"\n",
    "    nltk.download(\"punkt\")\n",
    "    tokens = word_tokenize(text)\n",
    "    # split into lower-case word tokens, in numpy array with shape of (seq, 1)\n",
    "    words = np.array([w.lower() for w in tokens], dtype=dtype).reshape(-1, 1)\n",
    "    # split words into chars, in numpy array with shape of (seq, 1, 1, 16)\n",
    "    chars = [[c for c in t][:16] for t in tokens]\n",
    "    chars = [cs + [\"\"] * (16 - len(cs)) for cs in chars]\n",
    "    chars = np.array(chars, dtype=dtype).reshape(-1, 1, 1, 16)\n",
    "    return words, chars\n",
    "\n",
    "\n",
    "def postprocess(context_words, answer):\n",
    "    \"\"\"Post-process results to show the chosen result\n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "    context_words : str\n",
    "        Original context\n",
    "\n",
    "    answer : InferResult\n",
    "        Triton inference result containing start and\n",
    "        end positions of desired answer\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    Numpy array containing the words from the context that\n",
    "    answer the given query.\n",
    "    \"\"\"\n",
    "\n",
    "    start = answer.as_numpy(\"start_pos\")[0]\n",
    "    end = answer.as_numpy(\"end_pos\")[0]\n",
    "    print(f\"start is {start}, end is {end}\")\n",
    "    return [w.encode() for w in context_words[start : end + 1].reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_utils.py\n",
    "\"\"\"download_models\n",
    "\n",
    "Downloads models needed for Triton example notebooks.\n",
    "\"\"\"\n",
    "import os\n",
    "import urllib\n",
    "from azure.storage.blob import BlobClient\n",
    "\n",
    "\n",
    "model_names = [\"densenet_onnx\", \"bidaf-9\"]\n",
    "\n",
    "\n",
    "def download_triton_models(prefix):\n",
    "    for model in model_names:\n",
    "        folder_path, model_file_path = _generate_paths(model, prefix)\n",
    "        url = f\"https://aka.ms/{model}-model\"\n",
    "        _download_model(model_file_path, folder_path, url)\n",
    "        print(f\"successfully downloaded model: {model}\")\n",
    "\n",
    "\n",
    "def delete_triton_models(prefix):\n",
    "    for model in model_names:\n",
    "        _, model_file_path = _generate_paths(model, prefix)\n",
    "        try:\n",
    "            os.remove(model_file_path)\n",
    "            print(f\"successfully deleted model: {model}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"model: {model} was already deleted\")\n",
    "\n",
    "\n",
    "def _download_model(model_file_path, folder_path, url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "\n",
    "    blob_client = BlobClient.from_blob_url(response.url)\n",
    "\n",
    "    # save the model if it does not already exist\n",
    "    if not os.path.exists(model_file_path):\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        with open(model_file_path, \"wb\") as my_blob:\n",
    "            download_stream = blob_client.download_blob()\n",
    "            my_blob.write(download_stream.readall())\n",
    "\n",
    "\n",
    "def _generate_paths(model, prefix):\n",
    "    folder_path = prefix.joinpath(\"models\", \"triton\", model, \"1\")\n",
    "    model_file_path = prefix.joinpath(folder_path, \"model.onnx\")\n",
    "    return folder_path, model_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile onnxruntimetrion.py\n",
    "\"\"\"\n",
    "onnxruntimetriton\n",
    "\n",
    "Offers the class InferenceSession which can be used as a drop-in replacement for the ONNX Runtime\n",
    "session object.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tritonclient.http as tritonhttpclient\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NodeArg:\n",
    "    def __init__(self, name, shape):\n",
    "        self.name = name\n",
    "        self.shape = shape\n",
    "\n",
    "\n",
    "class InferenceSession:\n",
    "    def __init__(self, path_or_bytes, sess_options=None, providers=[]):\n",
    "        self.client = tritonhttpclient.InferenceServerClient(\"localhost:8000\")\n",
    "        model_metadata = self.client.get_model_metadata(\n",
    "            model_name=path_or_bytes\n",
    "        )\n",
    "\n",
    "        self.request_count = 0\n",
    "        self.model_name = path_or_bytes\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.dtype_mapping = {}\n",
    "\n",
    "        for (src, dest) in (\n",
    "            (model_metadata[\"inputs\"], self.inputs),\n",
    "            (model_metadata[\"outputs\"], self.outputs),\n",
    "        ):\n",
    "            for element in src:\n",
    "                dest.append(NodeArg(element[\"name\"], element[\"shape\"]))\n",
    "                self.dtype_mapping[element[\"name\"]] = element[\"datatype\"]\n",
    "\n",
    "        self.triton_enabled = True\n",
    "\n",
    "    def get_inputs(self):\n",
    "        return self.inputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        return self.outputs\n",
    "\n",
    "    def run(self, output_names, input_feed, run_options=None):\n",
    "        inputs = []\n",
    "        for key, val in input_feed.items():\n",
    "            val = np.expand_dims(val, axis=0)\n",
    "            input = tritonhttpclient.InferInput(\n",
    "                key, val.shape, self.dtype_mapping[key]\n",
    "            )\n",
    "            input.set_data_from_numpy(val)\n",
    "            inputs.append(input)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for output_name in output_names:\n",
    "            output = tritonhttpclient.InferRequestedOutput(output_name)\n",
    "            outputs.append(output)\n",
    "\n",
    "        res = self.client.infer(\n",
    "            self.model_name,\n",
    "            inputs,\n",
    "            request_id=str(self.request_count),\n",
    "            outputs=outputs,\n",
    "        )\n",
    "        results = []\n",
    "        for output_name in output_names:\n",
    "            results.append(res.as_numpy(output_name))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score_densenet.py\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.contrib.services.aml_request import rawhttp\n",
    "from azureml.contrib.services.aml_response import AMLResponse\n",
    "from PIL import Image\n",
    "from utils import get_model_info, parse_model_http, triton_init, triton_infer\n",
    "from onnxruntimetriton import InferenceSession\n",
    "\n",
    "\n",
    "def preprocess(img, scaling):  # , dtype):\n",
    "    \"\"\"Pre-process an image to meet the size, type and format\n",
    "    requirements specified by the parameters.\n",
    "    \"\"\"\n",
    "    c = 3\n",
    "    h = 224\n",
    "    w = 224\n",
    "    format = \"FORMAT_NCHW\"\n",
    "\n",
    "    if c == 1:\n",
    "        sample_img = img.convert(\"L\")\n",
    "    else:\n",
    "        sample_img = img.convert(\"RGB\")\n",
    "\n",
    "    resized_img = sample_img.resize((w, h), Image.BILINEAR)\n",
    "    resized = np.array(resized_img)\n",
    "    if resized.ndim == 2:\n",
    "        resized = resized[:, :, np.newaxis]\n",
    "\n",
    "    # npdtype = triton_to_np_dtype(dtype)\n",
    "    typed = resized.astype(np.float32)\n",
    "    # typed = resized\n",
    "\n",
    "    if scaling == \"INCEPTION\":\n",
    "        scaled = (typed / 128) - 1\n",
    "    elif scaling == \"VGG\":\n",
    "        if c == 1:\n",
    "            scaled = typed - np.asarray((128,), dtype=npdtype)\n",
    "        else:\n",
    "            scaled = typed - np.asarray((123, 117, 104), dtype=npdtype)\n",
    "    else:\n",
    "        scaled = typed\n",
    "\n",
    "    # Swap to CHW if necessary\n",
    "    if format == \"FORMAT_NCHW\":\n",
    "        ordered = np.transpose(scaled, (2, 0, 1))\n",
    "    else:\n",
    "        ordered = scaled\n",
    "\n",
    "    # Channels are in RGB order. Currently model configuration data\n",
    "    # doesn't provide any information as to other channel orderings\n",
    "    # (like BGR) so we just assume RGB.\n",
    "    return ordered\n",
    "\n",
    "\n",
    "def postprocess(output_array):\n",
    "    \"\"\"Post-process results to show the predicted label.\"\"\"\n",
    "\n",
    "    output_array = output_array[0]\n",
    "    max_label = np.argmax(output_array)\n",
    "    final_label = label_dict[max_label]\n",
    "    return f\"{max_label} : {final_label}\"\n",
    "\n",
    "\n",
    "def init():\n",
    "    global session, label_dict\n",
    "    session = InferenceSession(path_or_bytes=\"densenet_onnx\")\n",
    "\n",
    "    model_dir = os.path.join(os.environ[\"AZUREML_MODEL_DIR\"], \"models\")\n",
    "    folder_path = os.path.join(model_dir, \"triton\", \"densenet_onnx\")\n",
    "    label_path = os.path.join(\n",
    "        model_dir, \"triton\", \"densenet_onnx\", \"densenet_labels.txt\"\n",
    "    )\n",
    "    label_file = open(label_path, \"r\")\n",
    "    labels = label_file.read().split(\"\\n\")\n",
    "    label_dict = dict(enumerate(labels))\n",
    "\n",
    "\n",
    "@rawhttp\n",
    "def run(request):\n",
    "    \"\"\"This function is called every time your webservice receives a request.\n",
    "\n",
    "    Notice you need to know the names and data types of the model inputs and\n",
    "    outputs. You can get these values by reading the model configuration file\n",
    "    or by querying the model metadata endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        outputs = []\n",
    "\n",
    "        for output in session.get_outputs():\n",
    "            outputs.append(output.name)\n",
    "\n",
    "        input_name = session.get_inputs()[0].name\n",
    "\n",
    "        reqBody = request.get_data(False)\n",
    "        img = Image.open(io.BytesIO(reqBody))\n",
    "        image_data = preprocess(img, scaling=\"INCEPTION\")\n",
    "\n",
    "        res = session.run(outputs, {input_name: image_data})\n",
    "\n",
    "        result = postprocess(output_array=res)\n",
    "\n",
    "        return AMLResponse(result, 200)\n",
    "    else:\n",
    "        return AMLResponse(\"bad request\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_service.py\n",
    "\"\"\"test_service.py\n",
    "\n",
    "Sends a specified image from the data directory to a deployed ML model\n",
    "and returns the result.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Test a deployed endpoint.\")\n",
    "    parser.add_argument(\n",
    "        \"--endpoint_name\",\n",
    "        type=str,\n",
    "        default=\"triton-densenet-onnx\",\n",
    "        help=\"name of the endpoint to test\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_file\",\n",
    "        type=str,\n",
    "        default=\"../../data/raw/triton/peacock.jpg\",\n",
    "        help=\"filename to run through the classifier\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ws = Workspace.from_config()\n",
    "    aks_service = AksWebservice(ws, args.endpoint_name)\n",
    "\n",
    "    # if (key) auth is enabled, fetch keys and include in the request\n",
    "    key1, _ = aks_service.get_keys()\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/octet-stream\",\n",
    "        \"Authorization\": \"Bearer \" + key1,\n",
    "    }\n",
    "\n",
    "    file_name = os.path.join(\n",
    "        os.path.abspath(os.path.dirname(__file__)),\n",
    "        \"..\",\n",
    "        \"data\",\n",
    "        args.data_file,\n",
    "    )\n",
    "    test_sample = open(file_name, \"rb\").read()\n",
    "    resp = requests.post(aks_service.scoring_uri, test_sample, headers=headers)\n",
    "    print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tritonhttpclient.py\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions\n",
    "# are met:\n",
    "#  * Redistributions of source code must retain the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer.\n",
    "#  * Redistributions in binary form must reproduce the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer in the\n",
    "#    documentation and/or other materials provided with the distribution.\n",
    "#  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived\n",
    "#    from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
    "# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
    "# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
    "# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
    "# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
    "# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
    "# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
    "# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "from geventhttpclient import HTTPClient\n",
    "from geventhttpclient.url import URL\n",
    "\n",
    "from urllib.parse import quote, quote_plus\n",
    "import rapidjson as json\n",
    "import numpy as np\n",
    "import gevent\n",
    "import gevent.pool\n",
    "import struct\n",
    "\n",
    "from tritonclientutils import *\n",
    "\n",
    "\n",
    "def _get_error(response):\n",
    "    \"\"\"\n",
    "    Returns the InferenceServerException object if response\n",
    "    indicates the error. If no error then return None\n",
    "    \"\"\"\n",
    "    if response.status_code != 200:\n",
    "        error_response = json.loads(response.read())\n",
    "        return InferenceServerException(msg=error_response[\"error\"])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _raise_if_error(response):\n",
    "    \"\"\"\n",
    "    Raise InferenceServerException if received non-Success\n",
    "    response from the server\n",
    "    \"\"\"\n",
    "    error = _get_error(response)\n",
    "    if error is not None:\n",
    "        raise error\n",
    "\n",
    "\n",
    "def _get_query_string(query_params):\n",
    "    params = []\n",
    "    for key, value in query_params.items():\n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                params.append(\n",
    "                    \"%s=%s\" % (quote_plus(key), quote_plus(str(item)))\n",
    "                )\n",
    "        else:\n",
    "            params.append(\"%s=%s\" % (quote_plus(key), quote_plus(str(value))))\n",
    "    if params:\n",
    "        return \"&\".join(params)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _get_inference_request(\n",
    "    inputs,\n",
    "    request_id,\n",
    "    outputs,\n",
    "    sequence_id,\n",
    "    sequence_start,\n",
    "    sequence_end,\n",
    "    priority,\n",
    "    timeout,\n",
    "):\n",
    "    infer_request = {}\n",
    "    parameters = {}\n",
    "    if request_id != \"\":\n",
    "        infer_request[\"id\"] = request_id\n",
    "    if sequence_id != 0:\n",
    "        parameters[\"sequence_id\"] = sequence_id\n",
    "        parameters[\"sequence_start\"] = sequence_start\n",
    "        parameters[\"sequence_end\"] = sequence_end\n",
    "    if priority != 0:\n",
    "        parameters[\"priority\"] = priority\n",
    "    if timeout is not None:\n",
    "        parameters[\"timeout\"] = timeout\n",
    "\n",
    "    infer_request[\"inputs\"] = [\n",
    "        this_input._get_tensor() for this_input in inputs\n",
    "    ]\n",
    "    if outputs:\n",
    "        infer_request[\"outputs\"] = [\n",
    "            this_output._get_tensor() for this_output in outputs\n",
    "        ]\n",
    "\n",
    "    if parameters:\n",
    "        infer_request[\"parameters\"] = parameters\n",
    "\n",
    "    request_body = json.dumps(infer_request)\n",
    "    json_size = len(request_body)\n",
    "    binary_data = None\n",
    "    for input_tensor in inputs:\n",
    "        raw_data = input_tensor._get_binary_data()\n",
    "        if raw_data is not None:\n",
    "            if binary_data is not None:\n",
    "                binary_data += raw_data\n",
    "            else:\n",
    "                binary_data = raw_data\n",
    "\n",
    "    if binary_data is not None:\n",
    "        request_body = struct.pack(\n",
    "            \"{}s{}s\".format(len(request_body), len(binary_data)),\n",
    "            request_body.encode(),\n",
    "            binary_data,\n",
    "        )\n",
    "        return request_body, json_size\n",
    "\n",
    "    return request_body, None\n",
    "\n",
    "\n",
    "class InferenceServerClient:\n",
    "    \"\"\"An InferenceServerClient object is used to perform any kind of\n",
    "    communication with the InferenceServer using http protocol.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The inference server URL, e.g. 'localhost:8000'.\n",
    "    connection_count : int\n",
    "        The number of connections to create for this client.\n",
    "        Default value is 1.\n",
    "    connection_timeout : float\n",
    "        The timeout value for the connection. Default value\n",
    "        is 60.0 sec.\n",
    "    network_timeout : float\n",
    "        The timeout value for the network. Default value is\n",
    "        60.0 sec\n",
    "    verbose : bool\n",
    "        If True generate verbose output. Default value is False.\n",
    "    max_greenlets : int\n",
    "        Determines the maximum allowed number of worker greenlets\n",
    "        for handling asynchronous inference requests. Default value\n",
    "        is None, which means there will be no restriction on the\n",
    "        number of greenlets created.\n",
    "\n",
    "    Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If unable to create a client.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url,\n",
    "        connection_count=1,\n",
    "        connection_timeout=60.0,\n",
    "        network_timeout=60.0,\n",
    "        verbose=False,\n",
    "        max_greenlets=None,\n",
    "    ):\n",
    "        if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n",
    "            url = \"http://\" + url\n",
    "        self._parsed_url = URL(url)\n",
    "        self._base_uri = self._parsed_url.request_uri\n",
    "        self._client_stub = HTTPClient.from_url(\n",
    "            self._parsed_url,\n",
    "            concurrency=connection_count,\n",
    "            connection_timeout=connection_timeout,\n",
    "            network_timeout=network_timeout,\n",
    "        )\n",
    "        self._pool = gevent.pool.Pool(max_greenlets)\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the client. Any future calls to server\n",
    "        will result in an Error.\n",
    "\n",
    "        \"\"\"\n",
    "        self._pool.join()\n",
    "        self._client_stub.close()\n",
    "\n",
    "    def _get(self, request_uri, headers, query_params):\n",
    "        \"\"\"Issues the GET request to the server\n",
    "\n",
    "         Parameters\n",
    "        ----------\n",
    "        request_uri: str\n",
    "            The request URI to be used in GET request.\n",
    "        headers: dict\n",
    "            Additional HTTP headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        geventhttpclient.response.HTTPSocketPoolResponse\n",
    "            The response from server.\n",
    "        \"\"\"\n",
    "        if self._base_uri is not None:\n",
    "            request_uri = self._base_uri + \"/\" + request_uri\n",
    "        if query_params is not None:\n",
    "            request_uri = request_uri + \"?\" + _get_query_string(query_params)\n",
    "\n",
    "        if self._verbose:\n",
    "            print(\"GET {}, headers {}\".format(request_uri, headers))\n",
    "\n",
    "        if headers is not None:\n",
    "            response = self._client_stub.get(request_uri, headers=headers)\n",
    "        else:\n",
    "            response = self._client_stub.get(request_uri)\n",
    "\n",
    "        if self._verbose:\n",
    "            print(response)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _post(self, request_uri, request_body, headers, query_params):\n",
    "        \"\"\"Issues the POST request to the server\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        request_uri: str\n",
    "            The request URI to be used in POST request.\n",
    "        request_body: str\n",
    "            The body of the request\n",
    "        headers: dict\n",
    "            Additional HTTP headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        geventhttpclient.response.HTTPSocketPoolResponse\n",
    "            The response from server.\n",
    "        \"\"\"\n",
    "        if self._base_uri is not None:\n",
    "            request_uri = self._base_uri + \"/\" + request_uri\n",
    "        if query_params is not None:\n",
    "            request_uri = request_uri + \"?\" + _get_query_string(query_params)\n",
    "\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                \"POST {}, headers {}\\n{}\".format(\n",
    "                    request_uri, headers, request_body\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if headers is not None:\n",
    "            response = self._client_stub.post(\n",
    "                request_uri=request_uri, body=request_body, headers=headers\n",
    "            )\n",
    "        else:\n",
    "            response = self._client_stub.post(\n",
    "                request_uri=request_uri, body=request_body\n",
    "            )\n",
    "\n",
    "        if self._verbose:\n",
    "            print(response)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def is_server_live(self, headers=None, query_params=None):\n",
    "        \"\"\"Contact the inference server and get liveness.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if server is live, False if server is not live.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If unable to get liveness.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        request_uri = \"v2/health/live\"\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "\n",
    "        return response.status_code == 200\n",
    "\n",
    "    def is_server_ready(self, headers=None, query_params=None):\n",
    "        \"\"\"Contact the inference server and get readiness.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if server is ready, False if server is not ready.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If unable to get readiness.\n",
    "\n",
    "        \"\"\"\n",
    "        request_uri = \"v2/health/ready\"\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "\n",
    "        return response.status_code == 200\n",
    "\n",
    "    def is_model_ready(\n",
    "        self, model_name, model_version=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Contact the inference server and get the readiness of specified model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name: str\n",
    "            The name of the model to check for readiness.\n",
    "        model_version: str\n",
    "            The version of the model to check for readiness. The default value\n",
    "            is an empty string which means then the server will choose a version\n",
    "            based on the model and internal policy.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if the model is ready, False if not ready.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If unable to get model readiness.\n",
    "\n",
    "        \"\"\"\n",
    "        if type(model_version) != str:\n",
    "            raise_error(\"model version must be a string\")\n",
    "        if model_version != \"\":\n",
    "            request_uri = \"v2/models/{}/versions/{}/ready\".format(\n",
    "                quote(model_name), model_version\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/models/{}/ready\".format(quote(model_name))\n",
    "\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "\n",
    "        return response.status_code == 200\n",
    "\n",
    "    def get_server_metadata(self, headers=None, query_params=None):\n",
    "        \"\"\"Contact the inference server and get its metadata.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON dict holding the metadata.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to get server metadata.\n",
    "\n",
    "        \"\"\"\n",
    "        request_uri = \"v2\"\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        content = response.read()\n",
    "        if self._verbose:\n",
    "            print(content)\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def get_model_metadata(\n",
    "        self, model_name, model_version=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Contact the inference server and get the metadata for specified model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name: str\n",
    "            The name of the model\n",
    "        model_version: str\n",
    "            The version of the model to get metadata. The default value\n",
    "            is an empty string which means then the server will choose\n",
    "            a version based on the model and internal policy.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON dict holding the metadata.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to get model metadata.\n",
    "\n",
    "        \"\"\"\n",
    "        if type(model_version) != str:\n",
    "            raise_error(\"model version must be a string\")\n",
    "        if model_version != \"\":\n",
    "            request_uri = \"v2/models/{}/versions/{}\".format(\n",
    "                quote(model_name), model_version\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/models/{}\".format(quote(model_name))\n",
    "\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        content = response.read()\n",
    "        if self._verbose:\n",
    "            print(content)\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def get_model_config(\n",
    "        self, model_name, model_version=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Contact the inference server and get the configuration for specified model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name: str\n",
    "            The name of the model\n",
    "        model_version: str\n",
    "            The version of the model to get configuration. The default value\n",
    "            is an empty string which means then the server will choose\n",
    "            a version based on the model and internal policy.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON dict holding the model config.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to get model configuration.\n",
    "\n",
    "        \"\"\"\n",
    "        if model_version != \"\":\n",
    "            request_uri = \"v2/models/{}/versions/{}/config\".format(\n",
    "                quote(model_name), model_version\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/models/{}/config\".format(quote(model_name))\n",
    "\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        content = response.read()\n",
    "        if self._verbose:\n",
    "            print(content)\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def get_model_repository_index(self, headers=None, query_params=None):\n",
    "        \"\"\"Get the index of model repository contents\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON dict holding the model repository index.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to get the repository index.\n",
    "\n",
    "        \"\"\"\n",
    "        request_uri = \"v2/repository/index\"\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=\"\",\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        content = response.read()\n",
    "        if self._verbose:\n",
    "            print(content)\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def load_model(self, model_name, headers=None, query_params=None):\n",
    "        \"\"\"Request the inference server to load or reload specified model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name : str\n",
    "            The name of the model to be loaded.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to load the model.\n",
    "\n",
    "        \"\"\"\n",
    "        request_uri = \"v2/repository/models/{}/load\".format(quote(model_name))\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=\"\",\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "        if self._verbose:\n",
    "            print(\"Loaded model '{}'\".format(model_name))\n",
    "\n",
    "    def unload_model(self, model_name, headers=None, query_params=None):\n",
    "        \"\"\"Request the inference server to unload specified model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name : str\n",
    "            The name of the model to be unloaded.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to unload the model.\n",
    "\n",
    "        \"\"\"\n",
    "        request_uri = \"v2/repository/models/{}/unload\".format(\n",
    "            quote(model_name)\n",
    "        )\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=\"\",\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "        if self._verbose:\n",
    "            print(\"Loaded model '{}'\".format(model_name))\n",
    "\n",
    "    def get_inference_statistics(\n",
    "        self, model_name=\"\", model_version=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Get the inference statistics for the specified model name and\n",
    "        version.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name : str\n",
    "            The name of the model to get statistics. The default value is\n",
    "            an empty string, which means statistics of all models will\n",
    "            be returned.\n",
    "        model_version: str\n",
    "            The version of the model to get inference statistics. The\n",
    "            default value is an empty string which means then the server\n",
    "            will return the statistics of all available model versions.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON dict holding the model inference statistics.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to get the model inference statistics.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if model_name != \"\":\n",
    "            if type(model_version) != str:\n",
    "                raise_error(\"model version must be a string\")\n",
    "            if model_version != \"\":\n",
    "                request_uri = \"v2/models/{}/versions/{}/stats\".format(\n",
    "                    quote(model_name), model_version\n",
    "                )\n",
    "            else:\n",
    "                request_uri = \"v2/models/{}/stats\".format(quote(model_name))\n",
    "        else:\n",
    "            request_uri = \"v2/models/stats\"\n",
    "\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        content = response.read()\n",
    "        if self._verbose:\n",
    "            print(content)\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def get_system_shared_memory_status(\n",
    "        self, region_name=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Request system shared memory status from the server.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        region_name : str\n",
    "            The name of the region to query status. The default\n",
    "            value is an empty string, which means that the status\n",
    "            of all active system shared memory will be returned.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON dict holding system shared memory status.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to get the status of specified shared memory.\n",
    "\n",
    "        \"\"\"\n",
    "        if region_name != \"\":\n",
    "            request_uri = \"v2/systemsharedmemory/region/{}/status\".format(\n",
    "                quote(region_name)\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/systemsharedmemory/status\"\n",
    "\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        content = response.read()\n",
    "        if self._verbose:\n",
    "            print(content)\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def register_system_shared_memory(\n",
    "        self, name, key, byte_size, offset=0, headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Request the server to register a system shared memory with the\n",
    "        following specification.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the region to register.\n",
    "        key : str\n",
    "            The key of the underlying memory object that contains the\n",
    "            system shared memory region.\n",
    "        byte_size : int\n",
    "            The size of the system shared memory region, in bytes.\n",
    "        offset : int\n",
    "            Offset, in bytes, within the underlying memory object to\n",
    "            the start of the system shared memory region. The default\n",
    "            value is zero.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to register the specified system shared memory.\n",
    "\n",
    "        \"\"\"\n",
    "        request_uri = \"v2/systemsharedmemory/region/{}/register\".format(\n",
    "            quote(name)\n",
    "        )\n",
    "\n",
    "        register_request = {\n",
    "            \"key\": key,\n",
    "            \"offset\": offset,\n",
    "            \"byte_size\": byte_size,\n",
    "        }\n",
    "        request_body = json.dumps(register_request)\n",
    "\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=request_body,\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                \"Registered system shared memory with name '{}'\".format(name)\n",
    "            )\n",
    "\n",
    "    def unregister_system_shared_memory(\n",
    "        self, name=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Request the server to unregister a system shared memory with the\n",
    "        specified name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the region to unregister. The default value is empty\n",
    "            string which means all the system shared memory regions will be\n",
    "            unregistered.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to unregister the specified system shared memory region.\n",
    "\n",
    "        \"\"\"\n",
    "        if name != \"\":\n",
    "            request_uri = \"v2/systemsharedmemory/region/{}/unregister\".format(\n",
    "                quote(name)\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/systemsharedmemory/unregister\"\n",
    "\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=\"\",\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "        if self._verbose:\n",
    "            if name is not \"\":\n",
    "                print(\n",
    "                    \"Unregistered system shared memory with name '{}'\".format(\n",
    "                        name\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\"Unregistered all system shared memory regions\")\n",
    "\n",
    "    def get_cuda_shared_memory_status(\n",
    "        self, region_name=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Request cuda shared memory status from the server.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        region_name : str\n",
    "            The name of the region to query status. The default\n",
    "            value is an empty string, which means that the status\n",
    "            of all active cuda shared memory will be returned.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The JSON dict holding cuda shared memory status.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to get the status of specified shared memory.\n",
    "\n",
    "        \"\"\"\n",
    "        if region_name != \"\":\n",
    "            request_uri = \"v2/cudasharedmemory/region/{}/status\".format(\n",
    "                quote(region_name)\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/cudasharedmemory/status\"\n",
    "\n",
    "        response = self._get(\n",
    "            request_uri=request_uri, headers=headers, query_params=query_params\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        content = response.read()\n",
    "        if self._verbose:\n",
    "            print(content)\n",
    "\n",
    "        return json.loads(content)\n",
    "\n",
    "    def register_cuda_shared_memory(\n",
    "        self,\n",
    "        name,\n",
    "        raw_handle,\n",
    "        device_id,\n",
    "        byte_size,\n",
    "        headers=None,\n",
    "        query_params=None,\n",
    "    ):\n",
    "        \"\"\"Request the server to register a system shared memory with the\n",
    "        following specification.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the region to register.\n",
    "        raw_handle : bytes\n",
    "            The raw serialized cudaIPC handle in base64 encoding.\n",
    "        device_id : int\n",
    "            The GPU device ID on which the cudaIPC handle was created.\n",
    "        byte_size : int\n",
    "            The size of the cuda shared memory region, in bytes.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to register the specified cuda shared memory.\n",
    "\n",
    "        \"\"\"\n",
    "        request_uri = \"v2/cudasharedmemory/region/{}/register\".format(\n",
    "            quote(name)\n",
    "        )\n",
    "\n",
    "        register_request = {\n",
    "            \"raw_handle\": {\"b64\": raw_handle},\n",
    "            \"device_id\": device_id,\n",
    "            \"byte_size\": byte_size,\n",
    "        }\n",
    "        request_body = json.dumps(register_request)\n",
    "\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=request_body,\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "        if self._verbose:\n",
    "            print(\"Registered cuda shared memory with name '{}'\".format(name))\n",
    "\n",
    "    def unregister_cuda_shared_memory(\n",
    "        self, name=\"\", headers=None, query_params=None\n",
    "    ):\n",
    "        \"\"\"Request the server to unregister a cuda shared memory with the\n",
    "        specified name.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the region to unregister. The default value is empty\n",
    "            string which means all the cuda shared memory regions will be\n",
    "            unregistered.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional\n",
    "            HTTP headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If unable to unregister the specified cuda shared memory region.\n",
    "\n",
    "        \"\"\"\n",
    "        if name != \"\":\n",
    "            request_uri = \"v2/cudasharedmemory/region/{}/unregister\".format(\n",
    "                quote(name)\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/cudasharedmemory/unregister\"\n",
    "\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=\"\",\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "        if self._verbose:\n",
    "            if name is not \"\":\n",
    "                print(\n",
    "                    \"Unregistered cuda shared memory with name '{}'\".format(\n",
    "                        name\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\"Unregistered all cuda shared memory regions\")\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        model_name,\n",
    "        inputs,\n",
    "        model_version=\"\",\n",
    "        outputs=None,\n",
    "        request_id=\"\",\n",
    "        sequence_id=0,\n",
    "        sequence_start=False,\n",
    "        sequence_end=False,\n",
    "        priority=0,\n",
    "        timeout=None,\n",
    "        headers=None,\n",
    "        query_params=None,\n",
    "    ):\n",
    "        \"\"\"Run synchronous inference using the supplied 'inputs' requesting\n",
    "        the outputs specified by 'outputs'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name: str\n",
    "            The name of the model to run inference.\n",
    "        inputs : list\n",
    "            A list of InferInput objects, each describing data for a input\n",
    "            tensor required by the model.\n",
    "        model_version: str\n",
    "            The version of the model to run inference. The default value\n",
    "            is an empty string which means then the server will choose\n",
    "            a version based on the model and internal policy.\n",
    "        outputs : list\n",
    "            A list of InferRequestedOutput objects, each describing how the output\n",
    "            data must be returned. If not specified all outputs produced\n",
    "            by the model will be returned using default settings.\n",
    "        request_id: str\n",
    "            Optional identifier for the request. If specified will be returned\n",
    "            in the response. Default value is an empty string which means no\n",
    "            request_id will be used.\n",
    "        sequence_id : int\n",
    "            The unique identifier for the sequence being represented by the\n",
    "            object. Default value is 0 which means that the request does not\n",
    "            belong to a sequence.\n",
    "        sequence_start: bool\n",
    "            Indicates whether the request being added marks the start of the\n",
    "            sequence. Default value is False. This argument is ignored if\n",
    "            'sequence_id' is 0.\n",
    "        sequence_end: bool\n",
    "            Indicates whether the request being added marks the end of the\n",
    "            sequence. Default value is False. This argument is ignored if\n",
    "            'sequence_id' is 0.\n",
    "        priority : int\n",
    "            Indicates the priority of the request. Priority value zero\n",
    "            indicates that the default priority level should be used\n",
    "            (i.e. same behavior as not specifying the priority parameter).\n",
    "            Lower value priorities indicate higher priority levels. Thus\n",
    "            the highest priority level is indicated by setting the parameter\n",
    "            to 1, the next highest is 2, etc. If not provided, the server\n",
    "            will handle the request using default setting for the model.\n",
    "        timeout : int\n",
    "            The timeout value for the request, in microseconds. If the request\n",
    "            cannot be completed within the time the server can take a\n",
    "            model-specific action such as terminating the request. If not\n",
    "            provided, the server will handle the request using default setting\n",
    "            for the model.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request.\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        InferResult\n",
    "            The object holding the result of the inference.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If server fails to perform inference.\n",
    "        \"\"\"\n",
    "\n",
    "        request_body, json_size = _get_inference_request(\n",
    "            inputs=inputs,\n",
    "            request_id=request_id,\n",
    "            outputs=outputs,\n",
    "            sequence_id=sequence_id,\n",
    "            sequence_start=sequence_start,\n",
    "            sequence_end=sequence_end,\n",
    "            priority=priority,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "\n",
    "        if json_size is not None:\n",
    "            if headers is None:\n",
    "                headers = {}\n",
    "            headers[\"Inference-Header-Content-Length\"] = json_size\n",
    "\n",
    "        if type(model_version) != str:\n",
    "            raise_error(\"model version must be a string\")\n",
    "        if model_version != \"\":\n",
    "            request_uri = \"v2/models/{}/versions/{}/infer\".format(\n",
    "                quote(model_name), model_version\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/models/{}/infer\".format(quote(model_name))\n",
    "\n",
    "        response = self._post(\n",
    "            request_uri=request_uri,\n",
    "            request_body=request_body,\n",
    "            headers=headers,\n",
    "            query_params=query_params,\n",
    "        )\n",
    "        _raise_if_error(response)\n",
    "\n",
    "        return InferResult(response, self._verbose)\n",
    "\n",
    "    def async_infer(\n",
    "        self,\n",
    "        model_name,\n",
    "        inputs,\n",
    "        model_version=\"\",\n",
    "        outputs=None,\n",
    "        request_id=\"\",\n",
    "        sequence_id=0,\n",
    "        sequence_start=False,\n",
    "        sequence_end=False,\n",
    "        priority=0,\n",
    "        timeout=None,\n",
    "        headers=None,\n",
    "        query_params=None,\n",
    "    ):\n",
    "        \"\"\"Run asynchronous inference using the supplied 'inputs' requesting\n",
    "        the outputs specified by 'outputs'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_name: str\n",
    "            The name of the model to run inference.\n",
    "        inputs : list\n",
    "            A list of InferInput objects, each describing data for a input\n",
    "            tensor required by the model.\n",
    "        model_version: str\n",
    "            The version of the model to run inference. The default value\n",
    "            is an empty string which means then the server will choose\n",
    "            a version based on the model and internal policy.\n",
    "        outputs : list\n",
    "            A list of InferRequestedOutput objects, each describing how the output\n",
    "            data must be returned. If not specified all outputs produced\n",
    "            by the model will be returned using default settings.\n",
    "        request_id: str\n",
    "            Optional identifier for the request. If specified will be returned\n",
    "            in the response. Default value is 'None' which means no request_id\n",
    "            will be used.\n",
    "        sequence_id : int\n",
    "            The unique identifier for the sequence being represented by the\n",
    "            object. Default value is 0 which means that the request does not\n",
    "            belong to a sequence.\n",
    "        sequence_start: bool\n",
    "            Indicates whether the request being added marks the start of the\n",
    "            sequence. Default value is False. This argument is ignored if\n",
    "            'sequence_id' is 0.\n",
    "        sequence_end: bool\n",
    "            Indicates whether the request being added marks the end of the\n",
    "            sequence. Default value is False. This argument is ignored if\n",
    "            'sequence_id' is 0.\n",
    "        priority : int\n",
    "            Indicates the priority of the request. Priority value zero\n",
    "            indicates that the default priority level should be used\n",
    "            (i.e. same behavior as not specifying the priority parameter).\n",
    "            Lower value priorities indicate higher priority levels. Thus\n",
    "            the highest priority level is indicated by setting the parameter\n",
    "            to 1, the next highest is 2, etc. If not provided, the server\n",
    "            will handle the request using default setting for the model.\n",
    "        timeout : int\n",
    "            The timeout value for the request, in microseconds. If the request\n",
    "            cannot be completed within the time the server can take a\n",
    "            model-specific action such as terminating the request. If not\n",
    "            provided, the server will handle the request using default setting\n",
    "            for the model.\n",
    "        headers: dict\n",
    "            Optional dictionary specifying additional HTTP\n",
    "            headers to include in the request\n",
    "        query_params: dict\n",
    "            Optional url query parameters to use in network\n",
    "            transaction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        InferAsyncRequest object\n",
    "            The handle to the asynchronous inference request.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If server fails to issue inference.\n",
    "        \"\"\"\n",
    "\n",
    "        def wrapped_post(request_uri, request_body, headers, query_params):\n",
    "            return self._post(request_uri, request_body, headers, query_params)\n",
    "\n",
    "        request_body, json_size = _get_inference_request(\n",
    "            inputs=inputs,\n",
    "            request_id=request_id,\n",
    "            outputs=outputs,\n",
    "            sequence_id=sequence_id,\n",
    "            sequence_start=sequence_start,\n",
    "            sequence_end=sequence_end,\n",
    "            priority=priority,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "\n",
    "        if json_size is not None:\n",
    "            if headers is None:\n",
    "                headers = {}\n",
    "            headers[\"Inference-Header-Content-Length\"] = json_size\n",
    "\n",
    "        if type(model_version) != str:\n",
    "            raise_error(\"model version must be a string\")\n",
    "        if model_version != \"\":\n",
    "            request_uri = \"v2/models/{}/versions/{}/infer\".format(\n",
    "                quote(model_name), model_version\n",
    "            )\n",
    "        else:\n",
    "            request_uri = \"v2/models/{}/infer\".format(quote(model_name))\n",
    "\n",
    "        g = self._pool.apply_async(\n",
    "            wrapped_post, (request_uri, request_body, headers, query_params)\n",
    "        )\n",
    "\n",
    "        g.start()\n",
    "\n",
    "        if self._verbose:\n",
    "            verbose_message = \"Sent request\"\n",
    "            if request_id is not \"\":\n",
    "                verbose_message = verbose_message + \" '{}'\".format(request_id)\n",
    "            print(verbose_message)\n",
    "\n",
    "        return InferAsyncRequest(g, self._verbose)\n",
    "\n",
    "\n",
    "class InferAsyncRequest:\n",
    "    \"\"\"An object of InferAsyncRequest class is used to describe\n",
    "    a handle to an ongoing asynchronous inference request.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    greenlet : gevent.Greenlet\n",
    "        The greenlet object which will provide the results.\n",
    "        For further details about greenlets refer\n",
    "        http://www.gevent.org/api/gevent.greenlet.html.\n",
    "\n",
    "    verbose : bool\n",
    "        If True generate verbose output. Default value is False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, greenlet, verbose=False):\n",
    "        self._greenlet = greenlet\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def get_result(self, block=True, timeout=None):\n",
    "        \"\"\"Get the results of the associated asynchronous inference.\n",
    "        Parameters\n",
    "        ----------\n",
    "        block : bool\n",
    "            If block is True, the function will wait till the\n",
    "            corresponding response is received from the server.\n",
    "            Default value is True.\n",
    "        timeout : int\n",
    "            The maximum wait time for the function. This setting is\n",
    "            ignored if the block is set False. Default is None,\n",
    "            which means the function will block indefinitely till\n",
    "            the corresponding response is received.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        InferResult\n",
    "            The object holding the result of the async inference.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If server fails to perform inference or failed to respond\n",
    "            within specified timeout.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self._greenlet.get(block=block, timeout=timeout)\n",
    "        except gevent.Timeout as e:\n",
    "            raise_error(\"failed to obtain inference response\")\n",
    "\n",
    "        _raise_if_error(response)\n",
    "        return InferResult(response, self._verbose)\n",
    "\n",
    "\n",
    "class InferInput:\n",
    "    \"\"\"An object of InferInput class is used to describe\n",
    "    input tensor for an inference request.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name of input whose data will be described by this object\n",
    "    shape : list\n",
    "        The shape of the associated input.\n",
    "    datatype : str\n",
    "        The datatype of the associated input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, shape, datatype):\n",
    "        self._name = name\n",
    "        self._shape = shape\n",
    "        self._datatype = datatype\n",
    "        self._parameters = {}\n",
    "        self._data = None\n",
    "        self._raw_data = None\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"Get the name of input associated with this object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The name of input\n",
    "        \"\"\"\n",
    "        return self._name\n",
    "\n",
    "    def datatype(self):\n",
    "        \"\"\"Get the datatype of input associated with this object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The datatype of input\n",
    "        \"\"\"\n",
    "        return self._datatype\n",
    "\n",
    "    def shape(self):\n",
    "        \"\"\"Get the shape of input associated with this object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            The shape of input\n",
    "        \"\"\"\n",
    "        return self._shape\n",
    "\n",
    "    def set_shape(self, shape):\n",
    "        \"\"\"Set the shape of input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : list\n",
    "            The shape of the associated input.\n",
    "        \"\"\"\n",
    "        self._shape = shape\n",
    "\n",
    "    def set_data_from_numpy(self, input_tensor, binary_data=True):\n",
    "        \"\"\"Set the tensor data from the specified numpy array for\n",
    "        input associated with this object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor : numpy array\n",
    "            The tensor data in numpy array format\n",
    "        binary_data : bool\n",
    "            Indicates whether to set data for the input in binary format\n",
    "            or explicit tensor within JSON. The default value is True,\n",
    "            which means the data will be delivered as binary data in the\n",
    "            HTTP body after the JSON object.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        InferenceServerException\n",
    "            If failed to set data for the tensor.\n",
    "        \"\"\"\n",
    "        if not isinstance(input_tensor, (np.ndarray,)):\n",
    "            raise_error(\"input_tensor must be a numpy array\")\n",
    "        dtype = np_to_triton_dtype(input_tensor.dtype)\n",
    "        if self._datatype != dtype:\n",
    "            raise_error(\n",
    "                \"got unexpected datatype {} from numpy array, expected {}\".format(\n",
    "                    dtype, self._datatype\n",
    "                )\n",
    "            )\n",
    "        valid_shape = True\n",
    "        if len(self._shape) != len(input_tensor.shape):\n",
    "            valid_shape = False\n",
    "        else:\n",
    "            for i in range(len(self._shape)):\n",
    "                if self._shape[i] != input_tensor.shape[i]:\n",
    "                    valid_shape = False\n",
    "        if not valid_shape:\n",
    "            raise_error(\n",
    "                \"got unexpected numpy array shape [{}], expected [{}]\".format(\n",
    "                    str(input_tensor.shape)[1:-1], str(self._shape)[1:-1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self._parameters.pop(\"shared_memory_region\", None)\n",
    "        self._parameters.pop(\"shared_memory_byte_size\", None)\n",
    "        self._parameters.pop(\"shared_memory_offset\", None)\n",
    "\n",
    "        if not binary_data:\n",
    "            self._parameters.pop(\"binary_data_size\", None)\n",
    "            self._raw_data = None\n",
    "            if self._datatype == \"BYTES\":\n",
    "                self._data = [val for val in input_tensor.flatten()]\n",
    "            else:\n",
    "                self._data = [val.item() for val in input_tensor.flatten()]\n",
    "        else:\n",
    "            self._data = None\n",
    "            if self._datatype == \"BYTES\":\n",
    "                self._raw_data = serialize_byte_tensor(input_tensor).tobytes()\n",
    "            else:\n",
    "                self._raw_data = input_tensor.tobytes()\n",
    "            self._parameters[\"binary_data_size\"] = len(self._raw_data)\n",
    "\n",
    "    def set_shared_memory(self, region_name, byte_size, offset=0):\n",
    "        \"\"\"Set the tensor data from the specified shared memory region.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        region_name : str\n",
    "            The name of the shared memory region holding tensor data.\n",
    "        byte_size : int\n",
    "            The size of the shared memory region holding tensor data.\n",
    "        offset : int\n",
    "            The offset, in bytes, into the region where the data for\n",
    "            the tensor starts. The default value is 0.\n",
    "\n",
    "        \"\"\"\n",
    "        self._data = None\n",
    "        self._raw_data = None\n",
    "        self._parameters.pop(\"binary_data_size\", None)\n",
    "\n",
    "        self._parameters[\"shared_memory_region\"] = region_name\n",
    "        self._parameters[\"shared_memory_byte_size\"] = byte_size\n",
    "        if offset != 0:\n",
    "            self._parameters[\"shared_memory_offset\"].int64_param = offset\n",
    "\n",
    "    def _get_binary_data(self):\n",
    "        \"\"\"Returns the raw binary data if available\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bytes\n",
    "            The raw data for the input tensor\n",
    "        \"\"\"\n",
    "        return self._raw_data\n",
    "\n",
    "    def _get_tensor(self):\n",
    "        \"\"\"Retrieve the underlying input as json dict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The underlying tensor specification as dict\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self._parameters.get(\"shared_memory_region\") is not None\n",
    "            or self._raw_data is not None\n",
    "        ):\n",
    "            return {\n",
    "                \"name\": self._name,\n",
    "                \"shape\": self._shape,\n",
    "                \"datatype\": self._datatype,\n",
    "                \"parameters\": self._parameters,\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"name\": self._name,\n",
    "                \"shape\": self._shape,\n",
    "                \"datatype\": self._datatype,\n",
    "                \"parameters\": self._parameters,\n",
    "                \"data\": self._data,\n",
    "            }\n",
    "\n",
    "\n",
    "class InferRequestedOutput:\n",
    "    \"\"\"An object of InferRequestedOutput class is used to describe a\n",
    "    requested output tensor for an inference request.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        The name of output tensor to associate with this object.\n",
    "    binary_data : bool\n",
    "        Indicates whether to return result data for the output in\n",
    "        binary format or explicit tensor within JSON. The default\n",
    "        value is True, which means the data will be delivered as\n",
    "        binary data in the HTTP body after JSON object. This field\n",
    "        will be unset if shared memory is set for the output.\n",
    "    class_count : int\n",
    "        The number of classifications to be requested. The default\n",
    "        value is 0 which means the classification results are not\n",
    "        requested.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, binary_data=True, class_count=0):\n",
    "        self._name = name\n",
    "        self._parameters = {}\n",
    "        if class_count != 0:\n",
    "            self._parameters[\"classification\"] = class_count\n",
    "        self._binary = binary_data\n",
    "        self._parameters[\"binary_data\"] = binary_data\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"Get the name of output associated with this object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The name of output\n",
    "        \"\"\"\n",
    "        return self._name\n",
    "\n",
    "    def set_shared_memory(self, region_name, byte_size, offset=0):\n",
    "        \"\"\"Marks the output to return the inference result in\n",
    "        specified shared memory region.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        region_name : str\n",
    "            The name of the shared memory region to hold tensor data.\n",
    "        byte_size : int\n",
    "            The size of the shared memory region to hold tensor data.\n",
    "        offset : int\n",
    "            The offset, in bytes, into the region where the data for\n",
    "            the tensor starts. The default value is 0.\n",
    "\n",
    "        \"\"\"\n",
    "        if \"classification\" in self._parameters:\n",
    "            raise_error(\"shared memory can't be set on classification output\")\n",
    "        if self._binary:\n",
    "            self._parameters[\"binary_data\"] = False\n",
    "\n",
    "        self._parameters[\"shared_memory_region\"] = region_name\n",
    "        self._parameters[\"shared_memory_byte_size\"] = byte_size\n",
    "        if offset != 0:\n",
    "            self._parameters[\"shared_memory_offset\"] = offset\n",
    "\n",
    "    def unset_shared_memory(self):\n",
    "        \"\"\"Clears the shared memory option set by the last call to\n",
    "        InferRequestedOutput.set_shared_memory(). After call to this\n",
    "        function requested output will no longer be returned in a\n",
    "        shared memory region.\n",
    "        \"\"\"\n",
    "\n",
    "        self._parameters[\"binary_data\"] = self._binary\n",
    "        self._parameters.pop(\"shared_memory_region\", None)\n",
    "        self._parameters.pop(\"shared_memory_byte_size\", None)\n",
    "        self._parameters.pop(\"shared_memory_offset\", None)\n",
    "\n",
    "    def _get_tensor(self):\n",
    "        \"\"\"Retrieve the underlying input as json dict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The underlying tensor as a dict\n",
    "        \"\"\"\n",
    "        return {\"name\": self._name, \"parameters\": self._parameters}\n",
    "\n",
    "\n",
    "class InferResult:\n",
    "    \"\"\"An object of InferResult class holds the response of\n",
    "    an inference request and provide methods to retrieve\n",
    "    inference results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result : dict\n",
    "        The inference response from the server\n",
    "    verbose : bool\n",
    "        If True generate verbose output. Default value is False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, response, verbose):\n",
    "        header_length = response.get(\"Inference-Header-Content-Length\")\n",
    "        if header_length is None:\n",
    "            content = response.read()\n",
    "            if verbose:\n",
    "                print(content)\n",
    "            self._result = json.loads(content)\n",
    "        else:\n",
    "            header_length = int(header_length)\n",
    "            content = response.read(length=header_length)\n",
    "            if verbose:\n",
    "                print(content)\n",
    "            self._result = json.loads(content)\n",
    "\n",
    "            # Maps the output name to the index in buffer for quick retrieval\n",
    "            self._output_name_to_buffer_map = {}\n",
    "            # Read the remaining data off the response body.\n",
    "            self._buffer = response.read()\n",
    "            buffer_index = 0\n",
    "            for output in self._result[\"outputs\"]:\n",
    "                parameters = output.get(\"parameters\")\n",
    "                if parameters is not None:\n",
    "                    this_data_size = parameters.get(\"binary_data_size\")\n",
    "                    if this_data_size is not None:\n",
    "                        self._output_name_to_buffer_map[\n",
    "                            output[\"name\"]\n",
    "                        ] = buffer_index\n",
    "                        buffer_index = buffer_index + this_data_size\n",
    "\n",
    "    def as_numpy(self, name):\n",
    "        \"\"\"Get the tensor data for output associated with this object\n",
    "        in numpy format\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the output tensor whose result is to be retrieved.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            The numpy array containing the response data for the tensor or\n",
    "            None if the data for specified tensor name is not found.\n",
    "        \"\"\"\n",
    "        if self._result.get(\"outputs\") is not None:\n",
    "            for output in self._result[\"outputs\"]:\n",
    "                if output[\"name\"] == name:\n",
    "                    datatype = output[\"datatype\"]\n",
    "                    has_binary_data = False\n",
    "                    parameters = output.get(\"parameters\")\n",
    "                    if parameters is not None:\n",
    "                        this_data_size = parameters.get(\"binary_data_size\")\n",
    "                        if this_data_size is not None:\n",
    "                            has_binary_data = True\n",
    "                            if this_data_size != 0:\n",
    "                                start_index = self._output_name_to_buffer_map[\n",
    "                                    name\n",
    "                                ]\n",
    "                                end_index = start_index + this_data_size\n",
    "                                if datatype == \"BYTES\":\n",
    "                                    # String results contain a 4-byte string length\n",
    "                                    # followed by the actual string characters. Hence,\n",
    "                                    # need to decode the raw bytes to convert into\n",
    "                                    # array elements.\n",
    "                                    np_array = deserialize_bytes_tensor(\n",
    "                                        self._buffer[start_index:end_index]\n",
    "                                    )\n",
    "                                else:\n",
    "                                    np_array = np.frombuffer(\n",
    "                                        self._buffer[start_index:end_index],\n",
    "                                        dtype=triton_to_np_dtype(datatype),\n",
    "                                    )\n",
    "                            else:\n",
    "                                np_array = np.empty(0)\n",
    "                    if not has_binary_data:\n",
    "                        np_array = np.array(\n",
    "                            output[\"data\"], dtype=triton_to_np_dtype(datatype)\n",
    "                        )\n",
    "                    np_array = np.resize(np_array, output[\"shape\"])\n",
    "                    return np_array\n",
    "        return None\n",
    "\n",
    "    def get_output(self, name):\n",
    "        \"\"\"Retrieves the output tensor corresponding to the named ouput.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the tensor for which Output is to be\n",
    "            retrieved.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            If an output tensor with specified name is present in\n",
    "            the infer resonse then returns it as a json dict,\n",
    "            otherwise returns None.\n",
    "        \"\"\"\n",
    "        for output in self._result[\"outputs\"]:\n",
    "            if output[\"name\"] == name:\n",
    "                return output\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_response(self):\n",
    "        \"\"\"Retrieves the complete response\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The underlying response dict.\n",
    "        \"\"\"\n",
    "        return self._result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "import tritonhttpclient\n",
    "\n",
    "\n",
    "def triton_init(url=\"localhost:8000\"):\n",
    "    \"\"\"Initializes the triton client to point at the specified URL\n",
    "\n",
    "    Parameter\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL on which to address the Triton server, defaults to\n",
    "        localhost:8000\n",
    "    \"\"\"\n",
    "    global triton_client\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url)\n",
    "    return triton_client\n",
    "\n",
    "\n",
    "def get_model_info():\n",
    "    \"\"\"Gets metadata for all models hosted behind the Triton endpoint.\n",
    "    Useful for confirming that your models were loaded into memory.\n",
    "\n",
    "    Prints the data to STDOUT.\n",
    "    \"\"\"\n",
    "    repo_index = triton_client.get_model_repository_index()\n",
    "    for model in repo_index:\n",
    "        model_name = model[\"name\"]\n",
    "        model_version = model[\"version\"]\n",
    "        (\n",
    "            input_meta,\n",
    "            input_config,\n",
    "            output_meta,\n",
    "            output_config,\n",
    "        ) = parse_model_http(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        )\n",
    "        print(\n",
    "            f\"Found model: {model_name}, version: {model_version}, \\\n",
    "              input meta: {input_meta}, input config: {input_config}, \\\n",
    "              output_meta: {output_meta}, output config: {output_config}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def parse_model_http(model_name, model_version=\"\"):\n",
    "    \"\"\"Check the configuration of a model to make sure it meets the\n",
    "    requirements for an image classification network (as expected by\n",
    "    this client)\n",
    "\n",
    "    Arguments\n",
    "    --------\n",
    "    model_name : str\n",
    "        Name of the model whose metadata you want to fetch\n",
    "\n",
    "    model_version : str\n",
    "        Optional, the version of the model, defaults to empty string.\n",
    "\n",
    "    From https://github.com/triton-inference-server/server/blob/master/src/clients/python/examples/image_client.py  # noqa\n",
    "    \"\"\"\n",
    "    model_metadata = triton_client.get_model_metadata(\n",
    "        model_name=model_name, model_version=model_version\n",
    "    )\n",
    "    model_config = triton_client.get_model_config(\n",
    "        model_name=model_name, model_version=model_version\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        model_metadata[\"inputs\"],\n",
    "        model_config[\"input\"],\n",
    "        model_metadata[\"outputs\"],\n",
    "        model_config[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def triton_infer(\n",
    "    input_mapping,\n",
    "    model_name,\n",
    "    binary_data=False,\n",
    "    binary_output=False,\n",
    "    class_count=0,\n",
    "):\n",
    "    \"\"\"Helper function for setting Triton inputs and executing a request\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    input_mapping : dict\n",
    "        A dictionary mapping strings to numpy arrays. The keys should\n",
    "        be the names of the model inputs, and the values should be the\n",
    "        inputs themselves.\n",
    "\n",
    "    model_name : str\n",
    "        The name of the model on which you are running inference.\n",
    "\n",
    "    binary_data : bool\n",
    "        Whether you are expecting binary input and output. Defaults to False\n",
    "\n",
    "    class_count : int\n",
    "        If the model is a classification model, the number of output classes.\n",
    "        Defaults to 0, indicating this is not a classification model.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    res : InferResult\n",
    "        Triton inference result containing output from running prediction\n",
    "    \"\"\"\n",
    "    input_meta, _, output_meta, _ = parse_model_http(model_name)\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    # Populate the inputs array\n",
    "    for in_meta in input_meta:\n",
    "        input_name = in_meta[\"name\"]\n",
    "        data = input_mapping[input_name]\n",
    "\n",
    "        input = tritonhttpclient.InferInput(\n",
    "            input_name, data.shape, in_meta[\"datatype\"]\n",
    "        )\n",
    "\n",
    "        input.set_data_from_numpy(data, binary_data=binary_data)\n",
    "        inputs.append(input)\n",
    "\n",
    "    # Populate the outputs array\n",
    "    for out_meta in output_meta:\n",
    "        output_name = out_meta[\"name\"]\n",
    "        output = tritonhttpclient.InferRequestedOutput(\n",
    "            output_name, binary_data=binary_output, class_count=class_count\n",
    "        )\n",
    "        outputs.append(output)\n",
    "\n",
    "    # Run inference\n",
    "    res = triton_client.infer(\n",
    "        model_name, inputs, request_id=\"0\", outputs=outputs\n",
    "    )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton.dockerfile\n",
    "FROM mcr.microsoft.com/azureml/aml-triton\n",
    "\n",
    "RUN pip install azureml-defaults\n",
    "# RUN pip install numpy inference-schema[numpy-support]\n",
    "RUN pip install pillow\n",
    "RUN pip install nvidia-pyindex\n",
    "RUN pip install tritonclient[http]\n",
    "# RUN apt-get update && apt-get install -y libcurl4-openssl-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile densenet_labels.txt\n",
    "TENCH\n",
    "GOLDFISH\n",
    "WHITE SHARK\n",
    "TIGER SHARK\n",
    "HAMMERHEAD SHARK\n",
    "ELECTRIC RAY\n",
    "STINGRAY\n",
    "ROOSTER\n",
    "HEN\n",
    "OSTRICH\n",
    "BRAMBLING\n",
    "GOLDFINCH\n",
    "HOUSE FINCH\n",
    "SNOWBIRD\n",
    "INDIGO FINCH\n",
    "ROBIN\n",
    "BULBUL\n",
    "JAY\n",
    "MAGPIE\n",
    "CHICKADEE\n",
    "WATER OUZEL\n",
    "KITE\n",
    "BALD EAGLE\n",
    "VULTURE\n",
    "GREAT GREY OWL\n",
    "FIRE SALAMANDER\n",
    "NEWT\n",
    "EFT\n",
    "SPOTTED SALAMANDER\n",
    "AXOLOTL\n",
    "BULL FROG\n",
    "TREE FROG\n",
    "TAILED FROG\n",
    "LOGGERHEAD\n",
    "LEATHERBACK TURTLE\n",
    "MUD TURTLE\n",
    "TERRAPIN\n",
    "BOX TURTLE\n",
    "BANDED GECKO\n",
    "COMMON IGUANA\n",
    "AMERICAN CHAMELEON\n",
    "WHIPTAIL\n",
    "AGAMA\n",
    "FRILLED LIZARD\n",
    "ALLIGATOR LIZARD\n",
    "GILA MONSTER\n",
    "GREEN LIZARD\n",
    "AFRICAN CHAMELEON\n",
    "KOMODO DRAGON\n",
    "AFRICAN CROCODILE\n",
    "AMERICAN ALLIGATOR\n",
    "TRICERATOPS\n",
    "THUNDER SNAKE\n",
    "RINGNECK SNAKE\n",
    "HOGNOSE SNAKE\n",
    "GREEN SNAKE\n",
    "KING SNAKE\n",
    "GARTER SNAKE\n",
    "WATER SNAKE\n",
    "VINE SNAKE\n",
    "NIGHT SNAKE\n",
    "BOA\n",
    "ROCK PYTHON\n",
    "COBRA\n",
    "GREEN MAMBA\n",
    "SEA SNAKE\n",
    "HORNED VIPER\n",
    "DIAMONDBACK\n",
    "SIDEWINDER\n",
    "TRILOBITE\n",
    "HARVESTMAN\n",
    "SCORPION\n",
    "GARDEN SPIDER\n",
    "BARN SPIDER\n",
    "GARDEN SPIDER\n",
    "BLACK WIDOW\n",
    "TARANTULA\n",
    "WOLF SPIDER\n",
    "TICK\n",
    "CENTIPEDE\n",
    "GROUSE\n",
    "PTARMIGAN\n",
    "RUFFED GROUSE\n",
    "PRAIRIE CHICKEN\n",
    "PEACOCK\n",
    "QUAIL\n",
    "PARTRIDGE\n",
    "AFRICAN GREY\n",
    "MACAW\n",
    "COCKATOO\n",
    "LORIKEET\n",
    "COUCAL\n",
    "BEE EATER\n",
    "HORNBILL\n",
    "HUMMINGBIRD\n",
    "JACAMAR\n",
    "TOUCAN\n",
    "DRAKE\n",
    "MERGANSER\n",
    "GOOSE\n",
    "BLACK SWAN\n",
    "TUSKER\n",
    "ECHIDNA\n",
    "PLATYPUS\n",
    "WALLABY\n",
    "KOALA\n",
    "WOMBAT\n",
    "JELLYFISH\n",
    "SEA ANEMONE\n",
    "BRAIN CORAL\n",
    "FLATWORM\n",
    "NEMATODE\n",
    "CONCH\n",
    "SNAIL\n",
    "SLUG\n",
    "SEA SLUG\n",
    "CHITON\n",
    "CHAMBERED NAUTILUS\n",
    "DUNGENESS CRAB\n",
    "ROCK CRAB\n",
    "FIDDLER CRAB\n",
    "KING CRAB\n",
    "AMERICAN LOBSTER\n",
    "SPINY LOBSTER\n",
    "CRAYFISH\n",
    "HERMIT CRAB\n",
    "ISOPOD\n",
    "WHITE STORK\n",
    "BLACK STORK\n",
    "SPOONBILL\n",
    "FLAMINGO\n",
    "LITTLE BLUE HERON\n",
    "AMERICAN EGRET\n",
    "BITTERN\n",
    "CRANE\n",
    "LIMPKIN\n",
    "EUROPEAN GALLINULE\n",
    "AMERICAN COOT\n",
    "BUSTARD\n",
    "RUDDY TURNSTONE\n",
    "RED-BACKED SANDPIPER\n",
    "REDSHANK\n",
    "DOWITCHER\n",
    "OYSTERCATCHER\n",
    "PELICAN\n",
    "KING PENGUIN\n",
    "ALBATROSS\n",
    "GREY WHALE\n",
    "KILLER WHALE\n",
    "DUGONG\n",
    "SEA LION\n",
    "CHIHUAHUA\n",
    "JAPANESE SPANIEL\n",
    "MALTESE DOG\n",
    "PEKINESE\n",
    "SHIH-TZU\n",
    "BLENHEIM SPANIEL\n",
    "PAPILLON\n",
    "TOY TERRIER\n",
    "RHODESIAN RIDGEBACK\n",
    "AFGHAN HOUND\n",
    "BASSET\n",
    "BEAGLE\n",
    "BLOODHOUND\n",
    "BLUETICK\n",
    "COONHOUND\n",
    "WALKER HOUND\n",
    "ENGLISH FOXHOUND\n",
    "REDBONE\n",
    "BORZOI\n",
    "IRISH WOLFHOUND\n",
    "ITALIAN GREYHOUND\n",
    "WHIPPET\n",
    "IBIZAN HOUND\n",
    "NORWEGIAN ELKHOUND\n",
    "OTTERHOUND\n",
    "SALUKI\n",
    "SCOTTISH DEERHOUND\n",
    "WEIMARANER\n",
    "STAFFORDSHIRE BULLTERRIER\n",
    "STAFFORDSHIRE TERRIER\n",
    "BEDLINGTON TERRIER\n",
    "BORDER TERRIER\n",
    "KERRY BLUE TERRIER\n",
    "IRISH TERRIER\n",
    "NORFOLK TERRIER\n",
    "NORWICH TERRIER\n",
    "YORKSHIRE TERRIER\n",
    "WIRE-HAIRED FOX TERRIER\n",
    "LAKELAND TERRIER\n",
    "SEALYHAM TERRIER\n",
    "AIREDALE\n",
    "CAIRN\n",
    "AUSTRALIAN TERRIER\n",
    "DANDIE DINMONT\n",
    "BOSTON BULL\n",
    "MINIATURE SCHNAUZER\n",
    "GIANT SCHNAUZER\n",
    "STANDARD SCHNAUZER\n",
    "SCOTCH TERRIER\n",
    "TIBETAN TERRIER\n",
    "SILKY TERRIER\n",
    "WHEATEN TERRIER\n",
    "WHITE TERRIER\n",
    "LHASA\n",
    "RETRIEVER\n",
    "CURLY-COATED RETRIEVER\n",
    "GOLDEN RETRIEVER\n",
    "LABRADOR RETRIEVER\n",
    "CHESAPEAKE BAY RETRIEVER\n",
    "SHORT-HAIRED POINTER\n",
    "VISLA\n",
    "ENGLISH SETTER\n",
    "IRISH SETTER\n",
    "GORDON SETTER\n",
    "BRITTANY SPANIEL\n",
    "CLUMBER\n",
    "ENGLISH SPRINGER\n",
    "WELSH SPRINGER SPANIEL\n",
    "COCKER SPANIEL\n",
    "SUSSEX SPANIEL\n",
    "IRISH WATERSPANIEL\n",
    "KUVASZ\n",
    "SCHIPPERKE\n",
    "GROENENDAEL\n",
    "MALINOIS\n",
    "BRIARD\n",
    "KELPIE\n",
    "KOMONDOR\n",
    "OLD ENGLISH SHEEPDOG\n",
    "SHETLAND SHEEPDOG\n",
    "COLLIE\n",
    "BORDER COLLIE\n",
    "BOUVIER DES FLANDRES\n",
    "ROTTWEILER\n",
    "GERMAN SHEPHERD\n",
    "DOBERMAN\n",
    "MINIATURE PINSCHER\n",
    "GREATER SWISS MOUNTAIN DOG\n",
    "BERNESE MOUNTAIN DOG\n",
    "APPENZELLER\n",
    "ENTLEBUCHER\n",
    "BOXER\n",
    "BULL MASTIFF\n",
    "TIBETAN MASTIFF\n",
    "FRENCH BULLDOG\n",
    "GREAT DANE\n",
    "SAINT BERNARD\n",
    "ESKIMO DOG\n",
    "MALAMUTE\n",
    "SIBERIAN HUSKY\n",
    "DALMATIAN\n",
    "AFFENPINSCHER\n",
    "BASENJI\n",
    "PUG\n",
    "LEONBERG\n",
    "NEWFOUNDLAND\n",
    "GREAT PYRENEES\n",
    "SAMOYED\n",
    "POMERANIAN\n",
    "CHOW\n",
    "KEESHOND\n",
    "BRABANCON GRIFFON\n",
    "PEMBROKE\n",
    "CARDIGAN\n",
    "TOY POODLE\n",
    "MINIATURE POODLE\n",
    "STANDARD POODLE\n",
    "MEXICAN HAIRLESS\n",
    "TIMBER WOLF\n",
    "WHITE WOLF\n",
    "RED WOLF\n",
    "COYOTE\n",
    "DINGO\n",
    "DHOLE\n",
    "AFRICAN HUNTING DOG\n",
    "HYENA\n",
    "RED FOX\n",
    "KIT FOX\n",
    "ARCTIC FOX\n",
    "GREY FOX\n",
    "TABBY\n",
    "TIGER CAT\n",
    "PERSIAN CAT\n",
    "SIAMESE CAT\n",
    "EGYPTIAN CAT\n",
    "COUGAR\n",
    "LYNX\n",
    "LEOPARD\n",
    "SNOW LEOPARD\n",
    "JAGUAR\n",
    "LION\n",
    "TIGER\n",
    "CHEETAH\n",
    "BROWN BEAR\n",
    "AMERICAN BLACK BEAR\n",
    "ICE BEAR\n",
    "SLOTH BEAR\n",
    "MONGOOSE\n",
    "MEERKAT\n",
    "TIGER BEETLE\n",
    "LADYBUG\n",
    "GROUND BEETLE\n",
    "LONG-HORNED BEETLE\n",
    "LEAF BEETLE\n",
    "DUNG BEETLE\n",
    "RHINOCEROS BEETLE\n",
    "WEEVIL\n",
    "FLY\n",
    "BEE\n",
    "ANT\n",
    "GRASSHOPPER\n",
    "CRICKET\n",
    "WALKING STICK\n",
    "COCKROACH\n",
    "MANTIS\n",
    "CICADA\n",
    "LEAFHOPPER\n",
    "LACEWING\n",
    "DRAGONFLY\n",
    "DAMSELFLY\n",
    "ADMIRAL\n",
    "RINGLET\n",
    "MONARCH\n",
    "CABBAGE BUTTERFLY\n",
    "SULPHUR BUTTERFLY\n",
    "LYCAENID\n",
    "STARFISH\n",
    "SEA URCHIN\n",
    "SEA CUCUMBER\n",
    "WOOD RABBIT\n",
    "HARE\n",
    "ANGORA\n",
    "HAMSTER\n",
    "PORCUPINE\n",
    "FOX SQUIRREL\n",
    "MARMOT\n",
    "BEAVER\n",
    "GUINEA PIG\n",
    "SORREL\n",
    "ZEBRA\n",
    "HOG\n",
    "WILD BOAR\n",
    "WARTHOG\n",
    "HIPPOPOTAMUS\n",
    "OX\n",
    "WATER BUFFALO\n",
    "BISON\n",
    "RAM\n",
    "BIGHORN\n",
    "IBEX\n",
    "HARTEBEEST\n",
    "IMPALA\n",
    "GAZELLE\n",
    "ARABIAN CAMEL\n",
    "LLAMA\n",
    "WEASEL\n",
    "MINK\n",
    "POLECAT\n",
    "BLACK-FOOTED FERRET\n",
    "OTTER\n",
    "SKUNK\n",
    "BADGER\n",
    "ARMADILLO\n",
    "THREE-TOED SLOTH\n",
    "ORANGUTAN\n",
    "GORILLA\n",
    "CHIMPANZEE\n",
    "GIBBON\n",
    "SIAMANG\n",
    "GUENON\n",
    "PATAS\n",
    "BABOON\n",
    "MACAQUE\n",
    "LANGUR\n",
    "COLOBUS\n",
    "PROBOSCIS MONKEY\n",
    "MARMOSET\n",
    "CAPUCHIN\n",
    "HOWLER MONKEY\n",
    "TITI\n",
    "SPIDER MONKEY\n",
    "SQUIRREL MONKEY\n",
    "MADAGASCAR CAT\n",
    "INDRI\n",
    "INDIAN ELEPHANT\n",
    "AFRICAN ELEPHANT\n",
    "LESSER PANDA\n",
    "GIANT PANDA\n",
    "BARRACOUTA\n",
    "EEL\n",
    "COHO\n",
    "ROCK BEAUTY\n",
    "ANEMONE FISH\n",
    "STURGEON\n",
    "GAR\n",
    "LIONFISH\n",
    "PUFFER\n",
    "ABACUS\n",
    "ABAYA\n",
    "ACADEMIC GOWN\n",
    "ACCORDION\n",
    "ACOUSTIC GUITAR\n",
    "AIRCRAFT CARRIER\n",
    "AIRLINER\n",
    "AIRSHIP\n",
    "ALTAR\n",
    "AMBULANCE\n",
    "AMPHIBIAN\n",
    "ANALOG CLOCK\n",
    "APIARY\n",
    "APRON\n",
    "ASHCAN\n",
    "ASSAULT RIFLE\n",
    "BACKPACK\n",
    "BAKERY\n",
    "BALANCE BEAM\n",
    "BALLOON\n",
    "BALLPOINT\n",
    "BAND AID\n",
    "BANJO\n",
    "BANNISTER\n",
    "BARBELL\n",
    "BARBER CHAIR\n",
    "BARBERSHOP\n",
    "BARN\n",
    "BAROMETER\n",
    "BARREL\n",
    "BARROW\n",
    "BASEBALL\n",
    "BASKETBALL\n",
    "BASSINET\n",
    "BASSOON\n",
    "BATHING CAP\n",
    "BATH TOWEL\n",
    "BATHTUB\n",
    "BEACH WAGON\n",
    "BEACON\n",
    "BEAKER\n",
    "BEARSKIN\n",
    "BEER BOTTLE\n",
    "BEER GLASS\n",
    "BELL COTE\n",
    "BIB\n",
    "BICYCLE-BUILT-FOR-TWO\n",
    "BIKINI\n",
    "BINDER\n",
    "BINOCULARS\n",
    "BIRDHOUSE\n",
    "BOATHOUSE\n",
    "BOBSLED\n",
    "BOLO TIE\n",
    "BONNET\n",
    "BOOKCASE\n",
    "BOOKSHOP\n",
    "BOTTLECAP\n",
    "BOW\n",
    "BOW TIE\n",
    "BRASS\n",
    "BRASSIERE\n",
    "BREAKWATER\n",
    "BREASTPLATE\n",
    "BROOM\n",
    "BUCKET\n",
    "BUCKLE\n",
    "BULLETPROOF VEST\n",
    "BULLET TRAIN\n",
    "BUTCHER SHOP\n",
    "CAB\n",
    "CALDRON\n",
    "CANDLE\n",
    "CANNON\n",
    "CANOE\n",
    "CAN OPENER\n",
    "CARDIGAN\n",
    "CAR MIRROR\n",
    "CAROUSEL\n",
    "CARPENTERS KIT\n",
    "CARTON\n",
    "CAR WHEEL\n",
    "CASH MACHINE\n",
    "CASSETTE\n",
    "CASSETTE PLAYER\n",
    "CASTLE\n",
    "CATAMARAN\n",
    "CD PLAYER\n",
    "CELLO\n",
    "CELLULAR TELEPHONE\n",
    "CHAIN\n",
    "CHAINLINK FENCE\n",
    "CHAIN MAIL\n",
    "CHAIN SAW\n",
    "CHEST\n",
    "CHIFFONIER\n",
    "CHIME\n",
    "CHINA CABINET\n",
    "CHRISTMAS STOCKING\n",
    "CHURCH\n",
    "CINEMA\n",
    "CLEAVER\n",
    "CLIFF DWELLING\n",
    "CLOAK\n",
    "CLOG\n",
    "COCKTAIL SHAKER\n",
    "COFFEE MUG\n",
    "COFFEEPOT\n",
    "COIL\n",
    "COMBINATION LOCK\n",
    "COMPUTER KEYBOARD\n",
    "CONFECTIONERY\n",
    "CONTAINER SHIP\n",
    "CONVERTIBLE\n",
    "CORKSCREW\n",
    "CORNET\n",
    "COWBOY BOOT\n",
    "COWBOY HAT\n",
    "CRADLE\n",
    "CRANE\n",
    "CRASH HELMET\n",
    "CRATE\n",
    "CRIB\n",
    "CROCK POT\n",
    "CROQUET BALL\n",
    "CRUTCH\n",
    "CUIRASS\n",
    "DAM\n",
    "DESK\n",
    "DESKTOP COMPUTER\n",
    "DIAL TELEPHONE\n",
    "DIAPER\n",
    "DIGITAL CLOCK\n",
    "DIGITAL WATCH\n",
    "DINING TABLE\n",
    "DISHRAG\n",
    "DISHWASHER\n",
    "DISK BRAKE\n",
    "DOCK\n",
    "DOGSLED\n",
    "DOME\n",
    "DOORMAT\n",
    "DRILLING PLATFORM\n",
    "DRUM\n",
    "DRUMSTICK\n",
    "DUMBBELL\n",
    "DUTCH OVEN\n",
    "ELECTRIC FAN\n",
    "ELECTRIC GUITAR\n",
    "ELECTRIC LOCOMOTIVE\n",
    "ENTERTAINMENT CENTER\n",
    "ENVELOPE\n",
    "ESPRESSO MAKER\n",
    "FACE POWDER\n",
    "FEATHER BOA\n",
    "FILE\n",
    "FIREBOAT\n",
    "FIRE ENGINE\n",
    "FIRE SCREEN\n",
    "FLAGPOLE\n",
    "FLUTE\n",
    "FOLDING CHAIR\n",
    "FOOTBALL HELMET\n",
    "FORKLIFT\n",
    "FOUNTAIN\n",
    "FOUNTAIN PEN\n",
    "FOUR-POSTER\n",
    "FREIGHT CAR\n",
    "FRENCH HORN\n",
    "FRYING PAN\n",
    "FUR COAT\n",
    "GARBAGE TRUCK\n",
    "GASMASK\n",
    "GAS PUMP\n",
    "GOBLET\n",
    "GO-KART\n",
    "GOLF BALL\n",
    "GOLFCART\n",
    "GONDOLA\n",
    "GONG\n",
    "GOWN\n",
    "GRAND PIANO\n",
    "GREENHOUSE\n",
    "GRILLE\n",
    "GROCERY STORE\n",
    "GUILLOTINE\n",
    "HAIR SLIDE\n",
    "HAIR SPRAY\n",
    "HALF TRACK\n",
    "HAMMER\n",
    "HAMPER\n",
    "HAND BLOWER\n",
    "HAND-HELD COMPUTER\n",
    "HANDKERCHIEF\n",
    "HARD DISC\n",
    "HARMONICA\n",
    "HARP\n",
    "HARVESTER\n",
    "HATCHET\n",
    "HOLSTER\n",
    "HOME THEATER\n",
    "HONEYCOMB\n",
    "HOOK\n",
    "HOOPSKIRT\n",
    "HORIZONTAL BAR\n",
    "HORSE CART\n",
    "HOURGLASS\n",
    "IPOD\n",
    "IRON\n",
    "JACK-O-LANTERN\n",
    "JEAN\n",
    "JEEP\n",
    "JERSEY\n",
    "JIGSAW PUZZLE\n",
    "JINRIKISHA\n",
    "JOYSTICK\n",
    "KIMONO\n",
    "KNEE PAD\n",
    "KNOT\n",
    "LAB COAT\n",
    "LADLE\n",
    "LAMPSHADE\n",
    "LAPTOP\n",
    "LAWN MOWER\n",
    "LENS CAP\n",
    "LETTER OPENER\n",
    "LIBRARY\n",
    "LIFEBOAT\n",
    "LIGHTER\n",
    "LIMOUSINE\n",
    "LINER\n",
    "LIPSTICK\n",
    "LOAFER\n",
    "LOTION\n",
    "LOUDSPEAKER\n",
    "LOUPE\n",
    "LUMBERMILL\n",
    "MAGNETIC COMPASS\n",
    "MAILBAG\n",
    "MAILBOX\n",
    "MAILLOT\n",
    "MAILLOT\n",
    "MANHOLE COVER\n",
    "MARACA\n",
    "MARIMBA\n",
    "MASK\n",
    "MATCHSTICK\n",
    "MAYPOLE\n",
    "MAZE\n",
    "MEASURING CUP\n",
    "MEDICINE CHEST\n",
    "MEGALITH\n",
    "MICROPHONE\n",
    "MICROWAVE\n",
    "MILITARY UNIFORM\n",
    "MILK CAN\n",
    "MINIBUS\n",
    "MINISKIRT\n",
    "MINIVAN\n",
    "MISSILE\n",
    "MITTEN\n",
    "MIXING BOWL\n",
    "MOBILE HOME\n",
    "MODEL T\n",
    "MODEM\n",
    "MONASTERY\n",
    "MONITOR\n",
    "MOPED\n",
    "MORTAR\n",
    "MORTARBOARD\n",
    "MOSQUE\n",
    "MOSQUITO NET\n",
    "MOTOR SCOOTER\n",
    "MOUNTAIN BIKE\n",
    "MOUNTAIN TENT\n",
    "MOUSE\n",
    "MOUSETRAP\n",
    "MOVING VAN\n",
    "MUZZLE\n",
    "NAIL\n",
    "NECK BRACE\n",
    "NECKLACE\n",
    "NIPPLE\n",
    "NOTEBOOK\n",
    "OBELISK\n",
    "OBOE\n",
    "OCARINA\n",
    "ODOMETER\n",
    "OIL FILTER\n",
    "ORGAN\n",
    "OSCILLOSCOPE\n",
    "OVERSKIRT\n",
    "OXCART\n",
    "OXYGEN MASK\n",
    "PACKET\n",
    "PADDLE\n",
    "PADDLEWHEEL\n",
    "PADLOCK\n",
    "PAINTBRUSH\n",
    "PAJAMA\n",
    "PALACE\n",
    "PANPIPE\n",
    "PAPER TOWEL\n",
    "PARACHUTE\n",
    "PARALLEL BARS\n",
    "PARK BENCH\n",
    "PARKING METER\n",
    "PASSENGER CAR\n",
    "PATIO\n",
    "PAY-PHONE\n",
    "PEDESTAL\n",
    "PENCIL BOX\n",
    "PENCIL SHARPENER\n",
    "PERFUME\n",
    "PETRI DISH\n",
    "PHOTOCOPIER\n",
    "PICK\n",
    "PICKELHAUBE\n",
    "PICKET FENCE\n",
    "PICKUP\n",
    "PIER\n",
    "PIGGY BANK\n",
    "PILL BOTTLE\n",
    "PILLOW\n",
    "PING-PONG BALL\n",
    "PINWHEEL\n",
    "PIRATE\n",
    "PITCHER\n",
    "PLANE\n",
    "PLANETARIUM\n",
    "PLASTIC BAG\n",
    "PLATE RACK\n",
    "PLOW\n",
    "PLUNGER\n",
    "POLAROID CAMERA\n",
    "POLE\n",
    "POLICE VAN\n",
    "PONCHO\n",
    "POOL TABLE\n",
    "POP BOTTLE\n",
    "POT\n",
    "POTTERS WHEEL\n",
    "POWER DRILL\n",
    "PRAYER RUG\n",
    "PRINTER\n",
    "PRISON\n",
    "PROJECTILE\n",
    "PROJECTOR\n",
    "PUCK\n",
    "PUNCHING BAG\n",
    "PURSE\n",
    "QUILL\n",
    "QUILT\n",
    "RACER\n",
    "RACKET\n",
    "RADIATOR\n",
    "RADIO\n",
    "RADIO TELESCOPE\n",
    "RAIN BARREL\n",
    "RECREATIONAL VEHICLE\n",
    "REEL\n",
    "REFLEX CAMERA\n",
    "REFRIGERATOR\n",
    "REMOTE CONTROL\n",
    "RESTAURANT\n",
    "REVOLVER\n",
    "RIFLE\n",
    "ROCKING CHAIR\n",
    "ROTISSERIE\n",
    "RUBBER ERASER\n",
    "RUGBY BALL\n",
    "RULE\n",
    "RUNNING SHOE\n",
    "SAFE\n",
    "SAFETY PIN\n",
    "SALTSHAKER\n",
    "SANDAL\n",
    "SARONG\n",
    "SAX\n",
    "SCABBARD\n",
    "SCALE\n",
    "SCHOOL BUS\n",
    "SCHOONER\n",
    "SCOREBOARD\n",
    "SCREEN\n",
    "SCREW\n",
    "SCREWDRIVER\n",
    "SEAT BELT\n",
    "SEWING MACHINE\n",
    "SHIELD\n",
    "SHOE SHOP\n",
    "SHOJI\n",
    "SHOPPING BASKET\n",
    "SHOPPING CART\n",
    "SHOVEL\n",
    "SHOWER CAP\n",
    "SHOWER CURTAIN\n",
    "SKI\n",
    "SKI MASK\n",
    "SLEEPING BAG\n",
    "SLIDE RULE\n",
    "SLIDING DOOR\n",
    "SLOT\n",
    "SNORKEL\n",
    "SNOWMOBILE\n",
    "SNOWPLOW\n",
    "SOAP DISPENSER\n",
    "SOCCER BALL\n",
    "SOCK\n",
    "SOLAR DISH\n",
    "SOMBRERO\n",
    "SOUP BOWL\n",
    "SPACE BAR\n",
    "SPACE HEATER\n",
    "SPACE SHUTTLE\n",
    "SPATULA\n",
    "SPEEDBOAT\n",
    "SPIDER WEB\n",
    "SPINDLE\n",
    "SPORTS CAR\n",
    "SPOTLIGHT\n",
    "STAGE\n",
    "STEAM LOCOMOTIVE\n",
    "STEEL ARCH BRIDGE\n",
    "STEEL DRUM\n",
    "STETHOSCOPE\n",
    "STOLE\n",
    "STONE WALL\n",
    "STOPWATCH\n",
    "STOVE\n",
    "STRAINER\n",
    "STREETCAR\n",
    "STRETCHER\n",
    "STUDIO COUCH\n",
    "STUPA\n",
    "SUBMARINE\n",
    "SUIT\n",
    "SUNDIAL\n",
    "SUNGLASS\n",
    "SUNGLASSES\n",
    "SUNSCREEN\n",
    "SUSPENSION BRIDGE\n",
    "SWAB\n",
    "SWEATSHIRT\n",
    "SWIMMING TRUNKS\n",
    "SWING\n",
    "SWITCH\n",
    "SYRINGE\n",
    "TABLE LAMP\n",
    "TANK\n",
    "TAPE PLAYER\n",
    "TEAPOT\n",
    "TEDDY\n",
    "TELEVISION\n",
    "TENNIS BALL\n",
    "THATCH\n",
    "THEATER CURTAIN\n",
    "THIMBLE\n",
    "THRESHER\n",
    "THRONE\n",
    "TILE ROOF\n",
    "TOASTER\n",
    "TOBACCO SHOP\n",
    "TOILET SEAT\n",
    "TORCH\n",
    "TOTEM POLE\n",
    "TOW TRUCK\n",
    "TOYSHOP\n",
    "TRACTOR\n",
    "TRAILER TRUCK\n",
    "TRAY\n",
    "TRENCH COAT\n",
    "TRICYCLE\n",
    "TRIMARAN\n",
    "TRIPOD\n",
    "TRIUMPHAL ARCH\n",
    "TROLLEYBUS\n",
    "TROMBONE\n",
    "TUB\n",
    "TURNSTILE\n",
    "TYPEWRITER KEYBOARD\n",
    "UMBRELLA\n",
    "UNICYCLE\n",
    "UPRIGHT\n",
    "VACUUM\n",
    "VASE\n",
    "VAULT\n",
    "VELVET\n",
    "VENDING MACHINE\n",
    "VESTMENT\n",
    "VIADUCT\n",
    "VIOLIN\n",
    "VOLLEYBALL\n",
    "WAFFLE IRON\n",
    "WALL CLOCK\n",
    "WALLET\n",
    "WARDROBE\n",
    "WARPLANE\n",
    "WASHBASIN\n",
    "WASHER\n",
    "WATER BOTTLE\n",
    "WATER JUG\n",
    "WATER TOWER\n",
    "WHISKEY JUG\n",
    "WHISTLE\n",
    "WIG\n",
    "WINDOW SCREEN\n",
    "WINDOW SHADE\n",
    "WINDSOR TIE\n",
    "WINE BOTTLE\n",
    "WING\n",
    "WOK\n",
    "WOODEN SPOON\n",
    "WOOL\n",
    "WORM FENCE\n",
    "WRECK\n",
    "YAWL\n",
    "YURT\n",
    "WEB SITE\n",
    "COMIC BOOK\n",
    "CROSSWORD PUZZLE\n",
    "STREET SIGN\n",
    "TRAFFIC LIGHT\n",
    "BOOK JACKET\n",
    "MENU\n",
    "PLATE\n",
    "GUACAMOLE\n",
    "CONSOMME\n",
    "HOT POT\n",
    "TRIFLE\n",
    "ICE CREAM\n",
    "ICE LOLLY\n",
    "FRENCH LOAF\n",
    "BAGEL\n",
    "PRETZEL\n",
    "CHEESEBURGER\n",
    "HOTDOG\n",
    "MASHED POTATO\n",
    "HEAD CABBAGE\n",
    "BROCCOLI\n",
    "CAULIFLOWER\n",
    "ZUCCHINI\n",
    "SPAGHETTI SQUASH\n",
    "ACORN SQUASH\n",
    "BUTTERNUT SQUASH\n",
    "CUCUMBER\n",
    "ARTICHOKE\n",
    "BELL PEPPER\n",
    "CARDOON\n",
    "MUSHROOM\n",
    "GRANNY SMITH\n",
    "STRAWBERRY\n",
    "ORANGE\n",
    "LEMON\n",
    "FIG\n",
    "PINEAPPLE\n",
    "BANANA\n",
    "JACKFRUIT\n",
    "CUSTARD APPLE\n",
    "POMEGRANATE\n",
    "HAY\n",
    "CARBONARA\n",
    "CHOCOLATE SAUCE\n",
    "DOUGH\n",
    "MEAT LOAF\n",
    "PIZZA\n",
    "POTPIE\n",
    "BURRITO\n",
    "RED WINE\n",
    "ESPRESSO\n",
    "CUP\n",
    "EGGNOG\n",
    "ALP\n",
    "BUBBLE\n",
    "CLIFF\n",
    "CORAL REEF\n",
    "GEYSER\n",
    "LAKESIDE\n",
    "PROMONTORY\n",
    "SANDBAR\n",
    "SEASHORE\n",
    "VALLEY\n",
    "VOLCANO\n",
    "BALLPLAYER\n",
    "GROOM\n",
    "SCUBA DIVER\n",
    "RAPESEED\n",
    "DAISY\n",
    "LADY SLIPPER\n",
    "CORN\n",
    "ACORN\n",
    "HIP\n",
    "BUCKEYE\n",
    "CORAL FUNGUS\n",
    "AGARIC\n",
    "GYROMITRA\n",
    "STINKHORN\n",
    "EARTHSTAR\n",
    "HEN-OF-THE-WOODS\n",
    "BOLETE\n",
    "EAR\n",
    "TOILET TISSUE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "name": "deploy-densenet-local",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
