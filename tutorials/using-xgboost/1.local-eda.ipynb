{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "To get started, we will prototype the workflow locally.\n",
    "\n",
    "**Warning:** this notebook may fail if your local machine does not have sufficient resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade dask distributed fastparquet adlfs xgboost pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "The data is modified from a Kaggle competition and hosted publicly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start a distributed Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "c = Client()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the Pythonic filesystem\n",
    "\n",
    "**Tip:** if you're not using public data, you need to provide data credentials. These can be retrieved through Azure ML Datastores, e.g.:\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore() # ws.datastores[\"my-datastore-name\"]\n",
    "\n",
    "storage_options = {\n",
    "    \"account_name\": ds.account_name,\n",
    "    \"account_key\": ds.account_key\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adlfs import AzureBlobFileSystem\n",
    "\n",
    "container_name = \"malware\"\n",
    "storage_options = {\"account_name\": \"azuremlexamples\"}\n",
    "\n",
    "fs = AzureBlobFileSystem(**storage_options)\n",
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list the processed (partitioned) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = fs.ls(f\"{container_name}/processed\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data into a (dask) dataframe - note pandas also accepts the ``storage_options`` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "for f in files:\n",
    "    if \"train\" in f:\n",
    "        df_train = dd.read_parquet(f\"az://{f}\", storage_options=storage_options)\n",
    "    elif \"test\" in f:\n",
    "        df_test = dd.read_parquet(f\"az://{f}\", storage_options=storage_options)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Explore the data - for the purpose of this tutorial, we will simply print out a sample of the train and test dataframes and compute other basic descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train[\"HasDetections\"].compute().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Prepare data for ML - for the purpose of this tutorial, we will simply ignore non-numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df_train.columns if df_train.dtypes[col] != \"object\"]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[cols].drop(\"HasDetections\", axis=1).values.persist()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train[\"HasDetections\"].values.persist()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost\n",
    "\n",
    "Now, we can use the ``xgboost.dask`` module for distributed XGBoost training through Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.dask.DaskDMatrix(c, X, y)\n",
    "dtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 2  # just see if it works\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"gamma\": 0,\n",
    "    \"max_depth\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.dask.train(c, params, dtrain, num_boost_round=num_boost_round)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "Optionally, save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"booster\"].save_model(\"xgboost.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
