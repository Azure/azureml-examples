{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"azureml-core<0.1.10\" --index-url https://azuremlsdktestpypi.azureedge.net/sdk-release/master/588E708E0DF342C4A80BD954289657CF --extra-index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"azureml-pipeline-core<0.1.10\" --index-url https://azuremlsdktestpypi.azureedge.net/sdk-release/master/588E708E0DF342C4A80BD954289657CF --extra-index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"azureml-pipeline-steps<0.1.10\" --index-url https://azuremlsdktestpypi.azureedge.net/sdk-release/master/588E708E0DF342C4A80BD954289657CF --extra-index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your AML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Synapse workspace to AML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import LinkedService, SynapseWorkspaceLinkedServiceConfiguration\n",
    "synapse_link_config = SynapseWorkspaceLinkedServiceConfiguration(\n",
    "    subscription_id=\"4faaaf21-663f-4391-96fd-47197c630979\",\n",
    "    resource_group=\"static_resources_synapse_test\",\n",
    "    name=\"synapsepetesting\"\n",
    ")\n",
    "\n",
    "linked_service = LinkedService.register(\n",
    "    workspace=ws,\n",
    "    name='synapselinkservice',\n",
    "    linked_service_config=synapse_link_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.linked_services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View all the linked services\n",
    "\n",
    "There is a MSI (system_assigned_identity_principal_id) created for each linked service, for example:\n",
    "\n",
    "name=synapselink,</p>\n",
    "type=Synapse, </p>\n",
    "linked_service_resource_id=/subscriptions/4faaaf21-663f-4391-96fd-47197c630979/resourceGroups/static_resources_synapse_test/providers/Microsoft.Synapse/workspaces/synapsetest2, </p>\n",
    "system_assigned_identity_principal_id=eb355d52-3806-4c5a-aec9-91447e8cfc2e </p>\n",
    "\n",
    "#### Make sure you grant spark admin role of the synapse workspace to MSI in synapse studio before you submit job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinkedService.list(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach Synapse spark pool as AML compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import SynapseCompute, ComputeTarget\n",
    "spark_pool_name = \"sparkpool1\"\n",
    "attached_synapse_name = \"synapsecompute\"\n",
    "\n",
    "attach_config = SynapseCompute.attach_configuration(\n",
    "        linked_service,\n",
    "        type=\"SynapseSpark\",\n",
    "        pool_name=spark_pool_name)\n",
    "\n",
    "synapse_compute=ComputeTarget.attach(\n",
    "        workspace=ws,\n",
    "        name=attached_synapse_name,\n",
    "        attach_configuration=attach_config)\n",
    "\n",
    "synapse_compute.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start an experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.dataset_factory import DataType\n",
    "\n",
    "dataset_name=\"blob_ds\"\n",
    "try:\n",
    "    dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "    print('Found existing dataset, use it.')\n",
    "except:\n",
    "    # create a TabularDataset from a delimited file behind a public web url and convert column \"Survived\" to boolean\n",
    "    web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\n",
    "    titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path, set_column_types={'Survived': DataType.to_bool()})\n",
    "    titanic_ds.register(ws,name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import HDFSOutputDatasetConfig\n",
    "output = HDFSOutputDatasetConfig(\n",
    "    \"synapse_step_output\",\n",
    "    destination=(ws.datastores['workspaceblobstore'],\"test2\")).register_on_complete(name=\"registered_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import RunConfiguration, Experiment\n",
    "\n",
    "run_config = RunConfiguration(framework=\"pyspark\")\n",
    "run_config.output_data = {output.name: output}\n",
    "\n",
    "run_config.target = attached_synapse_name\n",
    "\n",
    "run_config.spark.configuration[\"spark.driver.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.driver.cores\"] = 2 \n",
    "run_config.spark.configuration[\"spark.executor.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.executor.cores\"] = 1 \n",
    "run_config.spark.configuration[\"spark.executor.instances\"] = 1 \n",
    "\n",
    "from azureml.core import ScriptRunConfig \n",
    "\n",
    "script_run_config = ScriptRunConfig(source_directory = './script', \n",
    "                                    script= 'pyspark_job_exp.py', \n",
    "                                    arguments = ['args1','args2'], \n",
    "                                    run_config = run_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment \n",
    "exp = Experiment(workspace=ws, name=\"synapse-spark\") \n",
    "run = exp.submit(config=script_run_config) \n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Dataset, Environment,Datastore\n",
    "ws = Workspace.get(name='ws_canary_test', subscription_id='1aefdc5e-3a7c-4d71-a9f9-f5d3b03be19a', resource_group='rg_e2e_test_canary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "synapse_compute = ws.compute_targets[attached_synapse_name]\n",
    "synapse_compute\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, SynapseSparkStep\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "train_run_config = RunConfiguration()\n",
    "conda = CondaDependencies.create(\n",
    "    pip_indexurl='https://azuremlsdktestpypi.azureedge.net/sdk-release/master/588E708E0DF342C4A80BD954289657CF',\n",
    "    pip_packages=['azureml-sdk<0.1.1', 'azureml-dataprep[fuse,pandas]>=1.1.19', 'azureml-telemetry'],\n",
    "    pin_sdk_version=False\n",
    ")\n",
    "\n",
    "train_run_config.environment.python.conda_dependencies = conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import HDFSOutputDatasetConfig\n",
    "\n",
    "ds = Dataset.get_by_name(ws,name='blob_ds')\n",
    "input1 = ds.as_named_input('synapseinput')\n",
    "\n",
    "output1 = HDFSOutputDatasetConfig(\n",
    "    \"synapse_step_output\", destination=(ws.datastores['workspaceblobstore'],\"test1\")).register_on_complete(name=\"registered_dataset\")\n",
    "\n",
    "input2 = output1.as_input(\"input2\").as_download()\n",
    "\n",
    "\n",
    "step_1 = SynapseSparkStep(name = 'synapse-spark',\n",
    "                          file = 'pyspark_job_pipeline.py',\n",
    "                          source_directory=\"./script\", \n",
    "                          inputs=[input1],\n",
    "                          outputs=[output1],\n",
    "                          compute_target = synapse_compute,\n",
    "                          driver_memory = \"7g\",\n",
    "                          driver_cores = 4,\n",
    "                          executor_memory = \"7g\",\n",
    "                          executor_cores = 2,\n",
    "                          num_executors = 1)\n",
    "\n",
    "step_2 = PythonScriptStep(script_name=\"train.py\",\n",
    "                          arguments=[input2],\n",
    "                          inputs=[input2],\n",
    "                          compute_target=cpu_cluster_name,\n",
    "                          runconfig = train_run_config,\n",
    "                          source_directory=\"./script\",\n",
    "                          allow_reuse=False)\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[step_1, step_2])\n",
    "pipeline_run = pipeline.submit('two_steps', regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}