{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Bring your own data (Part 3 of 3)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "In the previous [Tutorial: Train a model in the cloud](2.train-model.ipynb) article, the CIFAR10 data was downloaded using the builtin `torchvision.datasets.CIFAR10` method in the PyTorch API. However, in many cases you are going to want to use your own data in a remote training run. This article focuses on the workflow you can leverage such that you can work with your own data in Azure Machine Learning. \n",
    "\n",
    "By the end of this tutorial you would have a better understanding of:\n",
    "\n",
    "- How to upload your data to Azure\n",
    "- Best practices for working with cloud data in Azure Machine Learning\n",
    "- Working with command-line arguments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- \n",
    "\n",
    "---\n",
    "\n",
    "## Your machine learning code\n",
    "\n",
    "By now you have your training script running in Azure Machine Learning, and can monitor the model performance. Let's _parametrize_ the training script by introducing\n",
    "arguments. Using arguments will allow you to easily compare different hyperparmeters.\n",
    "\n",
    "Presently our training script is set to download the CIFAR10 dataset on each run. The python code in [train-with-cloud-data-and-logging.py](../../code/models/pytorch/cifar10-cnn/train-with-cloud-data-and-logging.py) now uses **`argparse` to parametize the script.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding your machine learning code changes\n",
    "\n",
    "The script `train-with-cloud-data-and-logging.py` has leveraged the `argparse` library to set up the `--data-path`, `--learning-rate`, `--momentum`, and `--epochs` arguments:\n",
    "\n",
    "```python\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data-path\", type=str, help=\"Path to the training data\")\n",
    "parser.add_argument(\"--learning-rate\", type=float, default=0.001, help=\"Learning rate for SGD\")\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.9, help=\"Momentum for SGD\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=2, help=\"Number of epochs to train\")\n",
    "args = parser.parse_args()\n",
    "```\n",
    "\n",
    "The script was adapted to update the optimizer to use the user-defined parameters:\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),\n",
    "    lr=args.learning_rate,     # get learning rate from command-line argument\n",
    "    momentum=args.momentum,    # get momentum from command-line argument\n",
    ")\n",
    "```\n",
    "\n",
    "Similarly the training loop was adapted to update the number of epochs to train to use the user-defined parameters:\n",
    "```python\n",
    "for epoch in range(args.epochs):\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your data to Azure\n",
    "\n",
    "In order to run this script in Azure Machine Learning, you need to make your training data available in Azure. Your Azure Machine Learning workspace comes equipped with a _default_ **Datastore** - an Azure Blob storage account - that you can use to store your training data.\n",
    "\n",
    "> <span style=\"color:purple; font-weight:bold\">! NOTE <br>\n",
    "> Azure Machine Learning allows you to connect other cloud-based datastores that store your data. For more details, see [datastores documentation](./concept-data.md).</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "ws = Workspace.from_config(\"~/code/default.json\")\n",
    "\n",
    "datasets.CIFAR10(\".\", download=True)\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(\n",
    "    src_dir=\"cifar-10-batches-py\",\n",
    "    target_path=\"datasets/cifar10\",\n",
    "    overwrite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.remove(\"cifar-10-python.tar.gz\")\n",
    "shutil.rmtree(\"cifar-10-batches-py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `target_path` specifies the path on the datastore where the CIFAR10 data will be uploaded.\n",
    "\n",
    "## Submit your machine learning code to Azure Machine Learning\n",
    "\n",
    "As you have done previously, create a new Python control script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remote run",
     "batchai",
     "configure run",
     "use notebook widget",
     "get metrics",
     "use datastore"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2be8c7cbbdf4bf9a04bfe386b0727f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/getting-started-train-model-cloud-data-tutorial/runs/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0?wsid=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourcegroups/azureml-examples/workspaces/default\", \"run_id\": \"getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0\", \"run_properties\": {\"run_id\": \"getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0\", \"created_utc\": \"2020-10-07T19:35:14.090876Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"3205f1d4-48f1-4cff-b638-3844d2d2304f\", \"azureml.git.repository_uri\": \"https://github.com/azure/azureml-examples\", \"mlflow.source.git.repoURL\": \"https://github.com/azure/azureml-examples\", \"azureml.git.branch\": \"cody/cs-at\", \"mlflow.source.git.branch\": \"cody/cs-at\", \"azureml.git.commit\": \"2c456124445eff56a5432391df5498d5cbc5aed0\", \"mlflow.source.git.commit\": \"2c456124445eff56a5432391df5498d5cbc5aed0\", \"azureml.git.dirty\": \"True\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":20}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-10-07T19:35:59.826286Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-logs/55_azureml-execution-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt?sv=2019-02-02&sr=b&sig=yBGGOD%2BlFEprP3%2Bn6j209aa3Ev1TnK3bMtOBkYwSTUo%3D&st=2020-10-07T19%3A25%3A58Z&se=2020-10-08T03%3A35%3A58Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-logs/65_job_prep-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt?sv=2019-02-02&sr=b&sig=631clpJTcQbzz7lRVQkMb971ukkzRYvV%2B7w%2FAJv6TaQ%3D&st=2020-10-07T19%3A25%3A58Z&se=2020-10-08T03%3A35%3A58Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=lsp%2BTSY1LOQ1DvXSo7W9FmzMjzc8%2Fc4UW4iHYw9ObrA%3D&st=2020-10-07T19%3A25%3A58Z&se=2020-10-08T03%3A35%3A58Z&sp=r\", \"azureml-logs/75_job_post-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-logs/75_job_post-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt?sv=2019-02-02&sr=b&sig=n4SWaFYcxr%2BbtU8ErsOoUuVbRc1UeEF%2FIaBbdT9iNZo%3D&st=2020-10-07T19%3A25%3A59Z&se=2020-10-08T03%3A35%3A59Z&sp=r\", \"azureml-logs/process_info.json\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=OdZZZBqW24%2Fy1QouiME%2BuKIKlyODvn8N47HRfAvIuKM%3D&st=2020-10-07T19%3A25%3A59Z&se=2020-10-08T03%3A35%3A59Z&sp=r\", \"azureml-logs/process_status.json\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=m9V2R%2FpmJc3DwMHICbG03oIONjcSQ97%2B3q1CmJBwwkA%3D&st=2020-10-07T19%3A25%3A59Z&se=2020-10-08T03%3A35%3A59Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=B1a3zhyh3EOiOhAC38jYfSg5kb7X1cQ3H3jqW7UurPE%3D&st=2020-10-07T19%3A25%3A57Z&se=2020-10-08T03%3A35%3A57Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://default7955323798.blob.core.windows.net/azureml/ExperimentRun/dcid.getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=6ioWGCUCBvuukZ9qKgdZjebKJua%2Bw4lp%2FcIP7qKcNBY%3D&st=2020-10-07T19%3A25%3A57Z&se=2020-10-08T03%3A35%3A57Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_ce1e3da8e5217d8676fc95131adf4643c983fdd6c88b0b24a83d0b80e331194d_d.txt\"]], \"run_duration\": \"0:00:45\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020/10/07 19:35:39 logger.go:297: Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\\n2020/10/07 19:35:39 logger.go:297: Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\\n[2020-10-07T19:35:40.998037] Entering context manager injector.\\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['train-with-cloud-data-and-logging.py', '--data-path', 'DatasetConsumptionConfig:input_7b890f4d', '--learning-rate', '0.003', '--momentum', '0.92', '--epochs', '2'])\\nTraceback (most recent call last):\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/default/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/mounts/workspaceblobstore/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-setup/context_managers.py\\\", line 507, in _ensure_dataprep\\n    import azureml.dataprep\\nModuleNotFoundError: No module named 'azureml.dataprep'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/default/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/mounts/workspaceblobstore/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-setup/context_manager_injector.py\\\", line 309, in <module>\\n    cm_objects = [create_wrapped_context_manager(pair) for pair in options.inject]\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/default/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/mounts/workspaceblobstore/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-setup/context_manager_injector.py\\\", line 309, in <listcomp>\\n    cm_objects = [create_wrapped_context_manager(pair) for pair in options.inject]\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/default/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/mounts/workspaceblobstore/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-setup/context_manager_injector.py\\\", line 245, in create_wrapped_context_manager\\n    context_manager = cm_class(cm_config)\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/default/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/mounts/workspaceblobstore/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-setup/context_managers.py\\\", line 340, in __init__\\n    self._ensure_dataprep(config)\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/default/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/mounts/workspaceblobstore/azureml/getting-started-train-model-cloud-data-tutorial_1602099307_95eb62b0/azureml-setup/context_managers.py\\\", line 510, in _ensure_dataprep\\n    \\\"azureml-dataprep is not installed. Dataset cannot be used without azureml-dataprep. Please \\\"\\nImportError: azureml-dataprep is not installed. Dataset cannot be used without azureml-dataprep. Please make sure azureml-dataprep[fuse,pandas] is installed by specifying it in the conda dependencies. pandas is optional and should be only installed if you intend to create a pandas DataFrame from the dataset.\\n2020/10/07 19:35:42 logger.go:297: Failed to run the wrapper cmd with err: exit status 1\\n2020/10/07 19:35:42 logger.go:297: Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\\n2020/10/07 19:35:42 sysutils_linux.go:221: mpirun version string: {\\nIntel(R) MPI Library for Linux* OS, Version 2018 Update 3 Build 20180411 (id: 18329)\\nCopyright 2003-2018 Intel Corporation.\\n}\\n2020/10/07 19:35:42 sysutils_linux.go:225: MPI publisher: intel ; version: 2018\\n2020/10/07 19:35:42 logger.go:297: Process Exiting with Code:  1\\n\\nError occurred: AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.15.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.core import (\n",
    "    Workspace,\n",
    "    Experiment,\n",
    "    Environment,\n",
    "    ScriptRunConfig,\n",
    "    Dataset,\n",
    ")\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "ws = Workspace.from_config(\"~/code/default.json\")\n",
    "\n",
    "ds = Dataset.File.from_files(\n",
    "    path=(ws.get_default_datastore(), \"datasets/cifar10\")\n",
    ")\n",
    "env = Environment.from_conda_specification(\n",
    "    name=\"pytorch-env-tutorial\",\n",
    "    file_path=\"../../environments/pytorch-example.yml\",\n",
    ")\n",
    "exp = Experiment(\n",
    "    workspace=ws, name=\"getting-started-train-model-cloud-data-tutorial\"\n",
    ")\n",
    "src = ScriptRunConfig(\n",
    "    source_directory=\"../../code/models/pytorch/cifar10-cnn\",\n",
    "    script=\"train-with-cloud-data-and-logging.py\",\n",
    "    compute_target=\"cpu-cluster\",\n",
    "    environment=env,\n",
    "    arguments=[\n",
    "        \"--data-path\",\n",
    "        ds.as_mount(),\n",
    "        \"--learning-rate\",\n",
    "        0.003,\n",
    "        \"--momentum\",\n",
    "        0.92,\n",
    "        \"--epochs\",\n",
    "        2,\n",
    "    ],\n",
    ")\n",
    "\n",
    "run = exp.submit(src)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the control code\n",
    "\n",
    "The above control code has the following additional code compared to the control code written in [previous tutorial](03-train-model.ipynb)\n",
    "\n",
    "**`ds = Dataset.File.from_files(path=(datastore, 'datasets/cifar10'))`**: A Dataset is used to reference the data you uploaded to the Azure Blob Store. Datasets are an abstraction layer on top of your data that are designed to improve reliability and trustworthiness.\n",
    "\n",
    "\n",
    "**`src = ScriptRunConfig(...)`**: We modified the `ScriptRunConfig` to include a list of arguments that will be passed into training script. We also specified `ds.as_mount()`, which means the directory specified will be _mounted_ to the compute target.\n",
    "\n",
    "## Inspect the 70_driver_log log file\n",
    "\n",
    "In the navigate to the 70_driver_log.txt file - you should see the following output:\n",
    "\n",
    "```\n",
    "Processing 'input'.\n",
    "Processing dataset FileDataset\n",
    "{\n",
    "  \"source\": [\n",
    "    \"('workspaceblobstore', 'datasets/cifar10')\"\n",
    "  ],\n",
    "  \"definition\": [\n",
    "    \"GetDatastoreFiles\"\n",
    "  ],\n",
    "  \"registration\": {\n",
    "    \"id\": \"XXXXX\",\n",
    "    \"name\": null,\n",
    "    \"version\": null,\n",
    "    \"workspace\": \"Workspace.create(name='XXXX', subscription_id='XXXX', resource_group='X')\"\n",
    "  }\n",
    "}\n",
    "Mounting input to /tmp/tmp9kituvp3.\n",
    "Mounted input to /tmp/tmp9kituvp3 as folder.\n",
    "Exit __enter__ of DatasetContextManager\n",
    "Entering Run History Context Manager.\n",
    "Current directory:  /mnt/batch/tasks/shared/LS_root/jobs/dsvm-aml/azureml/tutorial-session-3_1600171983_763c5381/mounts/workspaceblobstore/azureml/tutorial-session-3_1600171983_763c5381\n",
    "Preparing to call script [ train.py ] with arguments: ['--data_path', '$input', '--learning_rate', '0.003', '--momentum', '0.92']\n",
    "After variable expansion, calling script [ train.py ] with arguments: ['--data_path', '/tmp/tmp9kituvp3', '--learning_rate', '0.003', '--momentum', '0.92']\n",
    "\n",
    "Script type = None\n",
    "===== DATA =====\n",
    "DATA PATH: /tmp/tmp9kituvp3\n",
    "LIST FILES IN DATA PATH...\n",
    "['cifar-10-batches-py', 'cifar-10-python.tar.gz']\n",
    "```\n",
    "\n",
    "Notice:\n",
    "\n",
    "1. Azure Machine Learning has mounted the blob store to the compute cluster automatically for you\n",
    "2. The ``ds.as_mount()`` used in the control script resolves to the mount point\n",
    "3. In the machine learning code we include a line to list the directorys under the data directory - you can see the list above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not going to use what you've created here, delete the resources you just created with this quickstart so you don't incur any charges for storage. In the Azure portal, select and delete your resource group."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "samkemp"
   }
  ],
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dkdc': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6813bb6deec483ed15ac37ef074baa52622250b2b65156cf2f3313d85d7e0391"
    }
   },
   "name": "Python 3.8.5 64-bit ('dkdc': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "notice": "Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}