{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy a model and monitor model data collectoin\n",
        "_**Create and deploy a model directly from a notebook and monitor model data collection**_\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## Contents\n",
        "1. [Introduction](#Introduction)\n",
        "1. [Setup](#Setup)\n",
        "1. [Data](#Data)\n",
        "1. [Train](#Train)\n",
        "    1. Viewing run results\n",
        "    1. Simple parameter sweep\n",
        "    1. Viewing experiment results\n",
        "    1. Select the best model\n",
        "1. [Deploy](#Deploy)\n",
        "    1. Register the model\n",
        "    1. Create a scoring file\n",
        "    1. Create the environment configuration (yml file for Conda and pip packages)\n",
        "    1. Deploy the model as web service on Azure Kubernetes Service (AKS)\n",
        "1. [Infere](#Infere)\n",
        "    1. Test the Web Service witt a few prediction request to collect model data\n",
        "    1. Inspect the Azure Blob Storage container to monitor the model data collection\n",
        "    1. Clean up\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "Azure Machine Learning provides capabilities to control all aspects of model training and deployment directly from a notebook using the AML Python SDK. The main objectives of this notebook is to: \n",
        "* Train a regression model to predict if a patien would be diabetic based on a public diabetes dataset (https://aka.ms/diabetes-data).  \n",
        "* Deploy that model as a web service in Azure Kubernetes Services, with ModelDataCollection enabled.\n",
        "* Submit a few prediction requests to collect model data\n",
        "* Monitor the model data collection in Azure Blobl Storage of the Machine Learning Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Setup\n",
        "Create an Azure Machine Learning servcie in Azure, and launch the studio. \n",
        "\n",
        "Create a Workspace and a Compute Instance to run this notebook.\n",
        "\n",
        "For this notebook we need the Azure ML SDK and access to our workspace.  The following cell imports the SDK, checks the version, and accesses our already configured AzureML workspace. \n",
        "\n",
        "See more detail on [Git Integration](https://docs.microsoft.com/en-us/azure/machine-learning/concept-train-model-git-integration#:~:text=Azure%20Machine%20Learning%20provides%20a%20shared%20file%20system,work%20with%20Git%20via%20the%20Git%20CLI%20experience) if you need to upload this notebook in AML."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Experiment, Workspace\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"This notebook was created using version 1.19.0 of the Azure ML SDK\")\n",
        "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep='\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "install"
        ],
        "gather": {
          "logged": 1609362381326
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Data\n",
        "We will use the diabetes dataset for this experiement (https://aka.ms/diabetes-data). \n",
        "\n",
        "The dataset consists of 8 baseline variables for n=10000 diabetes patients: Pregnancies, PlasmaGlucose, DiastolicBloodPressure, TricepsThickness, SerumInsulin, BMI, DiabetesPedigree, and Age.\n",
        "\n",
        "The dataset has one dichotomous outcome variable: Diebetic."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# load the diabetes dataset from the same folder where this notebook is located\r\n",
        "print(\"Loading Diabetes Data from the CSV file published in https://aka.ms/diabetes-data\")\r\n",
        "ds = pd.read_csv('https://aka.ms/diabetes-data')\r\n",
        "\r\n",
        "# Separate features and labels\r\n",
        "X, y = ds[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, ds['Diabetic'].values\r\n",
        "# Split data into training set and test set (80% Training and 20% Testing)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\r\n",
        "\r\n",
        "data = {\r\n",
        "    \"train\":{\"X\": X_train, \"y\": y_train},        \r\n",
        "    \"test\":{\"X\": X_test, \"y\": y_test}\r\n",
        "}\r\n",
        "\r\n",
        "print (\"Data contains\", len(data['train']['X']), \"training samples and\",len(data['test']['X']), \"test samples\")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1609362615944
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Train\n",
        "\n",
        "Let's use scikit-learn to train a simple Ridge regression model.  We use AML to record interesting information about the model in an Experiment.  An Experiment contains a series of trials called Runs.  During this trial we use AML in the following way:\n",
        "* We access an experiment from our AML workspace by name, which will be created if it doesn't exist\n",
        "* We use `start_logging` to create a new run in this experiment\n",
        "* We use `run.log()` to record a parameter, alpha, and an accuracy measure - the Mean Squared Error (MSE) to the run.  We will be able to review and compare these measures in the Azure Portal at a later time.\n",
        "* We store the resulting model in the **working** directory, which is automatically captured by AML when the run is complete.\n",
        "* We use `run.complete()` to indicate that the run is over and results can be captured and finalized"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "\n",
        "# Get an experiment object from Azure Machine Learning\n",
        "experiment = Experiment(workspace=ws, name=\"train-diabetes-regression\")\n",
        "\n",
        "# Create a run object in the experiment\n",
        "run =  experiment.start_logging()\n",
        "# Log the algorithm parameter alpha to the run; where alpha is between 0 and 1\n",
        "run.log('alpha', 0.03)\n",
        "\n",
        "# Create, fit, and test the scikit-learn Ridge regression model\n",
        "regression_model = Ridge(alpha=0.03)\n",
        "regression_model.fit(data['train']['X'], data['train']['y'])\n",
        "preds = regression_model.predict(data['test']['X'])\n",
        "\n",
        "# Output the Mean Squared Error to the notebook and to the run\n",
        "print('Mean Squared Error is', mean_squared_error(data['test']['y'], preds))\n",
        "run.log('mse', mean_squared_error(data['test']['y'], preds))\n",
        "\n",
        "# Save the model to the working directory \n",
        "model_file_name = 'diabetesregressionmodel.pkl'\n",
        "\n",
        "joblib.dump(value = regression_model, filename = model_file_name)\n",
        "\n",
        "# upload the model file explicitly into artifacts \n",
        "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
        "\n",
        "# Complete the run\n",
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "local run",
          "outputs upload"
        ],
        "gather": {
          "logged": 1609362803264
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viewing run results\n",
        "Azure Machine Learning stores all the details about the run in the Azure cloud.  Let's access those details by retrieving a link to the run using the default run output.  Clicking on the resulting link will take you to an interactive page presenting all run information."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1609362807711
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Deploy\n",
        "Now that we have trained the model, we want to deploy the model for real time inference.  The process of deploying a model involves\n",
        "* registering a model in your workspace\n",
        "* creating a scoring file containing init() and run() methods\n",
        "* creating an environment settings file describing packages necessary for your scoring file\n",
        "* creating a deployment configuration (for AKS in this example)\n",
        "* deploying the model and packages as a web service in an AKS cluster"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register a model\n",
        "When using `run.register_model()` we supply a `model_name` that is meaningful for our scenario and the `model_path` of the model relative to the run."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import Model\n",
        "   \n",
        "# Register the model with the workspace\n",
        "model = Model.register(model_path = \"diabetesregressionmodel.pkl\",\n",
        "                       model_name = \"diabetesregressionmodel.pkl\",\n",
        "                       tags = {'area': \"diabetes\", 'type': \"regression\"},\n",
        "                       description = \"Ridge regression model to predict diabetes\",\n",
        "                       workspace =ws)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "query history"
        ],
        "gather": {
          "logged": 1609363369684
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a model is registered, it is accessible from the list of models on the AML workspace.  If you register models with the same name multiple times, AML keeps a version history of those models for you.  The `Model.list()` lists all models in a workspace, and can be filtered by name, tags, or model properties.   "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all models called \"diabetesmodel\" and display their version numbers\n",
        "from azureml.core.model import Model\n",
        "models = Model.list(ws, name=\"diabetesregressionmodel.pkl\")\n",
        "for m in models:\n",
        "    print(m.name, m.version)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "register model from history"
        ],
        "gather": {
          "logged": 1609363371961
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a scoring file"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since your model file can essentially be anything you want it to be, you need to supply a scoring script that can load your model and then apply the model to new data. This script is your 'scoring file'. This scoring file is a python program containing, at a minimum, two methods init() and run(). The init() method is called once when your deployment is started so you can load your model and any other required objects. This method uses the get_model_path function to locate the registered model inside the docker container. The run() method is called interactively when the web service is called with one or more data samples to predict.\n",
        "\n",
        "The schema decorators for pandas and numpy are required to implement the automatic swagger schema generation for input and output variables\n",
        "\n",
        "The ModelDataCollector is included in the azureml-monitoring package. Make sure to install/upgrade to the latest package\n",
        "\n",
        "After a successful run of the this script, the score.py file be created in the working folder\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# upgrade azureml sdk and monitoring\r\n",
        "!pip install --upgrade azureml-sdk azureml-monitoring\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1609363615290
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile score.py\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from azureml.core.model import Model\n",
        "from azureml.monitoring import ModelDataCollector # for model data collection\n",
        "\n",
        "from inference_schema.schema_decorators import input_schema, output_schema\n",
        "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
        "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
        "\n",
        "def init():\n",
        "    global model, inputs_datacollection, predictions_datacollection\n",
        "    # collect inputs data send to, and predictions data received from, the model\n",
        "    # The datacollection can be found under the storage account of the workspace\n",
        "    # /modeldata/<subscriptionid>/<resourcegroup>/<workspace>/<webservice>/<model>/<version>/<designation>/<year>/<month>/<day>/data.csv\n",
        "    inputs_datacollection = ModelDataCollector(\"diabetesregressionmodel.pkl\", designation=\"inputs\", feature_names=[\"Pregnancies\", \"PlasmaGlucose\", \"DiastolicBloodPressure\", \"TricepsThickness\", \"SerumInsulin\", \"BMI\", \"DiabetesPedigree\", \"Age\"])\n",
        "    predictions_datacollection = ModelDataCollector(\"diabetesregressionmodel.pkl\", designation=\"predictions\", feature_names=[\"Diebetic\"])\n",
        "    model_path = Model.get_model_path('diabetesregressionmodel.pkl')\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "input_sample = pd.DataFrame(data=[{\n",
        "            \"Pregnancies\": 0,\n",
        "            \"PlasmaGlucose\": 171,\n",
        "            \"DiastolicBloodPressure\": 80,\n",
        "            \"TricepsThickness\": 34,\n",
        "            \"SerumInsulin\": 23,\n",
        "            \"BMI\": 43.51,\n",
        "            \"DiabetesPedigree\": 1.21,\n",
        "            \"Age\": 21,\n",
        "            }])\n",
        "output_sample = np.array([0])\n",
        "\n",
        "@input_schema('data', PandasParameterType(input_sample))\n",
        "@output_schema(NumpyParameterType(output_sample))\n",
        "\n",
        "def run(data):\n",
        "    try:\n",
        "        result = model.predict(data)\n",
        "\n",
        "        # save the inputs and prediction data collection from this run\n",
        "        inputs_datacollection.collect(data) \n",
        "        predictions_datacollection.collect(result) \n",
        "\n",
        "        return result.tolist()\n",
        "    except Exception as e:\n",
        "        error = str(e)\n",
        "        return error"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the environment settings \n",
        "\n",
        "The environment settings will also be exported into a yml file (myenv.yml) to verify the conda and pip packages.\n",
        "The yml file will be in the working folder for this deployment\n",
        "\n",
        "This step will create the python environment with the required conda and pip packages/dependencies. And then, it will create the inference configuration that will build the Docker container based on the scoring file and the environment configuration. The Docker image is transparent and will be created and registered behind the scenes with the AzureML SDK."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "environment_name = 'deploytocloudenv'\n",
        "env = Environment(environment_name)\n",
        "env.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy','scikit-learn'],pip_packages=['azureml-defaults','azureml-monitoring','inference-schema[numpy-support]'])\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
        "\n",
        "with open (\"myenv.yml\",\"w\") as f:\n",
        "   f.write(env.python.conda_dependencies.serialize_to_string())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1609364294775
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provision the AKS Cluster\r\n",
        "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it.\r\n",
        "\r\n",
        "This step will take a few minutes."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import AksCompute, ComputeTarget\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Choose a name for your AKS cluster\r\n",
        "aks_name = 'aks-cluster' \r\n",
        "\r\n",
        "# Verify that cluster does not exist already\r\n",
        "try:\r\n",
        "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # Use the default configuration (can also provide parameters to customize)\r\n",
        "    prov_config = AksCompute.provisioning_configuration()\r\n",
        "\r\n",
        "    # Create the cluster\r\n",
        "    aks_target = ComputeTarget.create(workspace = ws, \r\n",
        "                                    name = aks_name, \r\n",
        "                                    provisioning_configuration = prov_config)\r\n",
        "\r\n",
        "if aks_target.get_status() != \"Succeeded\":\r\n",
        "    aks_target.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1609364737929
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy the model as a web service to the AKS Cluster\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import AksWebservice\r\n",
        "\r\n",
        "# Set the web service configuration (using default here)\r\n",
        "# Model data collecoitn is not enabled by default. Enable here\r\n",
        "aks_config = AksWebservice.deploy_configuration(collect_model_data=True)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1609364911371
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\r\n",
        "import datetime\r\n",
        "\r\n",
        "aksservice_name ='aks-service'\r\n",
        "\r\n",
        "print(str(datetime.datetime.now()))\r\n",
        "# Create the webservice using all of the precreated configurations and our best model\r\n",
        "aksservice = Model.deploy(workspace=ws,\r\n",
        "                           name=aksservice_name,\r\n",
        "                           models=[model],\r\n",
        "                           inference_config=inference_config,\r\n",
        "                           deployment_config=aks_config,\r\n",
        "                           deployment_target=aks_target)\r\n",
        "\r\n",
        "# Wait for the service deployment to complete while displaying log output\r\n",
        "aksservice.wait_for_deployment(show_output = True)\r\n",
        "print(aksservice.state)\r\n",
        "print(aksservice.get_logs)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\r\n",
        "## Infere\r\n",
        "The webservice has now been deployed. Next, we will submit a few prediction requests to the webservice in order to collect model data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Raw dataset (Actual Diabetic is 1)\n",
        "test_sample = json.dumps({\"data\": [{\n",
        "        \"Pregnancies\": 9,\n",
        "        \"PlasmaGlucose\": 103,\n",
        "        \"DiastolicBloodPressure\": 78,\n",
        "        \"TricepsThickness\": 25,\n",
        "        \"SerumInsulin\": 309,\n",
        "        \"BMI\": 29.58,\n",
        "        \"DiabetesPedigree\": 1.28,\n",
        "        \"Age\": 43,}]})\n",
        "\n",
        "test_sample = bytes(test_sample,encoding = 'utf8')\n",
        "prediction = aksservice.run(input_data=test_sample)\n",
        "print(prediction)\n",
        "\n",
        "\n",
        "# Raw dataset (Actual Diabetic is 0)\n",
        "test_sample = json.dumps({\"data\": [{\n",
        "        \"Pregnancies\": 0,\n",
        "        \"PlasmaGlucose\": 171,\n",
        "        \"DiastolicBloodPressure\": 80,\n",
        "        \"TricepsThickness\": 34,\n",
        "        \"SerumInsulin\": 23,\n",
        "        \"BMI\": 43.51,\n",
        "        \"DiabetesPedigree\": 1.21,\n",
        "        \"Age\": 21,}]})\n",
        "\n",
        "test_sample = bytes(test_sample,encoding = 'utf8')\n",
        "prediction = aksservice.run(input_data=test_sample)\n",
        "print(prediction)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1609365478972
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect the collected model data\r\n",
        "Use the Azure Portal or the Storage Explorer tool to navigate to the storage account of the Azure Machine Learning Workspace\r\n",
        "\r\n",
        "Follow the path under the container 'modeldata' as described in the [ModelDataCollector Class](https://docs.microsoft.com/en-us/python/api/azureml-monitoring/azureml.monitoring.modeldatacollector.modeldatacollector?view=azure-ml-py). \r\n",
        "You will notice there are two csv files in two different containers; inputs and predictions.\r\n",
        "The csv files contain the input and prediction recrodsets from the run(s) above.\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean up"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the AKS web service to stop the compute and any associated billing."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "aksservice.delete()\n",
        "model.delete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "deploy service",
          "aci"
        ]
      }
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Training and deploying a model from a notebook",
    "deployment": [
      "Azure Container Instance"
    ],
    "authors": [
      {
        "name": "roastala"
      }
    ],
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "Local"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "Diabetes"
    ],
    "category": "tutorial",
    "framework": [
      "None"
    ],
    "friendly_name": "Train and deploy a model using Python SDK",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}