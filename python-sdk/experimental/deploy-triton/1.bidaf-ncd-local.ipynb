{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Triton Inference Server locally\n",
    "\n",
    "description: (preview) deploy a bi-directional attention flow (bidaf) Q&A model locally with Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: pip: command not found\n",
      "/bin/bash: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install --upgrade tritonclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "name": "import-workspace",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-core 1.27.0 (/home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.20.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.27.0 (/home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.20.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.27.0 (/home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.20.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.27.0 (/home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.20.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.27.0 (/home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages), Requirement.parse('azureml-core~=1.20.0')).\n",
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "name": "download-model",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "successfully downloaded model: densenet_onnx\n",
      "successfully downloaded model: bidaf-9\n"
     ]
    }
   ],
   "source": [
    "from src.model_utils import download_triton_models, delete_triton_models\n",
    "from pathlib import Path\n",
    "\n",
    "prefix = Path(\".\")\n",
    "download_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "name": "register-model",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Registering model bidaf-9-tutorial\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=bidaf-9-tutorial, id=bidaf-9-tutorial:2212, version=2212, tags={'area': 'Natural language processing', 'type': 'Question-answering'}, properties={})"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = prefix.joinpath(\"models\")\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"bidaf-9-tutorial\",\n",
    "    tags={\"area\": \"Natural language processing\", \"type\": \"Question-answering\"},\n",
    "    description=\"Question answering from ONNX model zoo\",\n",
    "    workspace=ws,\n",
    "    model_framework=Model.Framework.MULTI,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "Deploy to a pre-created [AksCompute](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.aks.akscompute?view=azure-ml-py#provisioning-configuration-agent-count-none--vm-size-none--ssl-cname-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--location-none--vnet-resourcegroup-name-none--vnet-name-none--subnet-name-none--service-cidr-none--dns-service-ip-none--docker-bridge-cidr-none--cluster-purpose-none--load-balancer-type-none-) named `aks-gpu-deploy`. For other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "name": "deploy-webservice",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading model bidaf-9-tutorial:2212 to /tmp/azureml__me4myf2/bidaf-9-tutorial/2212\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry \n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/6 : FROM nvcr.io/nvidia/tritonserver:20.06-py3\n",
      " ---> 171a7fd4d078\n",
      "Step 2/6 : ENV AZUREML_MODEL_DIR=azureml-models/bidaf-9-tutorial/2212\n",
      " ---> Running in 34ad55c96afb\n",
      " ---> edd10d37090f\n",
      "Step 3/6 : COPY azureml-app /var/azureml-app\n",
      " ---> 46b91b39800c\n",
      "Step 4/6 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY1NjA1NzVkLWZhMDYtNGU3ZC05NWZiLWY5NjJlNzRlZmQ3YSIsInJlc291cmNlR3JvdXBOYW1lIjoiYXp1cmVtbC1leGFtcGxlcyIsImFjY291bnROYW1lIjoiZGVmYXVsdCIsIndvcmtzcGFjZUlkIjoiMGUxNDk3NjQtMzcyMC00NjEwLWIwZjMtM2UzZjk3NDU0NGFjIn0sIm1vZGVscyI6eyJiaWRhZi05LXR1dG9yaWFsIjp7InZlcnNpb24iOjIyMTIsImlkIjoiYmlkYWYtOS10dXRvcmlhbDoyMjEyIiwiaW50ZXJuYWxJZCI6ImRkMTJjNTUyZGZkNDRlZTRhYjhlYTJlZjA1ZTc4MjdkIn19LCJtb2RlbHNJbmZvIjp7ImJpZGFmLTktdHV0b3JpYWwiOnsiMjIxMiI6eyJ2ZXJzaW9uIjoyMjEyLCJpZCI6ImJpZGFmLTktdHV0b3JpYWw6MjIxMiIsImludGVybmFsSWQiOiJkZDEyYzU1MmRmZDQ0ZWU0YWI4ZWEyZWYwNWU3ODI3ZCIsImRhdGFDb2xsZWN0b3JTdG9yYWdlUGF0aCI6Ii9tb2RlbGRhdGEvNjU2MDU3NWQtZmEwNi00ZTdkLTk1ZmItZjk2MmU3NGVmZDdhL2F6dXJlbWwtZXhhbXBsZXMvZGVmYXVsdC97d2Vic2VydmljZV9uYW1lfS9iaWRhZi05LXR1dG9yaWFsLzIyMTIvIn19fX0= | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in d1ffe46bf0e7\n",
      " ---> 8ba4697d21af\n",
      "Step 5/6 : ENTRYPOINT []\n",
      " ---> Running in 40d18cf7cb63\n",
      " ---> 983f86f2e456\n",
      "Step 6/6 : CMD [\"/opt/tritonserver/nvidia_entrypoint.sh\",\"tritonserver\",\"--model-repository=/var/azureml-app/azureml-models/bidaf-9-tutorial/2212/models/triton\",\"--strict-model-config=false\"]\n",
      " ---> Running in 60ee26cede9f\n",
      " ---> dd8e6f4989d8\n",
      "Successfully built dd8e6f4989d8\n",
      "Successfully tagged triton-bidaf-960705:latest\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:6789\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"triton-bidaf-9\" + str(randint(10000, 99999))\n",
    "\n",
    "config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.06 (build 13333626)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\n2021-05-13 20:33:26.045755: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\nI0513 20:33:26.995323 1 server.cc:120] Initializing Triton Inference Server\nE0513 20:33:26.997570 1 pinned_memory_manager.cc:192] failed to allocate pinned system memory: CUDA driver version is insufficient for CUDA runtime version\nI0513 20:33:27.698276 1 model_repository_manager.cc:786] loading: densenet_onnx:1\nI0513 20:33:27.698652 1 model_repository_manager.cc:786] loading: bidaf-9:1\nI0513 20:33:27.722765 1 onnx_backend.cc:195] Creating instance densenet_onnx_0_cpu on CPU using model.onnx\nI0513 20:33:27.725886 1 onnx_backend.cc:195] Creating instance bidaf-9_0_cpu on CPU using model.onnx\nI0513 20:33:28.271926 1 model_repository_manager.cc:967] successfully loaded 'bidaf-9' version 1\nI0513 20:33:28.313249 1 model_repository_manager.cc:967] successfully loaded 'densenet_onnx' version 1\nI0513 20:33:28.345944 1 grpc_server.cc:3199] Started GRPCInferenceService at 0.0.0.0:8001\nI0513 20:33:28.349268 1 http_server.cc:2546] Started HTTPService at 0.0.0.0:8000\nI0513 20:33:28.393374 1 http_server.cc:2565] Started Metrics Service at 0.0.0.0:8002\n\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk geventhttpclient python-rapidjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "name": "get-keys"
   },
   "outputs": [],
   "source": [
    "scoring_uri = service.scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*   Trying 127.0.0.1...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 6789 (#0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v $scoring_uri/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "name": "query-service",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages/tritonclientutils/__init__.py:33: DeprecationWarning: The package `tritonclientutils` is deprecated and will be removed in a future version. Please use instead `tritonclient.utils`\n",
      "  \"`tritonclient.utils`\", DeprecationWarning)\n",
      "/home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages/tritonhttpclient/__init__.py:33: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  \"`tritonclient.http`\", DeprecationWarning)\n",
      "[nltk_data] Downloading package punkt to /home/gopalv/nltk_data...\n",
      "start is 7, end is 8\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gopalv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[b'lazy', b'dog']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import tritonclient.http as tritonhttpclient\n",
    "from tritonclientutils import triton_to_np_dtype\n",
    "\n",
    "from src.bidaf_utils import preprocess, postprocess\n",
    "\n",
    "headers = {}\n",
    "\n",
    "triton_client = tritonhttpclient.InferenceServerClient(service.scoring_uri[7:])\n",
    "\n",
    "context = \"A quick brown fox jumped over the lazy dog.\"\n",
    "query = \"Which animal was lower?\"\n",
    "\n",
    "model_name = \"bidaf-9\"\n",
    "\n",
    "model_metadata = triton_client.get_model_metadata(\n",
    "    model_name=model_name, headers=headers\n",
    ")\n",
    "\n",
    "input_meta = model_metadata[\"inputs\"]\n",
    "output_meta = model_metadata[\"outputs\"]\n",
    "\n",
    "# We use the np.object data type for string data\n",
    "np_dtype = triton_to_np_dtype(input_meta[0][\"datatype\"])\n",
    "cw, cc = preprocess(context, np_dtype)\n",
    "qw, qc = preprocess(query, np_dtype)\n",
    "\n",
    "input_mapping = {\n",
    "    \"query_word\": qw,\n",
    "    \"query_char\": qc,\n",
    "    \"context_word\": cw,\n",
    "    \"context_char\": cc,\n",
    "}\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "# Populate the inputs array\n",
    "for in_meta in input_meta:\n",
    "    input_name = in_meta[\"name\"]\n",
    "    data = input_mapping[input_name]\n",
    "\n",
    "    input = tritonhttpclient.InferInput(input_name, data.shape, in_meta[\"datatype\"])\n",
    "\n",
    "    input.set_data_from_numpy(data, binary_data=False)\n",
    "    inputs.append(input)\n",
    "\n",
    "# Populate the outputs array\n",
    "for out_meta in output_meta:\n",
    "    output_name = out_meta[\"name\"]\n",
    "    output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)\n",
    "    outputs.append(output)\n",
    "\n",
    "# Run inference\n",
    "res = triton_client.infer(\n",
    "    model_name,\n",
    "    inputs,\n",
    "    request_id=\"0\",\n",
    "    outputs=outputs,\n",
    "    model_version=\"1\",\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "result = postprocess(context_words=cw, answer=res)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "delete-service",
    "tags": []
   },
   "outputs": [],
   "source": [
    "service.delete()\n",
    "delete_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try reading [our documentation](https://aka.ms/triton-aml-docs) to use Triton with your own models or check out the other notebooks in this folder for ways to do pre- and post-processing on the server. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "deploy-bidaf-aks",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}