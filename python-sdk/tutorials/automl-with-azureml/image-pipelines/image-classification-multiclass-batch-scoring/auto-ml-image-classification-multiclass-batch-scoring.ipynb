{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\r\n",
    "\r\n",
    "Licensed under the MIT License.\r\n",
    "\r\n",
    "# Batch Predictions for an Image Classification model trained using AutoML\r\n",
    "In this notebook, we go over how you can use [Azure Machine Learning service pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-batch-scoring-classification) to run a batch scoring image classification job. \r\n",
    "\r\n",
    "### Prerequisites\r\n",
    "\r\n",
    "> * Train an image classification model using AutoML for Images. [Refer to](https://github.com/swatig007/automlForImages/blob/main/MultiClass/AutoMLImage_MultiClass_SampleNotebook.ipynb) to know how to train an image classification model with AutoML.\r\n",
    "\r\n",
    "You will perform the following tasks:\r\n",
    "\r\n",
    "> * Register a Model we trained using AutoML for Image Classification.\r\n",
    "> * Create the Inference Dataset.\r\n",
    "> * Provision compute targets and create a Batch Scoring script.\r\n",
    "> * Use ParallelRunStep to do batch scoring.\r\n",
    "> * Build, run, and publish a pipeline.\r\n",
    "> * Enable a REST endpoint for the pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\r\n",
    "Batch Inference package can be installed through the following pip command.\r\n",
    "\r\n",
    "### Note: Only Python 3.6 and 3.7 are supported for this feature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Licensing Information - \n",
    "This preview software is made available to you on the condition that you agree to\n",
    "[your agreement][1] governing your use of Azure, and to the Supplemental Terms of Use for Microsoft Azure Previews[2], which supplement your agreement governing your use of Azure.\n",
    "If you do not have an existing agreement governing your use of Azure, you agree that \n",
    "your agreement governing use of Azure is the [Microsoft Online Subscription Agreement][3]\n",
    "(which incorporates the [Online Services Terms][4]).\n",
    "By using the software you agree to these terms. This software may collect data\n",
    "that is transmitted to Microsoft. Please see the [Microsoft Privacy Statement][5]\n",
    "to learn more about how Microsoft processes personal data.\n",
    "\n",
    "[1]: https://azure.microsoft.com/en-us/support/legal/\n",
    "[2]: https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/\n",
    "[3]: https://azure.microsoft.com/en-us/support/legal/subscription-agreement/\n",
    "[4]: http://www.microsoftvolumelicensing.com/DocumentSearch.aspx?Mode=3&DocumentTypeId=46\n",
    "[5]: http://go.microsoft.com/fwlink/?LinkId=248681"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workspace setup\r\n",
    "\r\n",
    "An [Azure ML Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#workspace) is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. In particular, an Azure ML Workspace coordinates storage, databases, and compute resources providing added functionality for machine learning experimentation, deployment, inference, and the monitoring of deployed models.\r\n",
    "\r\n",
    "Create an Azure ML Workspace within your Azure subscription, or load an existing workspace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## specify workspace parameters\r\n",
    "subscription_id='<my-subscription-id>'   \r\n",
    "resource_group='<my-resource-group>'   \r\n",
    "workspace_name='<my-workspace-name>'\r\n",
    "\r\n",
    "from azureml.core.workspace import Workspace\r\n",
    "\r\n",
    "ws = Workspace.create(name=workspace_name,\r\n",
    "                      subscription_id=subscription_id,\r\n",
    "                      resource_group=resource_group, \r\n",
    "                      exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workspace default datastore is used to store inference input images and outputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.datastore import Datastore\r\n",
    "\r\n",
    "def_data_store = ws.get_default_datastore()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create data objects\n",
    "\n",
    "### Note: Whitespaces in file paths are currently not supported for inference. This will be supported in the near future.\n",
    "\n",
    "When building pipelines, `Dataset` objects are used for reading data from workspace datastores, and `PipelineData`  objects are used for transferring intermediate data between pipeline steps.\n",
    "\n",
    "This batch scoring example only uses one pipeline step, but in use-cases with multiple steps, the typical flow will include:\n",
    "\n",
    "1. Using `Dataset` objects as **inputs** to fetch raw data, performing some transformations, then **outputs** a `PipelineData` object.\n",
    "1. Use the previous step's `PipelineData` **output object** as an *input object*, repeated for subsequent steps.\n",
    "\n",
    "For this scenario you create `Dataset` objects corresponding to the datastore directories for the input images. You also create a `PipelineData` object for the batch scoring output data. An object reference in the `outputs` array becomes available as an **input** for a subsequent pipeline step, for scenarios where there is more than one step. In this case we are just going to build a single step pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.dataset import Dataset\r\n",
    "from azureml.pipeline.core import PipelineData\r\n",
    "\r\n",
    "input_images = Dataset.File.from_files((def_data_store, \"fridgeObjects/**/*.jpg\"))\r\n",
    "\r\n",
    "output_dir = PipelineData(name=\"scores\", datastore=def_data_store)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to register the input datasets for batch scoring with the workspace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_images = input_images.register(workspace=ws, name=\"fridgeObjects_scoring_images\", create_new_version =  True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve the environmnent and metrics from the training run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.experiment import Experiment\r\n",
    "from azureml.core import Run\r\n",
    "\r\n",
    "experiment_name = 'automl-image-batchscoring' \r\n",
    "# Replace with the run id of the child training run (i.e. the one ending with HD_0)\r\n",
    "training_run_id = <my-training-run-id>\r\n",
    "exp = Experiment(ws, experiment_name)\r\n",
    "training_run = Run(exp, training_run_id)\r\n",
    "\r\n",
    "# The below will give only the requested metric\r\n",
    "metrics = training_run.get_metrics('accuracy')\r\n",
    "best_metric = max(metrics['accuracy'])\r\n",
    "print('best_metric:', best_metric)\r\n",
    "\r\n",
    "# Retrieve the training environment\r\n",
    "env = training_run.get_environment()\r\n",
    "print(env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register model with metric and environment tags\n",
    "\n",
    "Now you register the model to your workspace, which allows you to easily retrieve it in the pipeline process. In the `register()` static function, the `model_name` parameter is the key you use to locate your model throughout the SDK.\n",
    "Tag the model with the metrics and the environment used to train the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.model import Model\r\n",
    "\r\n",
    "tags = dict()\r\n",
    "tags['accuracy'] = best_metric\r\n",
    "tags['env_name'] = env.name\r\n",
    "tags['env_version'] = env.version\r\n",
    "\r\n",
    "model_name = 'fridgeObjectsClassifier'\r\n",
    "model = training_run.register_model(model_name=model_name, model_path='train_artifacts', tags=tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# List the models from the workspace\r\n",
    "models = Model.list(ws, name=model_name,latest=True)\r\n",
    "print(model.name)\r\n",
    "print(model.tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create and attach remote compute target\n",
    "\n",
    "Azure Machine Learning service pipelines cannot be run locally, and only run on cloud resources. Remote compute targets are reusable virtual compute environments where you run experiments and work-flows. Run the following code to create a GPU-enabled [`AmlCompute`](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.amlcompute.amlcompute?view=azure-ml-py) target, and attach it to your workspace. See the [conceptual article](https://docs.microsoft.com/azure/machine-learning/service/concept-compute-target) for more information on compute targets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\r\n",
    "from azureml.exceptions import ComputeTargetException\r\n",
    "compute_name = \"nc6cluster\"\r\n",
    "\r\n",
    "# checks to see if compute target already exists in workspace, else create it\r\n",
    "try:\r\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\r\n",
    "except ComputeTargetException:\r\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\",\r\n",
    "                                                   vm_priority=\"dedicated\", \r\n",
    "                                                   min_nodes=0, \r\n",
    "                                                   max_nodes=1)\r\n",
    "\r\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\r\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write a scoring script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To do the scoring, you create a batch scoring script `batch_scoring.py`, and write it to the scripts folder in current directory. The script takes a minibatch of input images, applies the classification model, and outputs the predictions to a results file.\r\n",
    "\r\n",
    "The script `batch_scoring.py` takes the following parameters, which get passed from the `ParallelRunStep` that you create later:\r\n",
    "\r\n",
    "- `--model_name`: the name of the model being used\r\n",
    "\r\n",
    "While creating the batch scoring script, Refer to the scoring scripts generated under the outputs folder of the Automl training runs. This will help to identify the right model settings to be used in the batch scoring script init method while loading the model.\r\n",
    "Note: The batch scoring script we generate in the subsequent step is different from the scoring script generated by the training runs in the below screenshot. We refer to it just to identify the right model settings to be used in the batch scoring script.\r\n",
    "\r\n",
    "![Training run outputs](../../media/outputs.PNG \"Training run outputs\")\r"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View the batch scoring script. Use the model settings as appropriate for your model.\r\n",
    "with open('./scripts/batch_scoring.py', 'r') as f:\r\n",
    "    print(f.read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and run the pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the parallel-run configuration to wrap the inference script\n",
    "Create the pipeline run configuration specifying the script, environment configuration, and parameters. Specify the compute target you already attached to your workspace as the target of execution of the script. This will set the run configuration of the ParallelRunStep we will define next.\n",
    "\n",
    "Refer this [site](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/machine-learning-pipelines/parallel-run) for more details on ParallelRunStep of Azure Machine Learning Pipelines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig\r\n",
    "\r\n",
    "parallel_run_config = ParallelRunConfig(\r\n",
    "    environment=env,\r\n",
    "    entry_script=\"batch_scoring.py\",\r\n",
    "    source_directory=\"scripts\",\r\n",
    "    output_action=\"append_row\",\r\n",
    "    append_row_file_name=\"parallel_run_step.txt\",\r\n",
    "    mini_batch_size=\"20\", # Num files to process in one call\r\n",
    "    error_threshold=1,\r\n",
    "    compute_target=compute_target,\r\n",
    "    process_count_per_node=2,\r\n",
    "    node_count=1\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the pipeline step\n",
    "\n",
    "A pipeline step is an object that encapsulates everything you need for running a pipeline including:\n",
    "\n",
    "* environment and dependency settings\n",
    "* the compute resource to run the pipeline on\n",
    "* input and output data, and any custom parameters\n",
    "* reference to a script to run during the step\n",
    "\n",
    "There are multiple classes that inherit from the parent class [`PipelineStep`](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps.parallelrunstep?view=azure-ml-py) to assist with building a step using certain frameworks and stacks. In this example, you use the [`ParallelRunStep`](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunstep?view=azure-ml-py) class to define your step logic using a scoring script. [`ParallelRunStep`] does executes the script in a distributed fashion.\n",
    "\n",
    "The pipelines infrastructure uses the `ArgumentParser` class to pass parameters into pipeline steps. For example, in the code below the first argument `--model_name` is given the property identifier `model_name`. In the `main()` function, this property is accessed using `Model.get_model_path(args.model_name)`.\n",
    "\n",
    "Note: The pipeline in this tutorial only has one step and writes the output to a file, but for multi-step pipelines, you also use `ArgumentParser` to define a directory to write output data for input to subsequent steps. See the [notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.ipynb) for an example of passing data between multiple pipeline steps using the `ArgumentParser` design pattern."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "parallel_step_name = \"batchscoring-\" + datetime.now().strftime(\"%Y%m%d%H%M\")\r\n",
    "\r\n",
    "arguments = [\"--model_name\", model_name]\r\n",
    "\r\n",
    "# Specify inference batch_size, otherwise uses default value. (This is different from the mini_batch_size above)\r\n",
    "# NOTE: Large batch sizes may result in OOM errors.\r\n",
    "# arguments = arguments + [\"--batch_size\", \"20\"]\r\n",
    "\r\n",
    "batch_score_step = ParallelRunStep(\r\n",
    "    name=parallel_step_name,\r\n",
    "    inputs=[input_images.as_named_input(\"input_images\")],\r\n",
    "    output=output_dir,\r\n",
    "    arguments=arguments,\r\n",
    "    parallel_run_config=parallel_run_config,\r\n",
    "    allow_reuse=False\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a list of all classes for different step types, see the [steps package](https://docs.microsoft.com/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the pipeline\r\n",
    "\r\n",
    "Now you run the pipeline. First create a `Pipeline` object with your workspace reference and the pipeline step you created. The `steps` parameter is an array of steps, and in this case there is only one step for batch scoring. To build pipelines with multiple steps, you place the steps in order in this array.\r\n",
    "\r\n",
    "Next use the `Experiment.submit()` function to submit the pipeline for execution. You also specify the custom parameter `param_batch_size`. The `wait_for_completion` function will output logs during the pipeline build process, which allows you to see current progress.\r\n",
    "\r\n",
    "Note: The first pipeline run takes roughly **15 minutes**, as all dependencies must be downloaded, a Docker image is created, and the Python environment is provisioned/created. Running it again takes significantly less time as those resources are reused. However, total run time depends on the workload of your scripts and processes running in each pipeline step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Experiment\r\n",
    "from azureml.pipeline.core import Pipeline\r\n",
    "\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[batch_score_step])\r\n",
    "pipeline_run = Experiment(ws, \"batch_scoring_automl_image\").submit(pipeline)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This will output information of the pipeline run, including the link to the details page of portal.\r\n",
    "pipeline_run"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wait the run for completion and show output log to console\r\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download and review output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import tempfile\r\n",
    "import os\r\n",
    "\r\n",
    "batch_run = pipeline_run.find_step_run(batch_score_step.name)[0]\r\n",
    "batch_output = batch_run.get_output_data(output_dir.name)\r\n",
    "\r\n",
    "target_dir = tempfile.mkdtemp()\r\n",
    "batch_output.download(local_path=target_dir)\r\n",
    "result_file = os.path.join(target_dir, batch_output.path_on_datastore, parallel_run_config.append_row_file_name)\r\n",
    "result_file\r\n",
    "\r\n",
    "# Print the first five lines of the output\r\n",
    "with open(result_file) as f:\r\n",
    "    for x in range(5):\r\n",
    "        print(next(f))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choose a random file for visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "import json\r\n",
    "\r\n",
    "with open(result_file, 'r') as f:\r\n",
    "    contents = f.readlines()\r\n",
    "\r\n",
    "rand_file = contents[random.randrange(len(contents))]\r\n",
    "prediction = json.loads(rand_file)\r\n",
    "print(prediction['filename'])\r\n",
    "print(prediction['probs'])\r\n",
    "print(prediction['labels'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download the image file from the datastore\r\n",
    "path = 'fridgeObjects'+'/'+prediction['filename'].split('/')[-2]+'/'+prediction['filename'].split('/')[-1]\r\n",
    "path_on_datastore = def_data_store.path(path)\r\n",
    "single_image_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)\r\n",
    "image = single_image_ds.download()[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%pip install --upgrade matplotlib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.image as mpimg\r\n",
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "\r\n",
    "IMAGE_SIZE = (18,12)\r\n",
    "plt.figure(figsize=IMAGE_SIZE)\r\n",
    "img_np=mpimg.imread(image)\r\n",
    "img = Image.fromarray(img_np.astype('uint8'),'RGB')\r\n",
    "x, y = img.size\r\n",
    "\r\n",
    "fig,ax = plt.subplots(1, figsize=(15,15))\r\n",
    "# Display the image\r\n",
    "ax.imshow(img_np)\r\n",
    "\r\n",
    "label_index = np.argmax(prediction['probs'])\r\n",
    "label = prediction['labels'][label_index]\r\n",
    "conf_score = prediction['probs'][label_index]\r\n",
    "\r\n",
    "display_text = '{} ({})'.format(label, round(conf_score, 3))\r\n",
    "print(display_text)\r\n",
    "\r\n",
    "color = 'red'\r\n",
    "plt.text(30, 30, display_text, color=color, fontsize=30)\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Publish and run from REST endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following code to publish the pipeline to your workspace. In your workspace in the portal, you can see metadata for the pipeline including run history and durations. You can also run the pipeline manually from the portal.\n",
    "\n",
    "Additionally, publishing the pipeline enables a REST endpoint to rerun the pipeline from any HTTP library on any platform."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\r\n",
    "    name=\"automl-image-batch-scoring\", description=\"Batch scoring using Automl for Image\", version=\"1.0\")\r\n",
    "\r\n",
    "published_pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run the pipeline from the REST endpoint, you first need an OAuth2 Bearer-type authentication header. This example uses interactive authentication for illustration purposes, but for most production scenarios requiring automated or headless authentication, use service principle authentication as [described in this notebook](https://aka.ms/pl-restep-auth).\n",
    "\n",
    "Service principle authentication involves creating an **App Registration** in **Azure Active Directory**, generating a client secret, and then granting your service principal **role access** to your machine learning workspace. You then use the [`ServicePrincipalAuthentication`](https://docs.microsoft.com/python/api/azureml-core/azureml.core.authentication.serviceprincipalauthentication?view=azure-ml-py) class to manage your auth flow. \n",
    "\n",
    "Both `InteractiveLoginAuthentication` and `ServicePrincipalAuthentication` inherit from `AbstractAuthentication`, and in both cases you use the `get_authentication_header()` function in the same way to fetch the header."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "\r\n",
    "interactive_auth = InteractiveLoginAuthentication()\r\n",
    "auth_header = interactive_auth.get_authentication_header()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the REST url from the `endpoint` property of the published pipeline object. You can also find the REST url in your workspace in the portal. Build an HTTP POST request to the endpoint, specifying your authentication header. Additionally, add a JSON payload object with the experiment name and the batch size parameter. As a reminder, the `process_count_per_node` is passed through to `ParallelRunStep` because you defined it is defined as a `PipelineParameter` object in the step configuration.\n",
    "\n",
    "Make the request to trigger the run. Access the `Id` key from the response dict to get the value of the run id."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\r\n",
    "\r\n",
    "rest_endpoint = published_pipeline.endpoint\r\n",
    "response = requests.post(rest_endpoint, \r\n",
    "                         headers=auth_header, \r\n",
    "                         json={\"ExperimentName\": \"batch_scoring\",\r\n",
    "                               \"ParameterAssignments\": {\"process_count_per_node\": 2}})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\r\n",
    "    response.raise_for_status()\r\n",
    "except Exception:    \r\n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\r\n",
    "                    \"Response Code: {}\\n\"\r\n",
    "                    \"Headers: {}\\n\"\r\n",
    "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\r\n",
    "\r\n",
    "run_id = response.json().get('Id')\r\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the run id to monitor the status of the new run. This will take another 10-15 min to run and will look similar to the previous pipeline run, so if you don't need to see another pipeline run, you can skip watching the full output."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\r\n",
    "\r\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"batch_scoring\"], run_id)\r\n",
    "published_pipeline_run"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wait the run for completion and show output log to console\r\n",
    "published_pipeline_run.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": [
     "sanpil",
     "trmccorm",
     "pansav"
    ]
   }
  ],
  "categories": [
   "tutorials"
  ],
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pyUdemy': conda)",
   "name": "python388jvsc74a57bd00f25b6eb4724eea488a4edd67dd290abce7d142c09986fc811384b5aebc0585a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f25b6eb4724eea488a4edd67dd290abce7d142c09986fc811384b5aebc0585a"
   }
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}