{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\r\n",
    "\r\n",
    "Licensed under the MIT License.\r\n",
    "\r\n",
    "# Training an Image Classification model using AutoML\r\n",
    "In this notebook, we go over how you can use AutoML for training an Image Classification model. We will use a small dataset to train the model, demonstrate how you can tune hyperparameters of the model to optimize model performance and deploy the model to use in inference scenarios."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important\r\n",
    "This feature is currently in public preview. This preview version is provided without a service-level agreement. Certain features might not be supported or might have constrained capabilities. For more information, see Supplemental Terms of Use for Microsoft Azure Previews."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workspace setup\r\n",
    "In order to train and deploy models in Azure ML, you will first need to set up a workspace.\r\n",
    "\r\n",
    "An [Azure ML Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#workspace) is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. In particular, an Azure ML Workspace coordinates storage, databases, and compute resources providing added functionality for machine learning experimentation, deployment, inference, and the monitoring of deployed models.\r\n",
    "\r\n",
    "Create an Azure ML Workspace within your Azure subscription, or load an existing workspace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# specify workspace parameters\r\n",
    "subscription_id = \"<my-subscription-id>\"\r\n",
    "resource_group = \"<my-resource-group>\"\r\n",
    "workspace_name = \"<my-workspace-name>\"\r\n",
    "\r\n",
    "from azureml.core.workspace import Workspace\r\n",
    "\r\n",
    "ws = Workspace.create(\r\n",
    "    name=workspace_name,\r\n",
    "    subscription_id=subscription_id,\r\n",
    "    resource_group=resource_group,\r\n",
    "    exist_ok=True,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1617264529681
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute target setup\r\n",
    "You will need to provide a [Compute Target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) that will be used for your AutoML model training. AutoML models for image tasks require GPU SKUs and support NC and ND families. We recommend using the NCsv3-series (with v100 GPUs) for faster training. Using a compute target with a multi-GPU VM SKU will leverage the multiple GPUs to speed up training. Additionally, setting up a compute target with multiple nodes will allow for faster model training by leveraging parallelism, when tuning hyperparameters for your model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\r\n",
    "\r\n",
    "cluster_name = \"gpu-cluster-nc6\"\r\n",
    "\r\n",
    "try:\r\n",
    "    compute_target = ws.compute_targets[cluster_name]\r\n",
    "    print(\"Found existing compute target.\")\r\n",
    "except KeyError:\r\n",
    "    print(\"Creating a new compute target...\")\r\n",
    "    compute_config = AmlCompute.provisioning_configuration(\r\n",
    "        vm_size=\"Standard_NC6\",\r\n",
    "        idle_seconds_before_scaledown=1800,\r\n",
    "        min_nodes=0,\r\n",
    "        max_nodes=4,\r\n",
    "    )\r\n",
    "\r\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
    "    \r\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\r\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\r\n",
    "compute_target.wait_for_completion(\r\n",
    "    show_output=True, min_node_count=None, timeout_in_minutes=20\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1617264539968
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Setup\r\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) in your workspace to track your model training runs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Experiment\r\n",
    "\r\n",
    "experiment_name = \"automl-image-multiclass\"\r\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset with input Training Data\r\n",
    "\r\n",
    "In order to generate models for computer vision, you will need to bring in labeled image data as input for model training in the form of an AzureML Labeled Dataset. You can either use a Labeled Dataset that you have exported from a Data Labeling project, or create a new Labeled Dataset with your labeled training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we use a toy dataset called Fridge Objects, which consists of 134 images of 4 classes of beverage container {can, carton, milk bottle, water bottle} photos taken on different backgrounds. We first download and unzip the data locally."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import urllib\r\n",
    "from zipfile import ZipFile\r\n",
    "\r\n",
    "# download data\r\n",
    "download_url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/image_classification/fridgeObjects.zip\"\r\n",
    "data_file = \"./fridgeObjects.zip\"\r\n",
    "urllib.request.urlretrieve(download_url, filename=data_file)\r\n",
    "\r\n",
    "# extract files\r\n",
    "with ZipFile(data_file, \"r\") as zip:\r\n",
    "    print(\"extracting files...\")\r\n",
    "    zip.extractall()\r\n",
    "    print(\"done\")\r\n",
    "# delete zip file\r\n",
    "os.remove(data_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a sample image from this dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import Image\r\n",
    "\r\n",
    "sample_image = \"./fridgeObjects/milk_bottle/99.jpg\"\r\n",
    "Image(filename=sample_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert the downloaded data to JSONL\n",
    "In this example, the fridge object dataset is stored in a directory. There are four different folders inside:\n",
    "\n",
    "- /water_bottle\n",
    "- /milk_bottle\n",
    "- /carton\n",
    "- /can\n",
    "\n",
    "This is most common data format for multiclass image classification. Each folder title corresponds to the image label for the images contained inside.\n",
    "\n",
    "In order to use this data to create an AzureML Datset, we first need to convert it to the required JSONL format.\n",
    "\n",
    "The following script is creating two .jsonl files (one for training and one for validation) in the parent folder of the dataset. The train / validation ratio corresponds to 20% of the data going into the validation file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import os\r\n",
    "\r\n",
    "src = \"./fridgeObjects/\"\r\n",
    "train_validation_ratio = 5\r\n",
    "\r\n",
    "# Retrieving default datastore that got automatically created when we setup a workspace\r\n",
    "workspaceblobstore = ws.get_default_datastore().name\r\n",
    "\r\n",
    "# Path to the training and validation files\r\n",
    "train_annotations_file = os.path.join(src, \"train_annotations.jsonl\")\r\n",
    "validation_annotations_file = os.path.join(src, \"validation_annotations.jsonl\")\r\n",
    "\r\n",
    "# sample json line dictionary\r\n",
    "json_line_sample = {\r\n",
    "    \"image_url\": \"AmlDatastore://\"\r\n",
    "    + workspaceblobstore\r\n",
    "    + \"/\"\r\n",
    "    + os.path.basename(os.path.dirname(src)),\r\n",
    "    \"label\": \"\",\r\n",
    "    \"label_confidence\": 1.0,\r\n",
    "}\r\n",
    "\r\n",
    "index = 0\r\n",
    "# Scan each sub directary and generate jsonl line\r\n",
    "with open(train_annotations_file, \"w\") as train_f:\r\n",
    "    with open(validation_annotations_file, \"w\") as validation_f:\r\n",
    "        for className in os.listdir(src):\r\n",
    "            subDir = src + className\r\n",
    "            if not os.path.isdir(subDir):\r\n",
    "                continue\r\n",
    "            # Scan each sub directary\r\n",
    "            print(\"Parsing \" + subDir)\r\n",
    "            for image in os.listdir(subDir):\r\n",
    "                json_line = dict(json_line_sample)\r\n",
    "                json_line[\"image_url\"] += f\"/{className}/{image}\"\r\n",
    "                json_line[\"label\"] = className\r\n",
    "\r\n",
    "                if index % train_validation_ratio == 0:\r\n",
    "                    # validation annotation\r\n",
    "                    validation_f.write(json.dumps(json_line) + \"\\n\")\r\n",
    "                else:\r\n",
    "                    # train annotation\r\n",
    "                    train_f.write(json.dumps(json_line) + \"\\n\")\r\n",
    "                index += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upload the JSONL file and images to Datastore \n",
    "In order to use the data for training in Azure ML, we upload it to our Azure ML Workspace via a [Datastore](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#datasets-and-datastores). The datastore provides a mechanism for you to upload/download data, and interact with it from your remote compute targets. It is an abstraction over Azure Storage.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieving default datastore that got automatically created when we setup a workspace\r\n",
    "ds = ws.get_default_datastore()\r\n",
    "ds.upload(src_dir=\"./fridgeObjects\", target_path=\"fridgeObjects\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we need to create an Azure ML [Dataset](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#datasets-and-datastores) from the data we uploaded to the Datastore. We create one dataset for training and one for validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.data import DataType\r\n",
    "from azureml.core import Dataset\r\n",
    "\r\n",
    "training_dataset_name = \"fridgeObjectsTrainingDataset\"\r\n",
    "if training_dataset_name in ws.datasets:\r\n",
    "    training_dataset = ws.datasets.get(training_dataset_name)\r\n",
    "    print(\"Found the training dataset\", training_dataset_name)\r\n",
    "else:\r\n",
    "    # create training dataset\r\n",
    "    training_dataset = Dataset.Tabular.from_json_lines_files(\r\n",
    "        path=ds.path(\"fridgeObjects/train_annotations.jsonl\"),\r\n",
    "        set_column_types={\"image_url\": DataType.to_stream(ds.workspace)},\r\n",
    "    )\r\n",
    "    training_dataset = training_dataset.register(\r\n",
    "        workspace=ws, name=training_dataset_name\r\n",
    "    )\r\n",
    "# create validation dataset\r\n",
    "validation_dataset_name = \"fridgeObjectsValidationDataset\"\r\n",
    "if validation_dataset_name in ws.datasets:\r\n",
    "    validation_dataset = ws.datasets.get(validation_dataset_name)\r\n",
    "    print(\"Found the validation dataset\", validation_dataset_name)\r\n",
    "else:\r\n",
    "    validation_dataset = Dataset.Tabular.from_json_lines_files(\r\n",
    "        path=ds.path(\"fridgeObjects/validation_annotations.jsonl\"),\r\n",
    "        set_column_types={\"image_url\": DataType.to_stream(ds.workspace)},\r\n",
    "    )\r\n",
    "    validation_dataset = validation_dataset.register(\r\n",
    "        workspace=ws, name=validation_dataset_name\r\n",
    "    )\r\n",
    "print(\"Training dataset name: \" + training_dataset.name)\r\n",
    "print(\"Validation dataset name: \" + validation_dataset.name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Validation dataset is optional. If no validation dataset is specified, by default 20% of your training data will be used for validation. You can control the percentage using the `split_ratio` argument - please refer to the documentation for more details.\n",
    "\n",
    "This is what the training dataset looks like"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_dataset.to_pandas_dataframe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuring your AutoML run for image tasks\n",
    "AutoML allows you to easily train models for Image Classification, Object Detection & Instance Segmentation on your image data. You can control the model algorithm to be used, specify hyperparameter values for your model as well as perform a sweep across the hyperparameter space to generate an optimal model. Parameters for configuring your AutoML runs for image related tasks are specified using the 'AutoMLImageConfig' - please refer to the [documentation](https://github.com/swatig007/automlForImages#readme) for the details on the parameters that can be used and their values. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When using AutoML for image tasks, you need to specify the model algorithms using the `model_name` parameter. You can either specify a single model or choose to sweep over multiple ones. Currently supported model algorithms for image classification: 'resnet18', 'resnet34', 'resnet50', 'mobilenetv2', 'seresnext'.\n",
    "\n",
    "### Using default hyperparameter values for the specified algorithm\n",
    "\n",
    "Before doing a large sweep to search for the optimal models and hyperparameters, we recommend trying the default values to get a first baseline. Next, you can explore multiple hyperparameters for the same model before sweeping over multiple models and their parameters. This is for employing a more iterative approach, because with multiple models and multiple hyperparameters for each (as we showcase in the next section), the search space grows exponentially and you need more iterations to find optimal configurations.\n",
    "\n",
    "If you wish to use the default hyperparameter values for a given algorithm (say seresnext), you can specify the config for your AutoML Image runs as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.automl.core.shared.constants import ImageTask\r\n",
    "from azureml.train.automl import AutoMLImageConfig\r\n",
    "from azureml.train.hyperdrive import GridParameterSampling, choice\r\n",
    "\r\n",
    "image_config_seresnext = AutoMLImageConfig(\r\n",
    "    task=ImageTask.IMAGE_CLASSIFICATION,\r\n",
    "    compute_target=compute_target,\r\n",
    "    training_data=training_dataset,\r\n",
    "    validation_data=validation_dataset,\r\n",
    "    hyperparameter_sampling=GridParameterSampling({\"model_name\": choice(\"seresnext\")}),\r\n",
    "    iterations=1,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submitting an AutoML run for Computer Vision tasks\n",
    "Once you've created the config settings for your run, you can submit an AutoML run using the config in order to train a vision model using your training dataset. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "automl_image_run = experiment.submit(image_config_seresnext)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "automl_image_run.wait_for_completion(wait_post_processing=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter sweeping for your AutoML models for computer vision tasks\n",
    "In this example, we use the AutoMLImageConfig to train an Image Classification model using seresnext and resnet50, both of which are pretrained on ILSVRC2012 image classification dataset (a.k.a. ImageNet), which includes over 1.2M training and 50K validation images with 1,000 classes.\n",
    "\n",
    "When using AutoML for Images, you can perform a hyperparameter sweep over a defined parameter space, to find the optimal model. In this example, we sweep over the hyperparameters for each algorithm, choosing from a range of values for learning_rate, optimizer, lr_scheduler, etc, to generate a model with the optimal 'accuracy'. If hyperparameter values are not specified, then default values are used for the specified algorithm. \n",
    "\n",
    "We use Random Sampling to pick samples from this parameter space and try a total of 20 iterations with these different samples, running 4 iterations at a time on our compute target, which has been previously set up using 4 nodes. Please note that the more parameters the space has, the more iterations you need to find optimal models.\n",
    "\n",
    "We also leverage the Bandit early termination policy that terminates poor performing configs (those that are not within 20% slack of the best perfroming config), thus significantly saving compute resources."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.automl.core.shared.constants import ImageTask\r\n",
    "from azureml.train.automl import AutoMLImageConfig\r\n",
    "from azureml.train.hyperdrive import (\r\n",
    "    GridParameterSampling,\r\n",
    "    RandomParameterSampling,\r\n",
    "    BayesianParameterSampling,\r\n",
    ")\r\n",
    "from azureml.train.hyperdrive import BanditPolicy\r\n",
    "from azureml.train.hyperdrive import choice, uniform\r\n",
    "\r\n",
    "parameter_space = {\r\n",
    "    \"model\": choice(\r\n",
    "        {\r\n",
    "            \"model_name\": choice(\"seresnext\"),\r\n",
    "            \"learning_rate\": uniform(0.0001, 0.01),\r\n",
    "            \"optimizer\": choice(\"sgd\", \"adam\"),\r\n",
    "            \"layers_to_freeze\": choice(0, 2, 4),\r\n",
    "        },\r\n",
    "        {\r\n",
    "            \"model_name\": choice(\"resnet50\"),\r\n",
    "            \"resize_size\": choice(288, 320, 352),  # model-specific\r\n",
    "            \"crop_size\": choice(\r\n",
    "                224, 256\r\n",
    "            ),  # model-specific, crop_size should be smaller or equal than resize_size\r\n",
    "        },\r\n",
    "    )\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "tuning_settings = {\r\n",
    "    \"iterations\": 20,\r\n",
    "    \"max_concurrent_iterations\": 4,\r\n",
    "    \"hyperparameter_sampling\": RandomParameterSampling(parameter_space),\r\n",
    "    \"early_termination_policy\": BanditPolicy(\r\n",
    "        evaluation_interval=2, slack_factor=0.2, delay_evaluation=6\r\n",
    "    ),\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "automl_image_config = AutoMLImageConfig(\r\n",
    "    task=ImageTask.IMAGE_CLASSIFICATION,\r\n",
    "    compute_target=compute_target,\r\n",
    "    training_data=training_dataset,\r\n",
    "    validation_data=validation_dataset,\r\n",
    "    **tuning_settings,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "automl_image_run = experiment.submit(automl_image_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "automl_image_run.wait_for_completion(wait_post_processing=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When doing a hyperparameter sweep, it can be useful to visualize the different configurations that were tried using the HyperDrive UI. You can navigate to this UI by going to the 'Child runs' tab in the UI of the main automl_image_run from above, which is the HyperDrive parent run. Then you can go into the 'Child runs' tab of this one. Alternatively, here below you can see directly the HyperDrive parent run and navigate to its 'Child runs' tab:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "hyperdrive_run = Run(experiment=experiment, run_id=automl_image_run.id + \"_HD\")\n",
    "hyperdrive_run"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register the optimal vision model from the AutoML run\n",
    "Once the run completes, we can register the model that was created from the best run (configuration that resulted in the best primary metric)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Register the model from the best run\n",
    "\n",
    "best_child_run = automl_image_run.get_best_child()\n",
    "model_name = best_child_run.properties[\"model_name\"]\n",
    "model = best_child_run.register_model(\n",
    "    model_name=model_name, model_path=\"outputs/model.pt\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy model as a web service\n",
    "Once you have your trained model, you can deploy the model on Azure. You can deploy your trained model as a web service on Azure Container Instances ([ACI](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance)) or Azure Kubernetes Service ([AKS](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service)). ACI is the perfect option for testing deployments, while AKS is better suited for for high-scale, production usage. \n",
    "In this tutorial, we will deploy the model as a web service in AKS."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will need to first create an AKS compute cluster, or use an existing AKS cluster. You can use either GPU or CPU VM SKUs for your deployment cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "# Choose a name for your cluster\n",
    "aks_name = \"cluster-aks-gpu\"\n",
    "\n",
    "# Check to see if the cluster already exists\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    print(\"Found existing compute target\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    # Provision AKS cluster with GPU machine\n",
    "    prov_config = AksCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", location=\"eastus2\"\n",
    "    )\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(\n",
    "        workspace=ws, name=aks_name, provisioning_configuration=prov_config\n",
    "    )\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, you will need to define the inference configuration, that describes how to set up the web-service containing your model. You can use the scoring script and the environment from the training run in your inference config. \n",
    "\n",
    "<b>Note:</b> To change the model's settings, open the downloaded scoring script and modify the model_settings variable <i>before</i> deploying the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "best_child_run.download_file(\n",
    "    \"outputs/scoring_file_v_1_0_0.py\", output_file_path=\"score.py\"\n",
    ")\n",
    "environment = best_child_run.get_environment()\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=environment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can then deploy the model as an AKS web service."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Deploy the model from the best run as an AKS web service\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "aks_config = AksWebservice.deploy_configuration(\n",
    "    autoscale_enabled=True, cpu_cores=1, memory_gb=50, enable_app_insights=True\n",
    ")\n",
    "\n",
    "aks_service = Model.deploy(\n",
    "    ws,\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=aks_config,\n",
    "    deployment_target=aks_target,\n",
    "    name=\"automl-image-test\",\n",
    "    overwrite=True,\n",
    ")\n",
    "aks_service.wait_for_deployment(show_output=True)\n",
    "print(aks_service.state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the web service\n",
    "Finally, let's test our deployed web service to predict new images. You can pass in any image. In this case, we'll use a random image from the dataset and pass it to te scoring URI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "# URL for the web service\n",
    "scoring_uri = aks_service.scoring_uri\n",
    "\n",
    "# If the service is authenticated, set the key or token\n",
    "key, _ = aks_service.get_keys()\n",
    "\n",
    "# Showed above\n",
    "sample_image = \"./test_image.jpg\"\n",
    "\n",
    "# Load image data\n",
    "data = open(sample_image, \"rb\").read()\n",
    "\n",
    "# Set the content type\n",
    "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
    "\n",
    "# If authentication is enabled, set the authorization header\n",
    "headers[\"Authorization\"] = f\"Bearer {key}\"\n",
    "\n",
    "# Make the request and display the response\n",
    "resp = requests.post(scoring_uri, data, headers=headers)\n",
    "print(resp.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize predictions\n",
    "Now that we have scored a test image, we can visualize the prediction for this image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%pip install --upgrade matplotlib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "IMAGE_SIZE = (18, 12)\n",
    "plt.figure(figsize=IMAGE_SIZE)\n",
    "img_np = mpimg.imread(sample_image)\n",
    "img = Image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n",
    "x, y = img.size\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "# Display the image\n",
    "ax.imshow(img_np)\n",
    "\n",
    "prediction = json.loads(resp.text)\n",
    "label_index = np.argmax(prediction[\"probs\"])\n",
    "label = prediction[\"labels\"][label_index]\n",
    "conf_score = prediction[\"probs\"][label_index]\n",
    "\n",
    "display_text = \"{} ({})\".format(label, round(conf_score, 3))\n",
    "print(display_text)\n",
    "\n",
    "color = \"red\"\n",
    "plt.text(30, 30, display_text, color=color, fontsize=30)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}