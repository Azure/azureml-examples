{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Deploy a model using Prebuilt Docker Images for Inference\n",
    "\n",
    "In this tutorial, you will deploy a PyTorch model using the Azure Machine Learning Python SDK. The PyTorch model is trained to classify chickens and turkeys by first using a pretrained ResNet18 model that has been trained on the [ImageNet](http://image-net.org/index) dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620920632022
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620920636972
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Subscription id: \" + ws.subscription_id,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Register model\n",
    "Register a file or folder as a model by calling [Model.register()](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-)\n",
    "\n",
    "In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620920642194
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# Register the model\n",
    "model = Model.register(\n",
    "    model_name=\"pytorch-birds-model\",\n",
    "    model_path=\"pytorch-birds/outputs/model.pt\",\n",
    "    workspace=ws,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy model as web service\n",
    "Once you have your trained model, you can deploy the model on Azure. \n",
    "\n",
    "In this tutorial, we will deploy the model as a web service in [Azure Container Instances (ACI)](https://docs.microsoft.com/en-us/azure/container-instances/).\n",
    "\n",
    "For more information on deploying models using Azure Machine Learning, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Write the requirements.txt\n",
    "Write the `requirements.txt`\n",
    "\n",
    "> **Pin each package version:**\n",
    "Even while prototyping, pin each package version in requirements.txt. \n",
    "For example, use scipy == 1.2.3 instead of just scipy or even scipy > 1.2.3. \n",
    "If you don't pin an exact version and scipy releases a new version, this can break your scoring script and cause failures during deployment and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "  torchvision==0.7.0\n",
    "  future==0.17.1\n",
    "  pillow==8.1.1\n",
    "  azureml.core==1.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Define the environment (Dynamic installation via requirements.txt)\n",
    "\n",
    "Then, we will need to create an Azure Machine Learning environment that specifies all of the scoring script's package dependencies. \n",
    "\n",
    "Here we assume the `requirements.txt` is in the right format and inlcudes all additional packages for the model.\n",
    "\n",
    "If deployment fails because of Python extensibility solution, there will be no logs available the [Azure Machine Learning Studio](https://ml.azure.com),`service.get_logs()` will return None. If there is a problem in the *init()* function of score.py, `service.get_logs()` will return logs for the same.\n",
    "\n",
    "For local debugging and validation, run the following command in `score.py` folder :\n",
    "\n",
    "```\n",
    "docker run -it -v $(pwd):/var/azureml-app -e AZUREML_EXTRA_REQUIREMENTS_TXT=\"requirements.txt\" mcr.microsoft.com/azureml/pytorch-1.6-ubuntu18.04-py37-cpu-inference:latest \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620920680233
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "myenv = Environment(name=\"my_pytroch_env\")\n",
    "myenv.docker.enabled = True\n",
    "myenv.docker.base_image = (\n",
    "    \"mcr.microsoft.com/azureml/pytorch-1.6-ubuntu18.04-py37-cpu-inference:latest \"\n",
    ")\n",
    "myenv.python.user_managed_dependencies = True\n",
    "myenv.register(workspace=ws)\n",
    "\n",
    "myenv.environment_variables = {\"AZUREML_EXTRA_REQUIREMENTS_TXT\": \"requirements.txt\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create scoring script\n",
    "\n",
    "First, we will create a scoring script that will be invoked by the web service call. Note that the scoring script must have two required functions:\n",
    "* `init()`: In this function, you typically load the model into a `global` object. This function is executed only once when the Docker container is started. \n",
    "* `run(input_data)`: In this function, the model is used to predict a value based on the input data. The input and output typically use JSON as serialization and deserialization format, but you are not limited to that.\n",
    "\n",
    "Refer to the scoring script `pytorch_score.py` for this tutorial. Our web service will use this file to predict whether an image is a chicken or a turkey. When writing your own scoring script, don't forget to test it locally first before you go and deploy the web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Use Azure Machine Learning Inference HTTP Server before deployment\n",
    "\n",
    "The local inference server allows users to quickly debug their score script. In the case that the underlying score script has a bug, the server will fail to initialize/serve and will instead throw an exception & LOC where the issues occurred at. It enables users to easily create cohesive validation gates in their CI/CD pipelines. In order to do so, simple boot up the server with the candidate script and run the test suite against the local endpoint.\n",
    "\n",
    "**How to use the Inference HTTP Server?**\n",
    "\n",
    "* Install the ***azureml-inference-server-http*** package\n",
    "\n",
    "```sh\n",
    "python -m pip install azureml-inference-server-http\n",
    "```\n",
    "\n",
    "* Start the server and set score.py as the entry script.\n",
    "```sh\n",
    "azmlinfsrv --entry_script score.py\n",
    "```\n",
    "\n",
    "* Let's send a scoring request to the server using curl.\n",
    "\n",
    "```sh\n",
    "curl -p 127.0.0.1:5001/score\n",
    "```\n",
    "\n",
    "[Learn more](https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http) about the Azure machine learning inference HTTP server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deploy to ACI container\n",
    "We are ready to deploy. \n",
    "\n",
    "Create an inference configuration which gives specifies the inferencing environment and scripts. \n",
    "\n",
    "Create a deployment configuration file to specify the number of CPUs and gigabytes of RAM needed for your ACI container. \n",
    "\n",
    "This cell will run for about 7-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620921222094
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Write InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"pytorch_score.py\", environment=myenv)\n",
    "inference_config.source_directory = \"./\"\n",
    "\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=2,\n",
    "    memory_gb=2,\n",
    "    tags={\"data\": \"birds\", \"method\": \"transfer learning\", \"framework\": \"pytorch\"},\n",
    "    description=\"Classify turkey/chickens using transfer learning with PyTorch\",\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=\"deploy-aci-pytorch\",\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=aciconfig,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "If your deployment fails for any reason and you need to redeploy, make sure to delete the service before you do so: `service.delete()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620921231450
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Get the web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620921235480
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Test the web service\n",
    "Finally, let's test our deployed web service. We will send the data as a JSON string to the web service hosted in ACI and use the SDK's `run` API to invoke the service. Here we will take an image from our validation data to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620921241805
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(Image.open(\"test_img.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620921244635
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def preprocess(image_file):\n",
    "    \"\"\"Preprocess the input image.\"\"\"\n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    image = data_transforms(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620921249226
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "input_data = preprocess(\"test_img.jpg\")\n",
    "result = service.run(input_data=json.dumps({\"data\": input_data.tolist()}))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Clean up\n",
    "Once you no longer need the web service, you can delete it with a simple API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1620921256650
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
