{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630469516879
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connect to workspace via config.json"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "ws"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1630469593164
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Register Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, managed dataset supports 4 different datastore types. \n",
        "* Azure Blob Container\n",
        "* Azure File Share\n",
        "* Azure Data Lake\n",
        "* Azure Data Lake Gen2\n",
        "\n",
        "\n",
        "[Datastore Documents](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Azure Blob Container Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore\n",
        "blob_datastore = Datastore.register_azure_blob_container(\n",
        "   workspace=ws, # workspace\n",
        "   datastore_name=\"<datastore name you want to register to workspace>\",\n",
        "   account_name=\"<Azure storage account name>\",\n",
        "   container_name=\"<container name in the Azure blob storage>\",\n",
        "   account_key=\"<access key for the storage account>\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630469601076
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "account key can be found from [storage account] -> [Settings] -> [Access Keys]"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Azure File Share Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore\n",
        "fileshare_datastore = Datastore.register_azure_file_share(\n",
        "   workspace=ws, # workspace\n",
        "   datastore_name=\"<datastore name you want to register to workspace>\",\n",
        "   account_name=\"<Azure storage account name>\",\n",
        "   file_share_name=\"<file share name in the Azure storage account>\",\n",
        "   account_key=\"<access key for the storage account>\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630469606760
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Azure Data Lake Storage Gen1 (ADLS Gen1) Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ADLS gen1 and gen2, you will need service principal to access. Service principal need to be assigned with proper RBAC roles to interact with. For example, for ADLS gen2, you will need to assign sp with Azure blob contrainer contributor/owner roles. \n",
        "\n",
        "[How to use portal to create Azure AD service principal.](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "adls_datastore = Datastore.register_azure_data_lake(\n",
        "                workspace=ws,\n",
        "                datastore_name=\"<datastore name you want to register to workspace>\",\n",
        "                subscription_id=\"<subscription id of ADLS Gen1 stoarge account>\",\n",
        "                resource_group=\"<resource group of ADLS Gen1 storage account>\",\n",
        "                store_name=\"<name of ADLS Gen1 storage account>\",\n",
        "                tenant_id=\"<tenant id of service principal>\", \n",
        "                client_id=\"<client id of service principal>\",\n",
        "                client_secret=\"<the secret of service principal>\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630469609953
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Azure Data Lake Storage Gen2 (ADLS Gen2) Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset, Datastore\n",
        "adls_datastore_gen2 = Datastore.register_azure_data_lake_gen2(\n",
        "                workspace=ws,\n",
        "                datastore_name=\"<datastore name you want to register to workspace>\",\n",
        "                account_name=\"<name of ADLS Gen2 storage account>\",\n",
        "                filesystem=\"<ADLS gen2 container name>\",\n",
        "                tenant_id=\"<tenant id of service principal>\", \n",
        "                client_id=\"<client id of service principal>\",\n",
        "                client_secret=\"<the secret of service principal>\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630469613170
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Methods for Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## get existing datastore\n",
        "from azureml.core import Datastore\n",
        "datastore = Datastore.get(ws, 'fileshare_datastore')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## show datastore details\n",
        "datastore.__dict__"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## unregister a datastore\n",
        "datastore.unregister()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Upload a Spark dataframe and register as a dataset] register_spark_dataframe\n",
        "\n",
        "```python\n",
        "def register_spark_dataframe(dataframe, target, name, show_progress=True):\n",
        "    \"\"\"Create a dataset from spark dataframe.\n",
        "\n",
        "    :param dataframe: In memory dataframe to be uploaded.\n",
        "    :type dataframe: pyspark.sql.DataFrame\n",
        "    :param target: The datastore path where the dataframe parquet data will be uploaded to.\n",
        "        A guid folder will be generated under the target path to avoid conflict.\n",
        "    :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
        "        or tuple(azureml.core.datastore.Datastore, str) object\n",
        "    :param name: The name of the registered dataset.\n",
        "    :type name: str\n",
        "    :param show_progress: Indicates whether to show progress in the console. Defaults to be True.\n",
        "    :type show_progress: bool, optional\n",
        "    :return: The registered dataset.\n",
        "    :rtype: azureml.data.TabularDataset\n",
        "    \"\"\"\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Currently, register_spark_dataframe only support Azure Blob storage, ADLS Gen1 and ADLS Gen2. File share is not supported.\n",
        "You can use Spark dataframe created by yourself or use the below sample code to get Spark dataframe from existing dataset. \n",
        "\n",
        "```python\n",
        "dstore = workspace.get_default_datastore()\n",
        "datastore_path = [(dstore, 'weather-data-florida/*/*/data.parquet')]\n",
        "dataset = Dataset.Tabular.from_parquet_files(path=datastore_path)\n",
        "spark_df = dataset.to_spark_dataframe()\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset=Dataset.Tabular.register_spark_dataframe(<spark dataframe>, <datastore>, \"<name of registered dataset>\", show_progress=True)\n",
        "# Using blob_datastore that we created earlier in this sample notebook\n",
        "dataset = Dataset.Tabular.register_spark_dataframe(spark_df, blob_datastore, \"ds_from_spark_df\", show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Upload a Pandas dataframe and register as a dataset] register_pandas_dataframe\n",
        "\n",
        "```python\n",
        "def register_pandas_dataframe(dataframe, target, name, show_progress=True):\n",
        "    \"\"\" Create a dataset from pandas dataframe.\n",
        "        Datastore type can only be azure data lake store or azure storage store.\n",
        "\n",
        "    :param dataframe: In memory dataframe to be uploaded.\n",
        "    :type dataframe: pandas.DataFrame\n",
        "    :param target: The datastore path where the dataframe parquet data will be uploaded to.\n",
        "        A guid folder will be generated under the target path to avoid conflict.\n",
        "    :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore or tuple(azureml.core.datastore.Datastore, str) object\n",
        "    :param name: The name of the registered dataset.\n",
        "    :type name: str\n",
        "    :param show_progress: Indicates whether to show progress in the console. Defaults to be True.\n",
        "    :type show_progress: bool, optional\n",
        "    :return: The registered dataset.\n",
        "    :rtype: azureml.data.TabularDataset\n",
        "    \"\"\"\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use Pandas dataframe created by yourself or use the below sample code to get Pandas dataframe from existing dataset. \n",
        "\n",
        "```python\n",
        "dstore = workspace.get_default_datastore()\n",
        "datastore_path = [(dstore, 'weather-data-florida/*/*/data.parquet')]\n",
        "dataset = Dataset.Tabular.from_parquet_files(path=datastore_path)\n",
        "pandas_df = dataset.to_pandas_dataframe()\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset=Dataset.Tabular.register_pandas_dataframe(<pandas dataframe>, <datastore>, \"<name of registered dataset>\", show_progress=True)\n",
        "# Using blob_datastore that we created earlier in this sample notebook\n",
        "dataset = Dataset.Tabular.register_pandas_dataframe(pandas_df, blob_datastore, \"ds_from_pandas_df\", show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630469736575
        }
      }
    }
  ],
  "metadata": {
    "pygments_lexer": "ipython3",
    "name": "python",
    "mimetype": "text/x-python",
    "npconvert_exporter": "python",
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "file_extension": ".py",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}