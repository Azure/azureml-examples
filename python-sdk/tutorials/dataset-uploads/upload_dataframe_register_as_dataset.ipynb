{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633029054756
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connect to workspace via config.json"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "workspace = Workspace.from_config()\n",
        "print(workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1633029061345
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, following types of datastores are supported: \n",
        "* Azure Blob Container\n",
        "* Azure File Share\n",
        "* Azure Data Lake\n",
        "* Azure Data Lake Gen2\n",
        "\n",
        "\n",
        "To learn more about datatstores, refer to [datastore documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload a Pandas dataframe and register as a dataset: register_pandas_dataframe()\n",
        "\n",
        "```python\n",
        "def register_pandas_dataframe(dataframe, target, name, show_progress=True):\n",
        "    \"\"\" Create a dataset from pandas dataframe.\n",
        "        Datastore type can only be azure data lake store or azure storage store.\n",
        "\n",
        "    :param dataframe: In memory dataframe to be uploaded.\n",
        "    :type dataframe: pandas.DataFrame\n",
        "    :param target: The datastore path where the dataframe parquet data will be uploaded to.\n",
        "        A guid folder will be generated under the target path to avoid conflict.\n",
        "    :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore or tuple(azureml.core.datastore.Datastore, str) object\n",
        "    :param name: The name of the registered dataset.\n",
        "    :type name: str\n",
        "    :param show_progress: Indicates whether to show progress in the console. Defaults to be True.\n",
        "    :type show_progress: bool, optional\n",
        "    :return: The registered dataset.\n",
        "    :rtype: azureml.data.TabularDataset\n",
        "    \"\"\"\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use Pandas dataframe created by yourself or use the below sample code to get Pandas dataframe from existing dataset. \n",
        "\n",
        "```python\n",
        "datastore = workspace.get_default_datastore()\n",
        "datastore_path = [(datastore, 'weather-data-florida/*/*/data.parquet')]\n",
        "dataset = Dataset.Tabular.from_parquet_files(path=datastore_path)\n",
        "pandas_df = dataset.to_pandas_dataframe()\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax:\n",
        "# dataset=Dataset.Tabular.register_pandas_dataframe(<pandas dataframe>, <datastore>, \"<name of registered dataset>\", show_progress=True)\n",
        "# Using workspace default datastore for uploading Pandas dataframe\n",
        "import pandas as pd\n",
        "from azureml.core import Dataset\n",
        "\n",
        "# Pandas dataframe from a local CSV file\n",
        "pandas_df = pd.read_csv(\"./data/titanic.csv\")\n",
        "# Getting workspace default datastore\n",
        "datastore = workspace.get_default_datastore()\n",
        "# Uploading Pandas dataframe and registering it as a dataset\n",
        "dataset = Dataset.Tabular.register_pandas_dataframe(pandas_df, datastore, \"ds_from_pandas_df\", show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1633029097380
        }
      }
    }
  ],
  "metadata": {
    "pygments_lexer": "ipython3",
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "mimetype": "text/x-python",
    "npconvert_exporter": "python",
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "file_extension": ".py",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}