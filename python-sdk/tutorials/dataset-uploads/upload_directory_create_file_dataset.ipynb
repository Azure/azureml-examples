{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "print(\"Core version:\", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630471040098
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connect to workspace via config.json"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "ws"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1630471072118
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import required modules"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "import azureml.dataprep"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading to Regular Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Azure Data Lake Storage Gen1 (ADLS Gen1) as Datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ADLS Gen1 and ADLS Gen2, you will need Service Principal (SP) to access data. Service principals need to be assigned proper RBAC roles for enabling access to data in the storage account. For example, for ADLS Gen2, you will need to assign Storage Blob Data Contributor/Owner roles to the SP. \n",
        "\n",
        "[How to use portal to create Azure AD service principal.](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "adlsgen1_datastore_name ='adlgen1store' # Datastore name\n",
        "store_name = '<your ADLS store name>' # ADLS Gen 1 storage account name\n",
        "subscription_id = '<your subscription id>'\n",
        "resource_group = '<your resource group>' # resource group for the ADLS Gen1 storage account\n",
        "tenant_id = '<your tenant id>' # tenant id for the service principal\n",
        "client_id = '<your SP client id>' # client id for the service principal\n",
        "client_secret = '<your SP client secret>' # the secret of service principal\n",
        "\n",
        "adls_datastore = Datastore.register_azure_data_lake(\n",
        "   workspace=ws,\n",
        "   datastore_name=adlsgen1_datastore_name,\n",
        "   subscription_id=subscription_id, # subscription for the ADLS Gen1 storage account\n",
        "   resource_group=resource_group, # resource group of ADLS Gen1 storage account\n",
        "   store_name=store_name, # ADLS Gen1 storage account name\n",
        "   tenant_id=tenant_id, # tenant id for the service principal\n",
        "   client_id=client_id, # client id for the service principal\n",
        "   client_secret=client_secret) # the secret of service principal"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload files to datastore and create a file dataset\n",
        "\n",
        "```python\n",
        "    def upload_directory(src_dir, target, pattern=None, overwrite=False, show_progress=True):\n",
        "        \"\"\"Upload source directory to target datastore and create a file dataset\n",
        "\n",
        "        :param src_dir: The local directory to upload.\n",
        "        :type src_dir: str\n",
        "        :param target: Required, the datastore path where the files will be uploaded to.\n",
        "        :type target: azureml.data.datapath.DataPath, azureml.core.datastore.Datastore\n",
        "            or tuple(azureml.core.datastore.Datastore, str) object\n",
        "        :param pattern: Optional, If provided, will filter all the path names matching the given pattern,\n",
        "            similar to Python glob package, supporting '*', '?', and character ranges expressed with [].\n",
        "        :type pattern: str\n",
        "        :param show_progress: Optional,\n",
        "            indicates whether to show progress of the upload in the console. Defaults to be True.\n",
        "        :type show_progress: bool\n",
        "        :return: The created file dataset.\n",
        "        :rtype: azureml.data.FileDataset\n",
        "        \"\"\"\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red' size=2>\r\n",
        "Please make sure both your account and the Service Pricipal used for the Azure Data Lake Store Gen1 are granted adequate permissions to access the folder in the data lake.\r\n",
        "</font>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ds1 = Dataset.File.upload_directory(src_dir='<your source folder for uploading>/',\n",
        "           target=DataPath(adls_datastore, '<upload path on the datastore>'),\n",
        "           show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# download files back to local\n",
        "ds1.download(\"./downloads\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# upload source files based on their path name using Pattern\n",
        "\n",
        "ds2 = Dataset.File.upload_directory(src_dir='<your source folder for uploading>/',\n",
        "           target=DataPath(adls_datastore, '<upload path on the datastore>'),\n",
        "           pattern='<source files filtering pattern>',\n",
        "           show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading to credential-less datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Azure Data Lake Storage Gen1 (ADLS Gen1) as credential-less datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "adlsgen1_datastore_name = 'adlgen1credlessstore' # Datastore name\n",
        "store_name = '<your ADLS store name>' # ADLS Gen1 storage account name\n",
        "subscription_id = '<your subscription id>' # subscription for the ADLS Gen1 storage account\n",
        "resource_group = '<your resource group>' # resource group for the ADLS Gen1 storage account\n",
        "\n",
        "adls_datastore_cred_less = Datastore.register_azure_data_lake(\n",
        "   workspace=ws,\n",
        "   datastore_name=adlsgen1_datastore_name, # Datastore name\n",
        "   subscription_id=subscription_id, # subscription id for the ADLS Gen1 storage account\n",
        "   resource_group=resource_group, # resource group for the ADLS Gen1 storage account\n",
        "   store_name=store_name) # ADLS Gen1 storage account name"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload files to cred-less datastore and create a file dataset\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ds3 = Dataset.File.upload_directory(src_dir='<your source folder for uploading>/',\n",
        "           target=DataPath(adls_datastore_cred_less, '<upload path on the datastore>'),\n",
        "           overwrite=True,\n",
        "           show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading to credential-less datastore using MSI"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create or Attach existing compute resource"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red' size=2>\r\n",
        "If using an existing compute target, please make sure the compute target has an identity attached, or type a new name to let the below script create a new one.\r\n",
        "</font>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# choose a name for your cluster\n",
        "cluster_name = '<your compute cluster name>'\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing compute target')\n",
        "except ComputeTargetException:\n",
        "    print('Creating a new compute target...')\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
        "                                                           max_nodes=4,\n",
        "                                                           identity_type='SystemAssigned')\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "\n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \n",
        "    # if no min node count is provided it uses the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "\n",
        "# use get_status() to get a detailed status for the current cluster. \n",
        "print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red' size=2>\n",
        "    Make sure that you grant access to the compute cluster created here in the Azure Data Lake Store Gen1 account:<br>    \n",
        "    (Not only the storage account, but also the specific folder to access)<br>    \n",
        "    <dd>&emsp;1. Go to the Azure Data Lake Store Gen1 portal, click on \"Data explorer\" </dd>\n",
        "    <dd>&emsp;2. Click on \"Access\" then click \"+\", </dd>\n",
        "    <dd>&emsp;3. In the opened page, click \"Select\" and search for below content:<br>\n",
        "        &emsp;&emsp;&emsp;\"{your workspace name}/computes/{the compute cluster name created above}\" </dd>\n",
        "    <dd>&emsp;4. Grant all needed permissions.</dd>\n",
        "</font>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the remote run script"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red' size=2>\r\n",
        "    Please replace credential-less datastore name created above and the datapath where files are uploaded.\r\n",
        "</font>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data/run.py\n",
        "def run():\n",
        "    from azureml.core import Dataset, Datastore\n",
        "    from azureml.data.datapath import DataPath\n",
        "    from azureml.core.run import Run\n",
        "    import glob\n",
        "\n",
        "    ws = Run.get_context().experiment.workspace\n",
        "    print('Got workspace')\n",
        "    print(ws)\n",
        "    print('Getting datastore')\n",
        "    dstore = Datastore.get(ws, 'adlgen1credlessstore')\n",
        "    print('Got datastore')\n",
        "    print(dstore)\n",
        "    datapath = DataPath(dstore, '/%s/' % 'dataset_from_compute')\n",
        "    print(datapath)\n",
        "    saved_dataset = Dataset.File.upload_directory(\n",
        "        src_dir='./',\n",
        "        target=datapath,\n",
        "        overwrite=True,\n",
        "        show_progress=True)\n",
        "    print(saved_dataset)\n",
        "\n",
        "run()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload files from remote context to target credential-less datastore"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
        "\n",
        "rc = RunConfiguration()\n",
        "rc.target = compute_target\n",
        "\n",
        "# create or load an experiment\n",
        "experiment = Experiment(workspace=ws, name='MyUploadingExperiment')\n",
        "# run a trial from the train.py code in your current directory\n",
        "config = ScriptRunConfig(source_directory='data', script='run.py', run_config=rc)\n",
        "run = experiment.submit(config)\n",
        "run.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    }
  ],
  "metadata": {
    "pygments_lexer": "ipython3",
    "name": "python",
    "mimetype": "text/x-python",
    "npconvert_exporter": "python",
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "file_extension": ".py",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}