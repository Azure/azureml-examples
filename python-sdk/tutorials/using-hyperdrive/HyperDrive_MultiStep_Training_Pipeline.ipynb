{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Step Training Pipeline with HyperDrive\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook covers the process of setting up a basic multi-step training pipeline that utilizes hyperparameter tuning within an Azure Machine Learning workspace.\r\n",
    "\r\n",
    "Steps in this notebook include:\r\n",
    "\r\n",
    "- Training Step (Hyperdrive step used for hyperparameter tuning)\r\n",
    "- Evaluation Step\r\n",
    "- Registration step\r\n",
    "\r\n",
    "This notebook is meant to serve as a guide or template for users who wish to do hyperparameter tuning within more complex multi-step pipelines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load dotenv extension\r\n",
    "\r\n",
    "%load_ext dotenv\r\n",
    "%dotenv\r\n",
    "\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "sys.path.append(\"./\")\r\n",
    "\r\n",
    "from azureml.core import Environment, Workspace, Experiment, ScriptRunConfig\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.data.data_reference import DataReference\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter, TrainingOutput\r\n",
    "from azureml.pipeline.steps import PythonScriptStep, HyperDriveStep\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "\r\n",
    "from azureml.train.hyperdrive import GridParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive import choice\r\n",
    "\r\n",
    "from src.common.attach_compute import get_compute\r\n",
    "from src.common.get_datastores import get_blob_datastore"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Workspace and Set Compute Target"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Interactive login \r\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "interactive_auth = InteractiveLoginAuthentication(force=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a workspace object from the `config.json` file in the running directory that you can [download from your AzureML](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace) portal:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Restore AML workspace from config.json file (can be downloaded through the portal)\r\n",
    "ws = Workspace.from_config()\r\n",
    "print('Workspace name: ' + ws.name, \r\n",
    "      'Azure region: ' + ws.location, \r\n",
    "      'Subscription id: ' + ws.subscription_id, \r\n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a compute target for our training job to run on, using the script helper `get_compute` imported from `attach_compute.py` file and the information provided within `.env` file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set compute target\r\n",
    "compute_target = get_compute(\r\n",
    "    workspace=ws,\r\n",
    "    compute_name=os.getenv(\"AML_CLUSTER_NAME\"),\r\n",
    "    vm_size=os.getenv(\"AML_CLUSTER_CPU_SKU\"),\r\n",
    "    vm_priority=os.environ.get(\"AML_CLUSTER_PRIORITY\", 'lowpriority'), \r\n",
    "    min_nodes=int(os.environ.get(\"AML_CLUSTER_MIN_NODES\", 0)),\r\n",
    "    max_nodes=int(os.environ.get(\"AML_CLUSTER_MAX_NODES\", 4)),\r\n",
    "    scale_down=int(os.environ.get(\"AML_CLUSTER_SCALE_DOWN\", 600)),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Datastores \r\n",
    "\r\n",
    "Create storage containers where data will be accessed, using the script helper `get_blob_datastore` imported from `attach_compute.py` file and the information provided within `.env` file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create root datastore\r\n",
    "root_datastore = get_blob_datastore(\r\n",
    "    ws, os.getenv(\"BLOB_DATASTORE_NAME\"), \r\n",
    "    os.getenv(\"STORAGE_NAME\"),\r\n",
    "    os.getenv(\"STORAGE_KEY\"), \r\n",
    "    os.getenv(\"STORAGE_CONTAINER\")\r\n",
    "    )\r\n",
    "\r\n",
    "# Create input and output data reference\r\n",
    "root_dir = DataReference(\r\n",
    "    datastore=root_datastore, \r\n",
    "    data_reference_name=\"data_reference\", \r\n",
    "    mode=\"mount\"\r\n",
    ")\r\n",
    "\r\n",
    "# Create pipeline data to hold model info in intermediate pipeline steps\r\n",
    "model_info_dir = PipelineData(\r\n",
    "        \"model_info_dir\",\r\n",
    "        datastore=root_datastore,\r\n",
    "        output_mode='mount',\r\n",
    "        output_overwrite=True,\r\n",
    "    )\r\n",
    "\r\n",
    "# Hyperdrive specific output (one file only)\r\n",
    "model_info_best = PipelineData(\r\n",
    "        \"model_info_best\",\r\n",
    "        datastore=root_datastore,\r\n",
    "        output_mode = 'mount',\r\n",
    "        training_output=TrainingOutput(\"Model\", model_file=\"outputs/model/model_info.json\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Pipeline Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Just an example how we can use parameters to provide different input folders and values\r\n",
    "model_name = PipelineParameter(name=\"model_name\", default_value=\"basic_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Custom Environment \r\n",
    "\r\n",
    "Create the Azure ML environment that encapsulates dependencies of our training script:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "\r\n",
    "# Specify package dependencies \r\n",
    "batch_conda_deps = CondaDependencies.create(\r\n",
    "    pip_packages=[\r\n",
    "        \"azureml==0.2.7\",\r\n",
    "        \"azureml-core==1.15.0\",\r\n",
    "        \"click==7.0\",\r\n",
    "        \"numpy==1.18.5\",\r\n",
    "        \"pandas==1.1.3\",\r\n",
    "        \"Pillow==7.2.0\",\r\n",
    "        \"tqdm==4.61.0\",\r\n",
    "    ]\r\n",
    ")\r\n",
    "\r\n",
    "# Create and set environment\r\n",
    "batch_env = Environment(name=\"train-env\")\r\n",
    "batch_env.docker.enabled = True\r\n",
    "\r\n",
    "# Set dependencies that will be used within environment (batch_conda_deps is a set of dependencies)\r\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\r\n",
    "\r\n",
    "# Set python version that environment will utilize\r\n",
    "batch_conda_deps.set_python_version('3.8.6')\r\n",
    "\r\n",
    "# Set up pipeline run configuration and set environment \r\n",
    "run_config = RunConfiguration()\r\n",
    "run_config.environment = batch_env"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create train step\r\n",
    "\r\n",
    "The cells in this section configure and instantiate the steps for training the model using HyperDrive, which carry most differences in transforming a standard training pipeline, without hyperparameter tuning, into a one with HyperDrive. \r\n",
    "\r\n",
    "In fact, all the previous steps will be required, with no change, for a standard pipeline as well. However, the train step for a standard pipeline will be much simpler, based on a single statement using `PythonScriptStep` class. Using HyperDrive, the `PythonScriptStep` is replaced by `HyperDriveStep` class. Running the `HyperDriveStep` requires two configuration steps to be prepared in advance: `run_config` and `hyperdrive_config`. \r\n",
    "\r\n",
    "We create the `run_config`, named `train_step_config` in this example, as a `ScriptRunConfig` object that specifies the configuration details of our training job, including our training script, environment to use, and the compute target to run on:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "source_directory = \"./\"\r\n",
    "\r\n",
    "# Create configurations for training step\r\n",
    "train_step_config = ScriptRunConfig(\r\n",
    "    script=\"src/pipeline/train.py\",\r\n",
    "    compute_target=compute_target,\r\n",
    "    source_directory=source_directory,\r\n",
    "    environment=batch_env\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we use the `run_config` created above to configure a run using `HyperDriveConfig` class. Before that, we first define the parameter space using grid sampling, which does a simple grid search over all possible values. In this example, we tune threshold, number of epochs, and learning rate as input parameters. \r\n",
    "\r\n",
    "We may also use an [early termination policy](https://docs.microsoft.com/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy) that automatically ends poorly performing runs to improve computational efficiency.\r\n",
    "\r\n",
    "In the `HyperDriveConfig` object, we need to specify the primary metric to be recorded in our training runs, which is `F_1` metric in our example. Notice that this metric needs to be logged in the training script. We also need to tell the service that we are looking to maximize `F_1`. Further, we set the maximum number of training runs to 500, and maximal concurrent jobs to 4, which is the same as the number of nodes in our compute cluster."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initiate sampling method and provide search space\r\n",
    "ps = GridParameterSampling( {\r\n",
    "        \"thr\": choice(0.05, 0.20, 0.35, 0.50, 0.65, 0.80, 0.95),\r\n",
    "        \"epoch\": choice(1, 20, 40, 60, 80, 100),\r\n",
    "        \"lr\": choice(0.0001, 0.001, 0.01, 0.1, 1, 5)\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "#  Early termination of low-performance runs\r\n",
    "early_termination_policy = BanditPolicy(evaluation_interval=5, slack_factor=0.2)\r\n",
    "\r\n",
    "# Configure hyperdrive config \r\n",
    "hd_config = HyperDriveConfig(\r\n",
    "    run_config=train_step_config, \r\n",
    "    hyperparameter_sampling=ps,\r\n",
    "    policy=early_termination_policy,\r\n",
    "    primary_metric_name=\"F_1\", \r\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "    max_total_runs=300,\r\n",
    "    max_concurrent_runs=4\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to use `HyperDriveStep` to run hyperparameter tuning as a step in pipeline:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a training step using HyperDrive\r\n",
    "train_step = HyperDriveStep(\r\n",
    "    name=\"train step\",\r\n",
    "    hyperdrive_config=hd_config,\r\n",
    "    estimator_entry_script_arguments=[\r\n",
    "        \"--root_dir\",\r\n",
    "        root_dir,\r\n",
    "        \"--model_info_dir\",\r\n",
    "        model_info_dir,\r\n",
    "        \"--model_info_best\",\r\n",
    "        model_info_best,\r\n",
    "    ],\r\n",
    "    inputs=[root_dir],\r\n",
    "    outputs=[model_info_dir, model_info_best],\r\n",
    "    allow_reuse=False,\r\n",
    ") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create evaluation step\r\n",
    "\r\n",
    "The following cell configures and instantiates the step for evaluation (also called test) of the trained model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create evaluation step to test the trained model\r\n",
    "eval_step = PythonScriptStep(\r\n",
    "    name=\"eval step\",\r\n",
    "    script_name=\"src/pipeline/evaluate.py\",\r\n",
    "    arguments=[\r\n",
    "        \"--root_dir\",\r\n",
    "        root_dir,\r\n",
    "        \"--model_info_dir\",\r\n",
    "        model_info_dir,\r\n",
    "        \"--model_info_best\",\r\n",
    "        model_info_best,\r\n",
    "    ],\r\n",
    "    inputs=[root_dir, model_info_dir, model_info_best],\r\n",
    "    outputs=[],\r\n",
    "    compute_target=compute_target,\r\n",
    "    source_directory=source_directory,\r\n",
    "    runconfig=run_config,\r\n",
    "    allow_reuse=False,\r\n",
    ")\r\n",
    "\r\n",
    "eval_step.run_after(train_step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create registration step\r\n",
    "\r\n",
    "The following cell configures and instantiates the step for registering the model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a step to register the model\r\n",
    "register_step = PythonScriptStep(\r\n",
    "    name=\"register step\",\r\n",
    "    script_name=\"src/pipeline/register.py\",\r\n",
    "    arguments=[\r\n",
    "        \"--root_dir\",\r\n",
    "        root_dir,\r\n",
    "        \"--model_name\",\r\n",
    "        model_name,\r\n",
    "        \"--model_info_dir\",\r\n",
    "        model_info_dir,\r\n",
    "    ],\r\n",
    "    inputs=[root_dir, model_info_dir],\r\n",
    "    outputs=[],\r\n",
    "    compute_target=compute_target,\r\n",
    "    source_directory=source_directory,\r\n",
    "    runconfig=run_config,\r\n",
    "    allow_reuse=False,\r\n",
    ")\r\n",
    "\r\n",
    "register_step.run_after(eval_step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure and publish pipeline to AML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create pipeline using existing steps\r\n",
    "training_pipeline = Pipeline(workspace=ws, steps=[train_step, eval_step, register_step])\r\n",
    "\r\n",
    "# Check if the pipeline is consistent \r\n",
    "training_pipeline.validate()\r\n",
    "\r\n",
    "# Publish pipeline\r\n",
    "published_pipeline = training_pipeline.publish(\r\n",
    "    name = \"hyperdrive_training_pipeline\",\r\n",
    "    description = \"Hyperdrive training pipeline experiment\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submit and run pipeline in AML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Submit the pipeline\r\n",
    "pipeline_run = Experiment(ws, 'Reza-hyperdrive-training').submit(training_pipeline)\r\n",
    "pipeline_run.wait_for_completion()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('azure-examples': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "eac405e3a102007e39b7d474266b3f29064fe1851bc7a4c3889e65ca4778a0ba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}