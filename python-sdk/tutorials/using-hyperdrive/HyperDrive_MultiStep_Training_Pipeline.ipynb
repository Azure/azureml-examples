{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Step Training Pipeline with HyperDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the process of setting up a basic multi-step training pipeline that utilizes hyperparameter tuning within an Azure Machine Learning workspace.\n",
    "\n",
    "Steps in this notebook include:\n",
    "\n",
    "- Training Step (Hyperdrive step used for hyperparameter tuning)\n",
    "- Evaluation Step\n",
    "- Registration step\n",
    "\n",
    "This notebook is meant to serve as a guide or template for users who wish to do hyperparameter tuning within more complex multi-step pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv extension\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "from azureml.core import Environment, Workspace, Experiment, ScriptRunConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import (\n",
    "    Pipeline,\n",
    "    PipelineData,\n",
    "    PipelineParameter,\n",
    "    TrainingOutput,\n",
    ")\n",
    "from azureml.pipeline.steps import PythonScriptStep, HyperDriveStep\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "from azureml.train.hyperdrive import (\n",
    "    GridParameterSampling,\n",
    "    BanditPolicy,\n",
    "    HyperDriveConfig,\n",
    "    PrimaryMetricGoal,\n",
    ")\n",
    "from azureml.train.hyperdrive import choice\n",
    "\n",
    "from src.common.attach_compute import get_compute\n",
    "from src.common.get_datastores import get_blob_datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive login\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication(force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a workspace object from the `config.json` file in the running directory that you can [download from your AzureML](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace) portal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore AML workspace from config.json file (can be downloaded through the portal)\n",
    "ws = Workspace.from_config()\n",
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Subscription id: \" + ws.subscription_id,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set a compute target for our training job to run on, using the script helper `get_compute` imported from `attach_compute.py` file. It will either utilize the default compute for the workspace when the `get_default` parameter is True, or take the information provided within `.env` file to create/configure a specified compute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set compute target\n",
    "compute_target = get_compute(\n",
    "    workspace=ws,\n",
    "    get_default=True,\n",
    "    get_default_type=\"CPU\",\n",
    "    compute_name=os.getenv(\"AML_CLUSTER_NAME\"),\n",
    "    vm_size=os.getenv(\"AML_CLUSTER_CPU_SKU\"),\n",
    "    vm_priority=os.environ.get(\"AML_CLUSTER_PRIORITY\"),\n",
    "    min_nodes=int(os.environ.get(\"AML_CLUSTER_MIN_NODES\")),\n",
    "    max_nodes=int(os.environ.get(\"AML_CLUSTER_MAX_NODES\")),\n",
    "    scale_down=int(os.environ.get(\"AML_CLUSTER_SCALE_DOWN\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Datastores \n",
    "\n",
    "Here we define the storage containers where data will be accessed, using the script helper `get_blob_datastore` imported from `get_datastores.py` file. It will either utilize the default datastore for the workspace when the `get_default` parameter is True, or take the information provided within `.env` file to register a specified datastore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create root datastore\n",
    "root_datastore = get_blob_datastore(\n",
    "    workspace=ws,\n",
    "    get_default=True,\n",
    "    data_store_name=os.getenv(\"BLOB_DATASTORE_NAME\"),\n",
    "    storage_name=os.getenv(\"STORAGE_NAME\"),\n",
    "    storage_key=os.getenv(\"STORAGE_KEY\"),\n",
    "    container_name=os.getenv(\"STORAGE_CONTAINER\"),\n",
    ")\n",
    "\n",
    "# Create input and output data reference\n",
    "root_dir = DataReference(\n",
    "    datastore=root_datastore, data_reference_name=\"data_reference\", mode=\"mount\"\n",
    ")\n",
    "\n",
    "# Create pipeline data to hold model info in intermediate pipeline steps\n",
    "model_info_dir = PipelineData(\n",
    "    \"model_info_dir\",\n",
    "    datastore=root_datastore,\n",
    "    output_mode=\"mount\",\n",
    "    output_overwrite=True,\n",
    ")\n",
    "\n",
    "# Hyperdrive specific output (one file only)\n",
    "model_info_best = PipelineData(\n",
    "    \"model_info_best\",\n",
    "    datastore=root_datastore,\n",
    "    output_mode=\"mount\",\n",
    "    training_output=TrainingOutput(\"Model\", model_file=\"outputs/model/model_info.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an example how we can use parameters to provide different input folders and values\n",
    "model_name = PipelineParameter(name=\"model_name\", default_value=\"basic_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Environment \n",
    "\n",
    "Create the Azure ML environment that encapsulates dependencies of our training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Specify package dependencies\n",
    "batch_conda_deps = CondaDependencies.create(\n",
    "    pip_packages=[\n",
    "        \"azureml==0.2.7\",\n",
    "        \"azureml-core==1.15.0\",\n",
    "        \"click==7.0\",\n",
    "        \"numpy==1.18.5\",\n",
    "        \"pandas==1.1.3\",\n",
    "        \"Pillow==7.2.0\",\n",
    "        \"tqdm==4.61.0\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create and set environment\n",
    "batch_env = Environment(name=\"train-env\")\n",
    "batch_env.docker.enabled = True\n",
    "\n",
    "# Set dependencies that will be used within environment (batch_conda_deps is a set of dependencies)\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "\n",
    "# Set python version that environment will utilize\n",
    "batch_conda_deps.set_python_version(\"3.8.6\")\n",
    "\n",
    "# Set up pipeline run configuration and set environment\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = batch_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train step\n",
    "\n",
    "The cells in this section configure and instantiate the steps for training the model using HyperDrive, which carry most differences in transforming a standard training pipeline, without hyperparameter tuning, into a one with HyperDrive. \n",
    "\n",
    "In fact, all the previous steps will be required, with no change, for a standard pipeline as well. However, the train step for a standard pipeline will be much simpler, based on a single statement using `PythonScriptStep` class. Using HyperDrive, the `PythonScriptStep` is replaced by `HyperDriveStep` class. Running the `HyperDriveStep` requires two configuration steps to be prepared in advance: `run_config` and `hyperdrive_config`. \n",
    "\n",
    "We create the `run_config`, named `train_step_config` in this example, as a `ScriptRunConfig` object that specifies the configuration details of our training job, including our training script, environment to use, and the compute target to run on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"./\"\n",
    "\n",
    "# Create configurations for training step\n",
    "train_step_config = ScriptRunConfig(\n",
    "    script=\"src/pipeline/train.py\",\n",
    "    compute_target=compute_target,\n",
    "    source_directory=source_directory,\n",
    "    environment=batch_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `run_config` created above to configure a run using `HyperDriveConfig` class. Before that, we first define the parameter space using grid sampling, which does a simple grid search over all possible values. In this example, we tune threshold, number of epochs, and learning rate as input parameters. \n",
    "\n",
    "We may also use an [early termination policy](https://docs.microsoft.com/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy) that automatically ends poorly performing runs to improve computational efficiency.\n",
    "\n",
    "In the `HyperDriveConfig` object, we need to specify the primary metric to be recorded in our training runs, which is `F_1` metric in our example. Notice that this metric needs to be logged in the training script. We also need to tell the service that we are looking to maximize `F_1`. Further, we set the maximum number of training runs to 500, and maximal concurrent jobs to 4, which is the same as the number of nodes in our compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate sampling method and provide search space\n",
    "ps = GridParameterSampling(\n",
    "    {\n",
    "        \"thr\": choice(0.05, 0.20),  # (0.05, 0.20, 0.35, 0.50, 0.65, 0.80, 0.95),\n",
    "        \"epoch\": choice(1, 20),  # (1, 20, 40, 60, 80, 100),\n",
    "        \"lr\": choice(0.0001, 0.001),  # (0.0001, 0.001, 0.01, 0.1, 1, 5),\n",
    "    }\n",
    ")\n",
    "\n",
    "#  Early termination of low-performance runs\n",
    "early_termination_policy = BanditPolicy(evaluation_interval=5, slack_factor=0.2)\n",
    "\n",
    "# Configure hyperdrive config\n",
    "hd_config = HyperDriveConfig(\n",
    "    run_config=train_step_config,\n",
    "    hyperparameter_sampling=ps,\n",
    "    policy=early_termination_policy,\n",
    "    primary_metric_name=\"F_1\",\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=300,\n",
    "    max_concurrent_runs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use `HyperDriveStep` to run hyperparameter tuning as a step in pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training step using HyperDrive\n",
    "train_step = HyperDriveStep(\n",
    "    name=\"train step\",\n",
    "    hyperdrive_config=hd_config,\n",
    "    estimator_entry_script_arguments=[\n",
    "        \"--root_dir\",\n",
    "        root_dir,\n",
    "        \"--model_info_dir\",\n",
    "        model_info_dir,\n",
    "        \"--model_info_best\",\n",
    "        model_info_best,\n",
    "    ],\n",
    "    inputs=[root_dir],\n",
    "    outputs=[model_info_dir, model_info_best],\n",
    "    allow_reuse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation step\n",
    "\n",
    "The following cell configures and instantiates the step for evaluation (also called test) of the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation step to test the trained model\n",
    "eval_step = PythonScriptStep(\n",
    "    name=\"eval step\",\n",
    "    script_name=\"src/pipeline/evaluate.py\",\n",
    "    arguments=[\n",
    "        \"--root_dir\",\n",
    "        root_dir,\n",
    "        \"--model_info_dir\",\n",
    "        model_info_dir,\n",
    "        \"--model_info_best\",\n",
    "        model_info_best,\n",
    "    ],\n",
    "    inputs=[root_dir, model_info_dir, model_info_best],\n",
    "    outputs=[],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config,\n",
    "    allow_reuse=False,\n",
    ")\n",
    "\n",
    "eval_step.run_after(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create registration step\n",
    "\n",
    "The following cell configures and instantiates the step for registering the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a step to register the model\n",
    "register_step = PythonScriptStep(\n",
    "    name=\"register step\",\n",
    "    script_name=\"src/pipeline/register.py\",\n",
    "    arguments=[\n",
    "        \"--root_dir\",\n",
    "        root_dir,\n",
    "        \"--model_name\",\n",
    "        model_name,\n",
    "        \"--model_info_dir\",\n",
    "        model_info_dir,\n",
    "    ],\n",
    "    inputs=[root_dir, model_info_dir],\n",
    "    outputs=[],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config,\n",
    "    allow_reuse=False,\n",
    ")\n",
    "\n",
    "register_step.run_after(eval_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and publish pipeline to AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline using existing steps\n",
    "training_pipeline = Pipeline(workspace=ws, steps=[train_step, eval_step, register_step])\n",
    "\n",
    "# Check if the pipeline is consistent\n",
    "training_pipeline.validate()\n",
    "\n",
    "# Publish pipeline\n",
    "published_pipeline = training_pipeline.publish(\n",
    "    name=\"hyperdrive_training_pipeline\",\n",
    "    description=\"Hyperdrive training pipeline experiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run pipeline in AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the pipeline\n",
    "pipeline_run = Experiment(ws, \"Reza-hyperdrive-training\").submit(training_pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eac405e3a102007e39b7d474266b3f29064fe1851bc7a4c3889e65ca4778a0ba"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('azure-examples': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
