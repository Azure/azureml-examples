{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Multi-Step Training Pipeline with Hyperparameter Tuning\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook covers the process of setting up a basic multi-step training pipeline that utilizes hyperparameter tuning within an Azure Machine Learning workspace.\r\n",
    "\r\n",
    "Steps in this notebook include:\r\n",
    "\r\n",
    "- Training Step (Hyperdrive step used for hyperparameter tuning)\r\n",
    "- Evaluation Step\r\n",
    "- Registration step\r\n",
    "\r\n",
    "This notebook is meant to serve as a guide or template for users who wish to do hyperparameter tuning within more complex multi-step pipelines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load dotenv extension\r\n",
    "\r\n",
    "%load_ext dotenv\r\n",
    "%dotenv\r\n",
    "\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "sys.path.append(\"./\")\r\n",
    "\r\n",
    "from azureml.core import Environment, Workspace, Experiment, ScriptRunConfig\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.data.data_reference import DataReference\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter, TrainingOutput\r\n",
    "from azureml.pipeline.steps import PythonScriptStep, HyperDriveStep\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "\r\n",
    "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive import choice\r\n",
    "\r\n",
    "from src.common.attach_compute import get_compute\r\n",
    "from src.common.get_datastores import get_blob_datastore"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Workspace and Set Compute Target"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Interactive login \r\n",
    "interactive_auth = InteractiveLoginAuthentication(force=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a workspace object from the `config.json` file in the running directory that you can [download from your AzureML](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace) portal:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Restore AML workspace from config.json file (can be downloaded through the portal)\r\n",
    "ws = Workspace.from_config()\r\n",
    "print('Workspace name: ' + ws.name, \r\n",
    "      'Azure region: ' + ws.location, \r\n",
    "      'Subscription id: ' + ws.subscription_id, \r\n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a compute target for our training job to run on, using the script helper `get_compute` imported from `attach_compute.py` file and the information provided within `.env` file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set compute target\r\n",
    "compute_target = get_compute(\r\n",
    "    workspace=ws,\r\n",
    "    compute_name=os.getenv(\"AML_CLUSTER_NAME\"),\r\n",
    "    vm_size=os.getenv(\"AML_CLUSTER_CPU_SKU\"),\r\n",
    "    vm_priority=os.environ.get(\"AML_CLUSTER_PRIORITY\", 'lowpriority'), \r\n",
    "    min_nodes=int(os.environ.get(\"AML_CLUSTER_MIN_NODES\", 0)),\r\n",
    "    max_nodes=int(os.environ.get(\"AML_CLUSTER_MAX_NODES\", 4)),\r\n",
    "    scale_down=int(os.environ.get(\"AML_CLUSTER_SCALE_DOWN\", 600)),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Datastores \r\n",
    "\r\n",
    "Create storage containers where data will be accessed, using the script helper `get_blob_datastore` imported from `attach_compute.py` file and the information provided within `.env` file:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create root datastore\r\n",
    "root_datastore = get_blob_datastore(\r\n",
    "    ws, os.getenv(\"BLOB_DATASTORE_NAME\"), \r\n",
    "    os.getenv(\"STORAGE_NAME\"),\r\n",
    "    os.getenv(\"STORAGE_KEY\"), \r\n",
    "    os.getenv(\"STORAGE_CONTAINER\")\r\n",
    "    )\r\n",
    "\r\n",
    "# Create input and output data reference\r\n",
    "root_dir = DataReference(\r\n",
    "    datastore=root_datastore, \r\n",
    "    data_reference_name=\"data_reference\", \r\n",
    "    mode=\"mount\"\r\n",
    ")\r\n",
    "\r\n",
    "# Create pipeline data to hold model info in intermediate pipeline steps\r\n",
    "model_info_dir = PipelineData(\r\n",
    "        \"model_info_dir\",\r\n",
    "        datastore=root_datastore,\r\n",
    "        output_mode='mount',\r\n",
    "        output_overwrite=True,\r\n",
    "    )\r\n",
    "\r\n",
    "# Hyperdrive specific output (one file only)\r\n",
    "model_info_best = PipelineData(\r\n",
    "        \"model_info_best\",\r\n",
    "        datastore=root_datastore,\r\n",
    "        output_mode = 'mount',\r\n",
    "        training_output=TrainingOutput(\"Model\", model_file=\"outputs/model/model_info.json\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Pipeline Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Just an example how we can use parameters to provide different input folders and values\r\n",
    "model_name = PipelineParameter(name=\"model_name\", default_value=\"basic_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Custom Environment \r\n",
    "\r\n",
    "Create the Azure ML environment that encapsulates our training script's dependencies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "\r\n",
    "# Specify package dependencies \r\n",
    "batch_conda_deps = CondaDependencies.create(\r\n",
    "    pip_packages=[\r\n",
    "        \"azureml==0.2.7\",\r\n",
    "        \"azureml-core==1.15.0\",\r\n",
    "        \"click==7.0\",\r\n",
    "        \"numpy==1.18.5\",\r\n",
    "        \"pandas==1.1.3\",\r\n",
    "        \"Pillow==7.2.0\",\r\n",
    "        \"tqdm==4.61.0\",\r\n",
    "    ]\r\n",
    ")\r\n",
    "\r\n",
    "# Create and set environment\r\n",
    "batch_env = Environment(name=\"train-env\")\r\n",
    "batch_env.docker.enabled = True\r\n",
    "\r\n",
    "# Set dependencies that will be used within environment (batch_conda_deps is a set of dependencies)\r\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\r\n",
    "\r\n",
    "# Set python version that environment will utilize\r\n",
    "batch_conda_deps.set_python_version('3.8.6')\r\n",
    "\r\n",
    "# Set up pipeline run configuration and set environment \r\n",
    "run_config = RunConfiguration()\r\n",
    "run_config.environment = batch_env"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure and instantiate pipeline steps\r\n",
    "\r\n",
    "Create and configure Training pipeline steps:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "source_directory = \"./\"\r\n",
    "\r\n",
    "# Create configurations for training step\r\n",
    "train_step_config = ScriptRunConfig(\r\n",
    "    script=\"src/pipeline/train.py\",\r\n",
    "    compute_target=compute_target,\r\n",
    "    source_directory=source_directory,\r\n",
    "    environment=batch_env\r\n",
    ")\r\n",
    "\r\n",
    "# Initiate sampling method and provide search space\r\n",
    "# In this basic case, we've specified different values for the initial_lr parameter\r\n",
    "ps = GridParameterSampling(\r\n",
    "    {\r\n",
    "        \"--initial_lr\": choice(0.00003, 0.00001, 0.0001),\r\n",
    "    })\r\n",
    "\r\n",
    "# Configure hyperdrive config using train config and sampling method specified\r\n",
    "hd_config = HyperDriveConfig(\r\n",
    "    run_config=train_step_config, \r\n",
    "    hyperparameter_sampling=ps,\r\n",
    "    primary_metric_name='accuracy', \r\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "    max_total_runs=4,\r\n",
    "    max_concurrent_runs=4\r\n",
    ")\r\n",
    "\r\n",
    "# Create training step using HyperDriveStep class to run experiment with hyperparameter tuning\r\n",
    "train_step = HyperDriveStep(\r\n",
    "    name=\"train step\",\r\n",
    "    hyperdrive_config=hd_config,\r\n",
    "    estimator_entry_script_arguments=[\r\n",
    "        \"--root_dir\",\r\n",
    "        root_dir,\r\n",
    "        \"--model_info_dir\",\r\n",
    "        model_info_dir,\r\n",
    "        \"--model_info_best\",\r\n",
    "        model_info_best,\r\n",
    "    ],\r\n",
    "    inputs=[root_dir],\r\n",
    "    outputs=[model_info_dir, model_info_best],\r\n",
    "    allow_reuse=False,\r\n",
    ")\r\n",
    "\r\n",
    "# Initialize other steps\r\n",
    "eval_step = PythonScriptStep(\r\n",
    "    name=\"eval step\",\r\n",
    "    script_name=\"src/pipeline/evaluate.py\",\r\n",
    "    arguments=[\r\n",
    "        \"--root_dir\",\r\n",
    "        root_dir,\r\n",
    "        \"--model_info_dir\",\r\n",
    "        model_info_dir,\r\n",
    "        \"--model_info_best\",\r\n",
    "        model_info_best,\r\n",
    "    ],\r\n",
    "    inputs=[root_dir, model_info_dir, model_info_best],\r\n",
    "    outputs=[],\r\n",
    "    compute_target=compute_target,\r\n",
    "    source_directory=source_directory,\r\n",
    "    runconfig=run_config,\r\n",
    "    allow_reuse=False,\r\n",
    ")\r\n",
    "\r\n",
    "eval_step.run_after(train_step)\r\n",
    "\r\n",
    "register_step = PythonScriptStep(\r\n",
    "    name=\"register step\",\r\n",
    "    script_name=\"src/pipeline/register.py\",\r\n",
    "    arguments=[\r\n",
    "        \"--root_dir\",\r\n",
    "        root_dir,\r\n",
    "        \"--model_name\",\r\n",
    "        model_name,\r\n",
    "        \"--model_info_dir\",\r\n",
    "        model_info_dir,\r\n",
    "    ],\r\n",
    "    inputs=[root_dir, model_info_dir],\r\n",
    "    outputs=[],\r\n",
    "    compute_target=compute_target,\r\n",
    "    source_directory=source_directory,\r\n",
    "    runconfig=run_config,\r\n",
    "    allow_reuse=False,\r\n",
    ")\r\n",
    "\r\n",
    "register_step.run_after(eval_step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure and publish pipeline to AML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create pipeline using existing steps\r\n",
    "training_pipeline = Pipeline(workspace=ws, steps=[train_step, eval_step, register_step])\r\n",
    "\r\n",
    "# Check if the pipeline is consistent \r\n",
    "training_pipeline.validate()\r\n",
    "\r\n",
    "# Publish pipeline\r\n",
    "published_pipeline = training_pipeline.publish(\r\n",
    "    name = \"hyperdrive_training_pipeline\",\r\n",
    "    description = \"Hyperdrive training pipeline experiment\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submit and run pipeline in AML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Submit the pipeline\r\n",
    "pipeline_run = Experiment(ws, 'Reza-hyperdrive-training').submit(training_pipeline)\r\n",
    "pipeline_run.wait_for_completion()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "571ec201d69d342dc510598094637a83d1bee9d3b477250a7d6cd7126f15823d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}