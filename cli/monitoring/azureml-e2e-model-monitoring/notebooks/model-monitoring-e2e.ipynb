{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## AzureML Model Monitoring through Operationalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample notebook, you will observe the end-to-end lifecycle of the Machine Learning (ML) operationalization process. You will follow the following steps to train your ML model, deploy it to production, and monitor it to ensure its continuous performance:\n",
    "\n",
    "1) Setup environment \n",
    "2) Register data assets\n",
    "3) Train the model\n",
    "4) Deploy the model\n",
    "5) Simulate inference requests\n",
    "6) Monitor the model\n",
    "\n",
    "Let's begin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, connect to your project workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1704936184660
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Connect to the project workspace\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a compute cluster to use to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmlCompute({'type': 'amlcompute', 'created_on': None, 'provisioning_state': 'Succeeded', 'provisioning_errors': None, 'name': 'cpu-cluster', 'description': None, 'tags': None, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/5f341982-4f40-4ecf-9cee-93ab5e24693f/resourceGroups/rg-sandbox-azureml-01/providers/Microsoft.MachineLearningServices/workspaces/mlw246jkl01/computes/cpu-cluster', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/nimoore-246jkl/code/Users/nimoore/model-monitoring-demo/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fe5dc678bb0>, 'resource_id': None, 'location': 'westus3', 'size': 'STANDARD_F4S_V2', 'min_instances': 0, 'max_instances': 1, 'idle_time_before_scale_down': 360.0, 'identity': None, 'ssh_public_access_enabled': True, 'ssh_settings': None, 'network_settings': <azure.ai.ml.entities._compute.compute.NetworkSettings object at 0x7fe5dc678d00>, 'tier': 'dedicated', 'enable_node_public_ip': True, 'subnet': None})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cluster_basic = AmlCompute(\n",
    "    name=\"cpu-cluster\",\n",
    "    type=\"amlcompute\",\n",
    "    size=\"STANDARD_F2S_V2\",  # you can replace it with other supported VM SKUs\n",
    "    location=ml_client.workspaces.get(ml_client.workspace_name).location,\n",
    "    min_instances=0,\n",
    "    max_instances=1,\n",
    "    idle_time_before_scale_down=360,\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(cluster_basic).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register data assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use some sample data to train our model. We will randomly split the dataset into reference and production sets. We add a timestamp column to simulate \"production-like\" data, since production data typically comes with timestamps. The dataset we are using in this example notebook has several columns related to credit card borrowers and contains a column on whether or not they defaulted on their credit card debt. We will train a model to predict `DEFAULT_NEXT_MONTH`, which is whether or not a borrower will default on their debt next month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Read the default_of_credit_card_clients dataset into a pandas data frame\n",
    "data_path = \"https://azuremlexamples.blob.core.windows.net/datasets/credit_card/default_of_credit_card_clients.csv\"\n",
    "df = pd.read_csv(data_path, header=1, index_col=0).rename(\n",
    "    columns={\"default payment next month\": \"DEFAULT_NEXT_MONTH\"}\n",
    ")\n",
    "\n",
    "# Split the data into production_data_df and reference_data_df\n",
    "# Use the iloc method to select the first 80% and the last 20% of the rows\n",
    "reference_data_df = df.iloc[: int(0.8 * len(df))].copy()\n",
    "production_data_df = df.iloc[int(0.8 * len(df)) :].copy()\n",
    "\n",
    "# Add a timestamp column in ISO8601 format\n",
    "timestamp = datetime.datetime.now() - datetime.timedelta(days=45)\n",
    "reference_data_df[\"TIMESTAMP\"] = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "production_data_df[\"TIMESTAMP\"] = [\n",
    "    timestamp + datetime.timedelta(minutes=i * 10)\n",
    "    for i in range(len(production_data_df))\n",
    "]\n",
    "production_data_df[\"TIMESTAMP\"] = production_data_df[\"TIMESTAMP\"].apply(\n",
    "    lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def write_df(df, local_path, file_name):\n",
    "    # Create directory if it does not exist\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    # Write data\n",
    "    df.to_csv(f\"{local_path}/{file_name}\", index=False)\n",
    "\n",
    "\n",
    "# Write data to local directory\n",
    "reference_data_dir_local_path = \"../data/reference\"\n",
    "production_data_dir_local_path = \"../data/production\"\n",
    "\n",
    "write_df(reference_data_df, reference_data_dir_local_path, \"01.csv\"),\n",
    "write_df(production_data_df, production_data_dir_local_path, \"01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mltable\n",
    "from mltable import MLTableHeaders, MLTableFileEncoding\n",
    "\n",
    "from azureml.fsspec import AzureMachineLearningFileSystem\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "def upload_data_and_create_data_asset(\n",
    "    local_path, remote_path, datastore_uri, data_name, data_version\n",
    "):\n",
    "    # Write MLTable file\n",
    "    tbl = mltable.from_delimited_files(\n",
    "        paths=[{\"pattern\": f\"{datastore_uri}{remote_path}*.csv\"}],\n",
    "        delimiter=\",\",\n",
    "        header=\"all_files_same_headers\",\n",
    "        infer_column_types=True,\n",
    "        include_path_column=False,\n",
    "        encoding=\"utf8\",\n",
    "    )\n",
    "\n",
    "    tbl.save(local_path)\n",
    "\n",
    "    # Instantiate file system\n",
    "    fs = AzureMachineLearningFileSystem(datastore_uri)\n",
    "\n",
    "    # Upload data\n",
    "    fs.upload(\n",
    "        lpath=local_path,\n",
    "        rpath=remote_path,\n",
    "        recursive=False,\n",
    "        **{\"overwrite\": \"MERGE_WITH_OVERWRITE\"},\n",
    "    )\n",
    "\n",
    "    # Define the Data asset object\n",
    "    data = Data(\n",
    "        path=f\"{datastore_uri}{remote_path}\",\n",
    "        type=AssetTypes.MLTABLE,\n",
    "        name=data_name,\n",
    "        version=data_version,\n",
    "    )\n",
    "\n",
    "    # Create the data asset in the workspace\n",
    "    ml_client.data.create_or_update(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Datastore uri for data\n",
    "datastore_uri = \"azureml://subscriptions/{}/resourcegroups/{}/workspaces/{}/datastores/workspaceblobstore/paths/\".format(\n",
    "    ml_client.subscription_id, ml_client.resource_group_name, ml_client.workspace_name\n",
    ")\n",
    "\n",
    "# Define paths\n",
    "reference_data_dir_remote_path = \"data/credit-default/reference/\"\n",
    "production_data_dir_remote_path = \"data/credit-default/production/\"\n",
    "\n",
    "# Define data asset names\n",
    "reference_data_asset_name = \"credit-default-reference\"\n",
    "production_data_asset_name = \"credit-default-production\"\n",
    "\n",
    "# Write data to remote directory and create data asset\n",
    "reference_data = upload_data_and_create_data_asset(\n",
    "    reference_data_dir_local_path,\n",
    "    reference_data_dir_remote_path,\n",
    "    datastore_uri,\n",
    "    reference_data_asset_name,\n",
    "    \"1\",\n",
    ")\n",
    "production_data = upload_data_and_create_data_asset(\n",
    "    production_data_dir_local_path,\n",
    "    production_data_dir_remote_path,\n",
    "    datastore_uri,\n",
    "    production_data_asset_name,\n",
    "    \"1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1704938023431
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading code (0.01 MBs): 100%|██████████| 7445/7445 [00:00<00:00, 82192.33it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: goofy_nutmeg_1yysd7rnvf\n",
      "Web View: https://ml.azure.com/runs/goofy_nutmeg_1yysd7rnvf?wsid=/subscriptions/5f341982-4f40-4ecf-9cee-93ab5e24693f/resourcegroups/rg-sandbox-azureml-01/workspaces/mlw246jkl01\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-01-11 09:22:19Z] Submitting 1 runs, first five are: ff07cd7b:b6d9c913-4324-4c2b-be13-cc78221c75df\n",
      "[2024-01-11 09:23:16Z] Completing processing run id b6d9c913-4324-4c2b-be13-cc78221c75df.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: goofy_nutmeg_1yysd7rnvf\n",
      "Web View: https://ml.azure.com/runs/goofy_nutmeg_1yysd7rnvf?wsid=/subscriptions/5f341982-4f40-4ecf-9cee-93ab5e24693f/resourcegroups/rg-sandbox-azureml-01/workspaces/mlw246jkl01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import load_job\n",
    "\n",
    "# Define training pipeline directory\n",
    "training_pipeline_path = \"../configurations/training_pipeline.yaml\"\n",
    "\n",
    "# Trigger training\n",
    "training_pipeline_definition = load_job(source=training_pipeline_path)\n",
    "training_pipeline_job = ml_client.jobs.create_or_update(training_pipeline_definition)\n",
    "\n",
    "ml_client.jobs.stream(training_pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model with AzureML managed online endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1704944532767
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_online_endpoint\n",
    "\n",
    "# Define endpoint directory\n",
    "endpoint_path = \"../endpoints/endpoint.yaml\"\n",
    "\n",
    "# Trigger endpoint creation\n",
    "endpoint_definition = load_online_endpoint(source=endpoint_path)\n",
    "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1704944880850
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint \"credit-default\" with provisioning state \"Succeeded\" is retrieved\n"
     ]
    }
   ],
   "source": [
    "# Check endpoint status\n",
    "endpoint = ml_client.online_endpoints.get(name=endpoint_definition.name)\n",
    "print(\n",
    "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the deployment configuration, the Model Data Collector (MDC) is enabled, so that inference data is collected for model monitoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1704946433311
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint credit-default exists\n",
      "\u001b[32mUploading code (0.01 MBs): 100%|██████████| 7431/7431 [00:00<00:00, 79754.43it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import load_online_deployment\n",
    "\n",
    "# Define deployment directory\n",
    "deployment_path = \"../endpoints/deployment.yaml\"\n",
    "\n",
    "# Trigger deployment creation\n",
    "deployment_definition = load_online_deployment(source=deployment_path)\n",
    "deployment = ml_client.online_deployments.begin_create_or_update(deployment_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1704946433813
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment \"main\" with provisioning state \"Updating\" is retrieved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........."
     ]
    }
   ],
   "source": [
    "# Check deployment status\n",
    "deployment = ml_client.online_deployments.get(\n",
    "    name=deployment_definition.name, endpoint_name=endpoint_definition.name\n",
    ")\n",
    "print(\n",
    "    f'Deployment \"{deployment.name}\" with provisioning state \"{deployment.provisioning_state}\" is retrieved'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Simulate production inference data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Generate Sample Data\n",
    "\n",
    "We generate sample inference data by taking the distribution for each input feature and adding a small amount of random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1704951318267
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define numeric and categotical feature columns\n",
    "NUMERIC_FEATURES = [\n",
    "    \"LIMIT_BAL\",\n",
    "    \"AGE\",\n",
    "    \"BILL_AMT1\",\n",
    "    \"BILL_AMT2\",\n",
    "    \"BILL_AMT3\",\n",
    "    \"BILL_AMT4\",\n",
    "    \"BILL_AMT5\",\n",
    "    \"BILL_AMT6\",\n",
    "    \"PAY_AMT1\",\n",
    "    \"PAY_AMT2\",\n",
    "    \"PAY_AMT3\",\n",
    "    \"PAY_AMT4\",\n",
    "    \"PAY_AMT5\",\n",
    "    \"PAY_AMT6\",\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_sample_inference_data(df_production, number_of_records=20):\n",
    "    # Sample records\n",
    "    df_sample = df_production.sample(n=number_of_records, replace=True)\n",
    "\n",
    "    # Generate numeric features with random noise\n",
    "    df_numeric_generated = pd.DataFrame(\n",
    "        {\n",
    "            feature: np.random.normal(\n",
    "                0, df_production[feature].std(), number_of_records\n",
    "            ).astype(np.int64)\n",
    "            for feature in NUMERIC_FEATURES\n",
    "        }\n",
    "    ) + df_sample[NUMERIC_FEATURES].reset_index(drop=True)\n",
    "\n",
    "    # Take categorical columns\n",
    "    df_categorical = df_sample[CATEGORICAL_FEATURES].reset_index(drop=True)\n",
    "\n",
    "    # Combine numerical and categorical columns\n",
    "    df_combined = pd.concat([df_numeric_generated, df_categorical], axis=1)\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gather": {
     "logged": 1704951319814
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import mltable\n",
    "import pandas as pd\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Load production / inference data\n",
    "data_asset = ml_client.data.get(\"credit-default-production\", version=\"1\")\n",
    "tbl = mltable.load(data_asset.path)\n",
    "df_production = tbl.to_pandas_dataframe()\n",
    "\n",
    "# Generate sample data for inference\n",
    "number_of_records = 20\n",
    "df_generated = generate_sample_inference_data(df_production, number_of_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Call Online Managed Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the endpoint with the sample data. Since your deployment was created with the Model Data Collector (MDC) enabled, the inference inputs and outputs will be collected in your workspace blob storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1704950639350
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "request_file_name = \"request.json\"\n",
    "\n",
    "# Request sample data\n",
    "data = {\"data\": df_generated.to_dict(orient=\"records\")}\n",
    "\n",
    "# Write sample data\n",
    "with open(request_file_name, \"w\") as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Call online endpoint\n",
    "result = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_definition.name,\n",
    "    deployment_name=deployment_definition.name,\n",
    "    request_file=request_file_name,\n",
    ")\n",
    "\n",
    "# Delete sample data\n",
    "os.remove(request_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a basic model monitor. Please feel free to augment it to meet the needs of your scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "...."
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    AlertNotification,\n",
    "    MonitoringTarget,\n",
    "    MonitorDefinition,\n",
    "    MonitorSchedule,\n",
    "    RecurrencePattern,\n",
    "    RecurrenceTrigger,\n",
    "    ServerlessSparkCompute,\n",
    ")\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id=\"subscription_id\",\n",
    "    resource_group_name=\"resource_group_name\",\n",
    "    workspace_name=\"workspace_name\",\n",
    ")\n",
    "\n",
    "# create the compute\n",
    "spark_compute = ServerlessSparkCompute(\n",
    "    instance_type=\"standard_e4s_v3\", runtime_version=\"3.3\"\n",
    ")\n",
    "\n",
    "# specify your online endpoint deployment\n",
    "monitoring_target = MonitoringTarget(\n",
    "    ml_task=\"classification\", endpoint_deployment_id=\"azureml:credit-default:main\"\n",
    ")\n",
    "\n",
    "\n",
    "# create alert notification object\n",
    "alert_notification = AlertNotification(emails=[\"abc@example.com\", \"def@example.com\"])\n",
    "\n",
    "# create the monitor definition\n",
    "monitor_definition = MonitorDefinition(\n",
    "    compute=spark_compute,\n",
    "    monitoring_target=monitoring_target,\n",
    "    alert_notification=alert_notification,\n",
    ")\n",
    "\n",
    "# specify the schedule frequency\n",
    "recurrence_trigger = RecurrenceTrigger(\n",
    "    frequency=\"day\", interval=1, schedule=RecurrencePattern(hours=3, minutes=15)\n",
    ")\n",
    "\n",
    "# create the monitor\n",
    "model_monitor = MonitorSchedule(\n",
    "    name=\"credit_default_monitor_basic\",\n",
    "    trigger=recurrence_trigger,\n",
    "    create_monitor=monitor_definition,\n",
    ")\n",
    "\n",
    "poller = ml_client.schedules.begin_create_or_update(model_monitor)\n",
    "created_monitor = poller.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an advanced model monitoring configuration. Feel free to augment it to meet the needs of your scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class PredictionDriftMetricThreshold: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class PredictionDriftSignal: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class DataQualityMetricsNumerical: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class DataQualityMetricsCategorical: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class DataQualityMetricThreshold: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class DataQualitySignal: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AlertNotification: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class MonitorDefinition: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class MonitorSchedule: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaselineDataRange: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProductionData: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class TrailingInputData: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class MonitorInputData: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class FixedInputData: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "..."
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import Input, MLClient\n",
    "from azure.ai.ml.constants import (\n",
    "    MonitorDatasetContext,\n",
    ")\n",
    "from azure.ai.ml.entities import (\n",
    "    AlertNotification,\n",
    "    DataDriftSignal,\n",
    "    DataQualitySignal,\n",
    "    PredictionDriftSignal,\n",
    "    DataDriftMetricThreshold,\n",
    "    DataQualityMetricThreshold,\n",
    "    PredictionDriftMetricThreshold,\n",
    "    NumericalDriftMetrics,\n",
    "    CategoricalDriftMetrics,\n",
    "    DataQualityMetricsNumerical,\n",
    "    DataQualityMetricsCategorical,\n",
    "    MonitorFeatureFilter,\n",
    "    MonitoringTarget,\n",
    "    MonitorDefinition,\n",
    "    MonitorSchedule,\n",
    "    RecurrencePattern,\n",
    "    RecurrenceTrigger,\n",
    "    ServerlessSparkCompute,\n",
    "    ReferenceData,\n",
    ")\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id=\"subscription_id\",\n",
    "    resource_group_name=\"resource_group_name\",\n",
    "    workspace_name=\"workspace_name\",\n",
    ")\n",
    "\n",
    "# create your compute\n",
    "spark_compute = ServerlessSparkCompute(\n",
    "    instance_type=\"standard_e4s_v3\", runtime_version=\"3.3\"\n",
    ")\n",
    "\n",
    "# specify the online deployment (if you have one)\n",
    "monitoring_target = MonitoringTarget(\n",
    "    ml_task=\"classification\", endpoint_deployment_id=\"azureml:credit-default:main\"\n",
    ")\n",
    "\n",
    "# training data to be used as baseline dataset\n",
    "reference_data_training = ReferenceData(\n",
    "    input_data=Input(type=\"mltable\", path=\"azureml:credit-default-reference:1\"),\n",
    "    data_column_names={\n",
    "        \"target_column\":\"DEFAULT_NEXT_MONTH\"\n",
    "    },\n",
    "    data_context=MonitorDatasetContext.TRAINING,\n",
    ")\n",
    "\n",
    "# create an advanced data drift signal\n",
    "features = MonitorFeatureFilter(top_n_feature_importance=10)\n",
    "\n",
    "metric_thresholds = DataDriftMetricThreshold(\n",
    "    numerical=NumericalDriftMetrics(jensen_shannon_distance=0.01),\n",
    "    categorical=CategoricalDriftMetrics(pearsons_chi_squared_test=0.02),\n",
    ")\n",
    "\n",
    "advanced_data_drift = DataDriftSignal(\n",
    "    reference_data=reference_data_training,\n",
    "    features=features,\n",
    "    metric_thresholds=metric_thresholds,\n",
    ")\n",
    "\n",
    "# create an advanced prediction drift signal\n",
    "metric_thresholds = PredictionDriftMetricThreshold(\n",
    "    categorical=CategoricalDriftMetrics(jensen_shannon_distance=0.01)\n",
    ")\n",
    "\n",
    "advanced_prediction_drift = PredictionDriftSignal(\n",
    "    reference_data=reference_data_training, metric_thresholds=metric_thresholds\n",
    ")\n",
    "\n",
    "# create an advanced data quality signal\n",
    "features = [\"SEX\", \"EDUCATION\", \"AGE\"]\n",
    "\n",
    "metric_thresholds = DataQualityMetricThreshold(\n",
    "    numerical=DataQualityMetricsNumerical(null_value_rate=0.01),\n",
    "    categorical=DataQualityMetricsCategorical(out_of_bounds_rate=0.02),\n",
    ")\n",
    "\n",
    "advanced_data_quality = DataQualitySignal(\n",
    "    reference_data=reference_data_training,\n",
    "    features=features,\n",
    "    metric_thresholds=metric_thresholds,\n",
    "    alert_enabled=False,\n",
    ")\n",
    "\n",
    "# put all monitoring signals in a dictionary\n",
    "monitoring_signals = {\n",
    "    \"data_drift_advanced\": advanced_data_drift,\n",
    "    \"data_quality_advanced\": advanced_data_quality,\n",
    "}\n",
    "\n",
    "# create alert notification object\n",
    "alert_notification = AlertNotification(emails=[\"abc@example.com\", \"def@example.com\"])\n",
    "\n",
    "# create the monitor definition\n",
    "monitor_definition = MonitorDefinition(\n",
    "    compute=spark_compute,\n",
    "    monitoring_target=monitoring_target,\n",
    "    monitoring_signals=monitoring_signals,\n",
    "    alert_notification=alert_notification,\n",
    ")\n",
    "\n",
    "# specify the frequency on which to run your monitor\n",
    "recurrence_trigger = RecurrenceTrigger(\n",
    "    frequency=\"day\", interval=1, schedule=RecurrencePattern(hours=3, minutes=15)\n",
    ")\n",
    "\n",
    "# create your monitor\n",
    "model_monitor = MonitorSchedule(\n",
    "    name=\"credit_default_monitor_advanced\",\n",
    "    trigger=recurrence_trigger,\n",
    "    create_monitor=monitor_definition,\n",
    ")\n",
    "\n",
    "poller = ml_client.schedules.begin_create_or_update(model_monitor)\n",
    "created_monitor = poller.result()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
