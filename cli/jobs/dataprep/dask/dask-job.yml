$schema: https://azuremlsdk2.blob.core.windows.net/latest/commandJob.schema.json

description: This sample shows how to run a distributed DASK job on AzureML. 
  The 24GB NYC Taxi dataset is read in CSV format by a 4 node DASK cluster, 
  processed and then written as job output in parquet format. 

code: 
  local_path: src

# This is the command that will start up the dask cluster and run the script `batch.py` with the following parameters
# for an interactive session, just remove the --script. That will just start the cluster
# and mount the dataset (which will be found under './AZURE_ML_INPUT0')
command: >-
  python startDask.py
  --script batch.py 
  --nyc_taxi_dataset {inputs.nyc_taxi_dataset} 
  
environment: 
  conda_file: file:dask-conda.yaml
  docker: 
    image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04

inputs:
  nyc_taxi_dataset:
    data: 
      path: https://azuremlexamples.blob.core.windows.net/datasets/nyctaxi/
    mode: mount

compute:
  # use a sku with lots of disk space and memory
  target: azureml:gpu-cluster
  instance_count: 4

distribution:
  # The job below is currently launched with `type: pytorch` since that 
  # gives the full flexibility of assigning the work to the
  # no pytorch is actually used in this job
  type: pytorch
  


