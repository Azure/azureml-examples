This example doesn't run actual distributed training but shows the distributed training environment available to training scripts with the various environment variables set for PyTorch environments - RANK, LOCAL_RANK, NODE_RANK, MASTER_ADDR and MASTER_PORT. It also shows how to override distributed training settings defined in a component in a job. 


