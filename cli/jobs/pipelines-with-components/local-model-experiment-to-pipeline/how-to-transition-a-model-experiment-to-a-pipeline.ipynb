{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How to transition a model experiment to a pipeline?\n",
                "\n",
                "Machine learning is a work that needs collaboration of many different roles, among which data scientist and machine learning engineer are the two major roles who will work together to train and productionize models. This article aims maching learning engineer, enabling them to productionize models built by data scientists.\n",
                "If you are not sure which role you are, please check the table below.\n",
                "\n",
                "|Role |Responsiblities |Tools |\n",
                "| --- | --- | --- |\n",
                "|data scientist|model development, model debug, data understanding, model experimentation, continuous training |VSCode/PyCharm/Jupyter Notebook, Python, PyTorch/TensorFlow, ML Platform|\n",
                "|machine learning engineer|engineering best practices, scaling, production training, model management, model deployment, application integration, MLOps|ML platform, Python, Docker, Kubernetes, ML pipeline|\n",
                "\n",
                "When a model is developped and goes to production, work will be handed over from data scientist (hereinafter referred to as DS) to machine learning engineer (hereinafter referred to as MLE).\n",
                "\n",
                "Data Scientist should provide:\n",
                "* a training script, it could be a Jupyter Notebook file or a python script depending on editor used by DS. This script includes what piece of sample data used, how data is processd, how model is trained, how metrics are defined to evaluate the model, and metrics baseline on sample data.\n",
                "* the environment to run this script, it could be a conda environment yaml or a requirements.txt.\n",
                "\n",
                "After these inputs are handed over from DS, a MLE's first job is to make this script run successfullly on local (laptop, Virtual Machine, CodeSpace or Azure ML Notebook, etc.) and then on cloud.\n",
                "\n",
                "We will take NYC taxi fares predicting as an example. Please find all datasets and codes [here in github](gitlink).\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Get it work\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get it work on local \n",
                "\n",
                "In order to make a script run on local, MLE needs to first go through code, understand logic in code, and refine code if necessary, rebuild environment, then run it locally. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Refine code\n",
                "\n",
                "When refining code, a MLE should take into consideration security, compliance, cost, company internal engineering practices, etc.\n",
                "\n",
                "For example, to imporve productivity, MLE can delete or comment parts of code for data visulization and expolartion, which will save compute time when runing on production environment, for example, code cells to check statistics or to view data distribution through histogram.\n",
                "\n",
                "Besides, if a .ipynb file is provided, you also need to convert notebook to python file using command below because Azure ML accepts .py file as job input when moving to cloud.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NbConvertApp] Converting notebook inputs_from_data_scientist/notebook.ipynb to script\n",
                        "[NbConvertApp] Writing 10286 bytes to inputs_from_data_scientist/script.py\n"
                    ]
                }
            ],
            "source": [
                "!jupyter nbconvert --to script --output script inputs_from_data_scientist/notebook.ipynb "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Rebuild environement to run script\n",
                "\n",
                "To run DS's script on local, MLE needs to reproduce the same environment.\n",
                "\n",
                "These are some frequent approaches, for example:\n",
                "\n",
                "* build a docker image and run container\n",
                "* create conda environment from yaml file \n",
                "* pip install requirements.txt\n",
                "\n",
                "Building a docker image and running script in container is recommended, it's OS independent and thus the best way to simulate running script in remote. We will take this method as an example below.\n",
                "\n",
                "First, list all the dependencies imported in script, and get denpendencies versions according to full list of requirements provided by DS.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import os\n",
                "import pandas as pd\n",
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "from sklearn.model_selection import train_test_split\n",
                "import pickle\n",
                "import numpy as np\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "import matplotlib.pyplot as plt "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "\n",
                "Now you can obtain a relatively short list of requirements.txt. In this example, pathlib/os/pickle are removed, because they are included in python. And plt is not necessary, since we already remove histogramm plotting code.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pathlib2==2.3.6\n",
                "pandas==1.3.3\n",
                "sklearn==0.0\n",
                "numpy==1.18.5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "Now you can write your dockerfile in which you set base image as python with a proper version, then install requirements, copy sample data and script, set command.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
                "FROM python:3.8.5\n",
                "\n",
                "# python installs\n",
                "COPY env/local/requirements.txt .\n",
                "RUN pip install -r requirements.txt && rm requirements.txt\n",
                "\n",
                "COPY data/sample_data /usr/python/data/sample_data\n",
                "COPY /1_script_run_on_local/src/script.py /usr/python/1_script_run_on_local/src/\n",
                "WORKDIR /usr/python\n",
                "\n",
                "# set command\n",
                "CMD [\"bash\", \"-c\", \"cd 1_script_run_on_local/src && python script.py && exit\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can then run these commands to build image and run python script in container to test it. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sending build context to Docker daemon  2.196GB\n",
                        "Step 1/7 : FROM python:3.8.5\n",
                        " ---> 28a4c88cdbbf\n",
                        "Step 2/7 : COPY env/local/requirements.txt .\n",
                        " ---> 12214c898ef2\n",
                        "Step 3/7 : RUN pip install -r requirements.txt && rm requirements.txt\n",
                        " ---> Running in 8a205c2db9e1\n",
                        "Collecting pathlib2==2.3.6\n",
                        "  Downloading pathlib2-2.3.6-py2.py3-none-any.whl (17 kB)\n",
                        "Collecting pandas==1.3.3\n",
                        "  Downloading pandas-1.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
                        "Collecting sklearn==0.0\n",
                        "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
                        "Collecting numpy==1.18.5\n",
                        "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
                        "Collecting azureml-mlflow==1.39.0\n",
                        "  Downloading azureml_mlflow-1.39.0-py3-none-any.whl (46 kB)\n",
                        "Collecting argparse==1.4.0\n",
                        "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
                        "Collecting six\n",
                        "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
                        "Collecting pytz>=2017.3\n",
                        "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
                        "Collecting python-dateutil>=2.7.3\n",
                        "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
                        "Collecting scikit-learn\n",
                        "  Downloading scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
                        "Collecting azureml-core~=1.39.0\n",
                        "  Downloading azureml_core-1.39.0.post1-py3-none-any.whl (2.5 MB)\n",
                        "Collecting mlflow-skinny\n",
                        "  Downloading mlflow_skinny-1.25.1-py3-none-any.whl (3.4 MB)\n",
                        "Collecting jsonpickle\n",
                        "  Downloading jsonpickle-2.1.0-py2.py3-none-any.whl (38 kB)\n",
                        "Collecting scipy>=1.1.0\n",
                        "  Downloading scipy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
                        "Collecting joblib>=0.11\n",
                        "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
                        "Collecting threadpoolctl>=2.0.0\n",
                        "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
                        "Collecting argcomplete<2.0\n",
                        "  Downloading argcomplete-1.12.3-py2.py3-none-any.whl (38 kB)\n",
                        "Collecting requests[socks]<3.0.0,>=2.19.1\n",
                        "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
                        "Collecting knack~=0.9.0\n",
                        "  Downloading knack-0.9.0-py3-none-any.whl (59 kB)\n",
                        "Collecting msal<2.0.0,>=1.15.0\n",
                        "  Downloading msal-1.17.0-py2.py3-none-any.whl (79 kB)\n",
                        "Collecting pyopenssl<22.0.0\n",
                        "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
                        "Collecting azure-mgmt-keyvault<10.0.0,>=0.40.0\n",
                        "  Downloading azure_mgmt_keyvault-9.3.0-py2.py3-none-any.whl (412 kB)\n",
                        "Collecting msrest<1.0.0,>=0.5.1\n",
                        "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
                        "Collecting humanfriendly<11.0,>=4.7\n",
                        "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
                        "Collecting adal<=1.2.7,>=1.2.0\n",
                        "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
                        "Collecting pathspec<1.0.0\n",
                        "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
                        "Collecting azure-core<1.22\n",
                        "  Downloading azure_core-1.21.1-py2.py3-none-any.whl (178 kB)\n",
                        "Collecting azure-mgmt-authorization<1.0.0,>=0.40.0\n",
                        "  Downloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\n",
                        "Collecting msal-extensions<0.4,>=0.3.0\n",
                        "  Downloading msal_extensions-0.3.1-py2.py3-none-any.whl (18 kB)\n",
                        "Collecting ndg-httpsclient<=0.5.1\n",
                        "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
                        "Collecting PyJWT<3.0.0\n",
                        "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
                        "Collecting urllib3<=1.26.7,>=1.23\n",
                        "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
                        "Collecting jmespath<1.0.0\n",
                        "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
                        "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<37.0.0\n",
                        "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
                        "Collecting paramiko<3.0.0,>=2.0.8\n",
                        "  Downloading paramiko-2.10.4-py2.py3-none-any.whl (212 kB)\n",
                        "Collecting contextlib2<22.0.0\n",
                        "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
                        "Collecting azure-common<2.0.0,>=1.1.12\n",
                        "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
                        "Collecting azure-graphrbac<1.0.0,>=0.40.0\n",
                        "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
                        "Collecting SecretStorage<4.0.0\n",
                        "  Downloading SecretStorage-3.3.2-py3-none-any.whl (15 kB)\n",
                        "Collecting backports.tempfile\n",
                        "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
                        "Collecting azure-mgmt-storage<20.0.0,>=16.0.0\n",
                        "  Downloading azure_mgmt_storage-19.1.0-py3-none-any.whl (1.8 MB)\n",
                        "Collecting docker<6.0.0\n",
                        "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
                        "Collecting pkginfo\n",
                        "  Downloading pkginfo-1.8.2-py2.py3-none-any.whl (26 kB)\n",
                        "Collecting msrestazure<=0.6.4,>=0.4.33\n",
                        "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
                        "Collecting azure-mgmt-containerregistry<9.0.0,>=8.2.0\n",
                        "  Downloading azure_mgmt_containerregistry-8.2.0-py2.py3-none-any.whl (928 kB)\n",
                        "Collecting azure-mgmt-resource<21.0.0,>=15.0.0\n",
                        "  Downloading azure_mgmt_resource-20.1.0-py3-none-any.whl (2.3 MB)\n",
                        "Collecting packaging<22.0,>=20.0\n",
                        "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
                        "Collecting databricks-cli>=0.8.7\n",
                        "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
                        "Collecting click>=7.0\n",
                        "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
                        "Collecting importlib-metadata!=4.7.0,>=3.7.0\n",
                        "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
                        "Collecting cloudpickle\n",
                        "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
                        "Collecting gitpython>=2.1.0\n",
                        "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
                        "Collecting entrypoints\n",
                        "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
                        "Collecting pyyaml>=5.1\n",
                        "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
                        "Collecting protobuf>=3.7.0\n",
                        "  Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
                        "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
                        "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
                        "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
                        "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
                        "Collecting certifi>=2017.4.17\n",
                        "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
                        "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
                        "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
                        "Collecting tabulate\n",
                        "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
                        "Collecting pygments\n",
                        "  Downloading Pygments-2.12.0-py3-none-any.whl (1.1 MB)\n",
                        "Collecting azure-mgmt-core<2.0.0,>=1.2.0\n",
                        "  Downloading azure_mgmt_core-1.3.0-py2.py3-none-any.whl (25 kB)\n",
                        "Collecting requests-oauthlib>=0.5.0\n",
                        "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
                        "Collecting isodate>=0.6.0\n",
                        "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
                        "Collecting portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\"\n",
                        "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
                        "Collecting pyasn1>=0.1.1\n",
                        "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
                        "Collecting cffi>=1.12\n",
                        "  Downloading cffi-1.15.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (446 kB)\n",
                        "Collecting pynacl>=1.0.1\n",
                        "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
                        "Collecting bcrypt>=3.1.3\n",
                        "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (61 kB)\n",
                        "Collecting jeepney>=0.6\n",
                        "  Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
                        "Collecting backports.weakref\n",
                        "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
                        "Collecting websocket-client>=0.32.0\n",
                        "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
                        "Collecting pyparsing!=3.0.5,>=2.0.2\n",
                        "  Downloading pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
                        "Collecting oauthlib>=3.1.0\n",
                        "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
                        "Collecting zipp>=0.5\n",
                        "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
                        "Collecting gitdb<5,>=4.0.1\n",
                        "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
                        "Collecting pycparser\n",
                        "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
                        "Collecting smmap<6,>=3.0.1\n",
                        "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
                        "Building wheels for collected packages: sklearn, databricks-cli\n",
                        "  Building wheel for sklearn (setup.py): started\n",
                        "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
                        "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=f0b76dfafef4c00164d4bb85b63830e1110f2e969f94361cb002293bd570aaee\n",
                        "  Stored in directory: /root/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
                        "  Building wheel for databricks-cli (setup.py): started\n",
                        "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
                        "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112628 sha256=8561c03799d674ffc6c5ff2f3b1dbef77942195d0df74574f840f6e574ced3a3\n",
                        "  Stored in directory: /root/.cache/pip/wheels/76/3a/2f/8a3d92bc72a413217b5c8649e2044d7161cacbe06e014361a3\n",
                        "Successfully built sklearn databricks-cli\n",
                        "Installing collected packages: six, pathlib2, numpy, pytz, python-dateutil, pandas, scipy, joblib, threadpoolctl, scikit-learn, sklearn, argcomplete, urllib3, charset-normalizer, idna, certifi, PySocks, requests, jsonpickle, jmespath, tabulate, pyyaml, pygments, knack, PyJWT, pycparser, cffi, cryptography, msal, pyopenssl, azure-common, azure-core, azure-mgmt-core, oauthlib, requests-oauthlib, isodate, msrest, azure-mgmt-keyvault, humanfriendly, adal, pathspec, msrestazure, azure-mgmt-authorization, portalocker, msal-extensions, pyasn1, ndg-httpsclient, pynacl, bcrypt, paramiko, contextlib2, azure-graphrbac, jeepney, SecretStorage, backports.weakref, backports.tempfile, azure-mgmt-storage, websocket-client, docker, pkginfo, azure-mgmt-containerregistry, azure-mgmt-resource, pyparsing, packaging, azureml-core, click, databricks-cli, zipp, importlib-metadata, cloudpickle, smmap, gitdb, gitpython, entrypoints, protobuf, mlflow-skinny, azureml-mlflow, argparse\n",
                        "Successfully installed PyJWT-2.3.0 PySocks-1.7.1 SecretStorage-3.3.2 adal-1.2.7 argcomplete-1.12.3 argparse-1.4.0 azure-common-1.1.28 azure-core-1.21.1 azure-graphrbac-0.61.1 azure-mgmt-authorization-0.61.0 azure-mgmt-containerregistry-8.2.0 azure-mgmt-core-1.3.0 azure-mgmt-keyvault-9.3.0 azure-mgmt-resource-20.1.0 azure-mgmt-storage-19.1.0 azureml-core-1.39.0.post1 azureml-mlflow-1.39.0 backports.tempfile-1.0 backports.weakref-1.0.post1 bcrypt-3.2.0 certifi-2021.10.8 cffi-1.15.0 charset-normalizer-2.0.12 click-8.1.3 cloudpickle-2.0.0 contextlib2-21.6.0 cryptography-36.0.2 databricks-cli-0.16.6 docker-5.0.3 entrypoints-0.4 gitdb-4.0.9 gitpython-3.1.27 humanfriendly-10.0 idna-3.3 importlib-metadata-4.11.3 isodate-0.6.1 jeepney-0.8.0 jmespath-0.10.0 joblib-1.1.0 jsonpickle-2.1.0 knack-0.9.0 mlflow-skinny-1.25.1 msal-1.17.0 msal-extensions-0.3.1 msrest-0.6.21 msrestazure-0.6.4 ndg-httpsclient-0.5.1 numpy-1.18.5 oauthlib-3.2.0 packaging-21.3 pandas-1.3.3 paramiko-2.10.4 pathlib2-2.3.6 pathspec-0.9.0 pkginfo-1.8.2 portalocker-2.4.0 protobuf-3.20.1 pyasn1-0.4.8 pycparser-2.21 pygments-2.12.0 pynacl-1.5.0 pyopenssl-21.0.0 pyparsing-3.0.8 python-dateutil-2.8.2 pytz-2022.1 pyyaml-6.0 requests-2.27.1 requests-oauthlib-1.3.1 scikit-learn-1.0.2 scipy-1.8.0 six-1.16.0 sklearn-0.0 smmap-5.0.0 tabulate-0.8.9 threadpoolctl-3.1.0 urllib3-1.26.7 websocket-client-1.3.2 zipp-3.8.0\n",
                        "\u001b[91mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
                        "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
                        "\u001b[0mRemoving intermediate container 8a205c2db9e1\n",
                        " ---> 2fc988bd5ccc\n",
                        "Step 4/7 : COPY data/sample_data /usr/python/data/sample_data\n",
                        " ---> de59fb8945bc\n",
                        "Step 5/7 : COPY /1_script_run_on_local/src/script.py /usr/python/1_script_run_on_local/src/\n",
                        " ---> c15930ba964a\n",
                        "Step 6/7 : WORKDIR /usr/python\n",
                        " ---> Running in 34133b6a7ed4\n",
                        "Removing intermediate container 34133b6a7ed4\n",
                        " ---> 34c3727633c0\n",
                        "Step 7/7 : CMD [\"bash\", \"-c\", \"cd 1_script_run_on_local/src && python script.py && exit\"]\n",
                        " ---> Running in 5a45c5682111\n",
                        "Removing intermediate container 5a45c5682111\n",
                        " ---> 872f7ad116f5\n",
                        "Successfully built 872f7ad116f5\n",
                        "Successfully tagged nyc_taxi_image:latest\n"
                    ]
                }
            ],
            "source": [
                "!docker build -t nyc_taxi_image -f env/local/Dockerfile ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "raw data files: \n",
                        "['yellowTaxiData.csv', 'greenTaxiData.csv']\n",
                        "(5000, 21)\n",
                        "(5000, 19)\n",
                        "['cost', 'distance', 'dropoff_datetime', 'dropoff_latitude', 'dropoff_longitude', 'passengers', 'pickup_datetime', 'pickup_latitude', 'pickup_longitude', 'store_forward', 'vendor']\n",
                        "green_columns:  {'vendorID': 'vendor', 'lpepPickupDatetime': 'pickup_datetime', 'lpepDropoffDatetime': 'dropoff_datetime', 'storeAndFwdFlag': 'store_forward', 'pickupLongitude': 'pickup_longitude', 'pickupLatitude': 'pickup_latitude', 'dropoffLongitude': 'dropoff_longitude', 'dropoffLatitude': 'dropoff_latitude', 'passengerCount': 'passengers', 'fareAmount': 'cost', 'tripDistance': 'distance'}\n",
                        "yellow_columns:  {'vendorID': 'vendor', 'tpepPickupDateTime': 'pickup_datetime', 'tpepDropoffDateTime': 'dropoff_datetime', 'storeAndFwdFlag': 'store_forward', 'startLon': 'pickup_longitude', 'startLat': 'pickup_latitude', 'endLon': 'dropoff_longitude', 'endLat': 'dropoff_latitude', 'passengerCount': 'passengers', 'fareAmount': 'cost', 'tripDistance': 'distance'}\n",
                        "data size before removing columns:  (5000, 21)\n",
                        "data size after removing columns:  (5000, 11)\n",
                        "data size before removing columns:  (5000, 19)\n",
                        "data size after removing columns:  (5000, 11)\n",
                        "cost                 0\n",
                        "distance             0\n",
                        "dropoff_datetime     0\n",
                        "dropoff_latitude     0\n",
                        "dropoff_longitude    0\n",
                        "passengers           0\n",
                        "pickup_datetime      0\n",
                        "pickup_latitude      0\n",
                        "pickup_longitude     0\n",
                        "store_forward        0\n",
                        "vendor               0\n",
                        "dtype: int64\n",
                        "cost                 0\n",
                        "distance             0\n",
                        "dropoff_datetime     0\n",
                        "dropoff_latitude     0\n",
                        "dropoff_longitude    0\n",
                        "passengers           0\n",
                        "pickup_datetime      0\n",
                        "pickup_latitude      0\n",
                        "pickup_longitude     0\n",
                        "store_forward        0\n",
                        "vendor               0\n",
                        "dtype: int64\n",
                        "combined data size: (10000, 11)\n",
                        "data size after removing cost outliers: 9948\n",
                        "data size after removing distance outliers: 9858\n",
                        "data size after filtering locations: (9743, 11)\n",
                        "data size after splitting datetime: (9743, 21)\n",
                        "(6820, 20)\n",
                        "Index(['distance', 'dropoff_latitude', 'dropoff_longitude', 'passengers',\n",
                        "       'pickup_latitude', 'pickup_longitude', 'store_forward', 'vendor',\n",
                        "       'pickup_weekday', 'pickup_month', 'pickup_monthday', 'pickup_hour',\n",
                        "       'pickup_minute', 'pickup_second', 'dropoff_weekday', 'dropoff_month',\n",
                        "       'dropoff_monthday', 'dropoff_hour', 'dropoff_minute', 'dropoff_second'],\n",
                        "      dtype='object')\n",
                        "training set score: 0.9402301722232183\n",
                        "Scored with the following model:\n",
                        "GradientBoostingRegressor()\n",
                        "Mean squared error: 6.02\n",
                        "Coefficient of determination: 0.93\n"
                    ]
                }
            ],
            "source": [
                " !docker run -it nyc_taxi_image:latest"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get it work on cloud\n",
                "\n",
                "After making sure your code can work on local, you can then move to cloud by submitting an Azure ML job. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Prerequisites\n",
                "\n",
                "To submit a job to Azure ML, you should install cli and set up environment on your local machine.\n",
                "\n",
                "Install latest version of azure cli, please refer to [How to install the Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli).\n",
                "\n",
                "Install latest version of azure ml cli and then set up default subscription, resource group and workspace (commands shown below). For more informations, please refer to [Install and set up the Maching learning CLI ](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli?tabs=public).\n",
                "\n",
                "Here is a cheatsheet for environment set-up.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!az login --use-device-code\n",
                "!az account set -s \"sub_id\"\n",
                "!az configure --defaults group=rg_name workspace=ws_name location=location"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Define inputs and outputs\n",
                "\n",
                "In preperation for moving to cloud, you need to define script interfaces (inputs and outputs), because you will use AZure ML datastore instead of local disk as data source.\n",
                "\n",
                "In this example, I define raw_data as input, model_output as output.\n",
                "\n",
                "Code modifications are:\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1. Import argparse package, add these two arguments.\n",
                "\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import argparse\n",
                "\n",
                "parser = argparse.ArgumentParser()\n",
                "parser.add_argument(\"--raw_data\", type=str, help=\"Path to raw data\")\n",
                "parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
                "\n",
                "\n",
                "args = parser.parse_args()\n",
                "\n",
                "lines = [\n",
                "    f\"Raw data path: {args.raw_data}\",\n",
                "    f\"model output path: {args.model_output}\",\n",
                "\n",
                "]\n",
                "\n",
                "for line in lines:\n",
                "    print(line)  "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "2. Replace raw_data with args.raw_data\n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read raw data from csv to dataframe\n",
                "# raw_data = './../data/sample_data/'\n",
                "print(\"raw data files: \")\n",
                "arr = os.listdir(args.raw_data)\n",
                "print(arr)\n",
                "\n",
                "green_data = pd.read_csv((Path(args.raw_data) / 'greenTaxiData.csv'))\n",
                "yellow_data = pd.read_csv((Path(args.raw_data) / 'yellowTaxiData.csv'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "3. Replace model_output with args.model_output\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Output the model \n",
                "# model_output = './model/'\n",
                "if not os.path.exists(args.model_output):\n",
                "    os.mkdir(args.model_output)\n",
                "pickle.dump(model, open((Path(args.model_output) / \"model.sav\"), \"wb\")) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "4. Add metrics and parameters logging code. Azure ML leverages MLflow to do experiment tracking. You need to import mlflow, then use mlflow.log_param() and mlflow.log_metric() instead of standard ouput print(). See more in [this article](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow-cli-runs?tabs=mlflow)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare predictions to actuals (testy)\n",
                "# The mean squared error\n",
                "# print(\"Scored with the following model:\\n{}\".format(model))\n",
                "# print(\"Mean squared error: %.2f\" % mean_squared_error(testy, predictions))\n",
                "# The coefficient of determination: 1 is perfect prediction\n",
                "# print(\"Coefficient of determination: %.2f\" % r2_score(testy, predictions))\n",
                "\n",
                "# Log params and metrics to AML\n",
                "\n",
                "mlflow.log_param(\"learning_rate\", learning_rate)\n",
                "mlflow.log_param(\"n_estimators\", n_estimators)\n",
                "\n",
                "mlflow.log_metric(\"mean_squared_error\", mean_squared_error(testy, predictions))\n",
                "mlflow.log_metric(\"r2_score\", r2_score(testy, predictions))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Create an Azure ML environment\n",
                "\n",
                "In the first place, modify dockerfile by deleting commands for copying data and script."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the second place, write your environment yaml file following [this schema instruction](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-environment). Remember to add argparse and azureml-mlflow in your requirements.txt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "azureml-mlflow==1.39.0\n",
                "argparse==1.4.0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the end, run this command to register environment on AML. Learn more about Azure ML environments management commands here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36mCommand group 'ml environment' is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus\u001b[0m\n",
                        "\u001b[32mUploading docker (0.0 MBs): 100%|███████████| 299/299 [00:00<00:00, 4402.55it/s]\u001b[0m\n",
                        "\u001b[39m\n",
                        "\n",
                        "{\n",
                        "  \"build\": {\n",
                        "    \"dockerfile_path\": \"Dockerfile\",\n",
                        "    \"path\": \"https://pmdev9225598307.blob.core.windows.net/azureml-blobstore-663bf81f-1924-4d17-a62c-3bc4a3984cab/LocalUpload/1109e35eb4795f412b2e811640506238/docker/\"\n",
                        "  },\n",
                        "  \"creation_context\": {\n",
                        "    \"created_at\": \"2022-04-29T04:46:03.163592+00:00\",\n",
                        "    \"created_by\": \"Yijun Zhang\",\n",
                        "    \"created_by_type\": \"User\",\n",
                        "    \"last_modified_at\": \"2022-04-29T04:46:03.163592+00:00\",\n",
                        "    \"last_modified_by\": \"Yijun Zhang\",\n",
                        "    \"last_modified_by_type\": \"User\"\n",
                        "  },\n",
                        "  \"id\": \"azureml:/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev/environments/nyc_taxi_image/versions/7\",\n",
                        "  \"name\": \"nyc_taxi_image\",\n",
                        "  \"os_type\": \"linux\",\n",
                        "  \"resourceGroup\": \"pipeline-pm\",\n",
                        "  \"tags\": {},\n",
                        "  \"version\": \"7\"\n",
                        "}\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!az ml environment create --file env/cloud/env.yml"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Now you can run the command below to list all environments created in workspace.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36mCommand group 'ml environment' is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus\u001b[0m\n",
                        "[\n",
                        "  {\n",
                        "    \"latest version\": \"7\",\n",
                        "    \"name\": \"nyc_taxi_image\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"test\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"0b32258cd1fc290ed0176979f5481357\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"6ca0e5ed1b7262c8bb953806eb9af5ee\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"r-mpg-environment\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"r-environment-2\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"r-environment-1\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"r-environment\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"2\",\n",
                        "    \"name\": \"pytorch_tabnet_env\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet_env_test5\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet_env_test4\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet_env_test3\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet_env_test2\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet_env_test1\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet_env_test\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet03\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet02\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pytorch_tabnet01\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"2\",\n",
                        "    \"name\": \"pytorch_tabnet\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"aec96af808d5bbc691e00deb997693cc\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"f65d393e7608d84bc70692a5f44e8f76\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"9f170aea3e06b69088f3b21da723cdba\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"f753647a2d06f3226ecdc0706d6e42dd\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"3\",\n",
                        "    \"name\": \"68ac741a826f46b7823f3fe755d5d33d\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"23945ecf67d9ed8908a26ba3d0e35ed5\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"3a2a275e7e26c81b1e2bee9f638e366f\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"b4185a67500137fd3834db6e68b2ff0c\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"dcc382fa9a0d5f45a53758c34db7f6ee\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"435079702c457bf1d54517f31d92f672\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"010485d56f904fcd4c64c493aadfef7a\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"2954242215fb17bf9c1a9e3a562fbf18\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"d3fc4885e52535f82f6294d34c0143e4\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"2\",\n",
                        "    \"name\": \"harper-sklearn\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"6\",\n",
                        "    \"name\": \"hz-aml176\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pte-191-dummy\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"tabnet-cpu-v3\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"tabnet-cpu-v2\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"3\",\n",
                        "    \"name\": \"tabnet_cpu__env\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"pipeline-base-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"27\",\n",
                        "    \"name\": \"AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"12\",\n",
                        "    \"name\": \"AzureML-tensorflow-2.6-ubuntu20.04-py38-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"35\",\n",
                        "    \"name\": \"AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"12\",\n",
                        "    \"name\": \"AzureML-tensorflow-2.7-ubuntu20.04-py38-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"11\",\n",
                        "    \"name\": \"AzureML-tensorflow-2.5-ubuntu20.04-py38-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"1\",\n",
                        "    \"name\": \"AzureML-responsibleai-0.18-ubuntu20.04-py38-cpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"16\",\n",
                        "    \"name\": \"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"36\",\n",
                        "    \"name\": \"AzureML-xgboost-0.9-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"3\",\n",
                        "    \"name\": \"AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11.0.3-gpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"36\",\n",
                        "    \"name\": \"AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"3\",\n",
                        "    \"name\": \"AzureML-pytorch-1.9-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"3\",\n",
                        "    \"name\": \"AzureML-minimal-ubuntu18.04-py37-cuda11.0.3-gpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"37\",\n",
                        "    \"name\": \"AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11.0.3-gpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"3\",\n",
                        "    \"name\": \"AzureML-pytorch-1.10-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"32\",\n",
                        "    \"name\": \"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"30\",\n",
                        "    \"name\": \"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"36\",\n",
                        "    \"name\": \"AzureML-tensorflow-1.15-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"3\",\n",
                        "    \"name\": \"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"36\",\n",
                        "    \"name\": \"AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"36\",\n",
                        "    \"name\": \"AzureML-minimal-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"36\",\n",
                        "    \"name\": \"AzureML-mlflow-ubuntu18.04-py37-cpu-inference\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"26\",\n",
                        "    \"name\": \"AzureML-pytorch-1.8-ubuntu18.04-py37-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"19\",\n",
                        "    \"name\": \"AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"33\",\n",
                        "    \"name\": \"AzureML-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"23\",\n",
                        "    \"name\": \"AzureML-Triton\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"40\",\n",
                        "    \"name\": \"AzureML-PyTorch-1.3-CPU\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"32\",\n",
                        "    \"name\": \"AzureML-VowpalWabbit-8.8.0\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"latest version\": \"12\",\n",
                        "    \"name\": \"AzureML-Designer-Score\"\n",
                        "  }\n",
                        "]\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!az ml environment list"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "Apart from that you can login Azure ML portal to check whether environment is registered correctly.\n",
                "\n",
                "![env](./images/env.png)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Prepare Azure ML job yaml file and submit a job\n",
                "\n",
                "What you should do is to wrap your python script into a standalone job through a yaml file. To begin with, you need to follow [this article](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-cli) to write your job yaml file, in which you define job name, description, environment used, code path and command to submit job, and interfaces, etc.\n",
                "\n",
                "Here is an example.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\n",
                "code: ./src\n",
                "command: >-\n",
                "  python script.py \n",
                "  --raw_data ${{inputs.raw_data}}\n",
                "  --model_output ${{outputs.model_output}}\n",
                "inputs:\n",
                "  raw_data: \n",
                "    type: uri_folder\n",
                "    path: ../sample_data \n",
                "outputs:\n",
                "  model_output: \n",
                "    type: uri_folder\n",
                "environment: azureml:nyc_taxi_image@latest\n",
                "compute: azureml:cpu-cluster\n",
                "display_name: nyc_taxi_regression\n",
                "experiment_name: nyc_taxi_regression\n",
                "description: Train a GBDT regression model on the NYC taxi dataset."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "The input can be a local path, Azure ML will upload your sample data to default datastore.\n",
                "\n",
                "After job yaml file prepared, you can run this command on your local environment to submit a job to Azure ML.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36mCommand group 'ml job' is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus\u001b[0m\n",
                        "\u001b[32mUploading src (0.01 MBs): 100%|█████████| 8821/8821 [00:00<00:00, 284877.96it/s]\u001b[0m\n",
                        "\u001b[39m\n",
                        "\n",
                        "{\n",
                        "  \"code\": \"/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev/codes/656aa7a0-5c23-4db7-ae39-2bc01e5d4c5c/versions/1\",\n",
                        "  \"command\": \"python script.py  --raw_data ${{inputs.raw_data}} --model_output ${{outputs.model_output}}\",\n",
                        "  \"compute\": \"azureml:cpu-cluster\",\n",
                        "  \"creation_context\": {\n",
                        "    \"created_at\": \"2022-04-29T04:59:02.197122+00:00\",\n",
                        "    \"created_by\": \"Yijun Zhang\",\n",
                        "    \"created_by_type\": \"User\"\n",
                        "  },\n",
                        "  \"description\": \"Train a GBDT regression model on the NYC taxi dataset.\",\n",
                        "  \"display_name\": \"nyc_taxi_regression\",\n",
                        "  \"environment\": \"azureml:nyc_taxi_image:7\",\n",
                        "  \"environment_variables\": {},\n",
                        "  \"experiment_name\": \"nyc_taxi_regression\",\n",
                        "  \"id\": \"azureml:/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev/jobs/gifted_shark_8ff5bz24rh\",\n",
                        "  \"inputs\": {\n",
                        "    \"raw_data\": {\n",
                        "      \"mode\": \"ro_mount\",\n",
                        "      \"path\": \"azureml:azureml://datastores/workspaceblobstore/paths/LocalUpload/1c2d0b4908fe99afe7e5d4d1e5af23e9/sample_data/\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"name\": \"gifted_shark_8ff5bz24rh\",\n",
                        "  \"outputs\": {\n",
                        "    \"default\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"path\": \"azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.gifted_shark_8ff5bz24rh\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"model_output\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_file\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"parameters\": {},\n",
                        "  \"properties\": {\n",
                        "    \"ContentSnapshotId\": \"2aa6c4a5-5d5b-4e0f-9110-0d0b02c48606\",\n",
                        "    \"_azureml.ComputeTargetType\": \"amlctrain\"\n",
                        "  },\n",
                        "  \"resourceGroup\": \"pipeline-pm\",\n",
                        "  \"resources\": {\n",
                        "    \"instance_count\": 1,\n",
                        "    \"properties\": {}\n",
                        "  },\n",
                        "  \"services\": {\n",
                        "    \"Studio\": {\n",
                        "      \"endpoint\": \"https://ml.azure.com/runs/gifted_shark_8ff5bz24rh?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/pipeline-pm/workspaces/pm-dev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
                        "      \"job_service_type\": \"Studio\"\n",
                        "    },\n",
                        "    \"Tracking\": {\n",
                        "      \"endpoint\": \"azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev?\",\n",
                        "      \"job_service_type\": \"Tracking\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"status\": \"Starting\",\n",
                        "  \"tags\": {},\n",
                        "  \"type\": \"command\"\n",
                        "}\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!az ml job create --f 2_standalone_job_run_on_cloud/2a_job_sample_data.yml --web"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "With this --web option, you are automatically directed to Azure ML job detail page where you can view job informations like run status, duration, logs, etc.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Get it reproducible\n",
                "\n",
                "In the first stage, you get your script work in remote with a sample data. To be reproducible, it is recommended to resubmit an Azure ML job with full size big data.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Prepare full data\n",
                "\n",
                "Azure ML datastores record connection information to your Azure storage where your full production data is located. For more details please refer to [Secure data access in Azure Machine Learning](https://review.docs.microsoft.com/en-us/azure/machine-learning/concept-data?branch=release-preview-aml-cli-v2-refresh#connect-to-storage-with-datastores).\n",
                "\n",
                "\n",
                "Suppose your data is now in cloud. Here we will take Azure File Share as an example. Your full size data are stored in \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "File shares/my_file_share_name/nyc_taxi/full_data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Prepare Azure ML job yaml file and submit a job\n",
                "\n",
                "What you need to do is to take job.yml of last step, modify input from local path to remote datastore path. \n",
                "\n",
                "From:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs:\n",
                "  raw_data: \n",
                "    type: uri_folder\n",
                "    path: ./sample_data "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "To:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs:\n",
                "  raw_data: \n",
                "    type: uri_folder\n",
                "    path: azureml://datastores/workspaceblobstore/paths/nyc_taxi_data/full_data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "In this yaml file, workspaceblobstore is datastore name. For more information about uri_folder path format see [here is a doc about uri format]().\n",
                "\n",
                "Attention, as you use full size data to reproduce your job, you might need to swich to a compute cluster with optimized memory. \n",
                "\n",
                "Then rerun this command to submit a job:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36mCommand group 'ml job' is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus\u001b[0m\n",
                        "{\n",
                        "  \"code\": \"/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev/codes/656aa7a0-5c23-4db7-ae39-2bc01e5d4c5c/versions/1\",\n",
                        "  \"command\": \"python script.py  --raw_data ${{inputs.raw_data}} --model_output ${{outputs.model_output}}\",\n",
                        "  \"compute\": \"azureml:cpu-cluster-ram\",\n",
                        "  \"creation_context\": {\n",
                        "    \"created_at\": \"2022-04-29T05:46:07.599122+00:00\",\n",
                        "    \"created_by\": \"Yijun Zhang\",\n",
                        "    \"created_by_type\": \"User\"\n",
                        "  },\n",
                        "  \"description\": \"Train a GBDT regression model on the NYC taxi dataset.\",\n",
                        "  \"display_name\": \"willing_apple_0yz3g8rkvn\",\n",
                        "  \"environment\": \"azureml:nyc_taxi_image:7\",\n",
                        "  \"environment_variables\": {},\n",
                        "  \"experiment_name\": \"nyc_taxi_regression\",\n",
                        "  \"id\": \"azureml:/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev/jobs/willing_apple_0yz3g8rkvn\",\n",
                        "  \"inputs\": {\n",
                        "    \"raw_data\": {\n",
                        "      \"mode\": \"ro_mount\",\n",
                        "      \"path\": \"azureml:azureml://datastores/workspaceblobstore/paths/nyc_taxi_data/full_data\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"name\": \"willing_apple_0yz3g8rkvn\",\n",
                        "  \"outputs\": {\n",
                        "    \"default\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"path\": \"azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.willing_apple_0yz3g8rkvn\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"model_output\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_file\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"parameters\": {},\n",
                        "  \"properties\": {\n",
                        "    \"ContentSnapshotId\": \"2aa6c4a5-5d5b-4e0f-9110-0d0b02c48606\",\n",
                        "    \"_azureml.ComputeTargetType\": \"amlctrain\"\n",
                        "  },\n",
                        "  \"resourceGroup\": \"pipeline-pm\",\n",
                        "  \"resources\": {\n",
                        "    \"instance_count\": 1,\n",
                        "    \"properties\": {}\n",
                        "  },\n",
                        "  \"services\": {\n",
                        "    \"Studio\": {\n",
                        "      \"endpoint\": \"https://ml.azure.com/runs/willing_apple_0yz3g8rkvn?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/pipeline-pm/workspaces/pm-dev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
                        "      \"job_service_type\": \"Studio\"\n",
                        "    },\n",
                        "    \"Tracking\": {\n",
                        "      \"endpoint\": \"azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev?\",\n",
                        "      \"job_service_type\": \"Tracking\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"status\": \"Starting\",\n",
                        "  \"tags\": {},\n",
                        "  \"type\": \"command\"\n",
                        "}\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!az ml job create --f 2_standalone_job_run_on_cloud/2b_job_full_data.yml --web"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Get it modulized\n",
                "\n",
                "Sometimes MLE will leverarge pipelines with modulized components to do production for many value adds: collaboration, cost effectiveness, etc. For more information about when and why to use pipelines, please refer to [here is concept doc]().\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Decompose code\n",
                "\n",
                "First of all, you need to go through code, understand AI workflow, decompose it into several steps, for example, data processing, feature engineering, training, prediction, scoring, etc.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define components\n",
                "\n",
                "In this NYC Taxi example, we are going to decompose script into 5 steps: data preperation, data transformation, training, prediction, scoring and define 5 components for each step. For more details about component, please refer to [this component concept article](https://docs.microsoft.com/en-us/azure/machine-learning/concept-component).\n",
                "\n",
                "Each component can be considered as a stanalone job, then a pipeline is responsible to schedule them together. Similar with migrating a single script from local to remote, what you need to do is to import necessary dependencies, define interfaces, modify code for each step, create yaml file for component definition. You can learn more about component yaml schema [here](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-component-command).\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Step 1:\n",
                "\n",
                "Input: NYC taxi dataset folder, including 2 .csv files\n",
                "\n",
                "Code: Take multiple taxi datasets (yellow and green), remove and rename columns, combine greed and yellow data.\n",
                "\n",
                "Output: Single combined data\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Step 2:\n",
                "\n",
                "Input: Output of step 1, combined data\n",
                "\n",
                "Code: Eliminate filers, filter out locations outside NYC, split the pickup and dropoff date into the day of the week, day of the month, and month values, etc.\n",
                "\n",
                "Output: Dataset filtered and created with 20+ features\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Step 3:\n",
                "\n",
                "Input: Output of step 2, processed data\n",
                "\n",
                "Code: Split data into X and Y, split the data into train/test set, train a GBDT model, log parameters\n",
                "\n",
                "Output: Trained model (pickle format) and data subset for test (.csv)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Step 4:\n",
                "\n",
                "Input: Output of step3, GBDT model and test data\n",
                "\n",
                "Code: Predict test dataset with trained model\n",
                "\n",
                "Output: Test data with predictions added as a column\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Step 5:\n",
                "\n",
                "Input: Output of step4, test data with predictions\n",
                "\n",
                "Code: Calculate and log metrics\n",
                "\n",
                "Output: None\n",
                "\n",
                "After all these, you have 5 python source codes and yaml file.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define and submit a pipeline\n",
                "\n",
                "After defining components, you could create a pipeline which connects a serie of components and submit whole complete AI workflow .\n",
                "\n",
                "What you need to do is first to write a yaml file which describes how pipeline is built, what compute resource used, inputs and outputs, etc. and then to submit a pipeline job using command line.\n",
                "\n",
                "You can refer to this article for [pipeline job yaml specification](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-pipeline).\n",
                "\n",
                "\n",
                "You can first test with sample file and then run it with full size data by switching pipeline input path from local to Azure ML datastore."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36mCommand group 'ml job' is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus\u001b[0m\n",
                        "{\n",
                        "  \"creation_context\": {\n",
                        "    \"created_at\": \"2022-04-29T04:50:43.592029+00:00\",\n",
                        "    \"created_by\": \"Yijun Zhang\",\n",
                        "    \"created_by_type\": \"User\"\n",
                        "  },\n",
                        "  \"display_name\": \"heroic_key_k2020pd2xd\",\n",
                        "  \"experiment_name\": \"nyc_taxi_regression\",\n",
                        "  \"id\": \"azureml:/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev/jobs/heroic_key_k2020pd2xd\",\n",
                        "  \"inputs\": {\n",
                        "    \"pipeline_job_input\": {\n",
                        "      \"mode\": \"ro_mount\",\n",
                        "      \"path\": \"azureml:azureml://datastores/workspaceblobstore/paths/LocalUpload/1c2d0b4908fe99afe7e5d4d1e5af23e9/sample_data/\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"jobs\": {\n",
                        "    \"predict_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:ce1ecdbb-ebcf-92e2-840d-386a32eefc9d:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"model_input\": \"${{parent.jobs.train_job.outputs.model_output}}\",\n",
                        "        \"test_data\": \"${{parent.jobs.train_job.outputs.test_data}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"predictions\": \"${{parent.outputs.pipeline_job_predictions}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"prep_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:9875734e-18d7-1875-0c50-d9259fa37cc2:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"raw_data\": \"${{parent.inputs.pipeline_job_input}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"prep_data\": \"${{parent.outputs.pipeline_job_prepped_data}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"score_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:d2b0cd03-92f2-2a00-07b8-304fe1504b96:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"predictions\": \"${{parent.jobs.predict_job.outputs.predictions}}\"\n",
                        "      },\n",
                        "      \"outputs\": {},\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"train_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:9074edc5-d5f7-4b4e-0262-57fb265df4e0:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"training_data\": \"${{parent.jobs.transform_job.outputs.transformed_data}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"model_output\": \"${{parent.outputs.pipeline_job_trained_model}}\",\n",
                        "        \"test_data\": \"${{parent.outputs.pipeline_job_test_data}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"transform_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:8a54f42e-63fd-ce03-1fac-fdd8315bd411:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"clean_data\": \"${{parent.jobs.prep_job.outputs.prep_data}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"transformed_data\": \"${{parent.outputs.pipeline_job_transformed_data}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"name\": \"heroic_key_k2020pd2xd\",\n",
                        "  \"outputs\": {\n",
                        "    \"pipeline_job_predictions\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_prepped_data\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_test_data\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_trained_model\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_transformed_data\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"properties\": {\n",
                        "    \"azureml.continue_on_step_failure\": \"False\",\n",
                        "    \"azureml.enforceRerun\": \"False\",\n",
                        "    \"azureml.parameters\": \"{}\",\n",
                        "    \"azureml.pipelineComponent\": \"pipelinerun\",\n",
                        "    \"azureml.runsource\": \"azureml.PipelineRun\",\n",
                        "    \"runSource\": \"CLIv2\",\n",
                        "    \"runType\": \"HTTP\"\n",
                        "  },\n",
                        "  \"resourceGroup\": \"pipeline-pm\",\n",
                        "  \"services\": {\n",
                        "    \"Studio\": {\n",
                        "      \"endpoint\": \"https://ml.azure.com/runs/heroic_key_k2020pd2xd?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/pipeline-pm/workspaces/pm-dev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
                        "      \"job_service_type\": \"Studio\"\n",
                        "    },\n",
                        "    \"Tracking\": {\n",
                        "      \"endpoint\": \"azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev?\",\n",
                        "      \"job_service_type\": \"Tracking\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"settings\": {\n",
                        "    \"continue_on_step_failure\": false,\n",
                        "    \"default_compute\": \"cpu-cluster-ram\",\n",
                        "    \"default_datastore\": \"workspaceblobstore\",\n",
                        "    \"force_rerun\": false\n",
                        "  },\n",
                        "  \"status\": \"Preparing\",\n",
                        "  \"tags\": {},\n",
                        "  \"type\": \"pipeline\"\n",
                        "}\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!az ml job create --f 3_pipeline_job_run_on_cloud/3a_pipeline_sample_data.yml --web"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[36mCommand group 'ml job' is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus\u001b[0m\n",
                        "{\n",
                        "  \"creation_context\": {\n",
                        "    \"created_at\": \"2022-04-29T05:46:00.373038+00:00\",\n",
                        "    \"created_by\": \"Yijun Zhang\",\n",
                        "    \"created_by_type\": \"User\"\n",
                        "  },\n",
                        "  \"display_name\": \"lemon_parsnip_ndvm7ktyj3\",\n",
                        "  \"experiment_name\": \"nyc_taxi_regression\",\n",
                        "  \"id\": \"azureml:/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev/jobs/lemon_parsnip_ndvm7ktyj3\",\n",
                        "  \"inputs\": {\n",
                        "    \"pipeline_job_input\": {\n",
                        "      \"mode\": \"ro_mount\",\n",
                        "      \"path\": \"azureml:azureml://datastores/workspaceblobstore/paths/nyc_taxi_data/full_data\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"jobs\": {\n",
                        "    \"predict_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:d6b35a61-b293-44fe-0b58-f316c0087bb5:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"model_input\": \"${{parent.jobs.train_job.outputs.model_output}}\",\n",
                        "        \"test_data\": \"${{parent.jobs.train_job.outputs.test_data}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"predictions\": \"${{parent.outputs.pipeline_job_predictions}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"prep_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:6d14f0a1-77dc-9da0-897a-f82b36a6fd4a:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"raw_data\": \"${{parent.inputs.pipeline_job_input}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"prep_data\": \"${{parent.outputs.pipeline_job_prepped_data}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"score_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:f781cd80-2f1f-3d32-15d7-62ba34a046fb:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"predictions\": \"${{parent.jobs.predict_job.outputs.predictions}}\"\n",
                        "      },\n",
                        "      \"outputs\": {},\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"train_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:ce7eced9-1e95-dd6a-e9cf-c266e6bddac4:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"training_data\": \"${{parent.jobs.transform_job.outputs.transformed_data}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"model_output\": \"${{parent.outputs.pipeline_job_trained_model}}\",\n",
                        "        \"test_data\": \"${{parent.outputs.pipeline_job_test_data}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    },\n",
                        "    \"transform_job\": {\n",
                        "      \"$schema\": \"{}\",\n",
                        "      \"code\": \"{}\",\n",
                        "      \"command\": \"{}\",\n",
                        "      \"component\": \"azureml:b22fd5ed-d55a-bc15-b665-4775bcc247e2:1\",\n",
                        "      \"environment_variables\": {},\n",
                        "      \"inputs\": {\n",
                        "        \"clean_data\": \"${{parent.jobs.prep_job.outputs.prep_data}}\"\n",
                        "      },\n",
                        "      \"outputs\": {\n",
                        "        \"transformed_data\": \"${{parent.outputs.pipeline_job_transformed_data}}\"\n",
                        "      },\n",
                        "      \"type\": \"command\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"name\": \"lemon_parsnip_ndvm7ktyj3\",\n",
                        "  \"outputs\": {\n",
                        "    \"pipeline_job_predictions\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_prepped_data\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_test_data\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_trained_model\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    },\n",
                        "    \"pipeline_job_transformed_data\": {\n",
                        "      \"mode\": \"rw_mount\",\n",
                        "      \"type\": \"uri_folder\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"properties\": {\n",
                        "    \"azureml.continue_on_step_failure\": \"False\",\n",
                        "    \"azureml.enforceRerun\": \"False\",\n",
                        "    \"azureml.parameters\": \"{}\",\n",
                        "    \"azureml.pipelineComponent\": \"pipelinerun\",\n",
                        "    \"azureml.runsource\": \"azureml.PipelineRun\",\n",
                        "    \"runSource\": \"CLIv2\",\n",
                        "    \"runType\": \"HTTP\"\n",
                        "  },\n",
                        "  \"resourceGroup\": \"pipeline-pm\",\n",
                        "  \"services\": {\n",
                        "    \"Studio\": {\n",
                        "      \"endpoint\": \"https://ml.azure.com/runs/lemon_parsnip_ndvm7ktyj3?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/pipeline-pm/workspaces/pm-dev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
                        "      \"job_service_type\": \"Studio\"\n",
                        "    },\n",
                        "    \"Tracking\": {\n",
                        "      \"endpoint\": \"azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/pipeline-pm/providers/Microsoft.MachineLearningServices/workspaces/pm-dev?\",\n",
                        "      \"job_service_type\": \"Tracking\"\n",
                        "    }\n",
                        "  },\n",
                        "  \"settings\": {\n",
                        "    \"continue_on_step_failure\": false,\n",
                        "    \"default_compute\": \"cpu-cluster-ram\",\n",
                        "    \"default_datastore\": \"workspaceblobstore\",\n",
                        "    \"force_rerun\": false\n",
                        "  },\n",
                        "  \"status\": \"Preparing\",\n",
                        "  \"tags\": {},\n",
                        "  \"type\": \"pipeline\"\n",
                        "}\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!az ml job create --f 3_pipeline_job_run_on_cloud/3b_pipeline_full_data.yml --web"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The command is the same as standalone job, except this time you are directed to pipeline job detail page where you can see your pipeline graph.\n",
                "\n",
                "![pipeline detail page](images/pipeline_detail_page.png)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8 - AzureML",
            "language": "python",
            "name": "python38-azureml"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
