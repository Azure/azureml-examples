$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: basic-chat-custom-docker
endpoint_name: basic-chat-endpoint
model:
  path: ./basic-chat
  properties:
    azureml.promptflow.source_flow_id: {insert random GUID}
    # Following are properties only for chat flow 
    # endpoint detail UI Test tab needs this property to know it's a chat flow
    azureml.promptflow.mode: chat
    # endpoint detail UI Test tab needs this property to know which is the input column for chat flow
    azureml.promptflow.chat_input: question
    # endpoint detail UI Test tab needs this property to know which is the output column for chat flow
    azureml.promptflow.chat_output: answer
environment: 
  build:
    path: image_build_with_reqirements
    dockerfile_path: Dockerfile
  # deploy prompt flow is BYOC, so we need to specify the inference config
  inference_config:
    liveness_route:
      path: /health
      port: 8080
    readiness_route:
      path: /health
      port: 8080
    scoring_route:
      path: /score
      port: 8080
instance_type: Standard_E16s_v3
instance_count: 1
environment_variables:
  # When there are multiple fields in the response, using this env variable will filter the fields to expose in the response.
  # For example, if there are 2 flow outputs: "answer", "context", and I only want to have "answer" in the endpoint response, I can set this env variable to '["answer"]'
  # PROMPTFLOW_RESPONSE_INCLUDED_FIELDS: '["category", "evidence"]'

  # if you want to deploy to serving mode, you need to set this env variable to "serving"
  PROMPTFLOW_RUN_MODE: serving
  # currently it is for pulling connections from workspace
  PRT_CONFIG_OVERRIDE: deployment.subscription_id=<subscription_id>,deployment.resource_group=<resource_group_name>,deployment.workspace_name=<workspace_name>,deployment.endpoint_name=basic-chat-endpoint,deployment.deployment_name=basic-chat-custom-docker