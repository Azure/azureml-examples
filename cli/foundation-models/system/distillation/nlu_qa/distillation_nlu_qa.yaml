type: distillation

name: "Distillation-nlu-qa-llama"
description: "Distill student model using a teacher model"
experiment_name: "Distillation-NLU-QA"

# Data Generation Properties
data_generation_type: label_generation
data_generation_task_type: nlu_qa

# Input data
training_data: azureml:train_nlu_qa:1
validation_data: azureml:validation_nlu_qa:1

# Teacher model serverless endpoint information
teacher_model_endpoint_connection: 
  type: serverless
  name: Meta-Llama-3-1-405B-Instruct-vum
  endpoint: https://Meta-Llama-3-1-405B-Instruct-vum.westus3.models.ai.azure.com/chat/completions
  api_key: EXAMPLE_API_KEY

# Model ID
student_model: azureml://registries/azureml-meta/models/Meta-Llama-3.1-8B-Instruct/versions/2

# Output distilled model
outputs:
  registered_model:
    type: mlflow_model
    name: llama-nlu-qa-distilled


# Teacher model related properties (OPTIONAL)
teacher_model_settings:
  inference_parameters:
    temperature: 0.1
    max_tokens: 100
    top_p: 0.95
  endpoint_request_settings:
    request_batch_size: 5
    min_endpoint_success_ratio: 0.7

# System prompt settings (OPTIONAL)
prompt_settings:
  enable_chain_of_thought: true

# For finetuning (OPTIONAL)
hyperparameters:
  learning_rate_multiplier: "0.2"
  n_epochs: "5"
  batch_size: "2"

# Resource for Data Generation Step (OPTIONAL)
resources:
  instance_types:
    - Standard_D2_v2