
[![Python code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![license: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)
# Operationalize 
Operationalizing models in production is about taking the model that has been trained and evaluated, and making it available for use in a live, production environment. 

The process of operationalizing a model involves several steps, including identifying the production infrastructure, testing the model in a production-like environment, optimizing the model for the target environment, monitoring the model's performance, and ensuring that it is continuously updated and improved as new data becomes available.

Successful operationalization of a model can lead to significant benefits, including increased efficiency, improved accuracy, and better decision-making capabilities. However, it also requires careful planning and ongoing maintenance to ensure that the model continues to perform effectively over time.

This section provides some of the inference optimization tools to maximize performance in the production environment.

## DeepSpeed MII
Deepspeed MII is the library for optimizing the inference execution for certain models. More details about using DeepSpeed MII for inference optimization is [here](./DeepSpeed-MII/README.md)

## ONNX conversion for inference speedups



## Distillation in the future

## Deploy model to MIR or hardware
### Monitoring