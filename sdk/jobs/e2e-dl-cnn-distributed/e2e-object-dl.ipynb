{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6b685d",
   "metadata": {},
   "source": [
    "TODO: does this work locally AND remotely?\n",
    "TODO: \"This is a N part series of tutorials\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aadfd2",
   "metadata": {},
   "source": [
    "# PyTorch image classification from prep to deployment (part I)\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial you should be able to use Azure Machine Learning (AzureML) to:\n",
    "- ingest a large dataset from a simple url\n",
    "- quickly implement basic commands for data preparation\n",
    "- assemble a pipeline with custom data preparation (python) scripts\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you need:\n",
    "- to have provisioned an AzureML workspace\n",
    "- to have permissions to create simple SKUs in your resource group\n",
    "- a python environment\n",
    "\n",
    "**Motivations** - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf27f86",
   "metadata": {},
   "source": [
    "# End-to-end scenario\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641f516",
   "metadata": {},
   "source": [
    "# 1. Set up the pipeline resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99101368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle to the workspace\n",
    "from azure.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ced11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    InteractiveBrowserCredential(), \n",
    "    # subscription_id = '<SUBSCRIPTION_ID>', \n",
    "    # resource_group = '<RESOURCE_GROUP>', \n",
    "    # workspace = '<AML_WORKSPACE_NAME>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405eae5b",
   "metadata": {},
   "source": [
    "# 2. Implement a reusable data preparation pipeline\n",
    "\n",
    "motivation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ab207",
   "metadata": {},
   "source": [
    "## 2.1. Unzip archives with a simple command (no code component)\n",
    "\n",
    "In our use case, we want to shortcut having to download locally and upload the data into AzureML. So first, we'll have to unzip the archives provided from the COCO web urls. Unzipping is not the most complex operation. Running it in a reusable manner in a cloud pipeline job can require a lot of boilerplate code. Instead of writing a python script for that purpose, we just want to run the command itself.\n",
    "\n",
    "Here, we will embed the unzip call in a [CommandComponent](TODO) that specifies inputs, outputs and environment of that command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88190d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "from azure.ml.entities import CommandComponent, JobInput, JobOutput\n",
    "\n",
    "download_unzip_component = CommandComponent(\n",
    "    name=\"download_and_unzip\", # optional: this will show in the UI\n",
    "\n",
    "    # this component has no code, just a simple unzip command\n",
    "    # TODO: command = \"wgetls -lr ${{inputs.archive_path}}; unzip ${{inputs.archive_path}} -d ${{outputs.extracted_data}}\",\n",
    "    command = \"curl -o local_archive.zip ${{inputs.url}} && unzip local_archive.zip -d ${{outputs.extracted_data}}\",\n",
    "\n",
    "    # inputs and outputs need to match with the command\n",
    "    inputs = {\n",
    "        'url': { 'type': 'string' }\n",
    "    },\n",
    "    outputs = {\n",
    "        'extracted_data': { 'type': 'path' }\n",
    "    },\n",
    "\n",
    "    # we're using a curated environment\n",
    "    environment = 'AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00903f3a",
   "metadata": {},
   "source": [
    "## 2.2. Write a reusable pipeline in python\n",
    "\n",
    "The component we just created can now be loaded as a [component](TODO): a reusable step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c17abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll package this unzip command as a component to use within a pipeline\n",
    "download_unzip_component_func = dsl.load_component(component=download_unzip_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a44973",
   "metadata": {},
   "source": [
    "This step can be used as a python function with arguments and parameters. The `inputs` and `ouputs` of the command component can be provided as python variables. We use the decorator `@dsl.pipeline` to construct an AzureML pipeline assembling components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "parse_annotations_func = dsl.load_component(yaml_file=\"./components/coco_extract_annotations/spec.yaml\")\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    compute=\"cpu-d14-v2\", #\"cpu-cluster\", # TODO: document\n",
    "    description=\"e2e images preparation\", # TODO: document\n",
    ")\n",
    "def coco_preparation_pipeline(train_archive_url, valid_archive_url, test_archive_url, annotations_archive_url, category_id, category_name):\n",
    "    # TODO: document\n",
    "    train_unzip_step = download_unzip_component_func(\n",
    "        url=train_archive_url\n",
    "    )\n",
    "\n",
    "    # TODO: document\n",
    "    valid_unzip_step = download_unzip_component_func(\n",
    "        url=valid_archive_url\n",
    "    )\n",
    "\n",
    "    # TODO: document\n",
    "    test_unzip_step = download_unzip_component_func(\n",
    "        url=test_archive_url\n",
    "    )\n",
    "\n",
    "    # TODO: document\n",
    "    annotations_unzip_step = download_unzip_component_func(\n",
    "        url=annotations_archive_url\n",
    "    )\n",
    "\n",
    "    parse_annotations_step = parse_annotations_func(\n",
    "        annotations_dir=annotations_unzip_step.outputs.extracted_data,\n",
    "        category_id=category_id,\n",
    "        category_name=category_name\n",
    "    )\n",
    "\n",
    "    # TODO: document\n",
    "    return {\n",
    "        \"train_images\": train_unzip_step.outputs.extracted_data,\n",
    "        \"valid_images\": valid_unzip_step.outputs.extracted_data,\n",
    "        \"test_images\": test_unzip_step.outputs.extracted_data,\n",
    "        \"train_annotations\": parse_annotations_step.outputs.train_annotations,\n",
    "        \"valid_annotations\": parse_annotations_step.outputs.valid_annotations,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d7718",
   "metadata": {},
   "source": [
    "The pipeline we just created, decorated by `@dsl.pipeline` can also be called from python, as a sub-pipeline within another pipeline, creating more complex workflows (we'll see in next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: document\n",
    "help(coco_preparation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40f0d1",
   "metadata": {},
   "source": [
    "## 2.3. Run an instance of this pipeline in AzureML\n",
    "\n",
    "When calling the pipeline function decorated with `@dsl.pipeline`, we will create an instance of this pipeline with the given arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "from azure.ml.entities import JobInput, JobOutput\n",
    "\n",
    "pipeline_instance = coco_preparation_pipeline(\n",
    "    train_archive_url=\"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    valid_archive_url=\"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    test_archive_url=\"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "    annotations_archive_url=\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    category_id=1,\n",
    "    category_name=\"contains_person\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232807f6",
   "metadata": {},
   "source": [
    "That instance can be submitted to AzureML and run as an experiment there. Use the [`MLClient`](TODO) to create this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    \n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    \n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149a504",
   "metadata": {},
   "source": [
    "![](media/image-prep-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26664b",
   "metadata": {},
   "source": [
    "# 3. Training a distributed gpu job\n",
    "\n",
    "To run a pytorch training on multiple gpus, you have multiple options.\n",
    "\n",
    "In the following, we'll use the `pytorch` distribution setting to run a job using [`DistributedDataParallel`](TODO): multiple instances of the script will be running on each node (one per gpu on that node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c4f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "training_func = dsl.load_component(yaml_file=\"./components/pytorch_dl_train/spec.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973d3888",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coco_model_training_devel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jeomhove\\source\\azureml-examples\\sdk\\jobs\\e2e-dl-cnn-distributed\\e2e-object-dl.ipynb Cell 23'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jeomhove/source/azureml-examples/sdk/jobs/e2e-dl-cnn-distributed/e2e-object-dl.ipynb#ch0000022?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jeomhove/source/azureml-examples/sdk/jobs/e2e-dl-cnn-distributed/e2e-object-dl.ipynb#ch0000022?line=32'>33</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: training_step\u001b[39m.\u001b[39moutputs\u001b[39m.\u001b[39mtrained_model\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jeomhove/source/azureml-examples/sdk/jobs/e2e-dl-cnn-distributed/e2e-object-dl.ipynb#ch0000022?line=33'>34</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jeomhove/source/azureml-examples/sdk/jobs/e2e-dl-cnn-distributed/e2e-object-dl.ipynb#ch0000022?line=35'>36</a>\u001b[0m \u001b[39m# TODO: document\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jeomhove/source/azureml-examples/sdk/jobs/e2e-dl-cnn-distributed/e2e-object-dl.ipynb#ch0000022?line=36'>37</a>\u001b[0m help(coco_model_training_devel)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'coco_model_training_devel' is not defined"
     ]
    }
   ],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    description=\"e2e images classification\", # TODO: document\n",
    ")\n",
    "def coco_model_training(train_images, valid_images, train_annotations, valid_annotations, model_name, epochs, profile):\n",
    "    training_step = training_func(\n",
    "        # inputs\n",
    "        train_images=train_images,\n",
    "        valid_images=valid_images,\n",
    "        train_annotations=train_annotations,\n",
    "        valid_annotations=valid_annotations,\n",
    "\n",
    "        # params\n",
    "        num_epochs=epochs,\n",
    "        register_model_as=model_name,\n",
    "\n",
    "        # params (profiling)\n",
    "        profile=profile, # turns on profiler (see train.py)\n",
    "        profile_export_format=\"print\" # report in markdown format (see train.py)\n",
    "    )\n",
    "    training_step.compute=\"gpu-cluster\"\n",
    "\n",
    "    # use process_count_per_instance to parallelize on multiple gpus\n",
    "    training_step.distribution.process_count_per_instance  = 4 # set to number of gpus on instance\n",
    "\n",
    "    # use instance_count to increase the number of nodes (machines)\n",
    "    training_step.resources.instance_count  = 2\n",
    "\n",
    "    # TODO: document\n",
    "    return {\n",
    "        \"model\": training_step.outputs.trained_model\n",
    "    }\n",
    "\n",
    "# TODO: document\n",
    "help(coco_model_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "from azure.ml.entities import JobInput, JobOutput\n",
    "\n",
    "pipeline_instance = coco_model_training(\n",
    "    train_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    valid_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    train_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    valid_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    epochs=10,\n",
    "    model_name=\"coco_model_person_dev\",\n",
    "    profile=True # turns on profiler (see train.py)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1d9d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute is not a known attribute of class <class 'azure.ml._restclient.v2021_10_01.models._models_py3.PipelineJob'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The url to see your live job running is returned by the sdk:\n",
      "https://ml.azure.com/runs/add2196c-763e-4277-be96-f87ee1a892b2?wsid=/subscriptions/48bbc269-ce89-4f6f-9a12-c6f91fcb772d/resourcegroups/aml3p-sandbox-eus2/workspaces/aml3p-sandbox&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the model to deploy. Here we use the latest version of our registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "#create an online deployment.\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name='blue',\n",
    "    endpoint_name='coco_resnet18_person',\n",
    "    model=model,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\",\n",
    "    code_local_path=deploy_dir,\n",
    "    scoring_script=\"score.py\",\n",
    "    instance_type='Standard_DS2_v2',\n",
    "    instance_count=1)\n",
    "\n",
    "ml_client.begin_create_or_update(blue_deployment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
