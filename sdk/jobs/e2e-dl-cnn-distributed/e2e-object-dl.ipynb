{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1aadfd2",
   "metadata": {},
   "source": [
    "# Distributed PyTorch Image Classification\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial you should be able to use Azure Machine Learning (AzureML) to:\n",
    "- ingest a large dataset from a simple url\n",
    "- quickly implement basic commands for data preparation\n",
    "- assemble a pipeline with custom data preparation (python) scripts\n",
    "- test and run a multi-node multi-gpu pytorch job\n",
    "- register and deploy a pytorch model\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you need:\n",
    "- to have provisioned an AzureML workspace\n",
    "- to have permissions to create simple SKUs in your resource group\n",
    "- a python environment\n",
    "\n",
    "**Motivations** - Let's consider the following scenario: we want to explore training different image classifiers on distinct kinds of problems, based on a large public dataset that is available at a given url. This ML pipeline will be future-looking, in particular we want:\n",
    "- **genericity**: to be fairly independent from the data we're ingesting (so that we could switch to internal proprietary data in the future),\n",
    "- **configurability**: to run different versions of that training with simple configuration changes,\n",
    "- **scalability**: to iterate on the pipeline on small sample, then smoothly transition to running at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641f516",
   "metadata": {},
   "source": [
    "### Connect to AzureML\n",
    "\n",
    "Before we dive in the code, we'll need to create an instance of MLClient to connect to Azure ML. Please provide the references to your workspace below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle to the workspace\n",
    "from azure.ml import MLClient\n",
    "\n",
    "# authentication package\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    InteractiveBrowserCredential(), \n",
    "    subscription_id = '<SUBSCRIPTION_ID>',\n",
    "    resource_group_name = '<RESOURCE_GROUP>',\n",
    "    workspace_name = '<AML_WORKSPACE_NAME>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405eae5b",
   "metadata": {},
   "source": [
    "# 1. Implement a reusable data preparation pipeline\n",
    "\n",
    "To develop our data preparation pipeline, there are a couple constraints that we're setting for ourselves:\n",
    "- we want to minimize the effort to ingest public data as it is used only as a learning opportunity,\n",
    "- we do not want to manipulate large data locally (ex: download/upload that data could take multiple hours),\n",
    "\n",
    "In this section, we'll achieve just that, by implementing the following:\n",
    "- a data ingestion and processing pipeline with simple shell commands (wget, unzip) using minimal boilerplate code,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ab207",
   "metadata": {},
   "source": [
    "## 1.1. Unzip archives with a simple command (no code)\n",
    "\n",
    "To train our classifier, we'll consume the [Common Objects in COntext (COCO) dataset](https://cocodataset.org/). If we were to use this locally, the sequence would be very basic: download 3 zip files, unzip each of them in a distinct folder for train/val/test, use python to extract annotations into a format we can use. We'll do just that, but in the cloud, without too much pain.\n",
    "\n",
    "The Azure ML SDK provides `entities` to implement any step of a workflow. In the example below, we create a `CommandComponent` with just a shell command. We parameterize this command by using a string template syntax provided by the SDK:\n",
    "\n",
    "> ```\n",
    "> curl -o local_archive.zip ${{inputs.url}} && unzip local_archive.zip -d ${{outputs.extracted_data}}\n",
    "> ```\n",
    "\n",
    "Creating the component just consists in declaring the names of the inputs, outputs, and specifying an environment. For this simple job we'll use a curated environment from AzureML. After that, we'll be able to reuse that component multiple times in our pipeline design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88190d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import CommandComponent, JobInput, JobOutput\n",
    "\n",
    "download_unzip_component = CommandComponent(\n",
    "    name=\"download_and_unzip\", # optional: this will show in the UI\n",
    "\n",
    "    # this component has no code, just a simple unzip command\n",
    "    command = \"curl -o local_archive.zip ${{inputs.url}} && unzip local_archive.zip -d ${{outputs.extracted_data}}\",\n",
    "\n",
    "    # I/O specifications, each using a specific key and type\n",
    "    inputs = {\n",
    "        # 'url' is the key of this input string\n",
    "        'url': { 'type': 'string' }\n",
    "    },\n",
    "    outputs = {\n",
    "        # 'extracted_data' will be the key to link this output to other steps in the pipeline\n",
    "        'extracted_data': { 'type': 'path' }\n",
    "    },\n",
    "\n",
    "    # we're using a curated environment\n",
    "    environment = 'AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00903f3a",
   "metadata": {},
   "source": [
    "The component we just created can now be loaded as a component: a reusable step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c17abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# we'll package this unzip command as a component to use within a pipeline\n",
    "download_unzip_component_func = dsl.load_component(component=download_unzip_component)\n",
    "\n",
    "?download_unzip_component_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a44973",
   "metadata": {},
   "source": [
    "## 1.2. Add a python script\n",
    "\n",
    "Next step in our pipeline is to implement a simple script to extract the annotations and format them for us. We've written that script in this repository, and it can be loaded as a component from its yaml specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ac9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "parse_annotations_func = dsl.load_component(yaml_file=\"./components/coco_extract_annotations/spec.yaml\")\n",
    "\n",
    "?parse_annotations_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe45054",
   "metadata": {},
   "source": [
    "## 1.3. Write a reusable pipeline\n",
    "\n",
    "We use the decorator `@dsl.pipeline` to construct an AzureML pipeline assembling the two components above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    compute=\"cpu-d14-v2\", #\"cpu-cluster\", # TODO: document\n",
    "    description=\"e2e images preparation\", # TODO: document\n",
    ")\n",
    "def coco_preparation_pipeline(train_archive_url, valid_archive_url, test_archive_url, annotations_archive_url, category_id, category_name):\n",
    "    # 1st instance using the command component above on the training data\n",
    "    train_unzip_step = download_unzip_component_func(\n",
    "        url=train_archive_url\n",
    "    )\n",
    "\n",
    "    # 2nd instance for validation data\n",
    "    valid_unzip_step = download_unzip_component_func(\n",
    "        url=valid_archive_url\n",
    "    )\n",
    "\n",
    "    # 3rd instance for testing data\n",
    "    test_unzip_step = download_unzip_component_func(\n",
    "        url=test_archive_url\n",
    "    )\n",
    "\n",
    "    # 4th instance for the annotations\n",
    "    annotations_unzip_step = download_unzip_component_func(\n",
    "        url=annotations_archive_url\n",
    "    )\n",
    "\n",
    "    # add the annotations processing after the unzip command\n",
    "    parse_annotations_step = parse_annotations_func(\n",
    "        # here we consume the output of the unzip step\n",
    "        annotations_dir=annotations_unzip_step.outputs.extracted_data,\n",
    "\n",
    "        # parameters for this step are given as pipeline parameters\n",
    "        # to allow for genericity (no hardcoded value)\n",
    "        category_id=category_id,\n",
    "        category_name=category_name\n",
    "    )\n",
    "\n",
    "    # outputs of this pipeline are coded as a dictionary\n",
    "    # keys can be used to assemble and link this pipeline with other pipelines\n",
    "    return {\n",
    "        \"train_images\": train_unzip_step.outputs.extracted_data,\n",
    "        \"valid_images\": valid_unzip_step.outputs.extracted_data,\n",
    "        \"test_images\": test_unzip_step.outputs.extracted_data,\n",
    "        \"train_annotations\": parse_annotations_step.outputs.train_annotations,\n",
    "        \"valid_annotations\": parse_annotations_step.outputs.valid_annotations,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d7718",
   "metadata": {},
   "source": [
    "The pipeline we just created, decorated by `@dsl.pipeline` can also be called from python, as a sub-pipeline within another pipeline, creating more complex workflows (we'll see in next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "?coco_preparation_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40f0d1",
   "metadata": {},
   "source": [
    "## 1.4.. Run an instance of this pipeline in AzureML\n",
    "\n",
    "When calling the pipeline function decorated with `@dsl.pipeline`, we will create an instance of this pipeline with the given arguments. In this scenario, we just provide the urls to the zip files we want to process, and the category of the objects we plan to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "from azure.ml.entities import JobInput, JobOutput\n",
    "\n",
    "pipeline_instance = coco_preparation_pipeline(\n",
    "    train_archive_url=\"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    valid_archive_url=\"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    test_archive_url=\"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "    annotations_archive_url=\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    category_id=1,\n",
    "    category_name=\"contains_person\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232807f6",
   "metadata": {},
   "source": [
    "That instance can be submitted to AzureML and run there. Use the `MLClient` to create the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    \n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    \n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149a504",
   "metadata": {},
   "source": [
    "Considering the side of the dataset, this job will take a couple hours to complete. The validation and annotations dataset are smaller, and should take a couple minutes only to unzip. So while we wait for the training dataset (110k+ images) to finalize, you can already go into AzureML and register the outputs of the pipeline as datasets (see below).\n",
    "\n",
    "<span style=\"color:red\">IMPORTANT</span> - To move forward with the next section, we'll need you to:\n",
    "- register the output of \"valid_unzip_step\" as dataset \"coco_val2017\"\n",
    "- register the 2nd output of \"Extract Coco Annotations\" as dataset \"coco_val2017_annotations\"\n",
    "\n",
    "![](media/image-prep-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26664b",
   "metadata": {},
   "source": [
    "# 2. Training a distributed gpu job\n",
    "\n",
    "To run a pytorch training on multiple gpus, you have multiple options.\n",
    "\n",
    "In the following, we'll use the `pytorch` distribution setting to run a job using [`DistributedDataParallel`](TODO): multiple instances of the script will be running on each node (one per gpu on that node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "training_func = dsl.load_component(yaml_file=\"./components/pytorch_dl_train/spec.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    description=\"e2e images classification\", # TODO: document\n",
    ")\n",
    "def coco_model_training(train_images, valid_images, train_annotations, valid_annotations, model_name, epochs, profile):\n",
    "    training_step = training_func(\n",
    "        # inputs\n",
    "        train_images=train_images,\n",
    "        valid_images=valid_images,\n",
    "        train_annotations=train_annotations,\n",
    "        valid_annotations=valid_annotations,\n",
    "\n",
    "        # params\n",
    "        num_epochs=epochs,\n",
    "        register_model_as=model_name,\n",
    "        num_workers=-1, # use all cpus (see train.py)\n",
    "\n",
    "        # params (profiling)\n",
    "        profile=profile, # turns on profiler (see train.py)\n",
    "        profile_export_format=\"tensorboard\" # report in markdown format (see train.py)\n",
    "    )\n",
    "    training_step.compute=\"gpu-cluster-eus\"\n",
    "\n",
    "    # use process_count_per_instance to parallelize on multiple gpus\n",
    "    training_step.distribution.process_count_per_instance  = 1 # set to number of gpus on instance\n",
    "\n",
    "    # use instance_count to increase the number of nodes (machines)\n",
    "    training_step.resources.instance_count  = 1\n",
    "\n",
    "    # TODO: document\n",
    "    return {\n",
    "        \"model\": training_step.outputs.trained_model\n",
    "    }\n",
    "\n",
    "# TODO: document\n",
    "help(coco_model_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "from azure.ml.entities import JobInput, JobOutput\n",
    "\n",
    "pipeline_instance = coco_model_training(\n",
    "    train_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    valid_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    train_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    valid_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    epochs=10,\n",
    "    model_name=\"coco_model_person_dev\",\n",
    "    profile=True # turns on profiler (see train.py)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "from azure.ml.entities import JobInput, JobOutput\n",
    "\n",
    "pipeline_instance = coco_model_training(\n",
    "    train_images=ml_client.datasets.get(\"coco_train2017\", version=1),\n",
    "    valid_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    train_annotations=ml_client.datasets.get(\"coco_train2017_annotations\", version=2),\n",
    "    valid_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    epochs=10,\n",
    "    model_name=\"coco_model_person_full\",\n",
    "    profile=True # turns on profiler (see train.py)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81532baf",
   "metadata": {},
   "source": [
    "# 4. Deploy the Model as an Online Endpoint\n",
    "\n",
    "TODO: what's specific about DL service here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick the latest version of the model\n",
    "model_name = \"coco_model_person_dev\"\n",
    "\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    ")\n",
    "\n",
    "print(latest_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Creating a unique name for the endpoint\n",
    "#online_endpoint_name='coco-person-'+str(uuid.uuid4())[:8]\n",
    "online_endpoint_name=\"coco-person-cc8eaedd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "#create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description='this is an online endpoint',\n",
    "    auth_mode='key',\n",
    "    tags={\n",
    "        'training_dataset': 'coco_train2017'\n",
    "    }\n",
    ")\n",
    "                     \n",
    "endpoint = ml_client.begin_create_or_update(endpoint)\n",
    "\n",
    "print(f\"Endpoint \\\"{endpoint.name}\\\" provisioning state: {endpoint.provisioning_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if the endpoint already exists\n",
    "endpoint = ml_client.online_endpoints.get(name = online_endpoint_name)\n",
    "\n",
    "print(f\"Endpint \\\"{endpoint.name}\\\" with provisioning state \\\"{endpoint.provisioning_state}\\\" is retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0496c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint._scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "registered_model_name = \"coco_model_person_dev\"\n",
    "\n",
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")\n",
    "\n",
    "#create an online deployment.\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name='red',\n",
    "    endpoint_name=endpoint.name,\n",
    "    model=f\"{registered_model_name}:{latest_model_version}\",\n",
    "    environment=\"AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu:11\",\n",
    "    code_local_path=\"./components/deployment/\",\n",
    "    scoring_script=\"score.py\",\n",
    "    instance_type='Standard_DS2_v2',\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e52c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_uri = endpoint.scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import traceback\n",
    "\n",
    "with open(\"./test_image.jpg\", mode='rb') as ifile:\n",
    "    image_64_encode = base64.b64encode(ifile.read()).decode('ascii')\n",
    "\n",
    "print(f\"URI: {endpoint.scoring_uri}\")\n",
    "\n",
    "request_payload = {\n",
    "    'rows': [\n",
    "        { 'image' : image_64_encode }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    r = requests.post(endpoint.scoring_uri, json=request_payload)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        print(f\"Endpoint returned: {r.json()}\")\n",
    "    else:\n",
    "        print(f\"Endpoint returned: {r.text}\")\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9188298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
