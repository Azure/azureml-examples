{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1aadfd2",
   "metadata": {},
   "source": [
    "# Distributed PyTorch Image Classification\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial you should be able to use Azure Machine Learning (AzureML) to:\n",
    "- quickly implement basic commands for data preparation\n",
    "- assemble a pipeline with custom data preparation (python) scripts\n",
    "- test and run a multi-node multi-gpu pytorch job\n",
    "- register and deploy a pytorch model\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you need:\n",
    "- to have provisioned an AzureML workspace\n",
    "- to have permissions to create simple SKUs in your resource group\n",
    "- a python environment\n",
    "\n",
    "**Motivations** - Let's consider the following scenario: we want to explore training different image classifiers on distinct kinds of problems, based on a large public dataset that is available at a given url. This ML pipeline will be future-looking, in particular we want:\n",
    "- **genericity**: to be fairly independent from the data we're ingesting (so that we could switch to internal proprietary data in the future),\n",
    "- **configurability**: to run different versions of that training with simple configuration changes,\n",
    "- **scalability**: to iterate on the pipeline on small sample, then smoothly transition to running at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641f516",
   "metadata": {},
   "source": [
    "### Connect to AzureML\n",
    "\n",
    "Before we dive in the code, we'll need to create an instance of MLClient to connect to Azure ML. Please provide the references to your workspace below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle to the workspace\n",
    "from azure.ml import MLClient\n",
    "\n",
    "# authentication package\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    InteractiveBrowserCredential(),\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405eae5b",
   "metadata": {},
   "source": [
    "# 1. Implement a reusable data preparation pipeline\n",
    "\n",
    "To develop our data preparation pipeline, there are a couple constraints that we're setting for ourselves:\n",
    "- we want to minimize the effort to ingest public data as it is used only as a learning opportunity,\n",
    "- we do not want to manipulate large data locally (ex: download/upload that data could take multiple hours),\n",
    "\n",
    "In this section, we'll achieve just that, by implementing the following:\n",
    "- a data ingestion and processing pipeline with simple shell commands (wget, unzip) using minimal boilerplate code,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ab207",
   "metadata": {},
   "source": [
    "## 1.1. Unzip archives with a simple command (no code)\n",
    "\n",
    "To train our classifier, we'll consume the [Common Objects in COntext (COCO) dataset](https://cocodataset.org/). If we were to use this locally, the sequence would be very basic: download 3 zip files, unzip each of them in a distinct folder for train/val/test, use python to extract annotations into a format we can use. We'll do just that, but in the cloud, without too much pain.\n",
    "\n",
    "The Azure ML SDK provides `entities` to implement any step of a workflow. In the example below, we create a `CommandComponent` with just a shell command. We parameterize this command by using a string template syntax provided by the SDK:\n",
    "\n",
    "> ```\n",
    "> curl -o local_archive.zip ${{inputs.url}} && unzip local_archive.zip -d ${{outputs.extracted_data}}\n",
    "> ```\n",
    "\n",
    "Creating the component just consists in declaring the names of the inputs, outputs, and specifying an environment. For this simple job we'll use a curated environment from AzureML. After that, we'll be able to reuse that component multiple times in our pipeline design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88190d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import CommandComponent, JobInput, JobOutput\n",
    "\n",
    "download_unzip_component = CommandComponent(\n",
    "    name=\"download_and_unzip\",  # optional: this will show in the UI\n",
    "    # this component has no code, just a simple unzip command\n",
    "    command=\"curl -o local_archive.zip ${{inputs.url}} && unzip local_archive.zip -d ${{outputs.extracted_data}}\",\n",
    "    # I/O specifications, each using a specific key and type\n",
    "    inputs={\n",
    "        # 'url' is the key of this input string\n",
    "        \"url\": {\"type\": \"string\"}\n",
    "    },\n",
    "    outputs={\n",
    "        # 'extracted_data' will be the key to link this output to other steps in the pipeline\n",
    "        \"extracted_data\": {\"type\": \"path\"}\n",
    "    },\n",
    "    # we're using a curated environment\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00903f3a",
   "metadata": {},
   "source": [
    "The component we just created can now be loaded as a component: a reusable step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c17abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# we'll package this unzip command as a component to use within a pipeline\n",
    "download_unzip_component_func = dsl.load_component(component=download_unzip_component)\n",
    "\n",
    "?download_unzip_component_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a44973",
   "metadata": {},
   "source": [
    "## 1.2. Add a python script\n",
    "\n",
    "Next step in our pipeline is to implement a simple script to extract the annotations and format them for us. We've written that script in this repository, and it can be loaded as a component from its yaml specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ac9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "parse_annotations_func = dsl.load_component(\n",
    "    yaml_file=\"./src/coco_extract_annotations/spec.yaml\"\n",
    ")\n",
    "\n",
    "?parse_annotations_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe45054",
   "metadata": {},
   "source": [
    "## 1.3. Write a reusable pipeline\n",
    "\n",
    "We use the decorator `@dsl.pipeline` to construct an AzureML pipeline assembling the two components above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    compute=\"cpu-cluster\",\n",
    "    description=\"e2e images preparation\",\n",
    ")\n",
    "def coco_preparation_pipeline(\n",
    "    train_archive_url,\n",
    "    valid_archive_url,\n",
    "    test_archive_url,\n",
    "    annotations_archive_url,\n",
    "    category_id,\n",
    "    category_name,\n",
    "):\n",
    "    # 1st instance using the command component above on the training data\n",
    "    train_unzip_step = download_unzip_component_func(url=train_archive_url)\n",
    "\n",
    "    # 2nd instance for validation data\n",
    "    valid_unzip_step = download_unzip_component_func(url=valid_archive_url)\n",
    "\n",
    "    # 3rd instance for testing data\n",
    "    test_unzip_step = download_unzip_component_func(url=test_archive_url)\n",
    "\n",
    "    # 4th instance for the annotations\n",
    "    annotations_unzip_step = download_unzip_component_func(url=annotations_archive_url)\n",
    "\n",
    "    # add the annotations processing after the unzip command\n",
    "    parse_annotations_step = parse_annotations_func(\n",
    "        # here we consume the output of the unzip step\n",
    "        annotations_dir=annotations_unzip_step.outputs.extracted_data,\n",
    "        # parameters for this step are given as pipeline parameters\n",
    "        # to allow for genericity (no hardcoded value)\n",
    "        category_id=category_id,\n",
    "        category_name=category_name,\n",
    "    )\n",
    "\n",
    "    # outputs of this pipeline are coded as a dictionary\n",
    "    # keys can be used to assemble and link this pipeline with other pipelines\n",
    "    return {\n",
    "        \"train_images\": train_unzip_step.outputs.extracted_data,\n",
    "        \"valid_images\": valid_unzip_step.outputs.extracted_data,\n",
    "        \"test_images\": test_unzip_step.outputs.extracted_data,\n",
    "        \"train_annotations\": parse_annotations_step.outputs.train_annotations,\n",
    "        \"valid_annotations\": parse_annotations_step.outputs.valid_annotations,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d7718",
   "metadata": {},
   "source": [
    "The pipeline we just created, decorated by `@dsl.pipeline` can also be called from python, as a sub-pipeline within another pipeline, creating more complex workflows (we'll see in next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "?coco_preparation_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40f0d1",
   "metadata": {},
   "source": [
    "## 1.4.. Run an instance of this pipeline in AzureML\n",
    "\n",
    "When calling the pipeline function decorated with `@dsl.pipeline`, we will create an instance of this pipeline with the given arguments. In this scenario, we just provide the urls to the zip files we want to process, and the category of the objects we plan to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = coco_preparation_pipeline(\n",
    "    train_archive_url=\"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    valid_archive_url=\"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    test_archive_url=\"http://images.cocodataset.org/zips/test2017.zip\",\n",
    "    annotations_archive_url=\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
    "    category_id=1,\n",
    "    category_name=\"contains_person\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232807f6",
   "metadata": {},
   "source": [
    "That instance can be submitted to AzureML and run there. Use the `MLClient` to create the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149a504",
   "metadata": {},
   "source": [
    "Considering the size of the dataset, this job will take a couple hours to complete. The validation and annotations dataset are smaller, and should take a couple minutes only to unzip. So while we wait for the training dataset (110k+ images) to finalize, you can already go into AzureML and register the outputs of the pipeline as datasets (see below).\n",
    "\n",
    "<span style=\"color:red\">IMPORTANT</span> - To move forward with the next section, we'll need you to:\n",
    "- register the output of \"valid_unzip_step\" as dataset \"coco_val2017\"\n",
    "- register the 2nd output of \"Extract Coco Annotations\" as dataset \"coco_val2017_annotations\"\n",
    "\n",
    "![](media/image-prep-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26664b",
   "metadata": {},
   "source": [
    "# 2. Training a distributed gpu job\n",
    "\n",
    "Implementing a distributed pytorch training is complex. Of course in this tutorial we've written one for you, but the point is: it takes time, it takes several iterations, each requiring you to try your code locally, then in the cloud, then try it at scale, until satisfied and then run a full blown production model training. This trial/error process can be made easier if we can create reusable code we can iterate on quickly, and that can be configured to run from small to large scale.\n",
    "\n",
    "So, to develop our training pipeline, we set a couple constraints for ourselves:\n",
    "- we want to minimize the effort to iterate on the pipeline code when porting it in the cloud,\n",
    "- we want to use the same code for small scale and large scale testing (so that we don't do not want to manipulate large data locally (ex: download/upload that data could take multiple hours),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16c184",
   "metadata": {},
   "source": [
    "## 2.1. Implement training and test on a sample\n",
    "\n",
    "We've implemented a distributed pytorch training script that we can load as a component using its yaml specification, like we did for other components before.\n",
    "\n",
    "Writing a pipeline to run it will be greatly simplified by the Azure ML SDK `@dsl.pipeline` decorator. For this, we've decided to parameterize this pipeline with relevant arguments (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl\n",
    "\n",
    "# load the component from its yaml specifications\n",
    "training_func = dsl.load_component(yaml_file=\"./src/pytorch_dl_train/spec.yaml\")\n",
    "\n",
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "@dsl.pipeline(\n",
    "    description=\"e2e images classification\",  # TODO: document\n",
    ")\n",
    "def coco_model_training(\n",
    "    train_images,  # dataset containing training images\n",
    "    valid_images,  # dataset containing validation images\n",
    "    train_annotations,  # annotations in CSV (see coco_extract_annotations/)\n",
    "    valid_annotations,  # annotations in CSV (see coco_extract_annotations/)\n",
    "    model_name,  # a name to register the model after training\n",
    "    epochs,  # the number of epochs\n",
    "    enabling_profiling,  # bonus: we've implemented pytorch profiling in our script\n",
    "):\n",
    "    # the training step is calling our training component with the right arguments\n",
    "    training_step = training_func(\n",
    "        # inputs\n",
    "        train_images=train_images,\n",
    "        valid_images=valid_images,\n",
    "        train_annotations=train_annotations,\n",
    "        valid_annotations=valid_annotations,\n",
    "        # params (some hardcoded, some given by pipeline parameters)\n",
    "        num_epochs=epochs,\n",
    "        register_model_as=model_name,\n",
    "        num_workers=-1,  # use all cpus (see train.py)\n",
    "        # params (profiling)\n",
    "        enabling_profiling=enabling_profiling,  # turns on profiler (see train.py)\n",
    "        profiler_export_format=\"markdown\",  # report in markdown format (see train.py)\n",
    "    )\n",
    "    # we set the name of the compute target for this training job\n",
    "    training_step.compute = \"gpu-cluster\"\n",
    "\n",
    "    # use process_count_per_instance to parallelize on multiple gpus\n",
    "    training_step.distribution.process_count_per_instance = (\n",
    "        1  # set to number of gpus on instance\n",
    "    )\n",
    "\n",
    "    # use instance_count to increase the number of nodes (machines)\n",
    "    training_step.resources.instance_count = 1\n",
    "\n",
    "    # outputs of this pipeline are coded as a dictionary\n",
    "    # keys can be used to assemble and link this pipeline with other pipelines\n",
    "    return {\"model\": training_step.outputs.trained_model}\n",
    "\n",
    "\n",
    "# TODO: document\n",
    "?coco_model_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e172ac",
   "metadata": {},
   "source": [
    "We can now test this code by running it on a smaller dataset in Azure ML. Here, we will use the validation set for training. Of course, the model will not be valid. But training will be short (8 mins on Standard_NC6 for 1 epoch) to allow us to iterate if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = coco_model_training(\n",
    "    # inputs: using validation set for training makes model invalid\n",
    "    train_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    valid_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    train_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    valid_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    # training parameters surfaced in the pipeline definition\n",
    "    epochs=1,  # 1 epoch only for testing, model isn't valid anyway\n",
    "    model_name=\"coco_model_person_dev\",\n",
    "    enabling_profiling=True,  # turns on profiler (see train.py)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b9c5f",
   "metadata": {},
   "source": [
    "Once we create that pipeline instance, we submit it through `MLClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf19ded",
   "metadata": {},
   "source": [
    "You can iterate on this design as much as you'd like, updating the local code of the component and re-submit the pipeline. You would have to go back to the beginning of this section (from the `dsl.load_component()` call) in order to refresh the pipeline definition with a new version of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782c68e",
   "metadata": {},
   "source": [
    "## 2.2. Use MLFlow to report metrics, logs, artifacts\n",
    "\n",
    "Azure ML natively integrated MLFlow so that if your code already supports MLFlow logging, you will not have to modify it to report your metrics within Azure ML.\n",
    "\n",
    "The component above is using MLFlow internally to report relevant metrics, logs and artifacts. Look for `mlflow` calls within the script `train.py`.\n",
    "\n",
    "To access this data, open the pipeline in the Azure ML Studio.\n",
    "\n",
    "![](media/pytorch_train_pipeline.png)\n",
    "\n",
    "Click on the component to display the Details panel. Choose **Metrics** to check the values reported using [`mlflow.log_metric()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric).\n",
    "\n",
    "![](media/pytorch_train_metrics.png)\n",
    "\n",
    "Your component can also report files using [`mlflow.log_artifact()`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact). NAvigate in the **Outputs+Logs** panel to find the artifacts. In our implementation, we use it to log a markdown file containing the output of [PyTorch profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-profiler-to-analyze-execution-time).\n",
    "\n",
    "![](media/pytorch_train_profile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d8b0f",
   "metadata": {},
   "source": [
    "## 2.3. Reuse the same pipeline on full scale data\n",
    "\n",
    "Once the pipeline is satisfying, running it full scale is actually just adjusting the inputs and parameters. We're creating a new instance below where we use training images as training set (as we should)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_instance = coco_model_training(\n",
    "    train_images=ml_client.datasets.get(\"coco_train2017\", version=1),\n",
    "    valid_images=ml_client.datasets.get(\"coco_val2017\", version=1),\n",
    "    train_annotations=ml_client.datasets.get(\"coco_train2017_annotations\", version=2),\n",
    "    valid_annotations=ml_client.datasets.get(\"coco_val2017_annotations\", version=2),\n",
    "    epochs=10,\n",
    "    model_name=\"coco_model_person_full\",\n",
    "    profile=False,  # turns off profiler (see train.py)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_instance,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_image_sample\",\n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "\n",
    "# get a URL for the status of the job\n",
    "print(\"The url to see your live job running is returned by the sdk:\")\n",
    "print(returned_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81532baf",
   "metadata": {},
   "source": [
    "# 3. Deploy the Model as an Online Endpoint\n",
    "\n",
    "As mentioned above, the model will be registered within the training pipeline at the end of the epochs. Next step is to deploy this registered model in an online endpoint for inferencing.\n",
    "\n",
    "An endpoint in AzureML is creating the URL behind which our model will be hosted. Creating that endpoint does not deploy the model. Instead, you can deploy several models behind this endpoint and manage traffic coming through each of them.\n",
    "\n",
    "In this tutorial, we'll just deploy one version of the model at full traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947aecb6",
   "metadata": {},
   "source": [
    "## 3.1. Create an endpoint\n",
    "\n",
    "We'll use the `MLClient` to create an endpoint with a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "import uuid\n",
    "\n",
    "# Generate a unique name for the endpoint\n",
    "# NOTE: endpoint names have to be unique at the Azure region level\n",
    "online_endpoint_name = \"coco-person-\" + str(uuid.uuid4())[:8]\n",
    "\n",
    "\n",
    "# Create an online endpoint object\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"online endpoint for pytorch image classifier\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"model\": \"vision\"},\n",
    ")\n",
    "\n",
    "# Use MLClient to create the endpoint\n",
    "endpoint = ml_client.begin_create_or_update(endpoint)\n",
    "\n",
    "# Report and track the status\n",
    "print(f'Endpoint \"{endpoint.name}\" provisioning state: {endpoint.provisioning_state}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea75f3",
   "metadata": {},
   "source": [
    "## 3.2. Deploy the model to the endpoint\n",
    "\n",
    "And in a second step, deploy the registered model in that endpoint. This operation could take approx 20 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import (\n",
    "    ManagedOnlineDeployment,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "# name of the model we want to deploy\n",
    "registered_model_name = \"coco_model_person_dev\"\n",
    "\n",
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")\n",
    "\n",
    "# create an online deployment\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=endpoint.name,\n",
    "    model=f\"{registered_model_name}:{latest_model_version}\",\n",
    "    environment=\"AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu:11\",\n",
    "    code_local_path=\"./src/deployment/\",\n",
    "    scoring_script=\"score.py\",\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e08b4e",
   "metadata": {},
   "source": [
    "## 3.3. Test the endpoint with REST calls\n",
    "\n",
    "To check our model deployment, we will try it out on two images sent to the endpoint using simple requests.\n",
    "\n",
    "We'll use the two images below from their public urls, download them and encode them to build a json payload.\n",
    "\n",
    "| `non_person_image.jpg` |  `person.jpg` |\n",
    "| :-- | :-- |\n",
    "| <img src=\"http://images.cocodataset.org/val2017/000000567197.jpg\" alt=\"a black and white picture of a warehouse\" width=\"30%\"> | <img src=\"http://images.cocodataset.org/val2017/000000274219.jpg\" alt=\"a black and white picture of a man\" width=\"30%\"> |\n",
    "\n",
    "The python code below will then call the endpoint URI and show you the returned response from the model we just deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import traceback\n",
    "\n",
    "# download each image locally\n",
    "non_person_image = requests.get(\n",
    "    \"http://images.cocodataset.org/val2017/000000567197.jpg\"\n",
    ").content\n",
    "with open(\"./non_person_image.jpg\", \"wb\") as handler:\n",
    "    handler.write(non_person_image)\n",
    "\n",
    "person_image = requests.get(\n",
    "    \"http://images.cocodataset.org/val2017/000000274219.jpg\"\n",
    ").content\n",
    "with open(\"./person_image.jpg\", \"wb\") as handler:\n",
    "    handler.write(person_image)\n",
    "\n",
    "# from local image, create encoded versions to send as requests\n",
    "with open(\"./non_person_image.jpg\", mode=\"rb\") as ifile:\n",
    "    non_person_image_encoded = base64.b64encode(ifile.read()).decode(\"ascii\")\n",
    "\n",
    "with open(\"./person_image.jpg\", mode=\"rb\") as ifile:\n",
    "    person_image_encoded = base64.b64encode(ifile.read()).decode(\"ascii\")\n",
    "\n",
    "request_payload = {\n",
    "    \"rows\": [{\"image\": non_person_image_encoded}, {\"image\": person_image_encoded}]\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(f\"URI: {endpoint.scoring_uri}\")\n",
    "    r = requests.post(endpoint.scoring_uri, json=request_payload)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        print(f\"Endpoint returned: {r.json()}\")\n",
    "    else:\n",
    "        print(f\"Endpoint returned: {r.text}\")\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
