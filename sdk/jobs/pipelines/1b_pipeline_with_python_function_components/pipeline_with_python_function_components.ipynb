{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build pipeline with dsl.command_component\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace. [Check this notebook for creating a workspace](/sdk/resources/workspace/workspace.ipynb) \n",
    "- A Compute Cluster. [Check this notebook to create a compute cluster](/sdk/resources/compute/compute.ipynb)\n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](/sdk/README.md#getting-started)\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Connect to your AML workspace from the Python SDK\n",
    "- Define `CommandComponent` using python function and dsl.command_component decorator\n",
    "- Create `Pipeline` using component defined by dsl.command_component\n",
    "\n",
    "**Motivations** - This notebook explains how to define `CommandComponent` via Python function and @dsl.command_component, then use command component to build pipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ml import MLClient, dsl\n",
    "from azure.ml.entities import JobInput\n",
    "import piprunpkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. When an access token is needed, it requests one using multiple identities(`EnvironmentCredential, ManagedIdentityCredential, SharedTokenCacheCredential, VisualStudioCodeCredential, AzureCliCredential, AzurePowerShellCredential`) in turn, stopping when one provides a token.\n",
    "Reference [here](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for more information.\n",
    "\n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "Reference [here](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python) for all available credentials if it does not work for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EnvironmentCredential.get_token failed: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "ManagedIdentityCredential.get_token failed: ManagedIdentityCredential authentication unavailable, no managed identity endpoint found.\n",
      "SharedTokenCacheCredential.get_token failed: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n",
      "VisualStudioCodeCredential.get_token failed: Failed to get Azure user details from Visual Studio Code.\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token('https://management.azure.com/.default')\n",
    "except Exception as ex:\n",
    "    # If exception happens when retrieve token, try exclude the failed credential like this then try again:\n",
    "    # Exclude VSCode credential:\n",
    "    # credential = DefaultAzureCredential(exclude_visual_studio_code_credential=True)\n",
    "    raise Exception(\"Failed to retrieve a token from the included credentials due to the following exception, try to add `exclude_xxx_credential=True` to `DefaultAzureCredential` and try again.\") from ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ml` to get a handle to the required Azure Machine Learning workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: D:\\practice\\azureml-examples\\sdk\\jobs\\.azureml\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x00000257BA71ED90>,\n",
      "         subscription_id=d511f82f-71ba-49a4-8233-d7be8a3650f4,\n",
      "         resource_group_name=mire2etesting,\n",
      "         workspace_name=gewa_ws)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": \"d511f82f-71ba-49a4-8233-d7be8a3650f4\",\n",
    "        \"resource_group\": \"mire2etesting\",\n",
    "        \"workspace_name\": \"gewa_ws\"\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith('<'):\n",
    "        print(\"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <WORKSPACE_NAME> in notebook cell\")\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "        config_path = \"../../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Retrieve or create an Azure Machine Learning compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    ml_client.compute.get(name=cluster_name)\n",
    "except Exception:\n",
    "    print('Creating a new compute target...')\n",
    "    from azure.ml.entities import AmlCompute\n",
    "    compute = AmlCompute(\n",
    "        name=cluster_name,\n",
    "        size=\"Standard_D2_v2\",\n",
    "        max_instances=2\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import components that are defined with python function\n",
    "\n",
    "We defined three sample component using dsl.command_component in [dsl_components.py](dsl_components.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pathlib import Path\n",
      "from random import randint\n",
      "from uuid import uuid4\n",
      "\n",
      "from azure.ml import dsl, ArtifactInput, ArtifactOutput\n",
      "from azure.ml.entities import Environment\n",
      "\n",
      "# init customer environment with conda YAML\n",
      "# the YAML file shall be put under your code folder.\n",
      "conda_env = Environment(\n",
      "    conda_file=Path(__file__).parent / \"conda.yaml\",\n",
      "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\"\n",
      ")\n",
      "\n",
      "\n",
      "@dsl.command_component(\n",
      "    name=\"dsl_train_model\",\n",
      "    display_name=\"Train\",\n",
      "    description=\"A dummy train component defined by dsl component.\",\n",
      "    version=\"0.0.2\",\n",
      "    # specify distribution type if needed\n",
      "    # distribution={'type': 'mpi'},\n",
      "    # specify customer environment, note that azure-ml must be included.\n",
      "    environment=conda_env,\n",
      "    # specify your code folder, default code folder is current file's parent\n",
      "    # code='.'\n",
      ")\n",
      "def train_model(\n",
      "    training_data: ArtifactInput,\n",
      "    max_epochs: int,\n",
      "    model_output: ArtifactOutput,\n",
      "    learning_rate=0.02,\n",
      "):\n",
      "    lines = [\n",
      "        f\"Training data path: {training_data}\",\n",
      "        f\"Max epochs: {max_epochs}\",\n",
      "        f\"Learning rate: {learning_rate}\",\n",
      "        f\"Model output path: {model_output}\",\n",
      "    ]\n",
      "\n",
      "    for line in lines:\n",
      "        print(line)\n",
      "\n",
      "    # Do the train and save the trained model as a file into the output folder.\n",
      "    # Here only output a dummy data for demo.\n",
      "    model = str(uuid4())\n",
      "    (Path(model_output) / \"model\").write_text(model)\n",
      "\n",
      "\n",
      "@dsl.command_component(\n",
      "    name=\"dsl_score_data\",\n",
      "    display_name=\"Score\",\n",
      "    description=\"A dummy score component defined by dsl component.\",\n",
      "    version=\"0.0.1\",\n",
      "    environment=conda_env,\n",
      ")\n",
      "def score_data(\n",
      "    model_input: ArtifactInput,\n",
      "    test_data: ArtifactInput,\n",
      "    score_output: ArtifactOutput,\n",
      "):\n",
      "\n",
      "    lines = [\n",
      "        f\"Model path: {model_input}\",\n",
      "        f\"Test data path: {test_data}\",\n",
      "        f\"Scoring output path: {score_output}\",\n",
      "    ]\n",
      "\n",
      "    for line in lines:\n",
      "        print(line)\n",
      "\n",
      "    # Load the model from input port\n",
      "    # Here only print the model as text since it is a dummy one\n",
      "    model = (Path(model_input) / \"model\").read_text()\n",
      "    print(\"Model:\", model)\n",
      "\n",
      "    # Do scoring with the input model\n",
      "    # Here only print text to output file as demo\n",
      "    (Path(score_output) / \"score\").write_text(\"scored with {}\".format(model))\n",
      "\n",
      "\n",
      "@dsl.command_component(\n",
      "    name=\"dsl_eval_model\",\n",
      "    display_name=\"Evaluate\",\n",
      "    description=\"A dummy evaluate component defined by dsl component.\",\n",
      "    version=\"0.0.1\",\n",
      "    environment=conda_env,\n",
      ")\n",
      "def eval_model(\n",
      "    scoring_result: ArtifactInput,\n",
      "    eval_output: ArtifactOutput,\n",
      "):\n",
      "    lines = [\n",
      "        f\"Scoring result path: {scoring_result}\",\n",
      "        f\"Evaluation output path: {eval_output}\",\n",
      "    ]\n",
      "\n",
      "    for line in lines:\n",
      "        print(line)\n",
      "\n",
      "    # Evaluate the incoming scoring result and output evaluation result.\n",
      "    # Here only output a dummy file for demo.\n",
      "    (Path(eval_output) / \"eval_result\").write_text(\"eval_result\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"src/dsl_components.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'CliV2AnonymousEnvironment' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'CliV2AnonymousEnvironment' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'CliV2AnonymousEnvironment' will not be used for anonymous registration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function train_model at 0x000001DC403E9430>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.dsl_components import train_model, score_data, eval_model\n",
    "\n",
    "print(train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also register dsl component functions to workspace use `ml_client.components.create_or_update()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function train_model at 0x000001DC403E9430>\n"
     ]
    }
   ],
   "source": [
    "print(train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sample pipeline job\n",
    "\n",
    "## 3.1 Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$schema: '{}'\n",
      "type: command\n",
      "inputs:\n",
      "  training_data: ${{parent.inputs.input_data}}\n",
      "  max_epochs: 5\n",
      "  learning_rate: ${{parent.inputs.learning_rate}}\n",
      "outputs: {}\n",
      "command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_train_model\n",
      "  --params --training_data ${{inputs.training_data}} --max_epochs ${{inputs.max_epochs}}\n",
      "  [--learning_rate ${{inputs.learning_rate}}] --model_output ${{outputs.model_output}}\n",
      "code: d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "environment_variables: {}\n",
      "component:\n",
      "  name: dsl_train_model\n",
      "  version: 0.0.2\n",
      "  display_name: Train\n",
      "  description: A dummy train component defined by dsl component.\n",
      "  type: command\n",
      "  inputs:\n",
      "    training_data:\n",
      "      type: uri_folder\n",
      "    max_epochs:\n",
      "      type: integer\n",
      "    learning_rate:\n",
      "      type: number\n",
      "      optional: true\n",
      "      default: '0.02'\n",
      "  outputs:\n",
      "    model_output:\n",
      "      type: uri_folder\n",
      "  command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_train_model\n",
      "    --params --training_data ${{inputs.training_data}} --max_epochs ${{inputs.max_epochs}}\n",
      "    [--learning_rate ${{inputs.learning_rate}}] --model_output ${{outputs.model_output}}\n",
      "  environment:\n",
      "    name: CliV2AnonymousEnvironment\n",
      "    tags: {}\n",
      "    version: e7b1221ce8d02665371bf65bbdfe4d40\n",
      "    image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
      "    conda_file:\n",
      "      name: project_environment\n",
      "      channels:\n",
      "      - defaults\n",
      "      dependencies:\n",
      "      - python=3.8.12\n",
      "      - pip=21.2.2\n",
      "      - pip:\n",
      "        - --extra-index-url=https://azuremlsdktestpypi.azureedge.net/test-sdk-cli-v2\n",
      "        - azure-ml==0.0.60488751\n",
      "  code: azureml:d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "  is_deterministic: true\n",
      "  tags:\n",
      "    codegenBy: dsl.component\n",
      "\n",
      "name: tidy_seal_c3lytbk5vp\n",
      "display_name: pipeline_with_python_function_components\n",
      "description: E2E dummy train-score-eval pipeline with components defined via python\n",
      "  function components\n",
      "type: pipeline\n",
      "inputs:\n",
      "  input_data:\n",
      "    mode: ro_mount\n",
      "    type: uri_file\n",
      "    path: azureml:wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv\n",
      "  test_data:\n",
      "    mode: ro_mount\n",
      "    type: uri_file\n",
      "    path: azureml:wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv\n",
      "  learning_rate: 0\n",
      "outputs:\n",
      "  eval_output: null\n",
      "  model_output: null\n",
      "tags: {}\n",
      "experiment_name: 1b_pipeline_with_python_function_components\n",
      "properties: {}\n",
      "jobs:\n",
      "  train_with_sample_data:\n",
      "    $schema: '{}'\n",
      "    type: command\n",
      "    inputs:\n",
      "      training_data: ${{parent.inputs.input_data}}\n",
      "      max_epochs: 5\n",
      "      learning_rate: ${{parent.inputs.learning_rate}}\n",
      "    outputs:\n",
      "      model_output: ${{parent.outputs.model_output}}\n",
      "    command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_train_model\n",
      "      --params --training_data ${{inputs.training_data}} --max_epochs ${{inputs.max_epochs}}\n",
      "      [--learning_rate ${{inputs.learning_rate}}] --model_output ${{outputs.model_output}}\n",
      "    code: d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "    environment_variables: {}\n",
      "    component:\n",
      "      name: dsl_train_model\n",
      "      version: 0.0.2\n",
      "      display_name: Train\n",
      "      description: A dummy train component defined by dsl component.\n",
      "      type: command\n",
      "      inputs:\n",
      "        training_data:\n",
      "          type: uri_folder\n",
      "        max_epochs:\n",
      "          type: integer\n",
      "        learning_rate:\n",
      "          type: number\n",
      "          optional: true\n",
      "          default: '0.02'\n",
      "      outputs:\n",
      "        model_output:\n",
      "          type: uri_folder\n",
      "      command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_train_model\n",
      "        --params --training_data ${{inputs.training_data}} --max_epochs ${{inputs.max_epochs}}\n",
      "        [--learning_rate ${{inputs.learning_rate}}] --model_output ${{outputs.model_output}}\n",
      "      environment:\n",
      "        name: CliV2AnonymousEnvironment\n",
      "        tags: {}\n",
      "        version: e7b1221ce8d02665371bf65bbdfe4d40\n",
      "        image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
      "        conda_file:\n",
      "          name: project_environment\n",
      "          channels:\n",
      "          - defaults\n",
      "          dependencies:\n",
      "          - python=3.8.12\n",
      "          - pip=21.2.2\n",
      "          - pip:\n",
      "            - --extra-index-url=https://azuremlsdktestpypi.azureedge.net/test-sdk-cli-v2\n",
      "            - azure-ml==0.0.60488751\n",
      "      code: azureml:d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "      is_deterministic: true\n",
      "      tags:\n",
      "        codegenBy: dsl.component\n",
      "  score_with_sample_data:\n",
      "    $schema: '{}'\n",
      "    type: command\n",
      "    inputs:\n",
      "      model_input: ${{parent.jobs.train_with_sample_data.outputs.model_output}}\n",
      "      test_data: ${{parent.inputs.test_data}}\n",
      "    outputs: {}\n",
      "    command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_score_data\n",
      "      --params --model_input ${{inputs.model_input}} --test_data ${{inputs.test_data}}\n",
      "      --score_output ${{outputs.score_output}}\n",
      "    code: d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "    environment_variables: {}\n",
      "    component:\n",
      "      name: dsl_score_data\n",
      "      version: 0.0.1\n",
      "      display_name: Score\n",
      "      description: A dummy score component defined by dsl component.\n",
      "      type: command\n",
      "      inputs:\n",
      "        model_input:\n",
      "          type: uri_folder\n",
      "        test_data:\n",
      "          type: uri_folder\n",
      "      outputs:\n",
      "        score_output:\n",
      "          type: uri_folder\n",
      "      command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_score_data\n",
      "        --params --model_input ${{inputs.model_input}} --test_data ${{inputs.test_data}}\n",
      "        --score_output ${{outputs.score_output}}\n",
      "      environment:\n",
      "        name: CliV2AnonymousEnvironment\n",
      "        tags: {}\n",
      "        version: e7b1221ce8d02665371bf65bbdfe4d40\n",
      "        image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
      "        conda_file:\n",
      "          name: project_environment\n",
      "          channels:\n",
      "          - defaults\n",
      "          dependencies:\n",
      "          - python=3.8.12\n",
      "          - pip=21.2.2\n",
      "          - pip:\n",
      "            - --extra-index-url=https://azuremlsdktestpypi.azureedge.net/test-sdk-cli-v2\n",
      "            - azure-ml==0.0.60488751\n",
      "      code: azureml:d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "      is_deterministic: true\n",
      "      tags:\n",
      "        codegenBy: dsl.component\n",
      "  eval_with_sample_data:\n",
      "    $schema: '{}'\n",
      "    type: command\n",
      "    inputs:\n",
      "      scoring_result: ${{parent.jobs.score_with_sample_data.outputs.score_output}}\n",
      "    outputs:\n",
      "      eval_output: ${{parent.outputs.eval_output}}\n",
      "    command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_eval_model\n",
      "      --params --scoring_result ${{inputs.scoring_result}} --eval_output ${{outputs.eval_output}}\n",
      "    code: d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "    environment_variables: {}\n",
      "    component:\n",
      "      name: dsl_eval_model\n",
      "      version: 0.0.1\n",
      "      display_name: Evaluate\n",
      "      description: A dummy evaluate component defined by dsl component.\n",
      "      type: command\n",
      "      inputs:\n",
      "        scoring_result:\n",
      "          type: uri_folder\n",
      "      outputs:\n",
      "        eval_output:\n",
      "          type: uri_folder\n",
      "      command: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_eval_model\n",
      "        --params --scoring_result ${{inputs.scoring_result}} --eval_output ${{outputs.eval_output}}\n",
      "      environment:\n",
      "        name: CliV2AnonymousEnvironment\n",
      "        tags: {}\n",
      "        version: e7b1221ce8d02665371bf65bbdfe4d40\n",
      "        image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
      "        conda_file:\n",
      "          name: project_environment\n",
      "          channels:\n",
      "          - defaults\n",
      "          dependencies:\n",
      "          - python=3.8.12\n",
      "          - pip=21.2.2\n",
      "          - pip:\n",
      "            - --extra-index-url=https://azuremlsdktestpypi.azureedge.net/test-sdk-cli-v2\n",
      "            - azure-ml==0.0.60488751\n",
      "      code: azureml:d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "      is_deterministic: true\n",
      "      tags:\n",
      "        codegenBy: dsl.component\n",
      "compute: azureml:cpu-cluster\n",
      "settings: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cluster_name = \"cpu-cluster\"\n",
    "# define a pipeline with dsl component\n",
    "@dsl.pipeline(\n",
    "    name='A-training-pipeline',\n",
    "    description='E2E dummy train-score-eval pipeline with components defined via python function components',\n",
    "    default_compute=cluster_name,\n",
    ")\n",
    "def pipeline_with_python_function_components(input_data, test_data, learning_rate):\n",
    "    # Call component obj as function: apply given inputs & parameters to create a node in pipeline\n",
    "    train_with_sample_data = train_model(\n",
    "        training_data=input_data, max_epochs=5, learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    print(train_with_sample_data)\n",
    "\n",
    "    score_with_sample_data = score_data(\n",
    "        model_input=train_with_sample_data.outputs.model_output, test_data=test_data\n",
    "    )\n",
    "\n",
    "    eval_with_sample_data = eval_model(scoring_result=score_with_sample_data.outputs.score_output)\n",
    "\n",
    "    # Return: pipeline outputs\n",
    "    return {\n",
    "        'eval_output': eval_with_sample_data.outputs.eval_output,\n",
    "        'model_output': train_with_sample_data.outputs.model_output,\n",
    "    }\n",
    "\n",
    "pipeline = pipeline_with_python_function_components(\n",
    "    input_data=JobInput(path=\"./data/Titanic.csv\", type=\"uri_file\"), \n",
    "    test_data=JobInput(path=\"./data/Titanic.csv\", type=\"uri_file\"), \n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\practice\\azureml-examples\\sdk\\jobs\\pipelines\\1b_pipeline_with_python_function_components\n",
      "DEBUG:docker.utils.config:Trying paths: ['C:\\\\Users\\\\xiaopwan\\\\.docker\\\\config.json', 'C:\\\\Users\\\\xiaopwan\\\\.dockercfg']\n",
      "DEBUG:docker.utils.config:Found file at path: C:\\Users\\xiaopwan\\.docker\\config.json\n",
      "DEBUG:docker.auth:Found 'auths' section\n",
      "DEBUG:docker.auth:Auth data for viennadroptest.azurecr.io is absent. Client might be using a credentials store instead.\n",
      "DEBUG:docker.auth:Found 'credsStore' section\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/images/cliv2anonymousenvironment:e7b1221ce8d02665371bf65bbdfe4d40/json HTTP/1.1\" 200 None\n",
      "INFO:piprunpkg.image_builder:image with name cliv2anonymousenvironment:e7b1221ce8d02665371bf65bbdfe4d40 already exist, skip image building...\n",
      "DEBUG:executor:docker image: cliv2anonymousenvironment:e7b1221ce8d02665371bf65bbdfe4d40\n",
      "DEBUG:docker.utils.config:Trying paths: ['C:\\\\Users\\\\xiaopwan\\\\.docker\\\\config.json', 'C:\\\\Users\\\\xiaopwan\\\\.dockercfg']\n",
      "DEBUG:docker.utils.config:Found file at path: C:\\Users\\xiaopwan\\.docker\\config.json\n",
      "DEBUG:docker.auth:Found 'auths' section\n",
      "DEBUG:docker.auth:Auth data for viennadroptest.azurecr.io is absent. Client might be using a credentials store instead.\n",
      "DEBUG:docker.auth:Found 'credsStore' section\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/1.1\" 200 None\n",
      "DEBUG:executor:working dir: d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src\n",
      "DEBUG:executor:volumn_mapping: {'wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv': '/train_with_sample_data/inputs/1/Titanic.csv', 'd:\\\\practice\\\\azureml-examples\\\\sdk\\\\jobs\\\\pipelines\\\\1b_pipeline_with_python_function_components\\\\./local-run-output\\\\train_with_sample_data': '/train_with_sample_data/outputs/1'}\n",
      "DEBUG:executor:template_params: {'inputs_training_data': '/train_with_sample_data/inputs/1/Titanic.csv', 'inputs_max_epochs': 5, 'inputs_learning_rate': 0.1, 'outputs_model_output': '/train_with_sample_data/outputs/1'}\n",
      "INFO:executor:Command run in docker: python -m azure.ml.dsl.executor --file dsl_components.py --name dsl_train_model --params --training_data /train_with_sample_data/inputs/1/Titanic.csv --max_epochs 5 --model_output /train_with_sample_data/outputs/1 --learning_rate 0.1\n",
      "DEBUG:executor:dirname: d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components, basename: src\n",
      "DEBUG:executor:binding d:/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/src to /train_with_sample_data/cwd/src in volumes\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.41/containers/train_with_sample_data/json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"DELETE /v1.41/containers/0019e3386aed9b0191b9f8d46060f12345f37f7a1a15d6edc4efbed01e07c722?v=False&link=False&force=False HTTP/1.1\" 204 0\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"POST /v1.41/containers/create?name=train_with_sample_data HTTP/1.1\" 400 None\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "400 Client Error for http+docker://localnpipe/v1.41/containers/create?name=train_with_sample_data: Bad Request (\"invalid mount config for type \"bind\": invalid mount path: 'wasbs:/demo@dprepdata.blob.core.windows.net/Titanic.csv' mount path must be absolute\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\api\\client.py:268\u001b[0m, in \u001b[0;36mAPIClient._raise_for_status\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=266'>267</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=267'>268</a>\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=268'>269</a>\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\requests\\models.py:943\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/requests/models.py?line=941'>942</a>\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/requests/models.py?line=942'>943</a>\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http+docker://localnpipe/v1.41/containers/create?name=train_with_sample_data",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\practice\\azureml-examples\\sdk\\jobs\\pipelines\\1b_pipeline_with_python_function_components\\pipeline_with_python_function_components.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/pipeline_with_python_function_components.ipynb#ch0000020?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39mgetcwd())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/pipeline_with_python_function_components.ipynb#ch0000020?line=5'>6</a>\u001b[0m output_root_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./local-run-output\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/pipeline_with_python_function_components.ipynb#ch0000020?line=6'>7</a>\u001b[0m run_pipeline(output_root_dir\u001b[39m=\u001b[39;49moutput_root_dir, pipeline_job\u001b[39m=\u001b[39;49mpipeline)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\piprunpkg\\executor.py:64\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m(output_root_dir, pipeline_job, ml_client, run_mode, curated_env_default_run_mode, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=60'>61</a>\u001b[0m \u001b[39melif\u001b[39;00m run_mode \u001b[39m==\u001b[39m PipelineJobLocalRunMode\u001b[39m.\u001b[39mDOCKER_BUILD:\n\u001b[0;32m     <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=61'>62</a>\u001b[0m     \u001b[39m# build image using job component environment\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=62'>63</a>\u001b[0m     image_name \u001b[39m=\u001b[39m image_builder\u001b[39m.\u001b[39mbuild_container_image(pipeline_job\u001b[39m.\u001b[39mname, node\u001b[39m.\u001b[39m_job)\n\u001b[1;32m---> <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=63'>64</a>\u001b[0m     success \u001b[39m=\u001b[39m _run_with_docker_image(image_name, node)\n\u001b[0;32m     <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=64'>65</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=65'>66</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound unsupported run mode: \u001b[39m\u001b[39m{\u001b[39;00mrun_mode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\piprunpkg\\executor.py:193\u001b[0m, in \u001b[0;36m_run_with_docker_image\u001b[1;34m(docker_image_name, node, docker_client)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=190'>191</a>\u001b[0m \u001b[39mif\u001b[39;00m working_dir_docker \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=191'>192</a>\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mworking_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m working_dir_docker\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=192'>193</a>\u001b[0m container \u001b[39m=\u001b[39m docker_client\u001b[39m.\u001b[39mcontainers\u001b[39m.\u001b[39mrun(docker_image_name, detach\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mounts\u001b[39m=\u001b[39mmnts, name\u001b[39m=\u001b[39mcontainer_name, command\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m/bin/bash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-c\u001b[39m\u001b[39m\"\u001b[39m, command_in_docker], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=193'>194</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m##########################################\u001b[39m\u001b[39m{\u001b[39;00mcontainer_name\u001b[39m}\u001b[39;00m\u001b[39m################################################\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/piprunpkg/executor.py?line=194'>195</a>\u001b[0m log_gen \u001b[39m=\u001b[39m container\u001b[39m.\u001b[39mlogs(stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\models\\containers.py:819\u001b[0m, in \u001b[0;36mContainerCollection.run\u001b[1;34m(self, image, command, stdout, stderr, remove, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=812'>813</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=813'>814</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mThe options \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnetwork\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnetwork_mode\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m can not be used \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=814'>815</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtogether.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=815'>816</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=817'>818</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=818'>819</a>\u001b[0m     container \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate(image\u001b[39m=\u001b[39mimage, command\u001b[39m=\u001b[39mcommand,\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=819'>820</a>\u001b[0m                             detach\u001b[39m=\u001b[39mdetach, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=820'>821</a>\u001b[0m \u001b[39mexcept\u001b[39;00m ImageNotFound:\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=821'>822</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mimages\u001b[39m.\u001b[39mpull(image, platform\u001b[39m=\u001b[39mplatform)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\models\\containers.py:878\u001b[0m, in \u001b[0;36mContainerCollection.create\u001b[1;34m(self, image, command, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=875'>876</a>\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39m_version\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=876'>877</a>\u001b[0m create_kwargs \u001b[39m=\u001b[39m _create_container_args(kwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=877'>878</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mcreate_container(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcreate_kwargs)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/models/containers.py?line=878'>879</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(resp[\u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\api\\container.py:428\u001b[0m, in \u001b[0;36mContainerApiMixin.create_container\u001b[1;34m(self, image, command, hostname, user, detach, stdin_open, tty, ports, environment, volumes, network_disabled, name, entrypoint, working_dir, domainname, host_config, mac_address, labels, stop_signal, networking_config, healthcheck, stop_timeout, runtime, use_config_proxy)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=415'>416</a>\u001b[0m     environment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proxy_configs\u001b[39m.\u001b[39minject_proxy_environment(\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=416'>417</a>\u001b[0m         environment\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=417'>418</a>\u001b[0m     ) \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=419'>420</a>\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_container_config(\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=420'>421</a>\u001b[0m     image, command, hostname, user, detach, stdin_open, tty,\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=421'>422</a>\u001b[0m     ports, environment, volumes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=425'>426</a>\u001b[0m     stop_timeout, runtime\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=426'>427</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=427'>428</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_container_from_config(config, name)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\api\\container.py:439\u001b[0m, in \u001b[0;36mContainerApiMixin.create_container_from_config\u001b[1;34m(self, config, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=434'>435</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=435'>436</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: name\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=436'>437</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=437'>438</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_json(u, data\u001b[39m=\u001b[39mconfig, params\u001b[39m=\u001b[39mparams)\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/docker/api/container.py?line=438'>439</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_result(res, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\api\\client.py:274\u001b[0m, in \u001b[0;36mAPIClient._result\u001b[1;34m(self, response, json, binary)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=271'>272</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_result\u001b[39m(\u001b[39mself\u001b[39m, response, json\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=272'>273</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (json \u001b[39mand\u001b[39;00m binary)\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=273'>274</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_for_status(response)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=275'>276</a>\u001b[0m     \u001b[39mif\u001b[39;00m json:\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=276'>277</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\api\\client.py:270\u001b[0m, in \u001b[0;36mAPIClient._raise_for_status\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=267'>268</a>\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=268'>269</a>\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/docker/api/client.py?line=269'>270</a>\u001b[0m     \u001b[39mraise\u001b[39;00m create_api_error_from_http_exception(e)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\docker\\errors.py:31\u001b[0m, in \u001b[0;36mcreate_api_error_from_http_exception\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python39/lib/site-packages/docker/errors.py?line=28'>29</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Python39/lib/site-packages/docker/errors.py?line=29'>30</a>\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m NotFound\n\u001b[1;32m---> <a href='file:///c%3A/Python39/lib/site-packages/docker/errors.py?line=30'>31</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mcls\u001b[39m(e, response\u001b[39m=\u001b[39mresponse, explanation\u001b[39m=\u001b[39mexplanation)\n",
      "\u001b[1;31mAPIError\u001b[0m: 400 Client Error for http+docker://localnpipe/v1.41/containers/create?name=train_with_sample_data: Bad Request (\"invalid mount config for type \"bind\": invalid mount path: 'wasbs:/demo@dprepdata.blob.core.windows.net/Titanic.csv' mount path must be absolute\")"
     ]
    }
   ],
   "source": [
    "from piprunpkg import run_pipeline\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "print(os.getcwd())\n",
    "output_root_dir = \"./local-run-output\"\n",
    "run_pipeline(output_root_dir=output_root_dir, pipeline_job=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job link: https://ml.azure.com/runs/good_dog_x3gl8dcyy7?wsid=/subscriptions/d511f82f-71ba-49a4-8233-d7be8a3650f4/resourcegroups/mire2etesting/workspaces/gewa_ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_samples</td><td>good_dog_x3gl8dcyy7</td><td>pipeline</td><td>Failed</td><td><a href=\"https://ml.azure.com/runs/good_dog_x3gl8dcyy7?wsid=/subscriptions/d511f82f-71ba-49a4-8233-d7be8a3650f4/resourcegroups/mire2etesting/workspaces/gewa_ws&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'input_data': <azure.ml.entities._job.pipeline._io.PipelineInput object at 0x00000257BCA788B0>, 'test_data': <azure.ml.entities._job.pipeline._io.PipelineInput object at 0x00000257BCA787F0>, 'learning_rate': <azure.ml.entities._job.pipeline._io.PipelineInput object at 0x00000257BCA786A0>}, 'outputs': {'eval_output': <azure.ml.entities._job.pipeline._io.PipelineOutput object at 0x00000257BCA78670>, 'model_output': <azure.ml.entities._job.pipeline._io.PipelineOutput object at 0x00000257BCA78730>}, 'component': _PipelineComponent({'components': {}, 'auto_increment_version': False, 'is_anonymous': True, 'name': '2a4303ad-56ea-485f-870a-5e57b83aaf0b', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x00000257BCA78610>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline_component', 'display_name': None, 'is_deterministic': True, 'inputs': {}, 'outputs': {}, 'yaml_str': None, 'other_parameter': {}, 'func': <function [component] None at 0x00000257BC995430>}), 'display_name': 'good_dog_x3gl8dcyy7', 'type': 'pipeline', 'status': 'Failed', 'log_files': None, 'name': 'good_dog_x3gl8dcyy7', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/Azure/azureml-examples', 'mlflow.source.git.branch': 'april-sdk-preview', 'mlflow.source.git.commit': 'da9e9cb19816a7ebf1d92a6b1323cd1951c79b86', 'azureml.git.dirty': 'False', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDKv2', 'runType': 'HTTP', 'azureml.parameters': '{\"learning_rate\":\"0.1\"}', 'azureml.continue_on_step_failure': 'False', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/d511f82f-71ba-49a4-8233-d7be8a3650f4/resourceGroups/mire2etesting/providers/Microsoft.MachineLearningServices/workspaces/gewa_ws/jobs/good_dog_x3gl8dcyy7', 'base_path': './', 'creation_context': <azure.ml._restclient.v2022_02_01_preview.models._models_py3.SystemData object at 0x00000257BCA838E0>, 'serialize': <msrest.serialization.Serializer object at 0x00000257BCA787C0>, 'experiment_name': 'pipeline_samples', 'compute': 'cpu-cluster', 'services': {'Tracking': <azure.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x00000257BCA836A0>, 'Studio': <azure.ml._restclient.v2022_02_01_preview.models._models_py3.JobService object at 0x00000257BCA83280>}, 'jobs': {'train_with_sample_data': {'code': {}, 'command': {}}, 'score_with_sample_data': {'code': {}, 'command': {}}, 'eval_with_sample_data': {'code': {}, 'command': {}}}, 'settings': <azure.ml.entities._job.pipeline.pipeline_job_settings.PipelineJobSettings object at 0x00000257BCA83670>, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(pipeline, experiment_name=\"pipeline_samples\")\n",
    "print(f'Job link: {pipeline_job.services[\"Studio\"].endpoint}')\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: good_dog_x3gl8dcyy7\n",
      "Web View: https://ml.azure.com/runs/good_dog_x3gl8dcyy7?wsid=/subscriptions/d511f82f-71ba-49a4-8233-d7be8a3650f4/resourcegroups/mire2etesting/workspaces/gewa_ws\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: good_dog_x3gl8dcyy7\n",
      "Web View: https://ml.azure.com/runs/good_dog_x3gl8dcyy7?wsid=/subscriptions/d511f82f-71ba-49a4-8233-d7be8a3650f4/resourcegroups/mire2etesting/workspaces/gewa_ws\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Exception : \n \"Detailed error not set on the Run. Please check the logs for details.\" ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\practice\\azureml-examples\\sdk\\jobs\\pipelines\\1b_pipeline_with_python_function_components\\pipeline_with_python_function_components.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/pipeline_with_python_function_components.ipynb#ch0000018?line=0'>1</a>\u001b[0m \u001b[39m# Wait until the job completes\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/practice/azureml-examples/sdk/jobs/pipelines/1b_pipeline_with_python_function_components/pipeline_with_python_function_components.ipynb#ch0000018?line=1'>2</a>\u001b[0m ml_client\u001b[39m.\u001b[39;49mjobs\u001b[39m.\u001b[39;49mstream(pipeline_job\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\azure\\ml\\_telemetry\\activity.py:160\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_telemetry/activity.py?line=156'>157</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_telemetry/activity.py?line=157'>158</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_telemetry/activity.py?line=158'>159</a>\u001b[0m     \u001b[39mwith\u001b[39;00m log_activity(logger, activity_name \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_telemetry/activity.py?line=159'>160</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\azure\\ml\\_operations\\job_operations.py:349\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_operations.py?line=345'>346</a>\u001b[0m job_object \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_job(name)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_operations.py?line=347'>348</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_operations.py?line=348'>349</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream_logs_until_completion(\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_operations.py?line=349'>350</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runs_operations, job_object, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_all_operations\u001b[39m.\u001b[39;49mall_operations[AzureMLResourceType\u001b[39m.\u001b[39;49mDATASTORE]\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_operations.py?line=350'>351</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_operations.py?line=351'>352</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_operations.py?line=352'>353</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\azure\\ml\\_operations\\job_ops_helper.py:283\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_ops_helper.py?line=280'>281</a>\u001b[0m         file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_ops_helper.py?line=281'>282</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_ops_helper.py?line=282'>283</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mException : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(json\u001b[39m.\u001b[39mdumps(error, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)))\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_ops_helper.py?line=284'>285</a>\u001b[0m file_handle\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Python39/lib/site-packages/azure/ml/_operations/job_ops_helper.py?line=285'>286</a>\u001b[0m file_handle\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;31mException\u001b[0m: Exception : \n \"Detailed error not set on the Run. Please check the logs for details.\" "
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of running a pipeline job [here](/sdk/jobs/pipelines/)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Create pipeline with dsl.command_component"
  },
  "interpreter": {
   "hash": "3e9e0e270b75c5e6da2e22113ba4f77b864d68f95da6601809c29e46c73ae6bb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
