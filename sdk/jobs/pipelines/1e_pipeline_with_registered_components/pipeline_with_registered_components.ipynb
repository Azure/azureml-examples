{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build pipeline with components\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace. [Check this notebook for creating a workspace](/sdk/resources/workspace/workspace.ipynb) \n",
    "- A Compute Cluster. [Check this notebook to create a compute cluster](/sdk/resources/compute/compute.ipynb)\n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](/sdk/README.md#getting-started)\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Connect to your AML workspace from the Python SDK\n",
    "- Define `CommandComponent` using YAML, `dsl.command_component`\n",
    "- Create components into workspace\n",
    "- Create `Pipeline` using registered components.\n",
    "\n",
    "**Motivations** - This notebook explains how to different method to create components via SDK then use these components to build pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.ml import MLClient, dsl, ArtifactInput, ArtifactOutput\n",
    "from azure.ml.entities import JobInput, load_component\n",
    "from azure.ml._constants import AssetTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. When an access token is needed, it requests one using multiple identities(`EnvironmentCredential, ManagedIdentityCredential, SharedTokenCacheCredential, VisualStudioCodeCredential, AzureCliCredential, AzurePowerShellCredential`) in turn, stopping when one provides a token.\n",
    "Reference [here](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for more information.\n",
    "\n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "Reference [here](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python) for all available credentials if it does not work for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token('https://management.azure.com/.default')\n",
    "except Exception as ex:\n",
    "    # If exception happens when retrieve token, try exclude the failed credential like this then try again:\n",
    "    # Exclude VSCode credential:\n",
    "    # credential = DefaultAzureCredential(exclude_visual_studio_code_credential=True)\n",
    "    raise Exception(\"Failed to retrieve a token from the included credentials due to the following exception, try to add `exclude_xxx_credential=True` to `DefaultAzureCredential` and try again.\") from ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. More advanced connection methods can be found [here](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get a handle to the workspace\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": \"<SUBSCRIPTION_ID>\",\n",
    "        \"resource_group\": \"<RESOURCE_GROUP>\",\n",
    "        \"workspace_name\": \"<WORKSPACE_NAME>\"\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith('<'):\n",
    "        print(\"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <WORKSPACE_NAME> in notebook cell\")\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "        config_path = \"../../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define and create components into workspace\n",
    "## 2.1 Load components definition from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '.'\n",
    "train_model = load_component(yaml_file=parent_dir + \"/train_model.yml\")\n",
    "score_data = load_component(yaml_file=parent_dir + \"/score_data.yml\")\n",
    "# print the component as yaml\n",
    "print(score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Register components into workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create component into workspace \n",
    "train_model = ml_client.components.create_or_update(train_model)\n",
    "score_data = ml_client.components.create_or_update(score_data)\n",
    "\n",
    "# print the component as yaml\n",
    "# NOTE: resources like code, environment are resolved to remote arm id.\n",
    "print(score_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a created component from workspace\n",
    "score_data = ml_client.components.get(name=score_data.name, version=score_data.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create components use dsl.component\n",
    "\n",
    "**Note:** dsl.component function need to write as separate py file, which will be included in component code snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile eval_src/dsl_component.py\n",
    "\n",
    "from azure.ml import dsl, ArtifactInput, ArtifactOutput\n",
    "\n",
    "@dsl.command_component(\n",
    "    name=\"eval_model\",\n",
    "    display_name=\"Eval Model\",\n",
    "    description=\"A dummy eval component defined by dsl component.\",\n",
    "    version=\"0.0.5\",\n",
    ")\n",
    "def eval_model(\n",
    "    scoring_result: ArtifactInput,\n",
    "    eval_output: ArtifactOutput,\n",
    "):\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    print (\"hello evaluation world...\")\n",
    "\n",
    "    lines = [\n",
    "        f'Scoring result path: {scoring_result}',\n",
    "        f'Evaluation output path: {eval_output}',\n",
    "    ]\n",
    "\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "\n",
    "    # Evaluate the incoming scoring result and output evaluation result.\n",
    "    # Here only output a dummy file for demo.\n",
    "    curtime = datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")\n",
    "    eval_msg = f\"Eval done at {curtime}\\n\"\n",
    "    (Path(eval_output) / 'eval_result.txt').write_text(eval_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# in notebook auto reload the component if any code change\n",
    "from eval_src.dsl_component import eval_model\n",
    "\n",
    "print(type(eval_model))\n",
    "help(eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # try get back the dsl.command_component defined component\n",
    "    eval_model = ml_client.components.get(name=\"eval_model\", version=\"0.0.5\")\n",
    "except:\n",
    "    # create if not exists\n",
    "    eval_model = ml_client.components.create_or_update(eval_model)\n",
    "\n",
    "print(eval_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sample pipeline job\n",
    "## 3.1 Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "@dsl.pipeline(\n",
    "    default_compute=\"cpu-cluster\",\n",
    "    description=\"E2E dummy train-score-eval pipeline with registered components\",\n",
    ")\n",
    "def pipeline_with_registered_components(\n",
    "    training_input,\n",
    "    test_input,\n",
    "    training_max_epochs=20,\n",
    "    training_learning_rate=1.8,\n",
    "    learning_rate_schedule=\"time-based\",\n",
    "):\n",
    "    # Call component obj as function: apply given inputs & parameters to create a node in pipeline\n",
    "    train_with_sample_data = train_model(\n",
    "        training_data=training_input,\n",
    "        max_epochs=training_max_epochs,\n",
    "        learning_rate=training_learning_rate,\n",
    "        learning_rate_schedule=learning_rate_schedule,\n",
    "    )\n",
    "\n",
    "    score_with_sample_data = score_data(\n",
    "        model_input=train_with_sample_data.outputs.model_output, \n",
    "        test_data=test_input\n",
    "    )\n",
    "    score_with_sample_data.outputs.score_output.mode = \"upload\"\n",
    "\n",
    "    eval_with_sample_data = eval_model(\n",
    "        scoring_result=score_with_sample_data.outputs.score_output\n",
    "    )\n",
    "\n",
    "    # Return: pipeline outputs\n",
    "    return {\n",
    "        \"trained_model\": train_with_sample_data.outputs.model_output,\n",
    "        \"scored_data\": score_with_sample_data.outputs.score_output,\n",
    "        \"evaluation_report\": eval_with_sample_data.outputs.eval_output,\n",
    "    }\n",
    "\n",
    "pipeline = pipeline_with_registered_components(\n",
    "    training_input = JobInput(type=AssetTypes.URI_FOLDER, path=parent_dir + \"/data/\"),\n",
    "    test_input = JobInput(type=AssetTypes.URI_FOLDER, path=parent_dir + \"/data/\"),\n",
    "    training_max_epochs=20,\n",
    "    training_learning_rate=1.8,\n",
    "    learning_rate_schedule=\"time-based\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(pipeline, experiment_name=\"pipeline_samples\")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of running a pipeline job [here](/sdk/jobs/pipelines/)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Register component and then use these components to build pipeline"
  },
  "interpreter": {
   "hash": "3e9e0e270b75c5e6da2e22113ba4f77b864d68f95da6601809c29e46c73ae6bb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
