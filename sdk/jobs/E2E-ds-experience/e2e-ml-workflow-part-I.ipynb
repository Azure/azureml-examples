{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbce700",
   "metadata": {},
   "source": [
    "# Using Azure ML Pipelines to Productionize E2E ML Workflows: Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe4d4d",
   "metadata": {},
   "source": [
    "**Learning Objectives** \n",
    "By the end of this tutorial, you should be able to use Azure Machine Learning (AML) to productionize your ML project. This means you will be able to:\n",
    "\n",
    "    - Connect to your AML workspace from the Python SDK\n",
    "    - Create AML Datasets\n",
    "    - Create AML Components from our Python code\n",
    "    - Create, validate and run AML pipelines  \n",
    "    - Deploy the newly-trained model as an endpoint\n",
    "    - Call the AML endpoint for inferencing\n",
    "    \n",
    "After part II, you should be able to:\n",
    "\n",
    "    - Load pre-registered Components\n",
    "    - Use multi-note clusters for data processing and model training\n",
    "    - Use AML for Hyper Parameter Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113b218",
   "metadata": {},
   "source": [
    "**Motivations** \n",
    "This tutorial is intended to introduce AML to data scientists who want to scale up or publish their ML projects. By completing a familiar end-to-end project, which starts by loading the data and ends by creating and calling an online inference endpoint, the user should become familiar with the core concepts of AML and their most common usage. Each step of this tutorial can be modified or performed in other ways that might have security or scalability advantages. We will cover some of those in the Part II of this tutorial, however, we suggest the reader use the provide links in each section to learn more on each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00c3c3",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "In order to benefit from this tutorial, you need to have:\n",
    "- basic understanding of Machine Learning projects workflow\n",
    "- an Azure subscription. If you don't have an Azure subscription, [create a free account](https://aka.ms/AMLFree) before you begin.\n",
    "- a working AML workspace. A workspace can be created via Azure Portal, Azure CLI, or Python SDK. [Read more](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python).\n",
    "- a Python environmnet\n",
    "- installed Azure Machine Learning Python SDK v2\n",
    "```python\n",
    "pip install azure-ml==0.0.139 --extra-index-url  https://azuremlsdktestpypi.azureedge.net/sdk-cli-v2\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28babe15",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this tutorial, we will create an AML pipeline to train a model for credit default prediction. The pipeline handles the data preparation, training and registering the trained model. Image below depicts the visual representation of the pipeline. \n",
    "<div>\n",
    "<img src=\"media/pipeline-overview.jpg\" style=\"width:60%\"/img>\n",
    "    <img src=\"media/metrics.jpg\" style=\"width:60%\"/>\n",
    "</div>\n",
    "Once the training is finished, we publish the model as an endpoint and evaluate the inferencing endpoint, by invoking it with sample queries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab7fbf",
   "metadata": {},
   "source": [
    "# 2. Set Up the Pipeline Resources\n",
    "Azure ML Framework can be used from CLI, Python SDK, or GUI. In this example we will use the Python SDK as indicated in the prerequisites section. The SDK will give us access to all the required packages. \n",
    "\n",
    "In this tutorial, we try to postpone the imports of the required packages to the section using that package, so that the user better understand the role of each entity. If you decide to run the cells out of the presented order, make sure you have imported the packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201c72d",
   "metadata": {},
   "source": [
    "## 2.1. Connect to the Workspace\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. \n",
    "Here we use the workspace identifier parameters to connect to the workspace. We use the default *interactive authentication* for this tutorial. More advanced connection methods can be found here [sdkv1link](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.authentication?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle to the workspace\n",
    "from azure.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e8f61",
   "metadata": {},
   "source": [
    "In the next cell, we enter the *Subscription ID*, *Resource Group* name and *Workspace* name. The result is a handler to the workspace that we can use to manage other resources and jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e25c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    InteractiveBrowserCredential(), \n",
    "    subscription_id = '<SUBSCRIPTION_ID>', \n",
    "    resource_group = '<RESOURCE_GROUP>', \n",
    "    workspace = '<AML_WORKSPACE_NAME>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402c1c7",
   "metadata": {},
   "source": [
    "## 2.2. Prepare Data\n",
    "The data we use for out training, is normally in one of the locations below:\n",
    "- Local Machine\n",
    "- Web\n",
    "- Big Data Storage services (e.g. Azure Blob, Azure Data Lake Storage, SQL... )\n",
    "\n",
    "AML uses a Dataset object to consume data. A Dataset object is a pointer to a data storage service and a path. In the section below, we consume some data from web url as one example. Datasets from other sources can be created as well. \n",
    "For more information on various sources of data and their consumption, please check [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Dataset\n",
    "\n",
    "web_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "\n",
    "credit_dataset = Dataset(\n",
    "    name=\"creditcard_defaults\",\n",
    "    paths=[dict(file=web_path)],\n",
    "    description=\"Dataset for credit card defaults\",\n",
    "    tags={'source_type':'web',\n",
    "          'source':'UCI ML Repo'\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e2d7a",
   "metadata": {},
   "source": [
    "This dataset object is ready to be consumed as a one-off object by the pipeline that we will define. We can also register the dataset to our workspace. This will enable us to:\n",
    "- reuse and share the dataset in future pipelines\n",
    "- use versions to track the modification to the dataset\n",
    "- use the dataset from AML designer which is AML's GUI for pipeline authoring\n",
    "\n",
    "Since this is the first time in this tutorial that we are making a call to the workspace, running the next cell should direct you to AML's web authentication page. Please login with your Azure credentials. Once you are logged in, you should receive a message that informs you that the authentication is complete and you can close the authentication window. You should then see the dataset registration completion below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_dataset = ml_client.create_or_update(credit_dataset)\n",
    "print(f\"Dataset with name \\\"{credit_dataset.name}\\\" was registered to workspace, the dataset version is {credit_dataset.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b4cb1",
   "metadata": {},
   "source": [
    "In future, one can fetch the same dataset from the workspace using \n",
    "```python\n",
    "credit_dataset = ml_client.datasets.get(\"<DATASET NAME>\", version='<VERSION>')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323154d5",
   "metadata": {},
   "source": [
    "## 2.3. Create a Compute Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b6b5a",
   "metadata": {},
   "source": [
    "Each step of an AML pipelines can use a different compute resource for running the specific job of that step. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like spark. \n",
    "\n",
    "In this section, we provision a Linux compute cluster for our tasks in this tutorial. Let's start by listing the available VM sizes available for use. You can check [here](https://azure.microsoft.com/en-ca/pricing/details/machine-learning/) for a full list on VM sizes and prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb72a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute, Compute\n",
    "import pandas as pd\n",
    "# Let's have a peak at the most important properties\n",
    "VM_dict = {\n",
    "    vm.name: {\n",
    "        \"family\": vm.family,\n",
    "        \"hdd_size\": vm.os_vhd_size_mb,\n",
    "        \"memory\": vm.memory_gb,\n",
    "        \"cpus\": vm.v_cp_us,\n",
    "        \"gpus\": vm.gpus,\n",
    "    }\n",
    "    for vm in ml_client.compute.list_sizes()\n",
    "}\n",
    "VM_df = pd.DataFrame.from_dict(VM_dict, orient='index')\n",
    "\n",
    "# Let's take a look at one VM Family, you can change the code and explore more\n",
    "VM_df[VM_df['family']=='standardDSv2Family']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9d632",
   "metadata": {},
   "source": [
    "For this tutorial we only need a basic cluster, let's pick *Standard_DS2_v2* and create am AML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the AML compute object with the intended parameters\n",
    "cluster_basic = AmlCompute(\n",
    "    # Name assigned to the compute cluster\n",
    "    name=\"cpu-cluster\",\n",
    "    \n",
    "    # AML Compte is AML's on-demand VM service\n",
    "    type=\"amlcompute\",\n",
    "   \n",
    "    # VM Family\n",
    "    size=\"Standard_DS2_v2\",\n",
    "    \n",
    "    # Minimum running nodes when there is no job running\n",
    "    min_instances=0,\n",
    "    \n",
    "    # nodes in cluster\n",
    "    max_instances=2,\n",
    "    \n",
    "    # How many seconds will the node running after the job termination\n",
    "    idle_time_before_scale_down=120,\n",
    "    \n",
    "    # dedicated or LowPriority. The latter is cheaper but there is a chance of job termination \n",
    "    tier='dedicated'\n",
    ")\n",
    "\n",
    "# Now, we pass the object to clinet's create_or_update method\n",
    "ml_client.begin_create_or_update(cluster_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccf792",
   "metadata": {},
   "source": [
    "## 2.4. Create a Job Environment\n",
    "So far, in the requirements section, we have created a development environment on our development machine. AML needs to know what environment to use for each step of the pipeline. We can use any published docker image as is, or add or required dependencies to the image.In this example, we create a conda environment for our jobs, using a [conda yaml file](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually) and add it to an Ubuntu image in Microsoft Container Registry. For more information on AML environments and Azure Container Registries, please check [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/concept-environments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8892ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d12731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.7\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - azureml-defaults==1.38.0\n",
    "    - azureml-mlflow==1.38.0\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - joblib==1.0.1\n",
    "    - xlrd==2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2959d0",
   "metadata": {},
   "source": [
    "Here we added some common packages we use in our pipelie:\n",
    "\n",
    "    - numpy=1.21.2\n",
    "    - pip=21.2.4\n",
    "    - scikit-learn=0.24.2\n",
    "    - scipy=1.7.1\n",
    "    - pandas>=1.1,<1.2\n",
    "    - joblib\n",
    "    - xlrd\n",
    "together with some AML specific packages:\n",
    "\n",
    "    - azureml-defaults==1.38.0\n",
    "    - azureml-mlflow==1.38.0\n",
    "    \n",
    "these AML packages are not mandatory to run AML jobs, however, adding those will let us interact with AML for logging metrics and registering models, all inside the AML job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ed22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Environment\n",
    "custom_env_name = 'aml-scikit-learn'\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name = custom_env_name,\n",
    "    description = 'Custom environment for Credit Card Defaults pipeline',\n",
    "    tags = {'scikit-learn':'0.24.2',\n",
    "           'azureml-defaults':'1.38.0'},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec44c39",
   "metadata": {},
   "source": [
    "# 3. Create the Training Pipeline\n",
    "AML pipelines are reusable ML workflows that usually consist of several components. AML defines these components in yaml files. One can directly write the yaml file or use the ComponentMethod to create a component. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e89b69",
   "metadata": {},
   "source": [
    "## 3.1. Create or Load Components\n",
    "Now that we have our workspace, compute and input data ready, let's work on the individual steps of our pipeline. \n",
    "\n",
    "### 3.1.1 Data Preparation Component\n",
    "\n",
    "Let's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_prep.py* python file.\n",
    "\n",
    "Let's first create a source folder for the data_prep component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1cb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_prep_src_dir = \"./components/data_prep\"\n",
    "os.makedirs(data_prep_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e39f3",
   "metadata": {},
   "source": [
    "This script performs the simple task of splitting the data into train and test datasets. \n",
    "AML mounts datasets as folders to the computes, therefore, we created an auxiliary `select_first_file` function to access the data file inside the mounted input folder.\n",
    "mlflow can be used to log the parameters and metrics during our pipeline run. A detailed guide on AML logging is available [here](https://github.com/Azure/azureml-examples/blob/sdk-preview/notebooks/mlflow/mlflow-v1-comparison.ipynb). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e23e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {data_prep_src_dir}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from azureml.core import Run\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# input and output arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data\")\n",
    "parser.add_argument(\"--split_ratio\", type=float, required=False, default=0.25)\n",
    "parser.add_argument(\"--train_data\")\n",
    "parser.add_argument(\"--test_data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "print(\"input data:\", select_first_file(args.data))\n",
    "\n",
    "credit_df = pd.read_excel(select_first_file(args.data), header=1, index_col=0)\n",
    "\n",
    "mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
    "mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
    "\n",
    "credit_train_df, credit_test_df = train_test_split(\n",
    "    credit_df,\n",
    "    test_size=args.split_ratio,\n",
    ")\n",
    "\n",
    "# output paths are mounted as folder, therefore, we are adding a filename to the path\n",
    "credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index = False)\n",
    "\n",
    "credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index = False)\n",
    "\n",
    "# Stop Logging\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137cba9",
   "metadata": {},
   "source": [
    "Now that we have a script that can perform the desired task, we can create an AML Component from it. AML support various types of components for performing ML tasks, such as running scripts, data transfer, etc. Here we use the general purpose CommandComponent that can run command line actions. This command line action can be directly calling system or running a script. The inputs/outputs are accessible in the command via the `${{parameter}}` notation.\n",
    "\n",
    "A component can be created by calling the component instantiators, or directly writing the defining yaml file. For the first component, we use the `CommandComponent` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the CommandComponent Package\n",
    "from azure.ml.entities import CommandComponent\n",
    "\n",
    "# importing the CommandComponent Package\n",
    "from azure.ml.entities import Code\n",
    "\n",
    "data_prep_component = CommandComponent(\n",
    "    # Name of the component\n",
    "    name=\"Data_Preparation\",\n",
    "    \n",
    "    # Component Version, no Version and the component will be automatically versioned \n",
    "#     version=\"26\",\n",
    "    \n",
    "    # The dictionary of the inputs. Each item is a dictionary itself.\n",
    "    inputs=dict(\n",
    "        data=dict(type=\"path\"),\n",
    "        split_ratio=dict(type=\"integer\"),\n",
    "    ),\n",
    "    \n",
    "    # The dictionary of the outputs. Each item is a dictionary itself.\n",
    "    outputs=dict(\n",
    "        train_data=dict(type=\"path\"),\n",
    "        test_data=dict(type=\"path\"),\n",
    "    ),\n",
    "    \n",
    "    # The source folder of the component\n",
    "    code=Code(local_path=data_prep_src_dir),\n",
    "    \n",
    "    # The environment the component job will be using\n",
    "    environment=pipeline_job_env,\n",
    "    \n",
    "    # The command that will be run in the component\n",
    "    command=\"python data_prep.py --data ${{inputs.data}} --split_ratio ${{inputs.split_ratio}} \"\n",
    "    \"--train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \",\n",
    ")\n",
    "\n",
    "data_prep_component = ml_client.create_or_update(data_prep_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc8d31",
   "metadata": {},
   "source": [
    "### 3.1.2. Training Component\n",
    "\n",
    "The second component that we will create will consume the training and test data, train a tree based model and then returns the output model. We use AML logging capabilities to record and visualize the learning progress.\n",
    "\n",
    "Once the model is trained, the model file is saved and registered to the workspace. This will allow us to use the registered model in inferencing endpoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_src_dir = \"./components/train\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c84dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {train_src_dir}/train.py\n",
    "import argparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Run\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# Start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# This line creates a handles to the current run. It is used for model registration\n",
    "run = Run.get_context()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "# input and output arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train_data\")\n",
    "parser.add_argument(\"--test_data\")\n",
    "parser.add_argument(\"--registered_model_name\")\n",
    "parser.add_argument(\"--model\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "train_df = pd.read_csv(select_first_file(args.train_data))\n",
    "\n",
    "# Extracting the label column\n",
    "y_train = train_df.pop(\"default payment next month\")\n",
    "\n",
    "# convert the dataframe values to array\n",
    "X_train = train_df.values\n",
    "\n",
    "# paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "test_df = pd.read_csv(select_first_file(args.test_data))\n",
    "\n",
    "# Extracting the label column\n",
    "y_test = test_df.pop(\"default payment next month\")\n",
    "\n",
    "# convert the dataframe values to array\n",
    "X_test = test_df.values\n",
    "\n",
    "print(f'Training with data of shape {X_train.shape}')\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# setting the full path of the model file\n",
    "model_file = os.path.join(args.model, \"model.pkl\")\n",
    "with open(model_file, \"wb\") as mf:\n",
    "    joblib.dump(clf, mf)\n",
    "\n",
    "\n",
    "# Registering the model to the workspace\n",
    "model = Model.register(\n",
    "    run.experiment.workspace,\n",
    "    model_name=args.registered_model_name,\n",
    "    model_path=model_file,\n",
    "    tags={\"type\": \"sklearn.GradientBoostingClassifier\"},\n",
    "    description = 'Model created in AML on credit card defaults dataset',\n",
    ")\n",
    "\n",
    "\n",
    "# Stop Logging\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca90aa",
   "metadata": {},
   "source": [
    "We observed how we create a component using `CommandComponnet` class. This time we use the yaml definition for defining our component. Each method has its own advantages. A yaml definition is more favored for code revision and history tracking, and the class method can be easier with built-in class documentation and code completion.\n",
    "\n",
    "For the environment, we can use one of the built-in AML environments. The tag `azureml`, tells the system to use look for the name in the previously built environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28074d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {train_src_dir}/train.yml\n",
    "# <component>\n",
    "name: TrainCreditDefaultsModel\n",
    "display_name: Train Credit Defaults Model\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "    train_data: \n",
    "        type: path\n",
    "    test_data: \n",
    "        type: path\n",
    "    registered_model_name:\n",
    "        type: string\n",
    "outputs:\n",
    "    model:\n",
    "        type: path\n",
    "code:\n",
    "    local_path: .\n",
    "environment:\n",
    "    azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\n",
    "command: >-\n",
    "    python train.py \n",
    "    --train_data ${{inputs.train_data}} \n",
    "    --test_data ${{inputs.test_data}} \n",
    "    --registered_model_name ${{inputs.registered_model_name}} \n",
    "    --model ${{outputs.model}}\n",
    "# </component>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This will be removed after confirmation\n",
    "# # importing the CommandComponent Package\n",
    "# from azure.ml.entities import CommandComponent\n",
    "\n",
    "# # importing the CommandComponent Package\n",
    "# from azure.ml.entities import Code\n",
    "\n",
    "\n",
    "# environment = \"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:5\"\n",
    "\n",
    "# train_component = CommandComponent(\n",
    "#     name=\"Train\",\n",
    "#     version=\"23\",\n",
    "#     inputs=dict(\n",
    "#         train_data=dict(type=\"path\"),\n",
    "#         test_data=dict(type=\"path\"),\n",
    "#         registered_model_name = {'type':'string'}\n",
    "#     ),\n",
    "#     outputs=dict(\n",
    "#         model=dict(type=\"path\"),\n",
    "#     ),\n",
    "#     code=Code(local_path=train_src_dir),\n",
    "#     environment=env,\n",
    "#     command=\"python train.py --train_data ${{inputs.train_data}} --test_data ${{inputs.test_data}} \"\n",
    "#     \"--registered_model_name ${{inputs.registered_model_name}} --model ${{outputs.model}} \",\n",
    "# )\n",
    "\n",
    "# ml_client.create_or_update(train_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b490a70",
   "metadata": {},
   "source": [
    "## 3.2. Creating the Pipeline from Components\n",
    "Now that both our components are defined, we can start creating the pipeline. First, we load the component, this way, we can use them in our pipeline like functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50699c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import dsl, MLClient\n",
    "from azure.ml.dsl import Pipeline\n",
    "from azure.ml.entities import Component as ComponentEntity, Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Let's load the data-perp component from the workspace\n",
    "data_prep_func = dsl.load_component(\n",
    "    client=ml_client,\n",
    "    name=data_prep_component.name,\n",
    "    version=data_prep_component.version,\n",
    ")\n",
    "\n",
    "# Let's load the train func from the local yaml file\n",
    "train_func = dsl.load_component(\n",
    "    yaml_file=os.path.join(train_src_dir,'train.yml')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a785f",
   "metadata": {},
   "source": [
    "Then we use these operators in a pipeline. We use a specific `@dsl` decorator that identifies the AML pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage.\n",
    "\n",
    "Pipeline can have inputs the we can use for different instantiations of the pipeline. Here, we used *input data*, *split ratio* and *registered model name* as variables. We then call teh components and connect them via their inputs/outputs. The outputs of each step can be accessed via the `.outputs` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e808dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an AML pipeline\n",
    "@dsl.pipeline(\n",
    "    compute=\"cpu-cluster\",\n",
    "    description=\"E2E data_perp-train pipeline\",\n",
    ")\n",
    "def credit_defaults_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_split_ratio,\n",
    "    pipeline_job_registered_model_name,\n",
    "):\n",
    "    data_prep_job = data_prep_func(\n",
    "        data=pipeline_job_data_input,\n",
    "        split_ratio=pipeline_job_split_ratio,\n",
    "    )\n",
    "    train_job = train_func(\n",
    "        train_data=data_prep_job.outputs.train_data,\n",
    "        test_data=data_prep_job.outputs.test_data,\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003bcf8",
   "metadata": {},
   "source": [
    "Let's now use our pipeline definition to instantiate a pipeline with our dataset, split rate of choice and the name we picked for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"credit_defaults_model\"\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = credit_defaults_pipeline(\n",
    "    pipeline_job_data_input=credit_dataset,\n",
    "    pipeline_job_split_ratio=0.2,\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192bb5b",
   "metadata": {},
   "source": [
    "## 3.3. Submitting a Job to AML Workspace\n",
    "It is now time to submit the job for running in AML. This time we use `create_or_update`  on `ml_client.jobs`. Here we also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in AML studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    \n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_registered_components\",\n",
    "    \n",
    "    # If there is no dependency, pipeline run will continue even after the failure of one component\n",
    "    continue_run_on_step_failure=True,\n",
    ")\n",
    "# get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44fa31f1",
   "metadata": {},
   "source": [
    "You can track the progress of your pipeline, by using the link generated in the cell above.\n",
    "Clicking on each component, will reveal more information on that one. \n",
    "There are two important parts to look for at this stage:\n",
    "- `Outputs+logs` > `user_logs` > `std_log.txt`\n",
    "This section shows the script run sdtout\n",
    "<div>\n",
    "<img src=\"media/user-logs.jpg\" style=\"width:60%\"/>\n",
    "</div>\n",
    "\n",
    "- `Outputs+logs` > `Metric`\n",
    "This section shows different logged metrics. In this example. mlflow `autologging`, has automatically logged the training metrics.\n",
    "\n",
    "<div>\n",
    "<img src=\"media/metrics.jpg\" style=\"width:55%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8b036",
   "metadata": {},
   "source": [
    "# 4. Deploy the Model as an Online Endpoint\n",
    "Let's learn how to deploy your machine learning model as a web service in the Azure cloud [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=azcli). \n",
    "A typical situation for a deployed machine learning service is that you need the following components:\n",
    "\n",
    " - Resources representing the specific model that you want deployed. We have done this step in our training component.\n",
    " - Code that you will be running in the service, that executes the model on a given input. This entry script receives data submitted to a deployed web service and passes it to the model. It then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns.\n",
    "\n",
    "The two things you need to accomplish in your entry script are:\n",
    "\n",
    "- Loading your model (using a function called init())\n",
    "- Running your model on input data (using a function called run())\n",
    "\n",
    "## 4.1. Creating an Inference Script\n",
    "In the following implementation the `init()` function loads the model, and the run function expects the data in `json` format with the input data stored under `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_dir = \"./deploy\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/score.py\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy\n",
    "import joblib\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
    "    You can write the logic here to perform init operations like caching the model in memory\n",
    "    \"\"\"\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    model_path = os.path.join(\n",
    "        os.getenv(\"AZUREML_MODEL_DIR\"), \"model.pkl\"\n",
    "    )\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "    logging.info(\"Init complete\")\n",
    "\n",
    "\n",
    "def run(raw_data):\n",
    "    \"\"\"\n",
    "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
    "    In the example we extract the data from the json input and call the scikit-learn model's predict()\n",
    "    method and return the result back\n",
    "    \"\"\"\n",
    "    logging.info(\"Request received\")\n",
    "    data = json.loads(raw_data)[\"data\"]\n",
    "    data = numpy.array(data)\n",
    "    result = model.predict(data)\n",
    "    logging.info(\"Request processed\")\n",
    "    return result.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95611fb7",
   "metadata": {},
   "source": [
    "## 4.2. Create a New Online Endpoint\n",
    "It is now straight forward to create an online endpoint. First, we create an endpoint by providing its description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4926162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "#create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(name='credit-defaults-online-endpoint',\n",
    "            description='this is an online endpoint',\n",
    "            auth_mode='key',\n",
    "            tags={'training_dataset': 'cradit_defaults',\n",
    "                 'model_type':'sklearn.GradientBoostingClassifier'})\n",
    "                     \n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd203d3",
   "metadata": {},
   "source": [
    "## 4.3. Deploy the Model to the Endpoint\n",
    "Then we deploy the model with the entry script as a deployment. Each endpoint can have multiple deployment and direct traffic to these deployment based on the provided rules. Here we create a single deployment that handles all the incoming traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an online deployment. Use the latest version of your registered model\n",
    "model = ml_client.models.get(name=registered_model_name, version=9)\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name='blue',\n",
    "    endpoint_name='credit-defaults-online-endpoint',\n",
    "    model=model,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\",\n",
    "    code_local_path=deploy_dir,\n",
    "    scoring_script=\"score.py\",\n",
    "    instance_type='Standard_DS2_v2',\n",
    "    instance_count=1)\n",
    "\n",
    "ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea1417",
   "metadata": {},
   "source": [
    "With the endpoint already published, we can run inference on any data. Let's create a sample request file following the design expected in the run method in the score script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c300b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {deploy_dir}/sample-request.json\n",
    "{\"data\": [\n",
    "    [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0], \n",
    "    [10,9,8,7,6,5,4,3,2,1, 10,9,8,7,6,5,4,3,2,1,10,9,8]\n",
    "]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name='credit-defaults-online-endpoint',\n",
    "    request_file=\"./deploy/sample-request.json\",\n",
    "    deployment_name='blue'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c35946",
   "metadata": {},
   "source": [
    "It is possible to use consume the Rest endpoint directly [sdkv1link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c0536",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "In part II of this tutorial, we will expand this work by using multi-node training and performing Hyper Parameter Optimization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk2.0",
   "language": "python",
   "name": "sdk2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
