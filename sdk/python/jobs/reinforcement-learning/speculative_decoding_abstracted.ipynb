{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speculative Decoding - Simplified\n",
        "\n",
        "This notebook demonstrates the speculative decoding workflow ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: config.json\n",
            "c:\\Users\\ynagaraj\\AppData\\Local\\anaconda3\\envs\\p310\\lib\\site-packages\\mlflow\\__init__.py:41: UserWarning: Versions of mlflow (3.1.4) and mlflow-skinny (2.22.1) are different. This may lead to unexpected behavior. Please install the same version of both packages.\n",
            "  mlflow.mismatch._check_version_mismatch()\n",
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Connected to registry: test_centralus\n",
            "✓ Connected to workspace: rtanase\n",
            "✓ Resource group: rtanase\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from rl_spec_dec_utils import setup_workspace, run_rl_training_pipeline, run_draft_model_pipeline, prepare_combined_model_for_deployment, deploy_speculative_decoding_endpoint, test_deployment\n",
        "\n",
        "# Setup Azure ML workspace and registry connections\n",
        "ml_client, registry_ml_client = setup_workspace(registry_name=\"test_centralus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Draft Model for Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train EAGLE3 draft model for speculative decoding\n",
        "draft_job, draft_status = run_draft_model_pipeline(\n",
        "    ml_client=ml_client,\n",
        "    registry_ml_client=registry_ml_client,\n",
        "    compute_cluster=\"h100-dedicated\",\n",
        "    num_epochs=1,\n",
        "    monitor=False  # Set to True to wait for completion\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Combined Model for Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download draft model, download base model, combine and register for deployment\n",
        "combined_model = prepare_combined_model_for_deployment(\n",
        "    ml_client=ml_client,\n",
        "    draft_job_name=draft_job.name,\n",
        "    base_model_hf_id=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    model_name=\"grpo-speculative-decoding\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deploy Speculative Decoding Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy managed online endpoint with speculative decoding\n",
        "endpoint_name = deploy_speculative_decoding_endpoint(\n",
        "    ml_client=ml_client,\n",
        "    combined_model=combined_model,\n",
        "    instance_type=\"Standard_NC24ads_A100_v4\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
