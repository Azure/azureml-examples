{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Reinforcement Learning (GRPO,REINFORCE++) with Speculative Decoding\n",
        "\n",
        "This notebook demonstrates an end-to-end workflow for:\n",
        "1. Training a model using **GRPO (Group Relative Policy Optimization)** on FinQA dataset\n",
        "2. Registering the fine-tuned model\n",
        "3. Creating a draft model for speculative decoding\n",
        "4. Deploying a speculative decoding endpoint for **2-3x faster inference**\n",
        "\n",
        "**Note**: Most operations are abstracted in `rl_spec_dec_utils.py` for cleaner code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: .\\config.json\n",
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Connected to registry: MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x000002AB9C02A7D0>,\n",
            "         subscription_id=72c03bf3-4e69-41af-9532-dfcdc3eefef4,\n",
            "         resource_group_name=rtanase,\n",
            "         workspace_name=None)\n",
            "‚úì Connected to workspace: rtanase\n",
            "‚úì Resource group: rtanase\n",
            "üîç Verifying datasets...\n",
            "  ‚úì train: c:\\gitRepos\\yeshsurya16\\azureml-examples\\sdk\\python\\jobs\\reinforcement-learning\\datasets\\train_finqa.jsonl\n",
            "  ‚úì validation: c:\\gitRepos\\yeshsurya16\\azureml-examples\\sdk\\python\\jobs\\reinforcement-learning\\datasets\\validation_finqa.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (run once)\n",
        "# %pip install azure-ai-ml azure-identity requests\n",
        "\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "from rl_spec_dec_utils import RLSpecDecPipeline, verify_datasets, DraftModelPipeline\n",
        "\n",
        "# Setup Azure credentials\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception:\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "# Connect to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "workspace = ml_client._workspaces.get(ml_client.workspace_name)\n",
        "\n",
        "# Create MLClient for AzureML registry 'test_centralus'\n",
        "\n",
        "registry_ml_client = MLClient(credential, registry_name=\"test_centralus\")\n",
        "\n",
        "print(f\"‚úì Connected to registry: {registry_ml_client}\")\n",
        "print(f\"‚úì Connected to workspace: {workspace.name}\")\n",
        "print(f\"‚úì Resource group: {ml_client.resource_group_name}\")\n",
        "\n",
        "# Verify datasets exist\n",
        "dataset_paths = verify_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Configuration loaded\n",
            "  Base model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
            "  RL component: arl_finetune_pipeline\n",
            "  Compute: h100-dedicated\n",
            "  Algorithm: GRPO (Group Relative Policy Optimization)\n"
          ]
        }
      ],
      "source": [
        "# Initialize pipeline manager\n",
        "pipeline = RLSpecDecPipeline(ml_client)\n",
        "\n",
        "# Configuration\n",
        "BASE_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "RL_COMPONENT_NAME = \"arl_finetune_pipeline\"  # Pipeline component in azureml registry\n",
        "COMPUTE_CLUSTER = \"h100-dedicated\"  # Your compute cluster name\n",
        "\n",
        "# Optional: Override default training parameters\n",
        "training_config = {\n",
        "    \"trainer_total_epochs\": 15,\n",
        "    \"actor_optim_lr\": 3e-6,\n",
        "    \"instance_type_finetune\": \"Standard_ND96isr_H100_v5\",\n",
        "    \"num_nodes_finetune\": 1,\n",
        "    \"number_of_gpu_to_use_finetuning\": 8,\n",
        "}\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"  Base model: {BASE_MODEL_ID}\")\n",
        "print(f\"  RL component: {RL_COMPONENT_NAME}\")\n",
        "print(f\"  Compute: {COMPUTE_CLUSTER}\")\n",
        "print(f\"  Algorithm: GRPO (Group Relative Policy Optimization)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Register Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Registering datasets...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AzureCliCredential.get_token_info failed: Failed to invoke the Azure CLI\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Training dataset: finqa_train_d19ac3ed\n",
            "  ‚úì Validation dataset: finqa_validation_d19ac3ed\n"
          ]
        }
      ],
      "source": [
        "# Register datasets in Azure ML\n",
        "train_asset, val_asset = pipeline.register_datasets(\n",
        "    train_path=dataset_paths[\"train\"],\n",
        "    val_path=dataset_paths[\"validation\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Submit RL Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Creating RL pipeline...\n",
            "  ‚úì Loading pipeline component: arl_finetune_pipeline\n",
            "  ‚úì Component loaded: arl_finetune_pipeline v0.0.56\n",
            "  ‚úì Submitting pipeline...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Job submitted: khaki_crayon_4s2f1qcyvm\n",
            "  üìä Studio URL: https://ml.azure.com/runs/khaki_crayon_4s2f1qcyvm?wsid=/subscriptions/72c03bf3-4e69-41af-9532-dfcdc3eefef4/resourcegroups/rtanase/workspaces/rtanase&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
          ]
        }
      ],
      "source": [
        "# Create and submit RL pipeline\n",
        "rl_job = pipeline.create_rl_pipeline(\n",
        "    registry_ml_client=registry_ml_client,\n",
        "    huggingface_id=BASE_MODEL_ID,\n",
        "    train_data_asset=train_asset,\n",
        "    val_data_asset=val_asset,\n",
        "    compute_cluster=COMPUTE_CLUSTER,\n",
        "    config=training_config,\n",
        "    pipeline_component_name=RL_COMPONENT_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Monitor Training Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Monitoring job: joyful_yak_gpblsnv78v\n",
            "   Checking every 60 seconds...\n",
            "   [18:40:32] Status: Running\n"
          ]
        }
      ],
      "source": [
        "# Monitor job until completion (this may take several hours)\n",
        "completed_job, status = pipeline.monitor_job(rl_job.name, poll_interval=60)\n",
        "\n",
        "if status != \"Completed\":\n",
        "    print(f\"\\n‚ö†Ô∏è  Job did not complete successfully: {status}\")\n",
        "    print(f\"Check logs at: {rl_job.studio_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Register Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register the fine-tuned model\n",
        "if status == \"Completed\":\n",
        "    registered_model = pipeline.register_model(\n",
        "        job=completed_job,\n",
        "        model_name_prefix=\"grpo-finqa-model\",\n",
        "        base_model_id=BASE_MODEL_ID,\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping model registration due to job failure.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Draft Model for Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Preparing draft model training pipeline...\n",
            "  ‚úì Draft model config saved: ./draft_config\\draft_model_config.json\n",
            "  ‚úì Draft training data: ./data_for_draft_model/train/sharegpt_train_small.jsonl\n",
            "  ‚úì Loading component: eagle3_chat_completion_pipeline\n",
            "  ‚úì Component loaded: eagle3_chat_completion_pipeline v0.0.1.visa01\n",
            "  ‚úì Submitting draft model training pipeline...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Job submitted: brave_leaf_wswl8gmkt8\n",
            "  üìä Studio URL: https://ml.azure.com/runs/brave_leaf_wswl8gmkt8?wsid=/subscriptions/72c03bf3-4e69-41af-9532-dfcdc3eefef4/resourcegroups/rtanase/workspaces/rtanase&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
          ]
        }
      ],
      "source": [
        "# Initialize draft model pipeline\n",
        "#if status == \"Completed\":\n",
        "import json\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "print(\"üéØ Preparing draft model training pipeline...\")\n",
        "\n",
        "# Configuration for draft model (EAGLE3 architecture)\n",
        "draft_model_config = {\n",
        "    \"architectures\": [\"LlamaForCausalLMEagle3\"],\n",
        "    \"bos_token_id\": 128000,\n",
        "    \"eos_token_id\": 128001,\n",
        "    \"hidden_act\": \"silu\",\n",
        "    \"hidden_size\": 4096,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 14336,\n",
        "    \"max_position_embeddings\": 2048,\n",
        "    \"model_type\": \"llama\",\n",
        "    \"num_attention_heads\": 32,\n",
        "    \"num_key_value_heads\": 8,\n",
        "    \"num_hidden_layers\": 1,  # Single layer for fast draft model\n",
        "    \"pad_token_id\": 0,\n",
        "    \"rms_norm_eps\": 1e-05,\n",
        "    \"tie_word_embeddings\": False,\n",
        "    \"torch_dtype\": \"float16\",\n",
        "    \"transformers_version\": \"4.28.1\",\n",
        "    \"use_cache\": True,\n",
        "    \"vocab_size\": 128256,\n",
        "    \"draft_vocab_size\": 32000\n",
        "}\n",
        "\n",
        "# Save draft model config\n",
        "config_dir = \"./draft_config\"\n",
        "import os\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "draft_config_path = os.path.join(config_dir, \"draft_model_config.json\")\n",
        "\n",
        "with open(draft_config_path, \"w\") as f:\n",
        "    json.dump(draft_model_config, f, indent=4)\n",
        "\n",
        "print(f\"  ‚úì Draft model config saved: {draft_config_path}\")\n",
        "\n",
        "# Dataset path for draft model training\n",
        "draft_train_data_path = \"./data_for_draft_model/train/sharegpt_train_small.jsonl\"\n",
        "\n",
        "# Verify dataset exists\n",
        "if not os.path.exists(draft_train_data_path):\n",
        "    raise FileNotFoundError(f\"Draft model training data not found: {draft_train_data_path}\")\n",
        "print(f\"  ‚úì Draft training data: {draft_train_data_path}\")\n",
        "\n",
        "# Base model for draft model training\n",
        "base_model_mlflow_path = \"azureml://registries/azureml-meta/models/Meta-Llama-3-8B-Instruct/versions/9\"\n",
        "\n",
        "# Component name\n",
        "draft_component_name = \"eagle3_chat_completion_pipeline\"\n",
        "\n",
        "# Get the component from workspace (as shown in spec_decod.ipynb)\n",
        "print(f\"  ‚úì Loading component: {draft_component_name}\")\n",
        "eagle3_comp = registry_ml_client.components.get(name=draft_component_name, label=\"latest\")\n",
        "print(f\"  ‚úì Component loaded: {eagle3_comp.name} v{eagle3_comp.version}\")\n",
        "\n",
        "# Define the pipeline\n",
        "@pipeline\n",
        "def speculative_decoding_draft_pipeline():\n",
        "    node = eagle3_comp(\n",
        "        mlflow_model_path=Input(type=AssetTypes.MLFLOW_MODEL, path=base_model_mlflow_path),\n",
        "        dataset_train_split=Input(type=AssetTypes.URI_FILE, path=draft_train_data_path),\n",
        "        dataset_validation_split=Input(type=AssetTypes.URI_FILE, path=draft_train_data_path),\n",
        "        draft_model_config=Input(type=AssetTypes.URI_FILE, path=draft_config_path),\n",
        "        compute_model_import=COMPUTE_CLUSTER,\n",
        "        compute_eagle3_training=COMPUTE_CLUSTER,\n",
        "        num_epochs=1,\n",
        "    )\n",
        "    return {\n",
        "        \"output_model\": node.outputs.output_model_path\n",
        "    }\n",
        "\n",
        "# Create pipeline job\n",
        "draft_job = speculative_decoding_draft_pipeline()\n",
        "\n",
        "# Submit the job\n",
        "print(\"  ‚úì Submitting draft model training pipeline...\")\n",
        "draft_job = ml_client.jobs.create_or_update(\n",
        "    draft_job, experiment_name=\"speculative-decoding-draft-model\"\n",
        ")\n",
        "\n",
        "print(f\"  ‚úì Job submitted: {draft_job.name}\")\n",
        "print(f\"  üìä Studio URL: {draft_job.studio_url}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7b. Download and Register Draft Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "DraftModelPipeline.download_draft_model() missing 1 required positional argument: 'self'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download draft model and prepare for deployment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#if status == \"Completed\" and draft_status == \"Completed\":\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Download draft model artifacts\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m draft_model_dir \u001b[38;5;241m=\u001b[39m \u001b[43mDraftModelPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_draft_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraft_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/draft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Download base model from HuggingFace (or use your trained model)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müì• Downloading base model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: DraftModelPipeline.download_draft_model() missing 1 required positional argument: 'self'"
          ]
        }
      ],
      "source": [
        "# Download draft model and prepare for deployment\n",
        "#if status == \"Completed\" and draft_status == \"Completed\":\n",
        "from rl_spec_dec_utils import DraftModelPipeline\n",
        "\n",
        "# Initialize DraftModelPipeline helper for download/upload operations\n",
        "draft_pipeline = DraftModelPipeline(ml_client)\n",
        "\n",
        "# Download draft model artifacts\n",
        "draft_model_dir = draft_pipeline.download_draft_model(\n",
        "    job_name=draft_job.name,\n",
        "    output_dir=\"./models/draft\"\n",
        ")\n",
        "\n",
        "# Download base model from HuggingFace (or use your trained model)\n",
        "print(\"\\n\\nüì• Downloading base model...\")\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "base_model_hf_id = \"nvidia/Llama-3.1-8B-Instruct-FP8\"  # Or use your model\n",
        "base_model_dir = \"./models/base\"\n",
        "\n",
        "snapshot_download(repo_id=base_model_hf_id, local_dir=base_model_dir)\n",
        "print(f\"  ‚úì Base model downloaded to: {base_model_dir}\")\n",
        "\n",
        "# Upload combined model for speculative decoding\n",
        "combined_model = draft_pipeline.upload_combined_model(\n",
        "    base_model_dir=base_model_dir,\n",
        "    draft_model_dir=draft_model_dir,\n",
        "    model_name=\"grpo-speculative-decoding\",\n",
        ")\n",
        "\n",
        "print(f\"\\n\\n‚úì Combined model ready for deployment: {combined_model.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Deploy Speculative Decoding Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy endpoint with speculative decoding using combined model\n",
        "if status == \"Completed\" and draft_status == \"Completed\":\n",
        "    from azure.ai.ml.entities import (\n",
        "        ManagedOnlineEndpoint,\n",
        "        ManagedOnlineDeployment,\n",
        "        Environment,\n",
        "        BuildContext,\n",
        "    )\n",
        "\n",
        "    endpoint_name = f\"spec-dec-grpo-{pipeline.guid}\"\n",
        "    deployment_name = \"speculative-deployment\"\n",
        "\n",
        "    print(f\"üåê Creating speculative decoding endpoint: {endpoint_name}\")\n",
        "\n",
        "    # Create endpoint\n",
        "    endpoint = ManagedOnlineEndpoint(\n",
        "        name=endpoint_name,\n",
        "        description=\"Speculative decoding endpoint with GRPO fine-tuned base model\",\n",
        "        auth_mode=\"key\",\n",
        "    )\n",
        "\n",
        "    ml_client.online_endpoints.begin_create_or_update(endpoint).wait()\n",
        "    print(f\"  ‚úì Endpoint created\")\n",
        "\n",
        "    # Create custom environment for SGLang speculative decoding\n",
        "    # Note: You need to create ./environment directory with Dockerfile and requirements\n",
        "    # See spec_decod.ipynb for environment setup details\n",
        "\n",
        "    # Create deployment with combined model\n",
        "    deployment = ManagedOnlineDeployment(\n",
        "        name=deployment_name,\n",
        "        endpoint_name=endpoint_name,\n",
        "        model=combined_model.id,\n",
        "        instance_type=\"Standard_NC24ads_A100_v4\",\n",
        "        instance_count=1,\n",
        "        environment_variables={\n",
        "            \"MODEL_BASE_PATH\": \"/var/azureml-app/azureml-models/\" + combined_model.name + \"/\" + str(combined_model.version) + \"/base\",\n",
        "            \"MODEL_DRAFT_PATH\": \"/var/azureml-app/azureml-models/\" + combined_model.name + \"/\" + str(combined_model.version) + \"/draft\",\n",
        "            \"SPECULATIVE_DECODING\": \"true\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(f\"  ‚úì Creating deployment (this takes 15-20 min)...\")\n",
        "    ml_client.online_deployments.begin_create_or_update(deployment).wait()\n",
        "\n",
        "    # Route traffic\n",
        "    endpoint.traffic = {deployment_name: 100}\n",
        "    ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
        "\n",
        "    print(f\"‚úì Speculative decoding endpoint deployed: {endpoint_name}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Skipping deployment due to training failures.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Speculative Decoding Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get endpoint credentials and test\n",
        "if status == \"Completed\" and draft_status == \"Completed\":\n",
        "    endpoint_info = pipeline.get_endpoint_details(endpoint_name)\n",
        "\n",
        "    print(f\"\\nüìç Endpoint: {endpoint_info['endpoint_name']}\")\n",
        "    print(f\"üîó URI: {endpoint_info['scoring_uri']}\")\n",
        "    print(f\"üîë Key: {endpoint_info['api_key'][:10]}...\\n\")\n",
        "\n",
        "    # Test the endpoint with a financial reasoning question\n",
        "    result = pipeline.test_endpoint(\n",
        "        scoring_uri=endpoint_info['scoring_uri'],\n",
        "        api_key=endpoint_info['api_key'],\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚ú® Speculative decoding enables 2-3x faster token generation!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Skipping endpoint test due to failures.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete endpoint and free up resources\n",
        "# ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()\n",
        "# print(f\"‚úì Endpoint deleted: {endpoint_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete end-to-end workflow:\n",
        "\n",
        "### ‚úÖ What We Accomplished:\n",
        "1. **RL Training (GRPO)**: Fine-tuned a base model on FinQA dataset using Group Relative Policy Optimization\n",
        "2. **Model Registration**: Registered the GRPO fine-tuned model in Azure ML\n",
        "3. **Draft Model Creation**: Trained an EAGLE3 draft model for speculative decoding\n",
        "4. **Model Combination**: Combined base and draft models into a single deployable artifact\n",
        "5. **Speculative Decoding Deployment**: Deployed an endpoint with SGLang for 2-3x faster inference\n",
        "6. **Testing**: Validated the speculative decoding endpoint with real queries\n",
        "\n",
        "### üöÄ Key Benefits:\n",
        "- **Faster Inference**: Speculative decoding provides 2-3x speedup in token generation\n",
        "- **Quality Preservation**: Produces identical outputs to standard decoding\n",
        "- **Cost Efficiency**: Reduced inference time leads to lower operational costs\n",
        "- **RL Optimization**: GRPO fine-tuning improves model reasoning on financial tasks\n",
        "\n",
        "### üìä Performance Gains:\n",
        "- **Request Throughput**: Higher requests per second\n",
        "- **Latency**: Lower end-to-end and inter-token latency\n",
        "- **TTFT**: Faster time to first token\n",
        "\n",
        "### üîß Components Used:\n",
        "- **RL Algorithm**: GRPO (critic-free reinforcement learning)\n",
        "- **Draft Model**: EAGLE3 architecture (1-layer transformer)\n",
        "- **Serving Engine**: SGLang for speculative decoding\n",
        "- **Infrastructure**: Azure ML pipelines and managed endpoints\n",
        "\n",
        "### üìö Next Steps:\n",
        "- Fine-tune hyperparameters for your specific use case\n",
        "- Experiment with different draft model architectures\n",
        "- Monitor production metrics and optimize further\n",
        "- Scale to multiple instances for production workloads"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
