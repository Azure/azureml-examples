{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure Reinforcement Learning (GRPO) with Speculative Decoding\n",
        "\n",
        "This notebook demonstrates the complete RL training and speculative decoding workflow at a high level. Most implementation details are deferred to `rl_spec_dec_utils.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: config.json\n",
            "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace setup complete, connected\n"
          ]
        }
      ],
      "source": [
        "from rl_spec_dec_utils import setup_workspace, run_rl_training_pipeline, run_draft_model_pipeline, prepare_combined_model_for_deployment, deploy_speculative_decoding_endpoint, test_deployment, download_and_register_hf_model\n",
        "\n",
        "ml_client, registry_ml_client = setup_workspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run RL Training Pipeline (GRPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run complete RL training pipeline: verify datasets, register data, train model, register model\n",
        "rl_job, status, registered_model = run_rl_training_pipeline(\n",
        "    base_model_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
        "    compute_cluster=\"k8s-a100-compute\",\n",
        "    config={\n",
        "        \"num_nodes_finetune\": 1,\n",
        "        \"trainer_total_epochs\": 0,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import json\n",
        "import shutil\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from huggingface_hub import snapshot_download\n",
        "from azure.ai.ml import MLClient, Input, dsl\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml.entities import (\n",
        "    Model,\n",
        "    KubernetesOnlineEndpoint,\n",
        "    KubernetesOnlineDeployment,\n",
        "    ProbeSettings,\n",
        "    OnlineRequestSettings,\n",
        "    Environment,\n",
        "    BuildContext,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hf_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "azureml_model_name = \"qwen-7b-base\"\n",
        "\n",
        "model = download_and_register_hf_model(\n",
        "    hf_model_id=hf_model_id,\n",
        "    azureml_model_name=azureml_model_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 Deploy base and RL models\n",
        "We have registered the Qwen-7B base and finetuned models, now we will deploy it as kubernetes endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ft_model = ml_client.models.get(name=\"grpo-finqa-3a436a81\", version=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading grpoenv (0.0 MBs): 100%|██████████| 38/38 [00:00<00:00, 1254.75it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#--------ENVIRONMENT--------\n",
        "environment_definition = Environment(\n",
        "    name=\"vllm-env\",\n",
        "    build=BuildContext(path=\"./grpoenv\"),\n",
        "    inference_config={\n",
        "         \"liveness_route\": {\n",
        "            \"port\": 8000,\n",
        "            \"path\": \"/health\"\n",
        "        },\n",
        "        \"readiness_route\": {\n",
        "            \"port\": 8000,\n",
        "            \"path\": \"/health\"\n",
        "        },\n",
        "        \"scoring_route\": {\n",
        "            \"port\": 8000,\n",
        "            \"path\": \"/\"\n",
        "        }\n",
        "    }\n",
        "\n",
        ")\n",
        "environment = ml_client.environments.create_or_update(environment_definition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint grpo-finqa-3a436a81-endpoint exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...................................."
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     13\u001b[0m probe_settings \u001b[38;5;241m=\u001b[39m ProbeSettings(\n\u001b[1;32m     14\u001b[0m     initial_delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m     15\u001b[0m     period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     failure_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m deployment \u001b[38;5;241m=\u001b[39m KubernetesOnlineDeployment(\n\u001b[1;32m     21\u001b[0m     name\u001b[38;5;241m=\u001b[39mdeployment_name,\n\u001b[1;32m     22\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mendpoint_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     ),\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_deployments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m endpoint\u001b[38;5;241m.\u001b[39mtraffic \u001b[38;5;241m=\u001b[39m {deployment_name: \u001b[38;5;241m100\u001b[39m}\n\u001b[1;32m     37\u001b[0m ml_client\u001b[38;5;241m.\u001b[39monline_endpoints\u001b[38;5;241m.\u001b[39mbegin_create_or_update(endpoint)\u001b[38;5;241m.\u001b[39mwait()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/azure/core/tracing/decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/azure/core/polling/_poller.py:341\u001b[0m, in \u001b[0;36mLROPoller.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;66;03m# Let's handle possible None in forgiveness here\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/8165\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/external_ignite/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
            "File \u001b[0;32m/anaconda/envs/external_ignite/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".."
          ]
        }
      ],
      "source": [
        "#--------ENDPOINT--------\n",
        "endpoint_name = f\"{ft_model.name}-endpoint\"\n",
        "endpoint = KubernetesOnlineEndpoint(\n",
        "    name=endpoint_name,\n",
        "    auth_mode=\"key\",\n",
        "    compute=\"k8s-a100-compute\",\n",
        ")\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).wait()\n",
        "\n",
        "#--------DEPLOYMENT--------\n",
        "model = ft_model\n",
        "deployment_name = f\"deployment-2\"\n",
        "environment_vars = {\n",
        "    \"MODEL_PATH\": \"/model/model_output\",\n",
        "{}\n",
        "probe_settings = ProbeSettings(\n",
        "    initial_delay=600,\n",
        "    period=10,\n",
        "    timeout=2,\n",
        "    success_threshold=1,\n",
        "    failure_threshold=30,\n",
        ")\n",
        "deployment = KubernetesOnlineDeployment(\n",
        "    name=deployment_name,\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=\"monogpu\",\n",
        "    model_mount_path=\"/model\",\n",
        "    instance_count=1,\n",
        "    environment=environment, \n",
        "    liveness_probe=probe_settings,\n",
        "    readiness_probe=probe_settings,\n",
        "    request_settings=OnlineRequestSettings(\n",
        "        request_timeout_ms=90000,\n",
        "        max_concurrent_requests_per_instance=4,\n",
        "    ),\n",
        ")\n",
        "ml_client.online_deployments.begin_create_or_update(deployment).wait()\n",
        "endpoint.traffic = {deployment_name: 100}\n",
        "ml_client.online_endpoints.begin_create_or_update(endpoint).wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Draft Model for Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train EAGLE3 draft model for speculative decoding\n",
        "draft_job, draft_status = run_draft_model_pipeline(\n",
        "    ml_client=ml_client,\n",
        "    registry_ml_client=registry_ml_client,\n",
        "    compute_cluster=\"shj-a100\",\n",
        "    num_epochs=1,\n",
        "    monitor=False,  # Set to True to wait for completion\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Combined Model for Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download draft model, download base model, combine and register for deployment\n",
        "combined_model = prepare_combined_model_for_deployment(\n",
        "    ml_client=ml_client,\n",
        "    draft_job_name=draft_job.name,\n",
        "    base_model_hf_id=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    model_name=\"grpo-speculative-decoding\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Deploy Speculative Decoding Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy managed online endpoint with speculative decoding\n",
        "endpoint_name = deploy_speculative_decoding_endpoint(\n",
        "    ml_client=ml_client,\n",
        "    combined_model=combined_model,\n",
        "    instance_type=\"monogpu\",\n",
        "    compute_name=\"shj-a100\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the deployed endpoint with a financial reasoning question\n",
        "result = test_deployment(ml_client, endpoint_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete endpoint and free up resources\n",
        "# ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()\n",
        "# print(f\"✓ Endpoint deleted: {endpoint_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This simplified notebook demonstrates the complete workflow:\n",
        "\n",
        "1. ✅ **Setup**: Connected to Azure ML workspace and registry\n",
        "2. ✅ **RL Training**: Trained GRPO model on FinQA dataset  \n",
        "3. ✅ **Draft Model**: Created EAGLE3 draft model for speculative decoding\n",
        "4. ✅ **Model Preparation**: Combined base and draft models\n",
        "5. ✅ **Deployment**: Deployed speculative decoding endpoint\n",
        "6. ✅ **Testing**: Validated 2-3x faster inference\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "external_ignite",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
