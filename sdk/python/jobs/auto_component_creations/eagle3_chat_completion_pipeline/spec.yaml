$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json
name: pipeline_draft_model_training
version: 0.0.0
type: pipeline
display_name: Draft Model Pipeline
description: Pipeline to train draft model for Speculative Decoding.


inputs:
  instance_type_model_import:
    type: string
    optional: true
    default: Standard_d12_v2
    description: Instance type to be used for model_import component in case of serverless compute, eg. standard_d12_v2. The parameter compute_model_import must be set to 'serverless' for instance_type to be used
  instance_type_eagle3_training:
    type: string
    optional: true
    default: Standard_nc24rs_v3
    description: Instance type to be used for eagle3_training component in case of serverless compute, eg. standard_nc24rs_v3. The parameter compute_eagle3_training must be set to 'serverless' for instance_type to be used
  shm_size_eagle3_training:
    type: string
    optional: true
    default: 5g
    description: Shared memory size to be used for eagle3_training component
  num_nodes_eagle3_training:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of nodes to be used for Eagle3 training (used for distributed training)
  number_of_gpu_to_use_eagle3_training:
    type: integer
    min: 1
    default: 1
    optional: true
    description: number of gpus to be used per node for Eagle3 training, should be equal to number of gpu per node in the compute SKU used for training

  # Model Import parameters
  huggingface_id:
    type: string
    description: The string can be any valid Hugging Face id from the [Hugging Face models webpage](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads). Models from Hugging Face are subject to third party license terms available on the Hugging Face model details page. It is your responsibility to comply with the model's license terms.
    optional: true

  pytorch_model_path:
    type: custom_model
    optional: true
    description: Pytorch model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  mlflow_model_path:
    type: mlflow_model
    optional: true
    description: MLflow model asset path. Special characters like \ and ' are invalid in the parameter value.
    mode: rw_mount

  # Dataset parameters
  dataset_train_split:
    type: uri_file
    optional: true
    description: Path to the training dataset in JSONL format
    mode: rw_mount

  dataset_validation_split:
    type: uri_file
    optional: true
    description: Path to the validation dataset in JSONL format
    mode: rw_mount

  # Eagle3 Training parameters
  draft_model_config:
    type: uri_file
    optional: true
    description: Path to draft model configuration JSON file. If not provided, will be auto-generated from target model.
    mode: ro_mount

  num_epochs:
    type: integer
    min: 1
    default: 2
    optional: true
    description: Number of training epochs for Eagle3

  training_batch_size:
    type: integer
    min: 1
    default: 2
    optional: true
    description: Batch size for Eagle3 training

  learning_rate:
    type: number
    default: 0.0001
    optional: true
    description: Learning rate for Eagle3 training

  max_length:
    type: integer
    default: 2048
    optional: true
    description: Maximum sequence length for Eagle3 training

  warmup_ratio:
    type: number
    default: 0.015
    optional: true
    description: Warmup ratio for learning rate scheduler

  max_grad_norm:
    type: number
    default: 0.5
    optional: true
    description: Maximum gradient norm for gradient clipping

  ttt_length:
    type: integer
    default: 7
    optional: true
    description: The length for Test-Time Training (TTT)

  chat_template:
    type: string
    default: llama3
    optional: true
    description: Chat template to use for formatting conversations

  attention_backend:
    type: string
    enum:
    - "flex_attention"
    - "sdpa"
    default: "flex_attention"
    optional: true
    description: Attention implementation backend to use

  tp_size:
    type: integer
    default: 1
    optional: true
    description: Tensor parallelism size

  dp_size:
    type: integer
    default: 1
    optional: true
    description: Data parallelism size

  draft_global_batch_size:
    type: integer
    default: 8
    optional: true
    description: Global batch size for draft model training

  draft_micro_batch_size:
    type: integer
    default: 1
    optional: true
    description: Micro batch size for draft model

  draft_accumulation_steps:
    type: integer
    default: 1
    optional: true
    description: Gradient accumulation steps for draft model

  log_steps:
    type: integer
    default: 50
    optional: true
    description: Log training metrics every N steps

  eval_interval:
    type: integer
    default: 1
    optional: true
    description: Evaluation interval in epochs

  save_interval:
    type: integer
    default: 1
    optional: true
    description: Checkpoint save interval in epochs

  seed:
    type: integer
    default: 0
    optional: true
    description: Random seed for reproducibility

  total_steps:
    type: integer
    optional: true
    description: Total training steps. If not provided, will be calculated as num_epochs * steps_per_epoch

  embedding_key:
    type: string
    default: "model.embed_tokens.weight"
    optional: true
    description: The key of the embedding weight to load from the target model

  dist_timeout:
    type: integer
    default: 20
    optional: true
    description: Timeout for collective communication in minutes

  resume:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: Whether to resume training from the last checkpoint

  resume_from_checkpoint:
    type: uri_folder
    optional: true
    description: Path to a checkpoint directory to resume training from. Used when resume is true and no checkpoints exist in output folder, or when resume is false to initialize from a pretrained draft model checkpoint.
    mode: ro_mount

  build_dataset_num_proc:
    type: integer
    default: 8
    optional: true
    description: Number of processes to use for building the dataset

  verbose:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: Enable verbose logging

  profile:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: Enable profiling

  profile_start_step:
    type: integer
    default: 30
    optional: true
    description: Step to start profiling

  profile_num_steps:
    type: integer
    default: 4
    optional: true
    description: Number of steps to profile

  profile_record_shapes:
    type: string
    enum:
    - "true"
    - "false"
    default: "false"
    optional: true
    description: Record tensor shapes during profiling

  # Compute parameters
  compute_model_import:
    type: string
    optional: true
    default: serverless
    description: compute to be used for model_import eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used
  compute_eagle3_training:
    type: string
    optional: true
    default: serverless
    description: compute to be used for eagle3_training eg. provide 'FT-Cluster' if your compute is named 'FT-Cluster'. Special characters like \ and ' are invalid in the parameter value. If compute cluster name is provided, instance_type field will be ignored and the respective cluster will be used

outputs:
  output_model_path:
    type: uri_folder
    description: Output folder containing trained Eagle3 draft model checkpoints
    mode: rw_mount

jobs:
  draft_model_import:
    type: command
    component: azureml://registries/reinforcement_learning_ignite/components/chat_completion_model_import/versions/0.0.1
    compute: '${{parent.inputs.compute_model_import}}'
    resources:
      instance_type: '${{parent.inputs.instance_type_model_import}}'
    inputs:
      huggingface_id: '${{parent.inputs.huggingface_id}}'
      pytorch_model_path: '${{parent.inputs.pytorch_model_path}}'
      mlflow_model_path: '${{parent.inputs.mlflow_model_path}}'
  draft_model_training:
    type: command
    component: azureml://registries/reinforcement_learning_ignite/components/eagle3_training/versions/0.0.1
    compute: '${{parent.inputs.compute_eagle3_training}}'
    distribution:
      type: pytorch
      process_count_per_instance: '${{parent.inputs.number_of_gpu_to_use_eagle3_training}}'
    resources:
      instance_count: '${{parent.inputs.num_nodes_eagle3_training}}'
      instance_type: '${{parent.inputs.instance_type_eagle3_training}}'
      shm_size: '${{parent.inputs.shm_size_eagle3_training}}'
    inputs:
      dataset_train_split: '${{parent.inputs.dataset_train_split}}'
      dataset_validation_split: '${{parent.inputs.dataset_validation_split}}'
      target_model_path: '${{parent.jobs.eagle3_model_import.outputs.output_dir}}'
      draft_model_config: '${{parent.inputs.draft_model_config}}'
      num_epochs: '${{parent.inputs.num_epochs}}'
      batch_size: '${{parent.inputs.training_batch_size}}'
      learning_rate: '${{parent.inputs.learning_rate}}'
      max_length: '${{parent.inputs.max_length}}'
      warmup_ratio: '${{parent.inputs.warmup_ratio}}'
      max_grad_norm: '${{parent.inputs.max_grad_norm}}'
      ttt_length: '${{parent.inputs.ttt_length}}'
      chat_template: '${{parent.inputs.chat_template}}'
      attention_backend: '${{parent.inputs.attention_backend}}'
      tp_size: '${{parent.inputs.tp_size}}'
      dp_size: '${{parent.inputs.dp_size}}'
      draft_global_batch_size: '${{parent.inputs.draft_global_batch_size}}'
      draft_micro_batch_size: '${{parent.inputs.draft_micro_batch_size}}'
      draft_accumulation_steps: '${{parent.inputs.draft_accumulation_steps}}'
      log_steps: '${{parent.inputs.log_steps}}'
      eval_interval: '${{parent.inputs.eval_interval}}'
      save_interval: '${{parent.inputs.save_interval}}'
      seed: '${{parent.inputs.seed}}'
      total_steps: '${{parent.inputs.total_steps}}'
      embedding_key: '${{parent.inputs.embedding_key}}'
      dist_timeout: '${{parent.inputs.dist_timeout}}'
      resume: '${{parent.inputs.resume}}'
      resume_from_checkpoint: '${{parent.inputs.resume_from_checkpoint}}'
      build_dataset_num_proc: '${{parent.inputs.build_dataset_num_proc}}'
      verbose: '${{parent.inputs.verbose}}'
      profile: '${{parent.inputs.profile}}'
      profile_start_step: '${{parent.inputs.profile_start_step}}'
      profile_num_steps: '${{parent.inputs.profile_num_steps}}'
      profile_record_shapes: '${{parent.inputs.profile_record_shapes}}'
    outputs:
      output_model_path: '${{parent.outputs.output_model_path}}'
