{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build pipeline with registered components\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with computer cluster - [Configure workspace](../../configuration.ipynb)\n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Connect to your AML workspace from the Python SDK\n",
    "- Define `CommandComponent` using YAML, `command_component` decorator\n",
    "- Create components into workspace\n",
    "- Create `Pipeline` using registered components.\n",
    "\n",
    "**Motivations** - This notebook explains different method to create components via SDK then use these components to build pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get a handle to the workspace\n",
    "\n",
    "We use config file to connect to a workspace. The Azure ML workspace should be configured with computer cluster. [Check this notebook for configure a workspace](../../configuration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cluster_name = \"cpu-cluster\"\n",
    "print(ml_client.compute.get(cluster_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define and create components into workspace\n",
    "## 2.1 Load components definition from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \".\"\n",
    "train_model = load_component(source=parent_dir + \"/train_model.yml\")\n",
    "score_data = load_component(source=parent_dir + \"/score_data.yml\")\n",
    "# print the component as yaml\n",
    "print(score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Register components into workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create component\n",
    "try:\n",
    "    train_model = ml_client.components.get(\n",
    "        name=train_model.name, version=train_model.version\n",
    "    )\n",
    "except:\n",
    "    train_model = ml_client.components.create_or_update(train_model)\n",
    "\n",
    "try:\n",
    "    score_data = ml_client.components.get(\n",
    "        name=score_data.name, version=score_data.version\n",
    "    )\n",
    "except:\n",
    "    score_data = ml_client.components.create_or_update(score_data)\n",
    "\n",
    "# print the component as yaml\n",
    "# NOTE: resources like code, environment are resolved to remote arm id.\n",
    "print(score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create components use component decorator\n",
    "\n",
    "**Note:** component decorated function need to write as separate py file, which will be included in component code snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting eval_src/component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eval_src/component.py\n",
    "\n",
    "from mldesigner import command_component, Input, Output\n",
    "\n",
    "@command_component(\n",
    "    display_name=\"Eval Model\",\n",
    "    version=\"0.0.9\",\n",
    ")\n",
    "def eval_model(\n",
    "    scoring_result: Input(type=\"uri_folder\"),\n",
    "    eval_output: Output(type=\"uri_folder\"),\n",
    "):\n",
    "    \"\"\"A dummy eval component defined by dsl component.\"\"\"\n",
    "\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    print (\"hello evaluation world...\")\n",
    "\n",
    "    lines = [\n",
    "        f'Scoring result path: {scoring_result}',\n",
    "        f'Evaluation output path: {eval_output}',\n",
    "    ]\n",
    "\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "\n",
    "    # Evaluate the incoming scoring result and output evaluation result.\n",
    "    # Here only output a dummy file for demo.\n",
    "    curtime = datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")\n",
    "    eval_msg = f\"Eval done at {curtime}\\n\"\n",
    "    (Path(eval_output) / 'eval_result.txt').write_text(eval_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# in notebook auto reload the component if any code change\n",
    "from eval_src.component import eval_model\n",
    "\n",
    "print(type(eval_model))\n",
    "help(eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the component if not exists\n",
    "try:\n",
    "    eval_model = ml_client.components.get(name=\"eval_model\", version=\"0.0.9\")\n",
    "except:\n",
    "    eval_model = ml_client.components.create_or_update(eval_model)\n",
    "\n",
    "print(eval_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sample pipeline job\n",
    "## 3.1 Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "@pipeline()\n",
    "def pipeline_with_registered_components(\n",
    "    training_input,\n",
    "    test_input,\n",
    "    training_max_epochs=20,\n",
    "    training_learning_rate=1.8,\n",
    "    learning_rate_schedule=\"time-based\",\n",
    "):\n",
    "    \"\"\"E2E dummy train-score-eval pipeline with registered components.\"\"\"\n",
    "    # Call component obj as function: apply given inputs & parameters to create a node in pipeline\n",
    "    train_with_sample_data = train_model(\n",
    "        training_data=training_input,\n",
    "        max_epochs=training_max_epochs,\n",
    "        learning_rate=training_learning_rate,\n",
    "        learning_rate_schedule=learning_rate_schedule,\n",
    "    )\n",
    "\n",
    "    score_with_sample_data = score_data(\n",
    "        model_input=train_with_sample_data.outputs.model_output, test_data=test_input\n",
    "    )\n",
    "    score_with_sample_data.outputs.score_output.mode = \"upload\"\n",
    "\n",
    "    eval_with_sample_data = eval_model(\n",
    "        scoring_result=score_with_sample_data.outputs.score_output\n",
    "    )\n",
    "\n",
    "    # Return: pipeline outputs\n",
    "    return {\n",
    "        \"trained_model\": train_with_sample_data.outputs.model_output,\n",
    "        \"scored_data\": score_with_sample_data.outputs.score_output,\n",
    "        \"evaluation_report\": eval_with_sample_data.outputs.eval_output,\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_job = pipeline_with_registered_components(\n",
    "    training_input=Input(type=\"uri_folder\", path=parent_dir + \"/data/\"),\n",
    "    test_input=Input(type=\"uri_folder\", path=parent_dir + \"/data/\"),\n",
    "    training_max_epochs=20,\n",
    "    training_learning_rate=1.8,\n",
    "    learning_rate_schedule=\"time-based\",\n",
    ")\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"cpu-cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_samples\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of running a pipeline job [here](../)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Register component and then use these components to build pipeline"
  },
  "interpreter": {
   "hash": "2b36b3e0fd12bdabecfca52fa8a9186c77ed407a10624692f08cbfbcbeac8e33"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
