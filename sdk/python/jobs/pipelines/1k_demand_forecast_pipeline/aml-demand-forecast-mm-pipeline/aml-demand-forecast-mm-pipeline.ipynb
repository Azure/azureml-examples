{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "\n",
    "## Demand Forecasting Using Many Models\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Compute](#Compute)\n",
    "1. [Data](#Data)\n",
    "1. [Import Components From Registry](#ImportComponents)\n",
    "1. [Create a Pipeline](#CreatePipeline)\n",
    "1. [Kick Off Pipeline Runs](#PipelineRuns)\n",
    "1. [Download Output](#DownloadOutput)\n",
    "1. [Compare Evaluation Results](#CompareResults)\n",
    "1. [Deployment](#Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The objective of this notebook is to illustrate how to use the component-based AutoML many models solution for demand forecasting tasks. It walks you through all stages of model evaluation and production process starting with data ingestion and concluding with batch endpoint deployment for production.\n",
    "\n",
    "We use a subset of UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)) with the objective of predicting electricity demand per consumer 24 hours ahead.\n",
    "\n",
    "There are a number of steps you need to take before you can put a model into production. A user needs to prepare the data, partition it into appropriate sets, select the best model, evaluate it against a baseline, and monitor the model in real life to collect enough observations on how it would perform had it been put in production. Some of these steps are time consuming, some require certain expertise in writing code. The steps shown in this notebook follow a typical thought process one follows before the model is put in production.\n",
    "\n",
    "Make sure you have executed the [configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1682992408484
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import yaml\n",
    "import azure.ai.ml\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import (\n",
    "    BatchEndpoint,\n",
    "    BatchDeployment,\n",
    "    AmlCompute,\n",
    "    PipelineComponentBatchDeployment,\n",
    ")\n",
    "\n",
    "print(f\"SDK version: {azure.ai.ml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential does not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "    print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.subscription_id\n",
    "output[\"Resource Group\"] = ws.resource_group\n",
    "output[\"Location\"] = ws.location\n",
    "pd.DataFrame(data=output, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute\n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "\n",
    "You will need to create a compute target for your AutoML run. In this tutorial, you will create AmlCompute as your training compute resource.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "\n",
    "Here, we use a 5 node cluster of the `STANDARD_DS15_V2` series for illustration purposes. You will need to adjust the compute type and the number of nodes based on your needs which can be driven by the speed needed for model selection, data size, etc. \n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "amlcompute_cluster_name = \"demand-fcst-mm-cluster\"\n",
    "\n",
    "try:\n",
    "    # Retrieve an already attached Azure Machine Learning Compute.\n",
    "    compute_target = ml_client.compute.get(amlcompute_cluster_name)\n",
    "except ResourceNotFoundError as e:\n",
    "    compute_target = AmlCompute(\n",
    "        name=amlcompute_cluster_name,\n",
    "        size=\"STANDARD_DS15_V2\",\n",
    "        type=\"amlcompute\",\n",
    "        min_instances=0,\n",
    "        max_instances=5,\n",
    "        idle_time_before_scale_down=600,\n",
    "    )\n",
    "    poller = ml_client.begin_create_or_update(compute_target)\n",
    "    poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data\n",
    "\n",
    "For illustration purposes we use the UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)). The original dataset contains electricity consumption data for 370 consumers measured at 15 minute intervals. In the data set for this demonstrations, we have aggregated to an hourly frequency and converted to the kilowatt hours (kWh) for 10 customers.\n",
    "\n",
    "The data for this notebook is located in the `automl-sample-notebook-data` container in the datastore and is publicly available. In the next few cells, we will download the train, test and inference datasets from the public datastore and store them locally in the _parquet_ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"https://automlsamplenotebookdata-adcuc7f7bqhhh8a4.b02.azurefd.net/automl-sample-notebook-data/uci-demand-pipeline-data-mm/train/uci_electro_small_mm_train.parquet\"\n",
    "test_data_path = \"https://automlsamplenotebookdata-adcuc7f7bqhhh8a4.b02.azurefd.net/automl-sample-notebook-data/uci-demand-pipeline-data-mm/test/uci_electro_small_mm_test.parquet\"\n",
    "inference_data_path = \"https://automlsamplenotebookdata-adcuc7f7bqhhh8a4.b02.azurefd.net/automl-sample-notebook-data/uci-demand-pipeline-data-mm/inference/uci_electro_small_mm_inference.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = \"datetime\"\n",
    "target_column_name = \"usage\"\n",
    "time_series_id_column_names = [\"customer_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_and_save_as_parquet(file_uri, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    data_frame = pd.read_parquet(file_uri)\n",
    "    file_name = os.path.split(file_uri)[-1]\n",
    "    data_path = os.path.join(output_folder, file_name)\n",
    "    data_frame.to_parquet(data_path, index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "create_folder_and_save_as_parquet(train_data_path, \"./data/train\")\n",
    "create_folder_and_save_as_parquet(test_data_path, \"./data/test\")\n",
    "create_folder_and_save_as_parquet(inference_data_path, \"./data/inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells read and print the first few rows of the training data as well as the number of unique time series in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./data/train/\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nseries = df.groupby(time_series_id_column_names).ngroups\n",
    "print(f\"Data contains {nseries} individual time series\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import Components From Registry\n",
    "\n",
    "An Azure Machine Learning component is a self-contained piece of code that does one step in a machine learning pipeline. A component is analogous to a function - it has a name, inputs, outputs, and a body. Components are the building blocks of the Azure Machine Learning pipelines. It's a good engineering practice to build a machine learning pipeline where each step has well-defined inputs and outputs. In Azure Machine Learning, a component represents one reusable step in a pipeline. Components are designed to help improve the productivity of pipeline building. Specifically, components offer:\n",
    "- Well-defined interface: Components require a well-defined interface (input and output). The interface allows the user to build steps and connect steps easily. The interface also hides the complex logic of a step and removes the burden of understanding how the step is implemented.\n",
    "\n",
    "- Share and reuse: As the building blocks of a pipeline, components can be easily shared and reused across pipelines, workspaces, and subscriptions. Components built by one team can be discovered and used by another team.\n",
    "\n",
    "- Version control: Components are versioned. The component producers can keep improving components and publish new versions. Consumers can use specific component versions in their pipelines. This gives them compatibility and reproducibility.\n",
    "\n",
    "For a more detailed information on this subject, refer to the this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2).\n",
    "\n",
    "To import components,  we need to get the registry. The following command obtains the public regsitry from which we will import components for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682992409837
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get registry for all components\n",
    "ml_client_registry = MLClient(credential=credential, registry_name=\"azureml\")\n",
    "print(ml_client_registry)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pull specific components and use them to build a pipeline of steps. For the illustration of the product evaluation workflow we will use the following components:\n",
    "- Data partitioning component: allows users to partion the data for many models runs, both, training and inference.\n",
    "- Many models training component: trains the best model per partition specified by users.\n",
    "- Many moodels inference componnet: generates forecast for each partition. This can be done on the test and inference sets.\n",
    "- Compute metrics component: calculates metrics per time series if the inference component was used on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_component_from_registry = ml_client_registry.components.get(\n",
    "    name=\"automl_tabular_data_partitioning\", label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Data partitioning component version: {partition_component_from_registry.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682992443218
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_component_from_registry = ml_client_registry.components.get(\n",
    "    name=\"automl_many_models_training\",\n",
    "    label=\"latest\",\n",
    ")\n",
    "print(\n",
    "    f\"Many models training component version: {train_component_from_registry.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996405805
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_component_from_registry = ml_client_registry.components.get(\n",
    "    name=\"automl_many_models_inference\", label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Many models inference component version: {train_component_from_registry.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996406713
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "compute_metrics_component = ml_client_registry.components.get(\n",
    "    name=\"compute_metrics\",\n",
    "    version=\"0.0.26\",  # label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Many models inference component version: {compute_metrics_component.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1682996406974
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## 6. Create a Pipeline\n",
    "\n",
    "Now that we imported the components we will build an evaluation pipeline. This pipeline will allow us to partition the data, train best models for each partition, genererate rolling forecasts on the test set, and, finally, calculate metrics on the test set output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Create a YML\n",
    "\n",
    "AzureML components can only receive specific object types such as strings, JSON/YML files, URI Folders and URI Files. Other object types are not accepted. Because of this, the settings needs to be passed into the training component in YML format.\n",
    "\n",
    "The following are the bare-minimum parameters needed to successfully train many models. For a finer control of the experiment a user may add other parameters to the config file. See the [forecast settings API doc](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.automl.forecastingjob#azure-ai-ml-automl-forecastingjob-set-forecast-settings) for a complete list of available parameters. \n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **task**                           | forecasting |\n",
    "| **primary_metric**                 | This is the metric that you want to optimize. Forecasting supports the following primary metrics<ul><li>`normalized_root_mean_squared_error`</li><li>`normalized_mean_absolute_error`</li><li>`spearman_correlation`</li><li>`r2_score`</li></ul> We recommend using either the normalized root mean squared error or normalized mean absolute erorr as a primary metric because they measure forecast accuracy. See the [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-automl-forecasting-faq#how-do-i-choose-the-primary-metric) for a more detailed discussion on this topic. |\n",
    "| **forecast_horizon**       | The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly). |\n",
    "| **label_column_name**      | The name of the target column we are trying to predict. |\n",
    "| **time_column_name**       | The name of your time column. |\n",
    "| **time_series_id_column_names** | The column names used to uniquely identify timeseries in data that has multiple rows with the same timestamp. |\n",
    "| **enable_early_stopping**  | Flag to enable early termination if the primary metric is no longer improving. |\n",
    "| **partition_column_names** | The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series. |\n",
    "| **allow_multi_partitions** | A flag that allows users to train one model per partition when each partition contians more than one unique time series. The dafault value is `False`. |\n",
    "| **track_child_runs**       | Flag to enable tracking of child runs. Only best run is tracked if the flag is set to False (this includes the model and metrics of the run). Defaults to `False`. We do not encourage to turn this on since it can lead to throttling, instead use `n_best_runs` if you really need to track more than one best run. |\n",
    "| **n_best_runs**            | Number of best runs to track per partition for a Many Models Run. Defaults to 1. |\n",
    "| **enable_early_stopping**  | Flag to enable early termination if the primary metric is no longer improving. |\n",
    "| **max_trials** | Represents the maximum number of trials an Automated ML job can try to run a training algorithm with different combination of hyperparameters. Its default value is set to 1000. If `enable_early_stopping` is defined, then the number of trials used to run training algorithms can be smaller.|\n",
    "| **timeout_minutes** | Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. This timeout includes setup, featurization and training runs but does not include the ensembling and model explainability runs at the end of the process since those actions need to happen once all the trials (children jobs) are done. If not specified, the default job's total timeout is 6 days (8,640 minutes). To specify a timeout less than or equal to 1 hour (60 minutes), make sure your dataset's size is not greater than 10,000,000 (rows times column) or an error results. |\n",
    "| **trial_timeout_minutes**  | Maximum time in minutes that each trial (child job) can run for before it terminates. If not specified, a value of 1 month or 43200 minutes is used. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_horizon = 24\n",
    "partition_column_names = [\"customer_id\"]\n",
    "allow_multi_partitions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Required parameters\n",
    "automl_settings = dict(\n",
    "    task=\"forecasting\",\n",
    "    primary_metric=\"normalized_root_mean_squared_error\",\n",
    "    debug_log=\"debug.txt\",\n",
    "    label_column_name=target_column_name,\n",
    "    time_column_name=time_column_name,\n",
    "    forecast_horizon=max_horizon,\n",
    "    time_series_id_column_names=time_series_id_column_names,\n",
    "    partition_column_names=partition_column_names,\n",
    "    max_trials=25,\n",
    "    timeout_minutes=60,\n",
    "    trial_timeout_minutes=5,\n",
    "    n_cross_validations=2,\n",
    "    forecast_step=max_horizon,\n",
    "    track_child_runs=False,\n",
    "    enable_early_stopping=True,\n",
    "    allow_multi_partitions=allow_multi_partitions,\n",
    ")\n",
    "pd.DataFrame(data=automl_settings, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save these settings as the `automl_settings.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"automl_settings.yml\", \"w\") as file:\n",
    "    yaml.dump(automl_settings, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Provide additional pipeline parameters\n",
    "\n",
    "The next set of parameters is necessary to build the pipeline of components. These parameters are specific to the many models training and/or inference components. Since both of these components rely on the Parallel run step (PRS) to train/inference multiple models at once, you will need to determine the appropriate number of workers and nodes for your use case. The `max_concurrency_per_node` is based off the number of cores of the compute VM. The `max_nodes` will determine the number of nodes to use, increasing the node count will speed up the training process.\n",
    "\n",
    "|Property|Description|\n",
    "|:-|:-|\n",
    "| **max_nodes**                     | The number of compute nodes in a cluster to be used for training and inferencing steps. We recommend to start with 3 and increase the node_count if the training time is taking too long. |\n",
    "| **max_concurrency_per_node**   | Process count per node. We recommend a 2:1 ratio for number of cores to the number of processes per node. For example, if a node has 16 cores then configure 8 **or less** process counts per node for optimal performance. |\n",
    "|**retrain_failed_model**| If training a model for any partition fails, should AutoML kick off a new child run for that partition? Possible values are `True` or `False`.|\n",
    "|**forecast_mode**| Type of forecat to perform on the test set. Can be `recursive` or `rolling`. Rolling forecast can be used for the evaluation purposes. |\n",
    "|**forecast_step**| The forecast step used for rolling forecast. See this [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast?view=azureml-api-2#evaluating-model-accuracy-with-a-rolling-forecast) for more details on this parameter. |\n",
    "| **parallel_step_timeout_in_seconds**         | Maximum amount of time in seconds that the `ParallelRunStep` class is allowed. This is optional but provides customers with greater control on exit criteria. This must be greater than `experiment_timeout_hours` by at least 300 seconds. |\n",
    "|**partition_column_names**| The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series. This parameter is identical to the one in the `automl_config` object.|\n",
    "|**compute_name**| Name of the compute to execute the pipeline on. |\n",
    "|**enable_event_logger**| Set this value to `True` to enable event logger. |\n",
    "| **input_type**               | Type of file format for the input data. Supported options are `csv` and `parquet`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline parameters\n",
    "pipeline_parameters = dict(\n",
    "    max_nodes=1,\n",
    "    max_concurrency_per_node=10,\n",
    "    retrain_failed_model=True,\n",
    "    forecast_mode=\"rolling\",\n",
    "    forecast_step=max_horizon,\n",
    "    parallel_step_timeout_in_seconds=3700,\n",
    "    partition_column_names=partition_column_names,\n",
    "    compute_name=amlcompute_cluster_name,\n",
    "    enable_event_logger=True,\n",
    "    input_type=\"parquet\",\n",
    ")\n",
    "print(\"Pipeline parameters\\n---\")\n",
    "display(pd.DataFrame(data=pipeline_parameters, index=[\"\"]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data partitioning component allows us to partition the large datasets and should be used when the data is too large to be partitioned on a single node. If your dataset is small relative to the RAM of a single node in your cluster, you most likely do not need this component since both training and inference components perform data partitionining as part of their internal work flow. The partitioning that takes place inside these components is done on a single node of the cluster the pipeline is executed on. \n",
    "\n",
    "If the data is too big to be handled internally (we are talking about size that are large than 2GB of data and the compute RAM of 28GB or less), you may want to use the partitioning component which uses spark cluster for the job. Since the data we are working with is is not big, we do not need the partitioning component. However, this notebook is written to handle both scenarios. If you choose to run a spark job, you need to specify a separate set of parameters to the pipeline builder which must incude the following:\n",
    "\n",
    "|Property|Description|\n",
    "|:-|:-|\n",
    "| **instance_type**            | A key that defines the compute instance type to be used for the serverless Spark compute. The following instance types are currently supported:<ul><li>`Standard_E4S_V3`</li><li>`Standard_E8S_V3`</li><li>`Standard_E16S_V3`</li><li>`Standard_E32S_V3`</li><li>`Standard_E64S_V3`</li></ul>|\n",
    "| **runtime_version**          | A key that defines the Spark runtime version. The following Spark runtime versions are currently supported:<ul><li>`3.3.0`</li><li>`3.4.0`</li></ul> |\n",
    "| **driver_cores**       | The he number of cores allocated for the Spark driver. |\n",
    "| **driver_memory**      | The allocated memory for the Spark exedriver, with a size unit suffix `k`, `m`, `g` or `t` (for example, `512m`, `2g`). |\n",
    "| **executor_cores**     | The number of cores allocated for the Spark executor. |\n",
    "| **executor_memory**    | The allocated memory for the Spark executor, with a size unit suffix `k`, `m`, `g` or `t` (for example, `512m`, `2g`). |\n",
    "| **executor_instances** | The number of Spark executor instances|\n",
    "\n",
    "All of these parameters are described in [this document](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-submit-spark-jobs?view=azureml-api-2&tabs=sdk). In this notebook we are using the serverless spark cluster, so we may not have the attached spark workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distinguish whether the pipeline uses the partitioning component, we use the `USE_PARTITIONING_COMPONENT` parameter. When it is set to `True`, partitioning component is added the pipeline in section 6.3. Since the dataset we are working with is small, there is no need for this component, so we set the parameter value to `False` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PARTITIONING_COMPONENT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Spark parameters (optional)\n",
    "if USE_PARTITIONING_COMPONENT:\n",
    "    spark_parameters = dict(\n",
    "        instance_type=\"Standard_E4S_V3\",\n",
    "        runtime_version=\"3.3.0\",\n",
    "        driver_cores=1,\n",
    "        driver_memory=\"2g\",\n",
    "        executor_cores=2,\n",
    "        executor_memory=\"2g\",\n",
    "        executor_instances=2,\n",
    "    )\n",
    "    print(\"Spark parameters\\n---\")\n",
    "    display(pd.DataFrame(data=spark_parameters, index=[\"\"]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Build a Pipeline\n",
    "\n",
    "Next, we build a pipeline from the imported components. Since this notebook is designed to illustrate the evaluation flow, we will string these components in the following fashion. First, we train the best model for each partition. Then, we generate a rolling forecast with the step size of 24 (hours) on the test set. This is done to mimic the evaluation process when a customer is tracking model's performance in real time and generates forecasts every 24 hours. Finally, we compute metrics based on the rolling forecast output from the previous step. You do not have to modify anything in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996407118
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "@pipeline(description=\"AutoML Forecasting Many Models Evaluation Pipeline\")\n",
    "def evaluation_pipeline(raw_data, inference_data, automl_config):\n",
    "    # 0. Extract pipeline parameters from the dictionary\n",
    "    partition_column_names = \" \".join(pipeline_parameters.get(\"partition_column_names\"))\n",
    "    compute_name = pipeline_parameters.get(\"compute_name\")\n",
    "    max_concurrency_per_node = pipeline_parameters.get(\"max_concurrency_per_node\")\n",
    "    parallel_step_timeout_in_seconds = pipeline_parameters.get(\n",
    "        \"parallel_step_timeout_in_seconds\", 3700\n",
    "    )\n",
    "    max_nodes = pipeline_parameters.get(\"max_nodes\", 1)\n",
    "    enable_event_logger = pipeline_parameters.get(\"enable_event_logger\", True)\n",
    "    retrain_failed_model = pipeline_parameters.get(\"retrain_failed_model\", True)\n",
    "    forecast_mode = pipeline_parameters.get(\"forecast_mode\", \"recursive\")\n",
    "    forecast_step = pipeline_parameters.get(\"forecast_step\", 1)\n",
    "    input_type = pipeline_parameters.get(\"input_type\", \"csv\")\n",
    "\n",
    "    if USE_PARTITIONING_COMPONENT:\n",
    "        # 1. Data partitioning step\n",
    "        partition_step = partition_component_from_registry(\n",
    "            raw_data=raw_data,\n",
    "            partition_column_names=partition_column_names,\n",
    "            input_type=pipeline_parameters.get(\"input_type\", \"csv\"),\n",
    "        )\n",
    "\n",
    "        partition_step.resources = {\n",
    "            \"instance_type\": spark_parameters.get(\"instance_type\", \"Standard_E4S_V3\"),\n",
    "            \"runtime_version\": str(spark_parameters.get(\"runtime_version\", \"3.3.0\")),\n",
    "        }\n",
    "        partition_step.conf = {\n",
    "            \"spark.driver.cores\": spark_parameters.get(\"driver_cores\", 1),\n",
    "            \"spark.driver.memory\": str(spark_parameters.get(\"driver_memory\", \"2g\")),\n",
    "            \"spark.executor.cores\": spark_parameters.get(\"executor_cores\", 2),\n",
    "            \"spark.executor.memory\": str(spark_parameters.get(\"executor_memory\", \"2g\")),\n",
    "            \"spark.executor.instances\": spark_parameters.get(\"executor_instances\", 2),\n",
    "        }\n",
    "        partition_step.outputs.partitioned_data.mode = \"direct\"\n",
    "\n",
    "        # 2. Model training step\n",
    "        training_node = train_component_from_registry(\n",
    "            raw_data=partition_step.outputs.partitioned_data,\n",
    "            automl_config=automl_config,\n",
    "            max_concurrency_per_node=max_concurrency_per_node,\n",
    "            parallel_step_timeout_in_seconds=parallel_step_timeout_in_seconds,\n",
    "            max_nodes=max_nodes,\n",
    "            retrain_failed_model=retrain_failed_model,\n",
    "            compute_name=compute_name,\n",
    "        )\n",
    "    else:\n",
    "        # 2. Model training step\n",
    "        training_node = train_component_from_registry(\n",
    "            raw_data=raw_data,\n",
    "            automl_config=automl_config,\n",
    "            max_concurrency_per_node=max_concurrency_per_node,\n",
    "            parallel_step_timeout_in_seconds=parallel_step_timeout_in_seconds,\n",
    "            max_nodes=max_nodes,\n",
    "            retrain_failed_model=retrain_failed_model,\n",
    "            compute_name=compute_name,\n",
    "        )\n",
    "\n",
    "    # 3. Inferencing step\n",
    "    inference_node = inference_component_from_registry(\n",
    "        raw_data=inference_data,\n",
    "        max_nodes=max_nodes,\n",
    "        max_concurrency_per_node=max_concurrency_per_node,\n",
    "        parallel_step_timeout_in_seconds=parallel_step_timeout_in_seconds,\n",
    "        optional_train_metadata=training_node.outputs.run_output,\n",
    "        forecast_mode=forecast_mode,\n",
    "        forecast_step=forecast_step,\n",
    "        compute_name=compute_name,\n",
    "    )\n",
    "\n",
    "    # 4. Metrics calculation step\n",
    "    compute_metrics_node = compute_metrics_component(\n",
    "        task=\"tabular-forecasting\",\n",
    "        prediction=inference_node.outputs.evaluation_data,\n",
    "        ground_truth=inference_node.outputs.evaluation_data,\n",
    "        evaluation_config=inference_node.outputs.evaluation_configs,\n",
    "    )\n",
    "    compute_metrics_node.compute = compute_name\n",
    "\n",
    "    # 5. Specify pipeline outputs\n",
    "    return {\n",
    "        \"output_files\": compute_metrics_node.outputs.evaluation_result,\n",
    "        \"forecast_output\": inference_node.outputs.raw_predictions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Kick Off Pipeline Runs\n",
    "\n",
    "Now that the pipeline is defined, we will use it to kick off several runs. First, we will kick off an experiment which will train, inference and evaluate the performance for the best AutoML model for each partition. Next, we will kick off the same pipeline which will only use the naive model for the same partitions. This will allow us to establish a baseline and compare performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Kick Off Best Many Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996407285
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = evaluation_pipeline(\n",
    "    raw_data=Input(type=AssetTypes.URI_FOLDER, path=\"./data/train\"),\n",
    "    inference_data=Input(type=AssetTypes.URI_FOLDER, path=\"./data/test\"),\n",
    "    automl_config=Input(type=AssetTypes.URI_FILE, path=\"./automl_settings.yml\"),\n",
    ")\n",
    "if not USE_PARTITIONING_COMPONENT:\n",
    "    pipeline_job.settings.default_compute = amlcompute_cluster_name\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682997237524
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_experiment_name = \"mm-experiment-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y%m%d\"\n",
    ")\n",
    "\n",
    "pipeline_submitted_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job,\n",
    "    experiment_name=evaluation_experiment_name,\n",
    "    skip_validation=True,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rehydrate run\n",
    "# RUN_ID = \"<Paste the PipelineRunId from the output of the previous cell.>\"\n",
    "# pipeline_submitted_job = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Kick Off the Baseline Experiment\n",
    "\n",
    "To establish a baseline, we will use the same pipeline as before with one minore change. We will add Naive model to the allowed model list and change the number of rolling origin cross validations (ROCV) to 2. Reducing the ROCV speeds up the runtime and is needed for model selection only, while in this run we have only one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_settings = automl_settings.copy()\n",
    "baseline_settings.update(\n",
    "    {\"allowed_training_algorithms\": [\"Naive\"], \"n_cross_validations\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we have done in section 6.3, we save the baseline experiment settings to as a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"automl_settings_base.yml\", \"w\") as file:\n",
    "    yaml.dump(baseline_settings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job_base = evaluation_pipeline(\n",
    "    raw_data=Input(type=AssetTypes.URI_FOLDER, path=\"./data/train\"),\n",
    "    inference_data=Input(type=AssetTypes.URI_FOLDER, path=\"./data/test\"),\n",
    "    automl_config=Input(type=\"uri_file\", path=\"./automl_settings_base.yml\"),\n",
    ")\n",
    "if not USE_PARTITIONING_COMPONENT:\n",
    "    pipeline_job_base.settings.default_compute = amlcompute_cluster_name\n",
    "print(pipeline_job_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_experiment_name = \"mm-experiment-base-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y%m%d\"\n",
    ")\n",
    "\n",
    "pipeline_submitted_job_base = ml_client.jobs.create_or_update(\n",
    "    pipeline_job_base,\n",
    "    experiment_name=base_experiment_name,\n",
    "    skip_validation=True,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job_base.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rehydrate baseline run\n",
    "# RUN_ID = \"<Paste the PipelineRunId from the output of the previous cell.>\"\n",
    "# pipeline_submitted_job_base = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Pipeline Output\n",
    "Next, we will download the output files generated by the compute metrics components for each executed pipeline and save them in the corresponfing subfolder of the `output` folder. First, we create corresponding output directories. Then, we execute the `ml_client.jobs.download` command which downloads experiments' outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directories\n",
    "mm_output_dir = os.path.join(os.getcwd(), \"output/many-models\")\n",
    "base_output_dir = os.path.join(os.getcwd(), \"output/base\")\n",
    "\n",
    "os.makedirs(mm_output_dir, exist_ok=True)\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    pipeline_submitted_job.name, download_path=mm_output_dir, output_name=\"output_files\"\n",
    ")\n",
    "ml_client.jobs.download(\n",
    "    pipeline_submitted_job_base.name,\n",
    "    download_path=base_output_dir,\n",
    "    output_name=\"output_files\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    pipeline_submitted_job.name,\n",
    "    download_path=mm_output_dir,\n",
    "    output_name=\"forecast_output\",\n",
    ")\n",
    "\n",
    "ml_client.jobs.download(\n",
    "    pipeline_submitted_job_base.name,\n",
    "    download_path=base_output_dir,\n",
    "    output_name=\"forecast_output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Compare Evaluation Results\n",
    "\n",
    "### 9.1. Examine Metrics\n",
    "\n",
    "In this section, we compare metrics for the 2 pipeline runs to quantify accuracy improvement of AutoML over the baseline model. First, we compare metrics that are calculated for the entire dataset. Since there are 10 unique time series in the test dataset, these individual metrics are aggregated into a single number. The non-normalized metrics can be misleading due to the difference in scales of each unique time series. The following [article (placeholder)](https://review.learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2&branch=pr-en-us-238443#forecasting-metrics-normalization-and-aggregation) explains this topic in a greater detail.\n",
    "\n",
    "The code in the next cell loads dataset metrics for each of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_artifacts_path = os.path.join(\n",
    "    \"named-outputs\", \"output_files\", \"evaluationResult\"\n",
    ")\n",
    "\n",
    "with open(os.path.join(mm_output_dir, metrics_artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_automl_series = json.load(f)\n",
    "\n",
    "with open(os.path.join(base_output_dir, metrics_artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_base_series = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge two dataframes to examine metrics side by side. The `metrics_all` data frame contains two columns which correspond to the scores from the many models and the baseline experiments, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_automl = (\n",
    "    pd.Series(metrics_automl_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "metrics_base = (\n",
    "    pd.Series(metrics_base_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "metrics_all = pd.DataFrame(\n",
    "    [metrics_automl_series, metrics_base_series], index=[\"score_automl\", \"score_base\"]\n",
    ").T\n",
    "metrics_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.1 Detailed Metrics\n",
    "\n",
    "Next, we will load and examine the detailed accuracy metrics since the aggregate metrics may not convey enough information to make a decision about product accuracy. It may be helpful to examine metrics at a more granular level. We will extract metrics per time series. To do this, we create a helper function `extract_specific_metric` which reads the JSON file and returns a specified metric for each time series. Even though the file contains the following metrics, we will  we will focus on the normalized root mean squared error (NRMSE) accuracy metric for illustration purposes. <ul>\n",
    "    <li> `explained_variance` </li>\n",
    "    <li> `mean_absolute_error` </li>\n",
    "    <li> `mean_absolute_percentage_error`</li>\n",
    "    <li> `median_absolute_error`</li>\n",
    "    <li> `normalized_median_absolute_error`</li>\n",
    "    <li> `normalized_root_mean_squared_error`</li>\n",
    "    <li> `normalized_root_mean_squared_error`</li>\n",
    "    <li> `normalized_root_mean_squared_log_error`</li>\n",
    "    <li> `r2_score`</li>\n",
    "    <li> `root_mean_squared_log_error`</li>\n",
    "    <li> `root_mean_squared_error`</li>\n",
    "    <li> `root_mean_squared_log_error`</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_specific_metric(path, metric_name):\n",
    "    with open(path) as f:\n",
    "        artifact = json.load(f)\n",
    "    all_metrics = pd.DataFrame(artifact[\"data\"])\n",
    "    index_scores = time_series_id_column_names + [metric_name]\n",
    "    return all_metrics[index_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_table_relative_path = os.path.join(\n",
    "    metrics_artifacts_path, \"artifacts\", \"forecast_time_series_id_distribution_table\"\n",
    ")\n",
    "automl_metric = extract_specific_metric(\n",
    "    os.path.join(mm_output_dir, metrics_table_relative_path),\n",
    "    \"normalized_root_mean_squared_error\",\n",
    ")\n",
    "\n",
    "base_metric = extract_specific_metric(\n",
    "    os.path.join(base_output_dir, metrics_table_relative_path),\n",
    "    \"normalized_root_mean_squared_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = automl_metric.merge(\n",
    "    base_metric,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"inner\",\n",
    "    suffixes=[\"_automl\", \"_base\"],\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Generate Time Series Plots\n",
    "\n",
    "Here, we generate forecast versus actuals plot for the test set for both the best many models and the baseline. Since we use rolling evaluation with the step size of 24 hours, this mimics the behavior of putting both models in production and monitoring their behavior for the duration of the test set. This step helps you make informed decisions about model performance and saves numerous costs associated with productionalizing the model and monitoring its performance in real life. \n",
    "\n",
    "In the next block of code, we, load the test set output for each of the runs and merge the data. Then, we generate and save time series plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_table_relative_path = os.path.join(\"named-outputs\", \"forecast_output\")\n",
    "\n",
    "forecast_column_name = \"automl_prediction\"\n",
    "base_forecast_column_name = \"base_predictions\"\n",
    "actual_column_name = \"automl_actual\"\n",
    "forecast_origin_column_name = \"forecast_origin\"\n",
    "\n",
    "automl_fcst = pd.read_parquet(os.path.join(mm_output_dir, forecast_table_relative_path))\n",
    "base_fcst = pd.read_parquet(os.path.join(base_output_dir, forecast_table_relative_path))\n",
    "\n",
    "merge_columns = time_series_id_column_names + [actual_column_name]\n",
    "merge_columns.extend([time_column_name, forecast_origin_column_name])\n",
    "\n",
    "backtest = automl_fcst.merge(\n",
    "    base_fcst.rename(columns={forecast_column_name: base_forecast_column_name}),\n",
    "    on=merge_columns,\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(f\"AutoML forecast table size: {automl_fcst.shape}\\n---\")\n",
    "print(f\"Base forecast table size: {base_fcst.shape}\\n---\")\n",
    "print(f\"Merged forecast table size: {backtest.shape}\\n---\")\n",
    "backtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper_scripts import draw_one_plot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plot_filename = \"forecast_vs_actual.pdf\"\n",
    "\n",
    "pdf = PdfPages(os.path.join(os.getcwd(), \"./output\", plot_filename))\n",
    "for _, one_forecast in backtest.groupby(time_series_id_column_names):\n",
    "    one_forecast[time_column_name] = pd.to_datetime(one_forecast[time_column_name])\n",
    "    one_forecast.sort_values(time_column_name, inplace=True)\n",
    "    draw_one_plot(\n",
    "        one_forecast,\n",
    "        time_column_name,\n",
    "        target_column_name,\n",
    "        time_series_id_column_names,\n",
    "        [actual_column_name, forecast_column_name, base_forecast_column_name],\n",
    "        pdf,\n",
    "        plot_predictions=True,\n",
    "    )\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(os.path.join(\"./output/forecast_vs_actual.pdf\"), width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deployment\n",
    "\n",
    "In this section, we will illustrate how to deploy and inference models using batch endpoint. Batch endpoints are endpoints that are used to do batch inferencing on large volumes of data in asynchronous way. Batch endpoints receive pointers to data and run jobs asynchronously to process the data in parallel on compute clusters and store outputs to a datastore for further analysis. For more information on batch endpoints see this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-batch?view=azureml-api-2).\n",
    "\n",
    "### 10.1. Create Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Creating a unique endpoint name by including a random suffix\n",
    "allowed_chars = string.ascii_lowercase + string.digits\n",
    "endpoint_suffix = \"\".join(random.choice(allowed_chars) for x in range(5))\n",
    "endpoint_name = \"sdk-many-models-\" + endpoint_suffix\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = BatchEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"A many models endpoint for component deployments\",\n",
    "    properties={\"ComponentDeployment.Enabled\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the Endpoint in the workspace usign the MLClient created earlier. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Create the Deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = PipelineComponentBatchDeployment(\n",
    "    name=\"sdk-many-models-deployment\",\n",
    "    description=\"A many models deployment.\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    component=inference_component_from_registry.id,\n",
    "    settings={\"default_compute\": amlcompute_cluster_name},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the deployment in the workspace usign the MLClient created earlier. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Invoke the Endpoint\n",
    "\n",
    "The next cell contians the command that invokes the endpoint for batch inference job. The `invoke` method contains the `inputs` parameter. This parameter contains the inputs necessary to execute the inference component on the endpoint. To convince yourself this is the case, compare the input parameters for the `inference_component_from_registry` in section 6.3 with the `inputs` we are proving in the next cell. They are identical.\n",
    "\n",
    "Notice, the the `forecast_mode` is set to `\"recursive\"`. In the evaluation pipeline this component was used to generate rolling forecast to evalaute model performance on the test set. For more details on rolling evaluation, see our [forecasting model evaluation article](placeholder). Here, we are using it to generate a forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=endpoint.name,\n",
    "    deployment_name=deployment.name,\n",
    "    inputs={\n",
    "        \"raw_data\": Input(type=AssetTypes.URI_FOLDER, path=\"./data/inference\"),\n",
    "        \"training_experiment_name\": Input(\n",
    "            type=\"string\", default=evaluation_experiment_name\n",
    "        ),\n",
    "        \"max_nodes\": Input(type=\"integer\", default=1),\n",
    "        \"max_concurrency_per_node\": Input(type=\"integer\", default=5),\n",
    "        \"compute_name\": Input(type=\"string\", default=amlcompute_cluster_name),\n",
    "        \"forecast_mode\": Input(type=\"string\", default=\"recursive\"),\n",
    "        \"parallel_step_timeout_in_seconds\": Input(type=\"integer\", default=3700),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will stream the job output to monitor the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = job.name\n",
    "batch_job = ml_client.jobs.get(name=job_name)\n",
    "print(f\"Batch job status: {batch_job.status}\\n---\")\n",
    "ml_client.jobs.stream(name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4. Download Forecast Output\n",
    "\n",
    "Finally, we download the forecast output and print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the run result was posted on Azure\n",
    "final_state = (\"Completed\", \"Failed\")\n",
    "while (\n",
    "    batch_job.status not in final_state\n",
    "    or len(list(ml_client.jobs.list(parent_job_name=batch_job.name))) < 3\n",
    "):\n",
    "    print(\"The runs were not posted... Re-trying in 10 seconds.\\n---\")\n",
    "    batch_job = ml_client.jobs.get(name=job_name)\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_output_dir = os.path.join(os.getcwd(), \"forecast\")\n",
    "\n",
    "for child in ml_client.jobs.list(parent_job_name=job.name):\n",
    "    print(f\"{child.name}\\n---\")\n",
    "    if (\n",
    "        child.properties[\"azureml.moduleName\"]\n",
    "        == \"automl_many_models_inference_collect_step\"\n",
    "    ):\n",
    "        print(\"Downloading data ...\\n---\")\n",
    "        for attempt in range(3):\n",
    "            print(f\"Attempt: {attempt}\")\n",
    "            try:\n",
    "                ml_client.jobs.download(\n",
    "                    child.name, download_path=fcst_output_dir, output_name=\"metadata\"\n",
    "                )\n",
    "                break\n",
    "            except BaseException:\n",
    "                sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_df = pd.read_parquet(\n",
    "    os.path.join(fcst_output_dir, \"named-outputs\", \"metadata\", \"raw_forecast\")\n",
    ")\n",
    "fcst_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5. [Optional] Delete the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint.name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
