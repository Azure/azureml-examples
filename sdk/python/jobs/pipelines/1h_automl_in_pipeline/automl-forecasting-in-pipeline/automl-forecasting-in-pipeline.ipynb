{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Forecasting Training and Inferencing using Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with computer cluster - [Configure workspace](../../configuration.ipynb)\n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section\n",
    "\n",
    "**Learning Objectives** - By the end of this tutorial, you should be able to:\n",
    "- Create a Forecasting AutoML task in pipeline.\n",
    "\n",
    "**Motivations** - This notebook explains how to use Forecasting AutoML task inside pipeline.\n",
    "\n",
    "In this notebook, we demonstrate how to use piplines to train and inference on AutoML Forecasting model. Two pipelines will be created: one for training AutoML model, and the other is for inference on AutoML model. We'll also demonstrate how to schedule the inference pipeline so you can get inference results periodically (with refreshed test dataset). Make sure you have executed the configuration notebook before running this notebook. In this notebook you will learn how to:\n",
    "\n",
    "- Configure AutoML forecasting tasks.\n",
    "- Create and register an AutoML model using AzureML pipeline.\n",
    "- Inference and schedule the pipeline using registered model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, command, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.automl import forecasting\n",
    "from azure.ai.ml.entities._job.automl.tabular.forecasting_settings import (\n",
    "    ForecastingSettings,\n",
    ")\n",
    "from azure.ai.ml.entities import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get a handle to the workspace\n",
    "\n",
    "We use config file to connect to a workspace. The Azure ML workspace should be configured with computer cluster. [Check this notebook for configure a workspace](../../configuration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "workspace = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.connections._subscription_id\n",
    "output[\"Resource Group\"] = workspace.resource_group\n",
    "output[\"Location\"] = workspace.location\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "outputDf = pd.DataFrame(data=output, index=[\"\"])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute \n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "\n",
    "You may need to talk to your workspace or IT admin to create the compute targets if you don't have permission.\n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cluster_name = \"forecast-step-cluster-v2\"\n",
    "\n",
    "try:\n",
    "    # Retrieve an already attached Azure Machine Learning Compute.\n",
    "    compute = ml_client.compute.get(cluster_name)\n",
    "except ResourceNotFoundError as e:\n",
    "    compute = AmlCompute(\n",
    "        name=cluster_name,\n",
    "        size=\"STANDARD_DS12_V2\",\n",
    "        type=\"amlcompute\",\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=120,\n",
    "    )\n",
    "    poller = ml_client.begin_create_or_update(compute)\n",
    "    poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "You are now ready to load the historical orange juice sales data. For demonstration purposes, we extract sales time-series for just a few of the stores. We will load the CSV file into a plain pandas DataFrame; the time column in the CSV is called _WeekStarting_, so it will be specially parsed into the datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = \"WeekStarting\"\n",
    "train = pd.read_csv(\"./data/train/oj-train.csv\", parse_dates=[time_column_name])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the DataFrame holds a quantity of weekly sales for an orange juice (OJ) brand at a single store. The data also includes the sales price, a flag indicating if the OJ brand was advertised in the store that week, and some customer demographic information based on the store location. For historical reasons, the data also include the logarithm of the sales quantity. The Dominick's grocery data is commonly used to illustrate econometric modeling techniques where logarithms of quantities are generally preferred.    \n",
    "\n",
    "The task is now to build a time-series model for the _Quantity_ column. It is important to note that this dataset is comprised of many individual time-series - one for each unique combination of _Store_ and _Brand_. To distinguish the individual time-series, we define the **time_series_id_column_names** - the columns whose values determine the boundaries between time-series: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_id_column_names = [\"Store\", \"Brand\"]\n",
    "nseries = train.groupby(time_series_id_column_names).ngroups\n",
    "print(\"Data contains {0} individual time-series.\".format(nseries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Splitting\n",
    "The test set will contain the final 4 weeks of observed sales for each time-series. The splits should be stratified by series, so we use a group-by statement on the time series identifier columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_periods = 4\n",
    "\n",
    "test = pd.read_csv(\"./data/test/oj-test.csv\", parse_dates=[time_column_name])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to datastore\n",
    "The [Machine Learning service workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace), is paired with the storage account, which contains the default data store. We will use it to upload the train data and create [Input](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.input?view=azure-python-preview) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLTable defined locally, with local data to be uploaded\n",
    "train_dataset = Input(type=AssetTypes.MLTABLE, path=\"./data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we will use our test data set from the pipeline run and we will need to upload it to URI directory to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Input(\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    path=\"./data/test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Building training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Modeling\n",
    "\n",
    "For forecasting tasks, AutoML uses pre-processing and estimation steps that are specific to time-series. AutoML will undertake the following pre-processing steps:\n",
    "* Detect time-series sample frequency (e.g. hourly, daily, weekly) and create new records for absent time points to make the series regular. A regular time series has a well-defined frequency and has a value at every sample point in a contiguous time span \n",
    "* Impute missing values in the target (via forward-fill) and feature columns (using median column values) \n",
    "* Create features based on time series identifiers to enable fixed effects across different series\n",
    "* Create time-based features to assist in learning seasonal patterns\n",
    "* Encode categorical variables to numeric quantities\n",
    "\n",
    "In this notebook, AutoML will train a single, regression-type model across **all** time-series in a given training set. This allows the model to generalize across related series.\n",
    "\n",
    "You are almost ready to start an AutoML training job. First, we need to define the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = \"Quantity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ForecastingSettings\n",
    "To define forecasting settings for your experiment training, you can leverage the ForecastingSettings class. The table below details the forecasting parameter we will be passing into our experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**time_series_id_column_names**|The column names used to uniquely identify the time series in data that has multiple rows with the same timestamp. If the time series identifiers are not defined, the data set is assumed to be one time series.|\n",
    "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
    "|**frequency**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information.\n",
    "|**cv_step_size**|Number of periods between two consecutive cross-validation folds. The default value is `None`, in which case AutoMl determines the cross-validation step size automatically. Or users could specify an integer value.|\n",
    "\n",
    "### forecasting() function parameters:\n",
    "\n",
    "The `forecasting()` factory function allows user to configure AutoML for the forecasting task for the most common scenarios with the following properties.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**target_column_name**|The name of the label column.|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**training_data**|The training data to be used for this experiment. You can use a registered MLTable in the workspace using the format `<mltable_name>:<version>` OR you can use a local file or folder as a MLTable. For e.g `Input(mltable='my_mltable:1')` OR `Input(mltable=MLTable(local_path=\"./data\"))` The parameter 'training_data' must always be provided.|\n",
    "|**compute**|The compute on which the AutoML job will run. In this example we are using a compute called 'cpu-cluster' present in the workspace. You can replace it with any other compute in the workspace.|\n",
    "|**n_cross_validations**|Number of cross-validation folds to use for model/pipeline selection. This can be set to \"auto\", in which case AutoMl determines the number of cross-validations automatically, if a validation set is not provided. Or, users could specify an integer value.|\n",
    "|**name**|The name of the Job/Run. This is an optional property. If not specified, a random name will be generated.\n",
    "|**experiment_name**|The name of the Experiment. An Experiment is like a folder with multiple runs in Azure ML Workspace that should be related to the same logical machine learning experiment. For example, if a user runs this notebook multiple times, there will be multiple runs associated with the same Experiment name.|\n",
    "|**enable_model_explainability**|If set to true, the explanations such as feature importance for the best model will be generated.|\n",
    "\n",
    "### set_limits() parameters:\n",
    "This is an optional configuration method to configure limits parameters such as timeouts.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**timeout_minutes**|Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. This timeout includes setup, featurization and training runs but does not include the ensembling and model explainability runs at the end of the process since those actions need to happen once all the trials (children jobs) are done. If not specified, the default job's total timeout is 6 days (8,640 minutes). To specify a timeout less than or equal to 1 hour (60 minutes), make sure your dataset's size is not greater than 10,000,000 (rows times column) or an error results. It is hard to say what the timeout limit should be because the runtimes depend on multiple factors such as number of unique time series in the dataset, length of time series, statistical properties of the data, etc. If your dataset is less than 10,000,000 observations, you can try to set the experiment to 1 hour. If you are seeing less than 30 child jobs completed in this time frame, increase the timeout limit and re-run the experiment.|\n",
    "|**trial_timeout_minutes**|Maximum time in minutes that each trial (child job) can run for before it terminates. If not specified, a value of 1 month or 43200 minutes is used.|\n",
    "|**max_trials**|The maximum number of trials/runs each with a different combination of algorithm and hyperparameters to try during an AutoML job. If not specified, the default is 1000 trials. If you are setting the `enable_early_termination=True` the number of trials will be smaller.|\n",
    "|**max_concurrent_trials**|Represents the maximum number of trials (children jobs) that would be executed in parallel. It's a good practice to set this number equal to the number of nodes in your cluster.|\n",
    "|**enable_early_termination**|Whether to enable early termination if the score is not improving over 10 iterations. Early stopping window starts only after first 20 iterations. This means that the first iteration where stopping can occur is the 31st.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_docker_conda = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    conda_file=\"./environment/preprocessing_env.yaml\",\n",
    "    name=\"pipeline-custom-environment\",\n",
    "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_str = \"ojmodel\"\n",
    "\n",
    "# Define pipeline\n",
    "@pipeline(\n",
    "    description=\"AutoML Forecasting Pipeline\",\n",
    ")\n",
    "def automl_forecasting(\n",
    "    forecasting_train_data,\n",
    "):\n",
    "    # define command function for preprocessing the model\n",
    "    preprocessing_command_func = command(\n",
    "        inputs=dict(\n",
    "            train_data=Input(type=\"mltable\"),\n",
    "        ),\n",
    "        outputs=dict(\n",
    "            preprocessed_train_data=Output(type=\"mltable\"),\n",
    "        ),\n",
    "        code=\"./preprocess.py\",\n",
    "        command=\"python preprocess.py \"\n",
    "        + \"--train_data ${{inputs.train_data}} \"\n",
    "        + \"--preprocessed_train_data ${{outputs.preprocessed_train_data}}\",\n",
    "        environment=\"pipeline-custom-environment@latest\",\n",
    "    )\n",
    "    preprocess_node = preprocessing_command_func(train_data=forecasting_train_data)\n",
    "\n",
    "    # define forecasting settings\n",
    "    forecasting_settings = ForecastingSettings(\n",
    "        time_column_name=time_column_name,\n",
    "        forecast_horizon=n_test_periods,\n",
    "        frequency=\"W-THU\",\n",
    "    )\n",
    "\n",
    "    # define the automl forecasting task with automl function\n",
    "    forecasting_node = forecasting(\n",
    "        training_data=preprocess_node.outputs.preprocessed_train_data,\n",
    "        target_column_name=target_column_name,\n",
    "        primary_metric=\"normalized_root_mean_squared_error\",\n",
    "        n_cross_validations=\"auto\",\n",
    "        forecasting_settings=forecasting_settings,\n",
    "        # currently need to specify outputs \"custom_model\" explictly to reference it in following nodes\n",
    "        outputs={\"best_model\": Output(type=AssetTypes.CUSTOM_MODEL)},\n",
    "    )\n",
    "\n",
    "    forecasting_node.set_limits(\n",
    "        timeout_minutes=15,\n",
    "        trial_timeout_minutes=5,\n",
    "    )\n",
    "\n",
    "    # define command function for registering the model\n",
    "    command_func = command(\n",
    "        inputs=dict(\n",
    "            model_input_path=Input(type=AssetTypes.CUSTOM_MODEL),\n",
    "            model_base_name=\"forecasting_example_model\",\n",
    "        ),\n",
    "        code=\"scripts/register_model.py\",\n",
    "        command=\"python register_model.py \"\n",
    "        + \"--model_path ${{inputs.model_input_path}} \"\n",
    "        + f\"--model_base_name {model_name_str}\",\n",
    "        environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n",
    "    )\n",
    "    register_model = command_func(model_input_path=forecasting_node.outputs.best_model)\n",
    "\n",
    "    return {\n",
    "        \"best_model\": forecasting_node.outputs.best_model,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an instance of a pipeline job\n",
    "pipeline_job_data = automl_forecasting(\n",
    "    forecasting_train_data=train_dataset,\n",
    ")\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job_data.settings.default_compute = cluster_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Submit pipeline job\n",
    "The pipeline will train AutoML model and register it in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job_data, experiment_name=\"pipeline_samples\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the output of a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(pipeline_job.name, download_path=\".\", output_name=\"best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the ID of the best run and download the artifacts associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "with open(os.path.join(\"named-outputs\", \"best_model\", \"MLmodel\"), \"r\") as f:\n",
    "    ml_model = yaml.safe_load(f)\n",
    "ml_model[\"run_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we know the run ID of the best run, we can instantiate the mlflow run object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "# Obtain the tracking URL from MLClient\n",
    "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
    "    name=ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "\n",
    "print(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "\n",
    "# Initialize MLFlow client\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "mlflow_best_run = mlflow_client.get_run(ml_model[\"run_id\"])\n",
    "\n",
    "print(\"Parent Run: \")\n",
    "print(mlflow_best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the artiracts for this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local folder\n",
    "import os\n",
    "\n",
    "local_dir = \"./artifact_downloads\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "# Download run's artifacts/outputs\n",
    "local_path = mlflow_client.download_artifacts(\n",
    "    mlflow_best_run.info.run_id, \"outputs\", local_dir\n",
    ")\n",
    "print(\"Artifacts downloaded in: {}\".format(local_path))\n",
    "print(\"Artifacts: {}\".format(os.listdir(local_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metrics for each run\n",
    "In this code we list all child runs, i.e., all runs that share the same parent run ID and end in the underscore followed by the order number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from mlflow.entities import RunStatus\n",
    "\n",
    "parent_run_id = ml_model[\"run_id\"][: ml_model[\"run_id\"].index(\"_\")]\n",
    "child_run_regex = re.compile(r\"[^_]+_\\d+$\")\n",
    "\n",
    "for child_run in filter(\n",
    "    lambda x: child_run_regex.match(x.name),\n",
    "    ml_client.jobs.list(parent_job_name=parent_run_id),\n",
    "):\n",
    "    mlflow_child_run = mlflow_client.get_run(child_run.name)\n",
    "    if RunStatus.from_string(mlflow_child_run.info.status) == RunStatus.FINISHED:\n",
    "        print(\n",
    "            f\"{child_run.name}: \"\n",
    "            f'{mlflow_child_run.data.metrics[\"normalized_root_mean_squared_error\"]}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to do the inference, for here we will demonstrate how to use the registered model and pipeline to do the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get Inference Pipeline Environment\n",
    "This environment can be created using the `yaml` file, which we have downloaded with other best run's artifacts into the `artifact_downloads` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env = Environment(\n",
    "    description=\"environment for automl inference\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    conda_file=os.path.join(\"artifact_downloads\", \"outputs\", \"conda_env_v_1_0_0.yml\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build and submit the inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference pipeline will create pipeline output object which can be downloaded after pipeline finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ds_name = \"oj-output\"\n",
    "# Define inference pipeline\n",
    "@pipeline(\n",
    "    description=\"AutoML Inference Pipeline\",\n",
    ")\n",
    "def automl_forecasting(\n",
    "    forecasting_inference_data,\n",
    "):\n",
    "    # define command function for registering the model\n",
    "    inference_func = command(\n",
    "        inputs=dict(\n",
    "            test_dataset=Input(type=AssetTypes.URI_FOLDER),\n",
    "            model_base_name=\"forecasting_example_model\",\n",
    "        ),\n",
    "        outputs=dict(output_dataset=Output(type=AssetTypes.URI_FOLDER)),\n",
    "        code=\"scripts/infer.py\",\n",
    "        command=(\n",
    "            \"python infer.py \"\n",
    "            \"--test_dataset ${{inputs.test_dataset}} \"\n",
    "            f\"--model_name {model_name_str} \"\n",
    "            f\"--target_column_name {target_column_name} \"\n",
    "            \"--output_dataset ${{outputs.output_dataset}} \"\n",
    "            f\"--output_dataset_name {output_ds_name}\"\n",
    "        ),\n",
    "        environment=env,\n",
    "    )\n",
    "\n",
    "    call_inferencing = inference_func(test_dataset=forecasting_inference_data)\n",
    "\n",
    "    return {\"output_dataset\": call_inferencing.outputs.output_dataset}\n",
    "\n",
    "\n",
    "pipeline_job_data = automl_forecasting(\n",
    "    forecasting_inference_data=test_dataset,\n",
    ")\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job_data.settings.default_compute = cluster_name\n",
    "\n",
    "pipeline_job_data.outputs.output_dataset.mode = \"rw_mount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job_data, experiment_name=\"pipeline_inference\"\n",
    ")\n",
    "inference_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(inference_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Get the predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    inference_job.name, download_path=\".\", output_name=\"output_dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = pd.read_csv(\n",
    "    os.path.join(\"named-outputs\", \"output_dataset\", f\"{output_ds_name}.csv\"),\n",
    "    parse_dates=[time_column_name],\n",
    ")\n",
    "inference_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Schedule Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is about how to schedule a pipeline for periodic predictions. For more info about pipeline schedule and pipeline endpoint, please follow this [notebook](https://github.com/Azure/azureml-examples/blob/83c67ec408f10e2e07b3a2a3e648023caa09e112/sdk/python/schedules/job-schedule.ipynb).<br>\n",
    "## 4.1. Define a schedule\n",
    "If `test_dataset` will be updated every 4 weeks on Friday 16:00 and the objective is to generate a 4 week (forecast_horizon) forecast, we can schedule our pipeline to run every 4 weeks at 16:00 to get daily inference results. You can refresh your test dataset (a newer version will be created) periodically when new data is available (i.e. target column in test dataset would have values in the beginning as context data, and followed by NaNs to be predicted). The inference pipeline will pick up context to further improve the forecast accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from azure.ai.ml.constants import TimeZone\n",
    "from azure.ai.ml.entities import (\n",
    "    JobSchedule,\n",
    "    RecurrenceTrigger,\n",
    "    RecurrencePattern,\n",
    ")\n",
    "\n",
    "schedule_name = \"OJ_Inference_schedule\"\n",
    "schedule_start_time = datetime.now()\n",
    "\n",
    "recurrence_trigger = RecurrenceTrigger(\n",
    "    frequency=\"week\",\n",
    "    interval=4,\n",
    "    schedule=RecurrencePattern(week_days=[\"Friday\"], hours=16, minutes=[0]),\n",
    "    start_time=schedule_start_time,\n",
    "    time_zone=TimeZone.UTC,\n",
    ")\n",
    "\n",
    "job_schedule = JobSchedule(\n",
    "    name=schedule_name, trigger=recurrence_trigger, create_job=pipeline_job_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.schedules.begin_create_or_update(schedule=job_schedule).wait()\n",
    "print(job_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. [Optional] Disable schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.schedules.begin_disable(name=schedule_name).wait()\n",
    "job_schedule.is_enabled\n",
    "ml_client.schedules.begin_delete(name=schedule_name).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 [Optional] Delete the compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.compute.begin_delete(name=cluster_name).wait()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "jialiu"
   }
  ],
  "category": "tutorial",
  "celltoolbar": "Raw Cell Format",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Orange Juice Sales"
  ],
  "deployment": [
   "Azure Container Instance"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML AutoML"
  ],
  "friendly_name": "Forecasting orange juice sales with deployment",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "tags": [
   "None"
  ],
  "task": "Forecasting",
  "vscode": {
   "interpreter": {
    "hash": "a3e1ce86190527341b095dce2d981b591205330162e59d5b85eea3038817dc05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
