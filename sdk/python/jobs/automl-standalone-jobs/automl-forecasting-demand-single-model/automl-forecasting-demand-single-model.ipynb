{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "**Demand Forecasting Using Many Models**\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Compute](#Compute)\n",
    "1. [Data](#Data)\n",
    "1. [Import Components From Registry](#ImportComponents)\n",
    "1. [Create a Pipeline](#Pipeline)\n",
    "1. [Kick Off Pipeline Runs](#PipelineRuns)\n",
    "1. [Download Output](#DownloadOutput)\n",
    "1. [Compare Evaluation Results](#CompareResults)\n",
    "1. [Deployment](#Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The objective of this notebook is to illustrate how to use the component-based AutoML single model solution. It walks you through all stages of model evaluation and production process starting with data ingestion and concluding with batch endpoint deployment for production. In this tutorial we will illustrate how to leverage AutoML and train a destributed TCN model ([link](placeholder)). However, the same notebook can be used to train a non-distributed TCN as well as the conventional ML models.\n",
    "\n",
    "We use a subset of UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)) with the objective of predicting electricity demand per consumer 24 hours ahead. The data was preprocessed using the [data prep notebook](https://github.com/Azure/azureml-examples/blob/main/v1/python-sdk/tutorials/automl-with-azureml/forecasting-data-preparation/auto-ml-forecasting-data-preparation.ipynb). Please refer to it for illustration on how to download the data from the source, aggregate to an hourly frequency, convert from wide to long format and upload to the Datastore. Here, we will work with the data that has been pre-processed and saved locally in the parquet format.\n",
    "\n",
    "There are a number of steps you need to take before you can put a model into production. A user needs to prepare the data, partition it into appropriate sets, select the best model, evaluate it against a baseline, and monitor the model in real life to collect enough observations on how it would perform had it been put in production. Some of these steps are time consuming, some require certain expertise in writing code. The steps shown in this notebook follow a typical thought process one follows before the model is put in production.\n",
    "\n",
    "Make sure you have executed the [configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1682992408484
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import yaml\n",
    "import azure.ai.ml\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import (\n",
    "    BatchEndpoint,\n",
    "    BatchDeployment,\n",
    "    AmlCompute,\n",
    "    PipelineComponentBatchDeployment,\n",
    ")\n",
    "from azure.ai.ml.entities._job.automl.tabular.forecasting_settings import (\n",
    "    ForecastingSettings,\n",
    ")\n",
    "\n",
    "print(f\"SDK version: {azure.ai.ml.__version__}\")\n",
    "os.environ[\n",
    "    \"AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED\"\n",
    "] = \"true\"  # TODO: Delete once CE is released"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential does not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
    "    print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Show Azure ML Workspace information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = ml_client.workspaces.get(name=ml_client.workspace_name)\n",
    "\n",
    "output = {}\n",
    "output[\"Workspace\"] = ml_client.workspace_name\n",
    "output[\"Subscription ID\"] = ml_client.connections._subscription_id\n",
    "output[\"Resource Group\"] = ws.resource_group\n",
    "output[\"Location\"] = ws.location\n",
    "pd.DataFrame(data=output, index=[\"\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute \n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "\n",
    "You will need to create a compute target for your AutoML run. In this tutorial, you will create AmlCompute as your training compute resource.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "\n",
    "Here, we use a 10 node cluster of the `STANDARD_NC6` series for illustration purposes. You will need to adjust the compute type and the number of nodes based on your needs which can be driven by the speed needed for model selection, data size, etc. \n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace, this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "amlcompute_cluster_name = \"demand-fcst-single-cluster\"\n",
    "\n",
    "try:\n",
    "    # Retrieve an already attached Azure Machine Learning Compute.\n",
    "    compute_target = ml_client.compute.get(amlcompute_cluster_name)\n",
    "except ResourceNotFoundError as e:\n",
    "    compute_target = AmlCompute(\n",
    "        name=amlcompute_cluster_name,\n",
    "        size=\"STANDARD_NC6\",\n",
    "        type=\"amlcompute\",\n",
    "        min_instances=0,\n",
    "        max_instances=10,\n",
    "        idle_time_before_scale_down=600,\n",
    "    )\n",
    "    poller = ml_client.begin_create_or_update(amlcompute_cluster_name)\n",
    "    poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data\n",
    "\n",
    "For illustration purposes we use the UCI electricity data ([link](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#)). The original dataset contains electricity consumption data for 370 consumers measured at 15 minute intervals. In the data set for this demonstrations, we have aggregated to an hourly frequency and convereted to the kilowatt hours (kWh) for 10 customers. Each customer is assigned to one of the two groups as denoted by the entries in the `group_id` column. The following cells read and print the first few rows of the training data as well as print the number of unique time series in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = \"datetime\"\n",
    "target_column_name = \"usage\"\n",
    "time_series_id_column_names = [\"group_id\", \"customer_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"train\"\n",
    "df = pd.read_parquet(f\"./data/{dataset_type}/uci_electro_small_{dataset_type}.parquet\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nseries = df.groupby(time_series_id_column_names).ngroups\n",
    "print(f\"Data contains {nseries} individual time-series\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[time_series_id_column_names].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLTable defined locally, with local data to be uploaded\n",
    "train_dataset = Input(type=AssetTypes.MLTABLE, path=\"./data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we will use our test data set from the pipeline run and we will need to upload it to URI directory to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Input(type=AssetTypes.URI_FOLDER, path=\"./data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import Components From Registry\n",
    "\n",
    "An Azure Machine Learning component is a self-contained piece of code that does one step in a machine learning pipeline. A component is analogous to a function - it has a name, inputs, outputs, and a body. Components are the building blocks of the Azure Machine Learning pipelines. It's a good engineering practice to build a machine learning pipeline where each step has well-defined inputs and outputs. In Azure Machine Learning, a component represents one reusable step in a pipeline. Components are designed to help improve the productivity of pipeline building. Specifically, components offer:\n",
    "- Well-defined interface: Components require a well-defined interface (input and output). The interface allows the user to build steps and connect steps easily. The interface also hides the complex logic of a step and removes the burden of understanding how the step is implemented.\n",
    "\n",
    "- Share and reuse: As the building blocks of a pipeline, components can be easily shared and reused across pipelines, workspaces, and subscriptions. Components built by one team can be discovered and used by another team.\n",
    "\n",
    "- Version control: Components are versioned. The component producers can keep improving components and publish new versions. Consumers can use specific component versions in their pipelines. This gives them compatibility and reproducibility.\n",
    "\n",
    "For a more detailed information on this subject, refer to the this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2).\n",
    "\n",
    "To import components,  we need to get the registry. The following command obtains the public regsitry from which we will import components for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682992409837
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get registry\n",
    "# TODO: Change registry name once the components are in the public registry\n",
    "ml_client_registry = MLClient(\n",
    "    credential=credential, registry_name=\"azureml-dev\"\n",
    ")  # \"ManyModels_HTS_BugBash\")\n",
    "print(ml_client_registry)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pull specific components and use them to build a pipeline of steps. For the illustration of the product evaluation workflow we will use the following components:\n",
    "- Data partitioning component: allows users to partion the data for many models runs, both, training and inference.\n",
    "- Many models training component: trains the best model per partition specified by users.\n",
    "- Many moodels inference componnet: generates forecast for each partition. This can be done on the test and inference sets.\n",
    "- Compute metrics component: calculates metrics per time series if the inference component was used on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996405805
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_component = ml_client_registry.components.get(\n",
    "    name=\"automl_forecasting_inference\", label=\"latest\"\n",
    ")\n",
    "print(f\"Many models inference component version: {inference_component.version}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996406713
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "compute_metrics_component = ml_client_registry.components.get(\n",
    "    name=\"compute_metrics\", label=\"latest\"\n",
    ")\n",
    "print(\n",
    "    f\"Many models inference component version: {compute_metrics_component.version}\\n---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1682996406974
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## 6. Create a Pipeline\n",
    "\n",
    "Now that we imported the components we will build an evaluation pipeline. This pipeline will allow us to train the best model, genererate rolling forecast on the test set, and calculate metrics on the test set output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Set Pipeline Parameters\n",
    "\n",
    "AzureML components can only receive specific object types such as strings, JSON/YML files, URI Folders and URI Files. Other object types are not accepted. Because of this, we will create the pipeline by utilizing the `pipeline_parameters` dictionary. Most of the parameters in this dictionary will define the settings for the model training step of the pipeline and the remaining ones will be used in inference and compute metrics components. To have a better understanding of what these settings represents, we will build this dictionary in sequential steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1. Training Step Parameters\n",
    "\n",
    "First, we create a set of parameters `automl_settings` which will be used to define the `forecasting()` factory function to kick off the model training stage. Think of this as the bare minimum settings that are necessary to define a forecasting job, and it contains the following properties:\n",
    "\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **task**               | forecasting |\n",
    "| **target_column_name** | The name of the column to target for predictions. It must always be specified. This parameter is applicable to `training_data`, `validation_data` and `test_data`. |\n",
    "| **primary_metric**     | This is the metric that you want to optimize. Forecasting supports the following primary metrics<ul><li>`normalized_root_mean_squared_error`</li><li>`normalized_mean_absolute_error`</li><li>`spearman_correlation`</li><li>`r2_score`</li></ul> We recommend using either the normalized root mean squared error (default metric) or normalized mean absolute error as a primary metric because they measure forecast accuracy. See the [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-automl-forecasting-faq#how-do-i-choose-the-primary-metric) for a more detailed discussion on this topic. |\n",
    "| **n_cross_validations** | Number of cross-validation folds to use for model/pipeline selection. This can be set to \"auto\", in which case AutoMl determines the number of cross-validations automatically, if a validation set is not provided. Or, users could specify an integer value. The default value is \"auto\". |\n",
    "\n",
    "Please note that the `forecasting()` function also requires a training and/or validation data. We will be providing this information as a separate parameter to the pipeline method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = dict(\n",
    "    task=\"forecasting\",\n",
    "    target_column_name=target_column_name,\n",
    "    primary_metric=\"normalized_root_mean_squared_error\",\n",
    "    n_cross_validations=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the forecasting specific parameters for the experiment which will be stored in the `forecast_settings` dictionary. Technically, there are only 2 parameters that are necessary for forecasting tasks (`forecast_horizon` and `time_column_name`). For a greater control over the experiment we also list optional parameters that users can set, they are marked with an asterisk $(*)$ in the table below. Feel free to uncomment and set the desired values. See the following [link placeholder]() for the detailed description for each of these parameters.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **time_column_name**               | The name of the time column in the data. |\n",
    "| **forecast_horizon** | The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly). |\n",
    "| **time_series_id_column_names***    | The column names used to uniquely identify the time series in data that has multiple rows with the same timestamp. If the time series identifiers are not defined, AutoML will detect them for you. |\n",
    "| **cv_step_size*** | Number of steps between cross-validation folds. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_settings = dict(\n",
    "    forecast_horizon=24,\n",
    "    time_column_name=time_column_name,\n",
    "    time_series_id_column_names=time_series_id_column_names,\n",
    "    cv_step_size=24,\n",
    "    # target_lags = None,\n",
    "    # target_rolling_window_size = None,\n",
    "    # frequency = None\n",
    "    # feature_lags = None,\n",
    "    # seasonality = None,\n",
    "    # use_stl = None,\n",
    "    # short_series_handling_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set parameters to configure limits such as timeouts and store them in the `training_limits` dictionary.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **timeout_minutes**          | Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. This timeout includes setup, featurization and training runs but does not include the ensembling and model explainability runs at the end of the process since those actions need to happen once all the trials (children jobs) are done. If not specified, the default job's total timeout is 6 days (8,640 minutes). To specify a timeout less than or equal to 1 hour (60 minutes), make sure your dataset's size is not greater than 10,000,000 (rows times column) or an error results. |\n",
    "| **trial_timeout_minutes**    | Maximum time in minutes that each trial (child job) can run for before it terminates. If not specified, a value of 1 month or 43200 minutes is used. |\n",
    "| **max_trials**               | Represents the maximum number of trials an Automated ML job can try to run a training algorithm with different combination of hyperparameters. Its default value is set to 1000. If `enable_early_termination` is defined, then the number of trials used to run training algorithms can be smaller. |\n",
    "| **max_concurrent_trials**    | The maximum number of trials (children jobs) that would be executed in parallel. It's highly recommended to set the number of concurrent runs to the number of nodes in the cluster (aml compute defined in `compute`). The default value is 1. |\n",
    "| **max_nodes**                | Maximum number of nodes to use in training. This value should be set only for the distirbuted TCN training. We encourage this value to be a multiple of max_concurrent_iterations. The multiple indicates the number of nodes that will be used by each concurrent iteration. Minimum acceptable value to kick off distributed training is 2. |\n",
    "| **enable_early_termination** | Represents whether to enable of experiment termination if the loss score doesn't improve after 'x' number of iterations. In an Automated ML job, no early stopping is applied on first 20 iterations. The early stopping window starts only after first 20 iterations. The default value is `True`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_limits = dict(\n",
    "    timeout_minutes=90,\n",
    "    trial_timeout_minutes=60,\n",
    "    max_concurrent_trials=2,  # 5,\n",
    "    max_trials=5,  # 20,\n",
    "    # max_nodes = 10, # 10,\n",
    "    enable_early_termination=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set parameters to configure training parameters such as enabling DNN and blocking/alowing specific models, and store them in the `training_settings` dictionary.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **enable_dnn_training**          | A flag to turn on or off the inclusion of DNN based models to try out during model selection. The default value is `False`. |\n",
    "| **training_mode**    |The training mode to use. The possible values are `distributed` and `non_distributed` (default value). When this parameter is set to `distributed` and `enablle_dnn_training=True`, a disitributed TCN run will be kicked off. |\n",
    "| **allowed_training_algorithms**               | A list of Time Series Forecasting algorithms to try out as base model for model training in an experiment. If it is omitted or set to `None`, then all supported algorithms are used during experiment, except algorithms specified in `blocked_training_algorithms`. The default value is `None`. |\n",
    "| **blocked_training_algorithms**               | A list of Time Series Forecasting algorithms to not run as base model while model training in an experiment. If it is omitted or set to `None`, then all supported algorithms are used during model training.  The default value is `None`.|\n",
    "| **enable_model_explainability**    | Represents a flag to turn on model explainability like feature importance, of best model evaluated by Automated ML system. The default value is `True`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_settings = dict(\n",
    "    enable_dnn_training=False,  # True,\n",
    "    training_mode=\"non_distributed\",  # \"distributed\",\n",
    "    allowed_training_algorithms=[\"LightGBM\"],  # [\"TCNForecaster\"],\n",
    "    blocked_training_algorithms=None,\n",
    "    enable_model_explainability=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.2. Other Pipeline Parameters\n",
    "<br>\n",
    "Next, we set declare the parameters that will be used be the inference and compute metrics components. We will store them in the `component_settings` dictionary.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "| **compute_name**                | The name of the AML compute infrastructure to execute the job on. |\n",
    "| **forecast_mode**               | Type of forecat to perform on the test set. Can be `recursive` or `rolling`. Rolling forecast can be used for the evaluation purpose. The default value is `recursive`. |\n",
    "| **forecast_step**               | The forecast step used for rolling forecast. See the following [link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast?view=azureml-api-2#evaluating-model-accuracy-with-a-rolling-forecast) for more details. |\n",
    "| **is_validation_data_provided** | Set this value to `True` if validation data is provided.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_settings = dict(\n",
    "    compute_name=amlcompute_cluster_name,\n",
    "    forecast_mode=\"rolling\",\n",
    "    forecast_step=24,\n",
    "    is_validation_data_provided=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all parameters explained and set, we save them all in the `pipeline_parameters` dictionary which will be used to build the evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_parameters = {\n",
    "    **automl_settings,\n",
    "    **forecast_settings,\n",
    "    **training_settings,\n",
    "    **training_limits,\n",
    "    **component_settings,\n",
    "}\n",
    "print(json.dumps(pipeline_parameters, indent=4), \"\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Build a Pipeline\n",
    "\n",
    "Next, we build a pipeline from the imported components. Since this notebook is designed to illustrate the evaluation flow, we will string these components in the following fashion. First, we train the best model. Then, we generate a rolling forecast with the step size of 24 (hours) on the test set. This is done to mimic the evaluation process when a customer is tracking model's performance in real time and generates forecasts every 24 hours. Finally, we compute metrics based on the rolling forecast output from the previous step. You do not have to modify anything in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1682996407118
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    description=\"AutoML Forecasting Single Model Evaluation Pipeline\",\n",
    ")\n",
    "def evaluation_pipeline(training_data, inference_data, validation_data=None):\n",
    "    # 0. Extract parameters from the dictionary\n",
    "    target_column_name = pipeline_parameters.get(\"target_column_name\")\n",
    "    primary_metric = pipeline_parameters.get(\n",
    "        \"primary_metric\", \"normalized_root_squared_error\"\n",
    "    )\n",
    "    n_cross_validations = pipeline_parameters.get(\"n_cross_validations\", \"auto\")\n",
    "\n",
    "    # -- 0.1 set_forecast_settings\n",
    "    time_column_name = pipeline_parameters.get(\"time_column_name\")\n",
    "    time_series_id_column_names = pipeline_parameters.get(\n",
    "        \"time_series_id_column_names\", None\n",
    "    )\n",
    "    country_or_region_for_holidays = pipeline_parameters.get(\n",
    "        \"country_or_region_for_holidays\", None\n",
    "    )\n",
    "    cv_step_size = pipeline_parameters.get(\"cv_step_size\", None)\n",
    "    forecast_horizon = pipeline_parameters.get(\"forecast_horizon\", None)\n",
    "    target_lags = pipeline_parameters.get(\"target_lags\", None)\n",
    "    target_rolling_window_size = pipeline_parameters.get(\n",
    "        \"target_rolling_window_size\", None\n",
    "    )\n",
    "    frequency = pipeline_parameters.get(\"frequency\", None)\n",
    "    feature_lags = pipeline_parameters.get(\"feature_lags\", None)\n",
    "    seasonality = pipeline_parameters.get(\"seasonality\", None)\n",
    "    use_stl = pipeline_parameters.get(\"use_stl\", None)\n",
    "    short_series_handling_config = pipeline_parameters.get(\n",
    "        \"short_series_handling_config\", None\n",
    "    )\n",
    "\n",
    "    # -- 0.2 set_training\n",
    "    enable_dnn_training = pipeline_parameters.get(\"enable_dnn_training\", False)\n",
    "    training_mode = pipeline_parameters.get(\"distributed\", None)\n",
    "    enable_model_explainability = pipeline_parameters.get(\"time_column_name\", True)\n",
    "    enable_stack_ensemble = pipeline_parameters.get(\"enable_stack_ensemble\", False)\n",
    "    enable_vote_ensemble = pipeline_parameters.get(\"enable_vote_ensemble\", True)\n",
    "    allowed_training_algorithms = pipeline_parameters.get(\n",
    "        \"allowed_training_algorithms\", None\n",
    "    )\n",
    "    blocked_training_algorithms = pipeline_parameters.get(\n",
    "        \"blocked_training_algorithms\", None\n",
    "    )\n",
    "\n",
    "    # -- 0.3 set_limits\n",
    "    max_concurrent_trials = pipeline_parameters.get(\"max_concurrent_trials\", None)\n",
    "    max_cores_per_trial = pipeline_parameters.get(\"max_cores_per_trial\", None)\n",
    "    max_nodes = pipeline_parameters.get(\"max_nodes\", None)\n",
    "    max_trials = pipeline_parameters.get(\"max_trials\", None)\n",
    "    timeout_minutes = pipeline_parameters.get(\"timeout_minutes\", None)\n",
    "    trial_timeout_minutes = pipeline_parameters.get(\"trial_timeout_minutes\", None)\n",
    "\n",
    "    # -- 0.4 component-specific settings\n",
    "    compute_name = pipeline_parameters.get(\"compute_name\")\n",
    "    max_nodes = pipeline_parameters.get(\"max_nodes\", 1)\n",
    "    forecast_mode = pipeline_parameters.get(\"forecast_mode\", \"recursive\")\n",
    "    forecast_step = pipeline_parameters.get(\"forecast_step\", 1)\n",
    "    forecast_quantiles = pipeline_parameters.get(\"forecast_quantiles\", None)\n",
    "    is_validation_data_provided = pipeline_parameters.get(\"forecast_quantiles\", False)\n",
    "\n",
    "    # 1. Model Training Step\n",
    "    # -- 1.1 Define the automl forecasting task with automl function\n",
    "    if is_validation_data_provided:\n",
    "        print(\"---\\n Wrong path \\n---\")\n",
    "        training_node = automl.forecasting(\n",
    "            compute=compute_name,\n",
    "            # experiment_name=experiment_name,\n",
    "            training_data=training_data,\n",
    "            validation_data=validation_data,\n",
    "            target_column_name=target_column_name,\n",
    "            primary_metric=primary_metric,\n",
    "            enable_model_explainability=enable_model_explainability,\n",
    "            outputs={\"best_model\": Output(type=AssetTypes.CUSTOM_MODEL)},\n",
    "        )\n",
    "    else:\n",
    "        training_node = automl.forecasting(\n",
    "            compute=compute_name,\n",
    "            # experiment_name=experiment_name,\n",
    "            training_data=training_data,\n",
    "            target_column_name=target_column_name,\n",
    "            primary_metric=primary_metric,\n",
    "            n_cross_validations=n_cross_validations,\n",
    "            enable_model_explainability=enable_model_explainability,\n",
    "            outputs={\"best_model\": Output(type=AssetTypes.CUSTOM_MODEL)},\n",
    "        )\n",
    "\n",
    "    # --  1.2 Define forecasting settings\n",
    "    training_node.set_forecast_settings(\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        time_column_name=time_column_name,\n",
    "        time_series_id_column_names=time_series_id_column_names,\n",
    "        country_or_region_for_holidays=country_or_region_for_holidays,\n",
    "        cv_step_size=cv_step_size,\n",
    "        target_lags=target_lags,\n",
    "        target_rolling_window_size=target_rolling_window_size,\n",
    "        frequency=frequency,\n",
    "        feature_lags=feature_lags,\n",
    "        seasonality=seasonality,\n",
    "        use_stl=use_stl,\n",
    "        short_series_handling_config=short_series_handling_config,\n",
    "    )\n",
    "\n",
    "    # -- 1.3 Set training parameters\n",
    "    training_node.set_training(\n",
    "        enable_dnn_training=enable_dnn_training,\n",
    "        training_mode=training_mode,\n",
    "        enable_model_explainability=enable_model_explainability,\n",
    "        allowed_training_algorithms=allowed_training_algorithms,\n",
    "    )\n",
    "\n",
    "    # -- 1.4 Set training limits. All limits are optional.\n",
    "    training_node.set_limits(\n",
    "        timeout_minutes=timeout_minutes,\n",
    "        trial_timeout_minutes=trial_timeout_minutes,\n",
    "        max_trials=max_trials,\n",
    "        max_concurrent_trials=max_concurrent_trials,\n",
    "        max_cores_per_trial=max_cores_per_trial,\n",
    "        max_nodes=max_nodes,\n",
    "    )\n",
    "\n",
    "    # 2. Inferencing step\n",
    "    inference_node = inference_component(\n",
    "        test_data=inference_data,\n",
    "        model_path=training_node.outputs.best_model,\n",
    "        target_column_name=target_column_name,\n",
    "        forecast_mode=forecast_mode,\n",
    "        forecast_step=forecast_step,\n",
    "        forecast_quantiles=forecast_quantiles,\n",
    "    )\n",
    "\n",
    "    # 3. Metrics calculation step\n",
    "    compute_metrics_node = compute_metrics_component(\n",
    "        task=\"tabular-forecasting\",\n",
    "        prediction=inference_node.outputs.inference_output_file,\n",
    "        ground_truth=inference_node.outputs.inference_output_file,\n",
    "        evaluation_config=inference_node.outputs.evaluation_config_output_file,\n",
    "    )\n",
    "    compute_metrics_node.compute = compute_name\n",
    "\n",
    "    # 4. Specify pipeline outputs\n",
    "    return {\n",
    "        \"output_files\": compute_metrics_node.outputs.evaluation_result,\n",
    "        \"output_model\": training_node.outputs.best_model,\n",
    "        \"forecast_output\": inference_node.outputs.inference_output_file,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Kick Off Pipeline Runs\n",
    "\n",
    "Now that the pipeline is defined, we will use it to kick off several runs. First, we will kick off an experiment which will train, inference and evaluate the performance for the best AutoML model for each partition. Next, we will kick off the same pipeline which will only use the naive model for the same partitions. This will allow us to establish a baseline and compare performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Kick Off Best Many Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682996407285
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = evaluation_pipeline(\n",
    "    training_data=Input(type=\"uri_folder\", path=\"./data/train\"),\n",
    "    inference_data=Input(type=\"uri_folder\", path=\"./data/test\"),\n",
    "    # validation_data=Input(type=\"uri_folder\", path=\"./data/valid\"),\n",
    ")\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = amlcompute_cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1682997237524
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"single-model-experiment-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y%m%d\"\n",
    ")\n",
    "\n",
    "pipeline_submitted_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rehydrate run\n",
    "# RUN_ID = \"<Paste the PipelineRunId from the output of the previous cell.>\"\n",
    "# pipeline_submitted_job = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Kick Off the Baseline Experiment\n",
    "\n",
    "To establish a baseline, we will use the same pipeline as before with one minore change. We will add Naive model to the allowed model list and change the number of rolling origin cross validations (ROCV) to 2. Reducing the ROCV speeds up the runtime and is needed for model selection only, while in this run we have only one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_parameters.update(\n",
    "    {\n",
    "        \"allowed_training_algorithms\": [\"Naive\"],\n",
    "        \"n_cross_validations\": 2,\n",
    "        \"enable_dnn_training\": False,\n",
    "        \"training_mode\": None,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job_base = evaluation_pipeline(\n",
    "    training_data=Input(type=\"uri_folder\", path=\"./data/train\"),\n",
    "    inference_data=Input(type=\"uri_folder\", path=\"./data/test\"),\n",
    ")\n",
    "print(pipeline_job_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline level compute\n",
    "pipeline_job_base.settings.default_compute = amlcompute_cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_experiment_name = (\n",
    "    \"single-model-experiment-base-\" + datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    ")\n",
    "\n",
    "pipeline_submitted_job_base = ml_client.jobs.create_or_update(\n",
    "    pipeline_job_base,\n",
    "    experiment_name=base_experiment_name,\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_submitted_job_base.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rehydrate baseline run\n",
    "# RUN_ID = \"<Paste the PipelineRunId from the output of the previous cell.>\"\n",
    "# pipeline_submitted_job_base = ml_client.jobs.get(RUN_ID)\n",
    "# pipeline_submitted_job_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Pipeline Output\n",
    "Next, we will download the output files generated by the compute metrics components for each executed pipeline and save them in the corresponfing subfolder of the `output` folder. First, we create corresponding output directories. Then, we execute the `ml_client.jobs.download` command which downloads experiments' outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directories\n",
    "automl_output_dir = os.path.join(os.getcwd(), \"output/automl\")\n",
    "base_output_dir = os.path.join(os.getcwd(), \"output/base\")\n",
    "\n",
    "os.makedirs(automl_output_dir, exist_ok=True)\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job.name,\n",
    "    download_path=automl_output_dir,\n",
    "    output_name=\"output_files\",\n",
    ")\n",
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job_base.name,\n",
    "    download_path=base_output_dir,\n",
    "    output_name=\"output_files\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job.name,\n",
    "    download_path=automl_output_dir,\n",
    "    output_name=\"forecast_output\",\n",
    ")\n",
    "\n",
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job_base.name,\n",
    "    download_path=base_output_dir,\n",
    "    output_name=\"forecast_output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.download(\n",
    "    name=pipeline_submitted_job.name,\n",
    "    download_path=automl_output_dir,\n",
    "    output_name=\"output_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 9. Compare Evaluation Results\n",
    "\n",
    "### 9.1. Examine Metrics\n",
    "\n",
    "In this section, we compare metrics for the 2 pipeline runs to quantify accuracy improvement of AutoML over the baseline model. First, we compare metrics that are calculated for the entire dataset. Since there are 10 unique time series in the test dataset, these individual metrics are aggregated into a single number. The non-normalized metrics can be misleading due to the difference in scales of each unique time series. The following [article (placeholder)](https://review.learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2&branch=pr-en-us-238443#forecasting-metrics-normalization-and-aggregation) explains this topic in a greater detail.\n",
    "\n",
    "The code in the next cell loads dataset metrics for each of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_artifacts_path = os.path.join(\n",
    "    \"named-outputs\", \"output_files\", \"evaluationResult\"\n",
    ")\n",
    "\n",
    "with open(os.path.join(automl_output_dir, metrics_artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_automl_series = json.load(f)\n",
    "\n",
    "with open(os.path.join(base_output_dir, metrics_artifacts_path, \"metrics.json\")) as f:\n",
    "    metrics_base_series = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge two dataframes to examine metrics side by side. The `metrics_all` data frame contains two columns which correspond to the scores from the many models and the baseline experiments, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_automl = (\n",
    "    pd.Series(metrics_automl_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "metrics_base = (\n",
    "    pd.Series(metrics_base_series).to_frame(name=\"score\").reset_index(drop=False)\n",
    ")\n",
    "metrics_all = pd.DataFrame(\n",
    "    [metrics_automl_series, metrics_base_series], index=[\"score_automl\", \"score_base\"]\n",
    ").T\n",
    "metrics_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.1 Detailed Metrics\n",
    "\n",
    "Next, we will load and examine the detailed accuracy metrics since the aggregate metrics may not convey enough information to make a decision about product accuracy. It may be helpful to examine metrics at a more granular level. We will extract metrics per time series. To do this, we create a helper function `extract_specific_metric` which reads the JSON file and returns a specified metric for each time series. Even though the file contains the following metrics, we will  we will focus on the normalized root mean squared error (NRMSE) accuracy metric for illustration purposes. <ul>\n",
    "    <li> `explained_variance` </li>\n",
    "    <li> `mean_absolute_error` </li>\n",
    "    <li> `mean_absolute_percentage_error`</li>\n",
    "    <li> `median_absolute_error`</li>\n",
    "    <li> `normalized_median_absolute_error`</li>\n",
    "    <li> `normalized_root_mean_squared_error`</li>\n",
    "    <li> `normalized_root_mean_squared_error`</li>\n",
    "    <li> `normalized_root_mean_squared_log_error`</li>\n",
    "    <li> `r2_score`</li>\n",
    "    <li> `root_mean_squared_log_error`</li>\n",
    "    <li> `root_mean_squared_error`</li>\n",
    "    <li> `root_mean_squared_log_error`</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_specific_metric(path, metric_name):\n",
    "    with open(path) as f:\n",
    "        artifact = json.load(f)\n",
    "    all_metrics = pd.DataFrame(artifact[\"data\"])\n",
    "    index_scores = [\"customer_id\"] + [metric_name]\n",
    "    return all_metrics[index_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_table_relative_path = os.path.join(\n",
    "    metrics_artifacts_path, \"artifacts\", \"forecast_time_series_id_distribution_table\"\n",
    ")\n",
    "automl_metric = extract_specific_metric(\n",
    "    os.path.join(automl_output_dir, metrics_table_relative_path),\n",
    "    \"normalized_root_mean_squared_error\",\n",
    ")\n",
    "\n",
    "base_metric = extract_specific_metric(\n",
    "    os.path.join(base_output_dir, metrics_table_relative_path),\n",
    "    \"normalized_root_mean_squared_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = automl_metric.merge(\n",
    "    base_metric,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"inner\",\n",
    "    suffixes=[\"_automl\", \"_base\"],\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Generate Time Series Plots\n",
    "\n",
    "Here, we generate forecast versus actuals plot for the test set for both the best many models and the baseline. Since we use rolling evaluation with the step size of 24 hours, this mimics the behavior of putting both models in production and monitoring their behavior for the duration of the test set. This step helps you make informed decisions about model performance and saves numerous costs associated with productionalizing the model and monitoring its performance in real life. \n",
    "\n",
    "In the next block of code, we, load the test set output for each of the runs and merge the data. Then, we generate and save time series plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_table_relative_path = os.path.join(\n",
    "    \"named-outputs\", \"forecast_output\", \"inference_output_file\"\n",
    ")\n",
    "\n",
    "forecast_column_name = \"automl_predictions\"\n",
    "base_forecast_column_name = \"base_predictions\"\n",
    "actual_column_name = \"automl_actuals\"\n",
    "forecast_origin_column_name = \"automl_forecast_origin\"\n",
    "\n",
    "automl_fcst = pd.read_json(\n",
    "    os.path.join(automl_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "automl_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "base_fcst = pd.read_json(\n",
    "    os.path.join(base_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "base_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "print(automl_fcst.head(3), \"\\n---\")\n",
    "print(base_fcst.head(3), \"\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_table_relative_path = os.path.join(\n",
    "    \"named-outputs\", \"forecast_output\", \"inference_output_file\"\n",
    ")\n",
    "\n",
    "forecast_column_name = \"automl_prediction\"\n",
    "base_forecast_column_name = \"base_prediction\"\n",
    "actual_column_name = \"automl_actual\"\n",
    "forecast_origin_column_name = \"automl_forecast_origin\"\n",
    "\n",
    "automl_fcst = pd.read_json(\n",
    "    os.path.join(automl_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "automl_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "base_fcst = pd.read_json(\n",
    "    os.path.join(base_output_dir, forecast_table_relative_path),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "base_fcst[forecast_origin_column_name] = pd.to_datetime(\n",
    "    automl_fcst[forecast_origin_column_name], unit=\"ms\"\n",
    ")\n",
    "\n",
    "merge_columns = [\"customer_id\"] + [actual_column_name]\n",
    "merge_columns.extend([time_column_name, forecast_origin_column_name])\n",
    "\n",
    "backtest = automl_fcst.merge(\n",
    "    base_fcst.rename(columns={forecast_column_name: base_forecast_column_name}),\n",
    "    on=merge_columns,\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(f\"AutoML forecast table size: {automl_fcst.shape}\\n---\")\n",
    "print(f\"Base forecast table size: {base_fcst.shape}\\n---\")\n",
    "print(f\"Merged forecast table size: {backtest.shape}\\n---\")\n",
    "backtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper_scripts import draw_one_plot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plot_filename = \"forecast_vs_actual.pdf\"\n",
    "\n",
    "pdf = PdfPages(os.path.join(os.getcwd(), \"./output\", plot_filename))\n",
    "for _, one_forecast in backtest.groupby(\"customer_id\"):\n",
    "    one_forecast[time_column_name] = pd.to_datetime(one_forecast[time_column_name])\n",
    "    one_forecast.sort_values(time_column_name, inplace=True)\n",
    "    draw_one_plot(\n",
    "        one_forecast,\n",
    "        time_column_name,\n",
    "        target_column_name,\n",
    "        [\"customer_id\"],\n",
    "        [actual_column_name, forecast_column_name, base_forecast_column_name],\n",
    "        pdf,\n",
    "        plot_predictions=True,\n",
    "    )\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(os.path.join(\"./output/forecast_vs_actual.pdf\"), width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Deployment\n",
    "\n",
    "In this section, we will illustrate how to deploy and inference models using batch endpoint. Batch endpoints are endpoints that are used to do batch inferencing on large volumes of data in asynchronous way. Batch endpoints receive pointers to data and run jobs asynchronously to process the data in parallel on compute clusters and store outputs to a datastore for further analysis. For more information on batch endpoints see this [link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-batch?view=azureml-api-2).\n",
    "\n",
    "### 10.1. Create Batch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete once components are in the public regitry\n",
    "from azure.ai.ml.constants._common import AZUREML_PRIVATE_FEATURES_ENV_VAR\n",
    "\n",
    "os.environ[AZUREML_PRIVATE_FEATURES_ENV_VAR] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Creating a unique endpoint name by including a random suffix\n",
    "allowed_chars = string.ascii_lowercase + string.digits\n",
    "endpoint_suffix = \"\".join(random.choice(allowed_chars) for x in range(5))\n",
    "endpoint_name = \"sdk-single-model-\" + endpoint_suffix\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = BatchEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"An endpoint for component deployments\",\n",
    "    properties={\"ComponentDeployment.Enabled\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the Endpoint in the workspace usign the MLClient created earlier. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Create the Deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = PipelineComponentBatchDeployment(\n",
    "    name=\"sdk-single-model-deployment\",\n",
    "    description=\"A single deployment.\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    component=inference_component.id,\n",
    "    settings={\"default_compute\": amlcompute_cluster_name},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command creates the deployment in the workspace usign the MLClient created earlier. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3. Invoke the Endpoint\n",
    "\n",
    "The next cell contians the command that invokes the endpoint for batch inference job. The `invoke` method contains the `inputs` parameter. This parameter contains the inputs necessary to execute the inference component on the endpoint. To convince yourself this is the case, compare the input parameters for the `inference_component_from_registry` in section 6.3 with the `inputs` we are proving in the next cell. They are identical.\n",
    "\n",
    "Notice, the the `forecast_mode` is set to `\"recursive\"`. In the evaluation pipeline this component was used to generate rolling forecast to evalaute model performance on the test set. For more details on rolling evaluation, see our [forecasting model evaluation article](placeholder). Here, we are using it to generate a forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"single-model-experiment-\" + datetime.datetime.now().strftime(\n",
    "    \"%Y%m%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain run ID for the best model. Will use it\n",
    "with open(\n",
    "    os.path.join(automl_output_dir, \"named-outputs\", \"output_model\", \"MLmodel\"), \"r\"\n",
    ") as f:\n",
    "    ml_model = yaml.safe_load(f)\n",
    "run_id = ml_model[\"run_id\"]\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Input(\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    path=f\"azureml://jobs/{run_id}/outputs/artifacts/outputs/model.pkl\",\n",
    ")\n",
    "\n",
    "# # another option is to uplaod the model\n",
    "# best_model = Input(\n",
    "#     type=AssetTypes.MLFLOW_MODEL,\n",
    "#     path=os.path.join(automl_output_dir, \"named-outputs\", \"output_model\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=endpoint.name,\n",
    "    deployment_name=deployment.name,\n",
    "    inputs={\n",
    "        \"test_data\": Input(type=AssetTypes.URI_FOLDER, path=\"./data/inference\"),\n",
    "        \"model_path\": Input(type=AssetTypes.MLFLOW_MODEL, default=best_model),\n",
    "        \"target_column_name\": Input(type=\"string\", default=target_column_name),\n",
    "        \"forecast_mode\": Input(type=\"string\", default=\"recursive\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will stream the job output to monitor the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = job.name\n",
    "batch_job = ml_client.jobs.get(name=job_name)\n",
    "print(f\"Batch job status: {batch_job.status}\\n---\")\n",
    "ml_client.jobs.stream(name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4. Download Forecast Output\n",
    "\n",
    "Finally, we download the forecast output and print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_output_dir = os.path.join(os.getcwd(), \"forecast\")\n",
    "\n",
    "for child in ml_client.jobs.list(parent_job_name=job.name):\n",
    "    print(f\"{child.name}\\n---\")\n",
    "    if (\n",
    "        child.properties[\"azureml.moduleName\"]\n",
    "        == \"automl_many_model_inferencing_collect\"\n",
    "    ):\n",
    "        print(\"Downloading data ...\\n---\")\n",
    "        ml_client.jobs.download(\n",
    "            child.name, download_path=fcst_output_dir, output_name=\"metadata\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_df = pd.read_parquet(\n",
    "    os.path.join(fcst_output_dir, \"named-outputs\", \"metadata\", \"raw_forecast\")\n",
    ")\n",
    "fcst_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5. [Optional] Delete the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint.name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
