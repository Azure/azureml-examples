{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MLFlow model locally and try predictions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. You need to have run successfully the training notebook related to this model, available in this same folder, where at the end of the notebook, after training the model, it downloads the 'artifacts' with the MLFlow model folder (\"./artifact_downloads/outputs/mlflow-model\").\n",
    "\n",
    "2. Create a conda environment with the 'conda.yaml' file provided within the \"mlflow-model\" folder, doing like the following:\n",
    "     1. if you are running this notebook on a windows machine, Please remove \"Pycocotools\" and \"recordclass\" lines from conda.yaml and have c++ build tools( https://visualstudio.microsoft.com/visual-cpp-build-tools/ ) installed before running the below steps\n",
    "\n",
    "   1. (base) /> conda env create --file conda.yaml --name automl-model-image-instance-segmentation-env\n",
    "   \n",
    "   1. (base) /> conda activate automl-model-image-instance-segmentation-env\n",
    "   \n",
    "   1. automl-model-image-instance-segmentation-env) /> conda install jupyter nb_conda\n",
    "\n",
    "3. Run Jupyter and make sure you are using the related 'automl-model-image-instance-segmentation-env' Kernel.\n",
    "\n",
    "4. Run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the MLFlow model files were downloaded successfully by the training notebook, you should see the files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Local dir where you have downloaded and saved the artifacts\n",
    "local_dir = \"./artifact_downloads\"\n",
    "\n",
    "mlflow_model_dir = os.path.join(local_dir, \"outputs\", \"mlflow-model\")\n",
    "\n",
    "# Show the contents of the MLFlow model folder\n",
    "os.listdir(mlflow_model_dir)\n",
    "\n",
    "# You should see a list of files such as the following:\n",
    "# ['artifacts', 'conda.yaml', 'MLmodel', 'python_env.yaml', 'python_model.pkl', 'requirements.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data into a Pandas DataFrame\n",
    "\n",
    "Load some test images into a Pandas DataFrame in order to try some predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to a different location if you downloaded data at a different location\n",
    "dataset_parent_dir = \"./data\"\n",
    "dataset_name = \"odFridgeObjectsMask\"\n",
    "\n",
    "test_image_paths = [\n",
    "    os.path.join(dataset_parent_dir, dataset_name, \"images\", \"31.jpg\"),\n",
    "    os.path.join(dataset_parent_dir, dataset_name, \"images\", \"62.jpg\"),\n",
    "    os.path.join(dataset_parent_dir, dataset_name, \"images\", \"93.jpg\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    data=[\n",
    "        base64.encodebytes(read_image(image_path)).decode(\"utf-8\")\n",
    "        for image_path in test_image_paths\n",
    "    ],\n",
    "    columns=[\"image\"],\n",
    ")\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model in memory\n",
    "\n",
    "Load the model using MLflow flavor. Check MLmodel under the downloaded folder (artifact_downloads/outputs/mlflow-model). For this particular example (and for AutoML for Images scenario), MLmodel file will describe python_function flavor. We show how to load model using pyfunc flavor. For more information on MLflow flavors, visit: https://www.mlflow.org/docs/latest/models.html#storage-format\n",
    "\n",
    "Loading the models locally assume that you are running the notebook in an environment compatible with the model. The list of dependencies that is expected by the model is specified in the MLFlow model produced by AutoML (in the 'conda.yaml' file within the mlflow-model folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way #1: Get the MLFlow model from the downloaded MLFlow model files\n",
    "\n",
    "pyfunc_model = mlflow.pyfunc.load_model(mlflow_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "\n",
    "result = pyfunc_model.predict(test_df).to_json(orient=\"records\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize detections\n",
    "Now that we have scored test images, we can visualize the bounding boxes and masks for the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.lines import Line2D\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "sample_image = test_image_paths[0]\n",
    "detections = json.loads(result)[0]\n",
    "\n",
    "IMAGE_SIZE = (18, 12)\n",
    "plt.figure(figsize=IMAGE_SIZE)\n",
    "img_np = mpimg.imread(sample_image)\n",
    "img = Image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n",
    "x, y = img.size\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "# Display the image\n",
    "ax.imshow(img_np)\n",
    "\n",
    "# draw box and label for each detection\n",
    "for detect in detections[\"boxes\"]:\n",
    "    label = detect[\"label\"]\n",
    "    box = detect[\"box\"]\n",
    "    polygon = detect[\"polygon\"]\n",
    "    conf_score = detect[\"score\"]\n",
    "    if conf_score > 0.6:\n",
    "        ymin, xmin, ymax, xmax = (\n",
    "            box[\"topY\"],\n",
    "            box[\"topX\"],\n",
    "            box[\"bottomY\"],\n",
    "            box[\"bottomX\"],\n",
    "        )\n",
    "        topleft_x, topleft_y = x * xmin, y * ymin\n",
    "        width, height = x * (xmax - xmin), y * (ymax - ymin)\n",
    "        print(\n",
    "            f\"{detect['label']}: [{round(topleft_x, 3)}, {round(topleft_y, 3)}, \"\n",
    "            f\"{round(width, 3)}, {round(height, 3)}], {round(conf_score, 3)}\"\n",
    "        )\n",
    "\n",
    "        color = np.random.rand(3)  #'red'\n",
    "        rect = patches.Rectangle(\n",
    "            (topleft_x, topleft_y),\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=2,\n",
    "            edgecolor=color,\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n",
    "\n",
    "        polygon_np = np.array(polygon[0])\n",
    "        polygon_np = polygon_np.reshape(-1, 2)\n",
    "        polygon_np[:, 0] *= x\n",
    "        polygon_np[:, 1] *= y\n",
    "        poly = patches.Polygon(polygon_np, True, facecolor=color, alpha=0.4)\n",
    "        ax.add_patch(poly)\n",
    "        poly_line = Line2D(\n",
    "            polygon_np[:, 0],\n",
    "            polygon_np[:, 1],\n",
    "            linewidth=2,\n",
    "            marker=\"o\",\n",
    "            markersize=8,\n",
    "            markerfacecolor=color,\n",
    "        )\n",
    "        ax.add_line(poly_line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "You can see further examples of other AutoML tasks such as Regression, Image-Classification, NLP-Text-Classification, Time-Series-Forcasting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "289dd8ba56d14462f802fc494f900f580a323d52d317644c669d12d1794296a4"
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
