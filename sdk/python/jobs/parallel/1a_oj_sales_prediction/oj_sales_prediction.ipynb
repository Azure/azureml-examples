{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Orange juice sales prediction example \\[Paralle job\\] \\[SDK example\\]\n",
        "\n",
        "**Key notes for this example**\n",
        "- How to use **parallel job** for **many model training** scenario.\n",
        "- How to use parallel job **run_function** task with predefined **entry_script**.\n",
        "- How to pre-cook data into **mltable with partition setting**.\n",
        "- How to use **mltable** with **tabular data** as the **input of parallel job**.\n",
        "- How to use **partition_keys** in parallel job to consume data with partitions. \n",
        "- How to use **append_row_to** to aggregate returns to **uri_file** output.\n",
        "- How to use parallel job settings:\n",
        "  - error threshold\n",
        "  - mini_batch_error_threshold\n",
        "  - environment_variables\n",
        "\n",
        "To get the same example with CLI + Yaml experience, please refer to: [link]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Connect to Azure Machine Learning Workspace\n",
        "## 1.1 Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1673838207609
        },
        "name": "required-library"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input, Output, load_component\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml.entities import Environment, ResourceConfiguration\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "from azure.ai.ml.parallel import parallel_run_function, RunFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Configure credential\n",
        "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
        "\n",
        "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1673838211196
        },
        "name": "credential"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
            "Attempted credentials:\n",
            "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
            "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot.this issue.\n",
            "\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n",
            "\tSharedTokenCacheCredential: Azure Active Directory error '(invalid_grant) AADSTS700082: The refresh token has expired due to inactivity. The token was issued on 2022-06-20T03:07:49.9957858Z and was inactive for 90.00:00:00.\n",
            "Trace ID: 8963f824-5ed2-4e2c-8f3f-d2794cf30200\n",
            "Correlation ID: ebce3ab4-4914-45da-96b4-30aa0461e253\n",
            "Timestamp: 2023-01-17 08:27:06Z'\n",
            "Content: {\"error\":\"invalid_grant\",\"error_description\":\"AADSTS700082: The refresh token has expired due to inactivity. The token was issued on 2022-06-20T03:07:49.9957858Z and was inactive for 90.00:00:00.\\r\\nTrace ID: 8963f824-5ed2-4e2c-8f3f-d2794cf30200\\r\\nCorrelation ID: ebce3ab4-4914-45da-96b4-30aa0461e253\\r\\nTimestamp: 2023-01-17 08:27:06Z\",\"error_codes\":[700082],\"timestamp\":\"2023-01-17 08:27:06Z\",\"trace_id\":\"8963f824-5ed2-4e2c-8f3f-d2794cf30200\",\"correlation_id\":\"ebce3ab4-4914-45da-96b4-30aa0461e253\",\"error_uri\":\"https://login.microsoftonline.com/error?code=700082\"}\n",
            "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Get a handle to the workspace\n",
        "\n",
        "We use config file to connect to a workspace. The Azure ML workspace should be configured with computer cluster. [Check this notebook for configure a workspace](../../configuration.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1673858642606
        },
        "name": "workspace"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: d:\\Code\\azureml-examples\\sdk\\python\\jobs\\parallel\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AmlCompute({'type': 'amlcompute', 'created_on': None, 'provisioning_state': 'Succeeded', 'provisioning_errors': None, 'name': 'cpu-cluster', 'description': None, 'tags': {}, 'properties': {}, 'id': '/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastus2/computes/cpu-cluster', 'Resource__source_path': None, 'base_path': 'd:\\\\Code\\\\azureml-examples\\\\sdk\\\\python\\\\jobs\\\\parallel\\\\1a_oj_sales_prediction', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001BD754A7488>, 'resource_id': None, 'location': 'eastus2', 'size': 'STANDARD_D2_V2', 'min_instances': 0, 'max_instances': 3, 'idle_time_before_scale_down': 1800.0, 'identity': None, 'ssh_public_access_enabled': True, 'ssh_settings': None, 'network_settings': None, 'tier': 'dedicated', 'subnet': None})\n"
          ]
        }
      ],
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "\n",
        "# Retrieve an already attached Azure Machine Learning Compute.\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "print(ml_client.compute.get(cpu_compute_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Define components and jobs in pipeline\n",
        "\n",
        "## 2.1 Load existing command component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1673860079582
        },
        "name": "load-from-yaml"
      },
      "outputs": [],
      "source": [
        "# load existing command component to partition the single csv data to mltable.\n",
        "partition_data = load_component(source=\"./src/partition_data/partition_data.yml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Declare parallel job by `parallel_run_function`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1673860081288
        },
        "name": "parallel-job-for-file-data"
      },
      "outputs": [],
      "source": [
        "# Declare parallel job with run_function task\n",
        "many_model_training_with_partition_keys = parallel_run_function(\n",
        "    name=\"train_many_models_with_partition_keys\",\n",
        "    display_name=\"Train Many Models With Partition Keys\",\n",
        "    description=\"parallel job to train many models with partition_keys on mltable input\",\n",
        "    tags={\n",
        "        \"azureml_parallel_example\": \"oj_many-model_sdk\",\n",
        "    },\n",
        "    inputs=dict(\n",
        "        data_source=Input(\n",
        "            type=AssetTypes.MLTABLE,\n",
        "            description=\"Input mltable with predefined partition format.\",\n",
        "            mode=InputOutputModes.DIRECT,   # [Important] To use 'partition_keys', input MLTable is required to use 'direct' mode.\n",
        "        ),\n",
        "        drop_cols=Input(\n",
        "            type=\"string\",\n",
        "            description=\"Columns need to be dropped before training. Split by comma.\",\n",
        "        ),\n",
        "        target_col=Input(\n",
        "            type=\"string\",\n",
        "            description=\"The column name for label of the input data.\",\n",
        "        ),\n",
        "        date_col=Input(\n",
        "            type=\"string\",\n",
        "            description=\"The column name for datatime. This will be used for generating time-series lagging data.\",\n",
        "        ),\n",
        "        lagging_orders=Input(\n",
        "            type=\"string\",\n",
        "            description=\"List of int which indicate how to generate lagging data for time-series input. Split by comma.\",\n",
        "        ),\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        model_perf=Output(\n",
        "            type=AssetTypes.URI_FILE,\n",
        "            mode=InputOutputModes.RW_MOUNT,\n",
        "        ),\n",
        "        model_folder=Output(\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            mode=InputOutputModes.RW_MOUNT,\n",
        "        ),\n",
        "    ),\n",
        "    input_data=\"${{inputs.data_source}}\",   # Define which input data will be splitted into mini-batches\n",
        "    partition_keys=[\"Store\", \"Brand\"],      # Use 'partition_keys' as the data division method. This method requires MLTable input with partition setting pre-defined in MLTable artifact.\n",
        "    instance_count=2,                       # Use 2 nodes from compute cluster to run this parallel job.\n",
        "    max_concurrency_per_instance=1,         # Create 2 worker processors in each compute node to execute mini-batches.\n",
        "    error_threshold=-1,                     # Monitor the failures of item processed by the gap between mini-batch input count and returns. 'Many model training' scenario doesn't fit this setting and '-1' means ignore counting failure items by mini-batch returns.\n",
        "    mini_batch_error_threshold=5,           # Monitor the failed mini-batch by exception, time out, or null return. When failed mini-batch count is higher than this setting, the parallel job will be marked as 'failed'.\n",
        "    retry_settings=dict(\n",
        "        max_retries=2,                      # Define how many retries when mini-batch execution is failed by exception, time out, or null return.\n",
        "        timeout=60,                         # Define the timeout in second for each mini-batch execution.\n",
        "    ),\n",
        "    logging_level=\"DEBUG\",\n",
        "    environment_variables={\n",
        "      \"AZUREML_PARALLEL_EXAMPLE\": \"oj_many-model_sdk\",\n",
        "    },\n",
        "    task=RunFunction(\n",
        "        code=\"./src/parallel_train/\",\n",
        "        entry_script=\"parallel_train.py\",\n",
        "        environment=Environment(\n",
        "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "            conda_file=\"./src/parallel_train/conda.yml\",\n",
        "        ),\n",
        "        program_arguments=\"--drop_cols ${{inputs.drop_cols}} \"  # Passthrough input parameters into parallel_train script.\n",
        "        \"--target_col ${{inputs.target_col}} \"\n",
        "        \"--date_col ${{inputs.date_col}} \"\n",
        "        \"--lagging_orders ${{inputs.lagging_orders}} \"\n",
        "        \"--model_folder ${{outputs.model_folder}} \",\n",
        "        append_row_to=\"${{outputs.model_perf}}\",                # Define where to output the aggregated returns from each mini-batches.\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Build pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1673860087207
        },
        "name": "build-pipeline"
      },
      "outputs": [],
      "source": [
        "# Declare the overall input of the job.\n",
        "input_oj_data = Input(\n",
        "    path=\"./oj_sales_data/oj_sales_data.csv\", type=AssetTypes.URI_FILE, mode=InputOutputModes.RO_MOUNT\n",
        ")\n",
        "\n",
        "# Declare pipeline structure.\n",
        "@pipeline(\n",
        "    display_name=\"parallel job for oj many model training\",\n",
        ")\n",
        "def partition_job_in_pipeline(\n",
        "    pipeline_input_data,\n",
        "):\n",
        "    # Declare 1st data partition command job. \n",
        "    partition_job = partition_data(\n",
        "        data_source=pipeline_input_data,\n",
        "        partition_keys=\"Store,Brand\",\n",
        "    )\n",
        "\n",
        "    # Declare 2nd parallel model training job. \n",
        "    parallel_train = many_model_training_with_partition_keys(\n",
        "        data_source=partition_job.outputs.tabular_output_data,\n",
        "        drop_cols=\"Revenue,Advert,Store,Brand\",\n",
        "        target_col=\"Quantity\",\n",
        "        date_col=\"WeekStarting\",\n",
        "        lagging_orders=\"1,2,3,4,5,6\",\n",
        "    )\n",
        "\n",
        "    # User could override parallel job run-level property when invoke that parallel job/component in pipeline.\n",
        "    parallel_train.resources.instance_count = 3\n",
        "    parallel_train.max_concurrency_per_instance = 2\n",
        "    parallel_train.mini_batch_error_threshold = 10\n",
        "    parallel_train.outputs.model_perf.path = f\"azureml://datastores/workspaceblobstore/paths/${{name}}/my_append_output.csv\"\n",
        "\n",
        "# Create pipeline instance\n",
        "my_job = partition_job_in_pipeline(\n",
        "    pipeline_input_data=input_oj_data,\n",
        ")\n",
        "\n",
        "# Set pipeline level compute\n",
        "my_job.tags.update\n",
        "my_job.settings.default_compute = \"cpu-cluster\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1673860094391
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display_name: parallel job for oj many model training\n",
            "type: pipeline\n",
            "inputs:\n",
            "  pipeline_input_data:\n",
            "    mode: ro_mount\n",
            "    type: uri_file\n",
            "    path: azureml:./oj_sales_data/oj_sales_data.csv\n",
            "jobs:\n",
            "  partition_job:\n",
            "    type: command\n",
            "    inputs:\n",
            "      data_source:\n",
            "        path: ${{parent.inputs.pipeline_input_data}}\n",
            "      partition_keys: Store,Brand\n",
            "    code: D:/Code/azureml-examples/sdk/python/jobs/parallel/1a_oj_sales_prediction/src/partition_data\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: partition_data\n",
            "      version: '1'\n",
            "      display_name: Partition input data by keys\n",
            "      type: command\n",
            "      inputs:\n",
            "        data_source:\n",
            "          type: uri_file\n",
            "        partition_keys:\n",
            "          type: string\n",
            "      outputs:\n",
            "        tabular_output_data:\n",
            "          type: mltable\n",
            "      command: python partition_data.py --data_source ${{inputs.data_source}} --partition_keys\n",
            "        ${{inputs.partition_keys}} --tabular_output_data ${{outputs.tabular_output_data}}\n",
            "      environment:\n",
            "        name: CliV2AnonymousEnvironment\n",
            "        version: f0f91b763ec6b7c07485b03c34ff4adf\n",
            "        image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
            "        conda_file:\n",
            "          name: prs-env\n",
            "          channels:\n",
            "          - conda-forge\n",
            "          dependencies:\n",
            "          - python=3.7.6\n",
            "          - pip\n",
            "          - pip:\n",
            "            - mlflow\n",
            "            - azureml-dataset-runtime[pandas,fuse]\n",
            "            - azureml-telemetry\n",
            "            - pandas\n",
            "            - pillow\n",
            "            - azureml-core\n",
            "            - scikit-learn~=0.20.0\n",
            "            - cloudpickle==1.1.1\n",
            "            - tensorflow==1.15.2\n",
            "            - mltable==1.0.0\n",
            "      code: D:/Code/azureml-examples/sdk/python/jobs/parallel/1a_oj_sales_prediction/src/partition_data\n",
            "      is_deterministic: true\n",
            "  parallel_train:\n",
            "    type: parallel\n",
            "    inputs:\n",
            "      data_source:\n",
            "        path: ${{parent.jobs.partition_job.outputs.tabular_output_data}}\n",
            "      drop_cols: Revenue,Advert,Store,Brand\n",
            "      target_col: Quantity\n",
            "      date_col: WeekStarting\n",
            "      lagging_orders: 1,2,3,4,5,6\n",
            "    outputs:\n",
            "      model_perf:\n",
            "        mode: rw_mount\n",
            "        type: uri_file\n",
            "        path: azureml://datastores/workspaceblobstore/paths/${name}/my_append_output.csv\n",
            "      model_folder:\n",
            "        mode: rw_mount\n",
            "        type: uri_folder\n",
            "    resources:\n",
            "      instance_count: 3\n",
            "    component:\n",
            "      name: train_many_models_with_partition_keys\n",
            "      display_name: Train Many Models With Partition Keys\n",
            "      description: parallel job to train many models with partition_keys on mltable\n",
            "        input\n",
            "      type: parallel\n",
            "      inputs:\n",
            "        data_source:\n",
            "          type: mltable\n",
            "          description: Input mltable with predefined partition format.\n",
            "          mode: direct\n",
            "        drop_cols:\n",
            "          type: string\n",
            "          description: Columns need to be dropped before training. Split by comma.\n",
            "        target_col:\n",
            "          type: string\n",
            "          description: The column name for label of the input data.\n",
            "        date_col:\n",
            "          type: string\n",
            "          description: The column name for datatime. This will be used for generating\n",
            "            time-series lagging data.\n",
            "        lagging_orders:\n",
            "          type: string\n",
            "          description: List of int which indicate how to generate lagging data for\n",
            "            time-series input. Split by comma.\n",
            "      outputs:\n",
            "        model_perf:\n",
            "          type: uri_file\n",
            "          mode: rw_mount\n",
            "        model_folder:\n",
            "          type: uri_folder\n",
            "          mode: rw_mount\n",
            "      resources:\n",
            "        instance_count: 2\n",
            "      is_deterministic: true\n",
            "      tags:\n",
            "        azureml_parallel_example: oj_many-model_sdk\n",
            "      input_data: ${{inputs.data_source}}\n",
            "      retry_settings:\n",
            "        timeout: 60\n",
            "        max_retries: 2\n",
            "      mini_batch_error_threshold: 5\n",
            "      max_concurrency_per_instance: 1\n",
            "      error_threshold: -1\n",
            "      task:\n",
            "        type: run_function\n",
            "        code: ./src/parallel_train/\n",
            "        entry_script: parallel_train.py\n",
            "        program_arguments: '--drop_cols ${{inputs.drop_cols}} --target_col ${{inputs.target_col}}\n",
            "          --date_col ${{inputs.date_col}} --lagging_orders ${{inputs.lagging_orders}}\n",
            "          --model_folder ${{outputs.model_folder}} '\n",
            "        append_row_to: ${{outputs.model_perf}}\n",
            "        environment:\n",
            "          name: CliV2AnonymousEnvironment\n",
            "          version: f0f91b763ec6b7c07485b03c34ff4adf\n",
            "          image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
            "          conda_file:\n",
            "            name: prs-env\n",
            "            channels:\n",
            "            - conda-forge\n",
            "            dependencies:\n",
            "            - python=3.7.6\n",
            "            - pip\n",
            "            - pip:\n",
            "              - mlflow\n",
            "              - azureml-dataset-runtime[pandas,fuse]\n",
            "              - azureml-telemetry\n",
            "              - pandas\n",
            "              - pillow\n",
            "              - azureml-core\n",
            "              - scikit-learn~=0.20.0\n",
            "              - cloudpickle==1.1.1\n",
            "              - tensorflow==1.15.2\n",
            "              - mltable==1.0.0\n",
            "      partition_keys:\n",
            "      - Store\n",
            "      - Brand\n",
            "      logging_level: DEBUG\n",
            "    input_data: ${{inputs.data_source}}\n",
            "    retry_settings:\n",
            "      timeout: 60\n",
            "      max_retries: 2\n",
            "    mini_batch_error_threshold: 10\n",
            "    max_concurrency_per_instance: 2\n",
            "    error_threshold: -1\n",
            "    task:\n",
            "      type: run_function\n",
            "      code: ./src/parallel_train/\n",
            "      entry_script: parallel_train.py\n",
            "      program_arguments: '--drop_cols ${{inputs.drop_cols}} --target_col ${{inputs.target_col}}\n",
            "        --date_col ${{inputs.date_col}} --lagging_orders ${{inputs.lagging_orders}}\n",
            "        --model_folder ${{outputs.model_folder}} '\n",
            "      append_row_to: ${{outputs.model_perf}}\n",
            "      environment:\n",
            "        name: CliV2AnonymousEnvironment\n",
            "        version: f0f91b763ec6b7c07485b03c34ff4adf\n",
            "        image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
            "        conda_file:\n",
            "          name: prs-env\n",
            "          channels:\n",
            "          - conda-forge\n",
            "          dependencies:\n",
            "          - python=3.7.6\n",
            "          - pip\n",
            "          - pip:\n",
            "            - mlflow\n",
            "            - azureml-dataset-runtime[pandas,fuse]\n",
            "            - azureml-telemetry\n",
            "            - pandas\n",
            "            - pillow\n",
            "            - azureml-core\n",
            "            - scikit-learn~=0.20.0\n",
            "            - cloudpickle==1.1.1\n",
            "            - tensorflow==1.15.2\n",
            "            - mltable==1.0.0\n",
            "    partition_keys:\n",
            "    - Store\n",
            "    - Brand\n",
            "    logging_level: DEBUG\n",
            "    environment_variables:\n",
            "      AZUREML_PARALLEL_EXAMPLE: oj_many-model_sdk\n",
            "settings:\n",
            "  default_compute: azureml:cpu-cluster\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(my_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Submit pipeline job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1673858666260
        },
        "name": "submit-pipeline"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading partition_data (0.0 MBs): 100%|##########| 4549/4549 [00:00<00:00, 5236.08it/s]\n",
            "\u001b[39m\n",
            "\n",
            "\u001b[32mUploading parallel_train (0.0 MBs): 100%|##########| 4068/4068 [00:00<00:00, 4093.37it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>parallel examples</td><td>patient_stamp_yg13f5zblq</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/patient_stamp_yg13f5zblq?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/alainli-rg/workspaces/alainli-prs-eastUS2&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
            ],
            "text/plain": [
              "PipelineJob({'inputs': {'pipeline_input_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x000001BD75AB4408>}, 'outputs': {}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001BD75B7D988>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'parallel job for oj many model training', 'is_deterministic': None, 'inputs': {'pipeline_input_data': {}}, 'outputs': {}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'partition_job': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'partition_job', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001BD75AC3648>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'partition_keys': 'Store,Brand', 'data_source': '${{parent.inputs.pipeline_input_data}}'}, 'job_outputs': {}, 'inputs': {'partition_keys': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000001BD75830B48>, 'data_source': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000001BD75830B08>}, 'outputs': {}, 'component': 'azureml_anonymous:b519272a-6b2b-477b-97b1-45963cdc30ff', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'ac4aecfd-2d84-469c-a87b-1fb055270831', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'parallel_train': Parallel({'init': False, 'type': 'parallel', 'status': None, 'log_files': None, 'name': 'parallel_train', 'description': None, 'tags': {'azureml_parallel_example': 'oj_many-model_sdk'}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': 'd:\\\\Code\\\\azureml-examples\\\\sdk\\\\python\\\\jobs\\\\parallel\\\\1a_oj_sales_prediction', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001BD75B70F48>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Train Many Models With Partition Keys', 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'drop_cols': 'Revenue,Advert,Store,Brand', 'target_col': 'Quantity', 'date_col': 'WeekStarting', 'lagging_orders': '1,2,3,4,5,6', 'data_source': '${{parent.jobs.partition_job.outputs.tabular_output_data}}'}, 'job_outputs': {'model_perf': {'type': 'uri_file', 'path': 'azureml://datastores/workspaceblobstore/paths/${name}/my_append_output.csv', 'mode': 'rw_mount'}, 'model_folder': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'inputs': {'drop_cols': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000001BD75B708C8>, 'target_col': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000001BD75B703C8>, 'date_col': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000001BD75BAB748>, 'lagging_orders': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000001BD75BAB688>, 'data_source': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000001BD75BAB6C8>}, 'outputs': {'model_perf': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000001BD75BABA08>, 'model_folder': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000001BD75BABA88>}, 'component': 'azureml_anonymous:63f67520-fc8e-4e2d-89c2-df222c5767cf', 'referenced_control_flow_node_instance_id': None, 'kwargs': {}, 'instance_id': 'c89883cd-577a-4c24-aa87-faba27a863df', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'task': {'type': 'run_function', 'code': './src/parallel_train/', 'entry_script': 'parallel_train.py', 'program_arguments': '--drop_cols ${{inputs.drop_cols}} --target_col ${{inputs.target_col}} --date_col ${{inputs.date_col}} --lagging_orders ${{inputs.lagging_orders}} --model_folder ${{outputs.model_folder}} ', 'append_row_to': '${{outputs.model_perf}}', 'environment': {'name': 'CliV2AnonymousEnvironment', 'version': 'f0f91b763ec6b7c07485b03c34ff4adf', 'image': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04', 'conda_file': {'name': 'prs-env', 'channels': ['conda-forge'], 'dependencies': ['python=3.7.6', 'pip', {'pip': ['mlflow', 'azureml-dataset-runtime[pandas,fuse]', 'azureml-telemetry', 'pandas', 'pillow', 'azureml-core', 'scikit-learn~=0.20.0', 'cloudpickle==1.1.1', 'tensorflow==1.15.2', 'mltable==1.0.0']}]}}}, 'mini_batch_size': None, 'partition_keys': '\"[\\\\\"Store\\\\\", \\\\\"Brand\\\\\"]\"', 'input_data': '${{inputs.data_source}}', 'retry_settings': {'timeout': 60, 'max_retries': 2}, 'logging_level': 'DEBUG', 'max_concurrency_per_instance': 2, 'error_threshold': -1, 'mini_batch_error_threshold': 10, 'resources': {'instance_count': 3}, 'environment_variables': {'AZUREML_PARALLEL_EXAMPLE': 'oj_many-model_sdk'}})}, 'job_types': {'command': 1, 'parallel': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'patient_stamp_yg13f5zblq', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/Azure/azureml-examples.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'c2e89456c99502442448842635d09ef20e4d76f9', 'azureml.git.dirty': 'True', 'azureml.DevPlatv2': 'true', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.defaultComputeName': 'cpu-cluster', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastUS2/jobs/patient_stamp_yg13f5zblq', 'Resource__source_path': None, 'base_path': 'd:\\\\Code\\\\azureml-examples\\\\sdk\\\\python\\\\jobs\\\\parallel\\\\1a_oj_sales_prediction', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000001BD75B7D2C8>, 'serialize': <msrest.serialization.Serializer object at 0x000001BD75B86908>, 'display_name': 'parallel job for oj many model training', 'experiment_name': 'parallel examples', 'compute': None, 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x000001BD75B7D9C8>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x000001BD75B7D848>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    my_job, \n",
        "    experiment_name=\"parallel examples\",\n",
        ")\n",
        "pipeline_job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "stream-pipeline"
      },
      "outputs": [],
      "source": [
        "# wait until the job completes\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    }
  ],
  "metadata": {
    "description": {
      "description": "Create pipeline with parallel node to do batch inference"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "vnext",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1648a01f415a15976454e88ab551f1eeb39d06522c1fdad5697f49923f4699e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
