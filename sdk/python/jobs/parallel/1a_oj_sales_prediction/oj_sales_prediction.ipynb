{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Orange juice sales prediction example \\[Paralle job\\] \\[SDK example\\]\n",
        "\n",
        "**Key notes for this example**\n",
        "- How to use **parallel job** for **many model training** scenario.\n",
        "- How to use parallel job **run_function** task with predefined **entry_script**.\n",
        "- How to pre-cook data into **mltable with partition setting**.\n",
        "- How to use **mltable** with **tabular data** as the **input of parallel job**.\n",
        "- How to use **partition_keys** in parallel job to consume data with partitions. \n",
        "- How to use **append_row_to** to aggregate returns to **uri_file** output.\n",
        "- How to use parallel job settings:\n",
        "  - error threshold\n",
        "  - mini_batch_error_threshold\n",
        "  - environment_variables\n",
        "\n",
        "To get the same example with CLI + Yaml experience, please refer to: [link]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Connect to Azure Machine Learning Workspace\n",
        "## 1.1 Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673838207609
        },
        "name": "required-library"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input, Output, load_component\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml.entities import Environment, ResourceConfiguration\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "from azure.ai.ml.parallel import parallel_run_function, RunFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Configure credential\n",
        "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
        "\n",
        "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673838211196
        },
        "name": "credential"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Get a handle to the workspace\n",
        "\n",
        "We use config file to connect to a workspace. The Azure ML workspace should be configured with computer cluster. [Check this notebook for configure a workspace](../../configuration.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673858642606
        },
        "name": "workspace"
      },
      "outputs": [],
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "\n",
        "# Retrieve an already attached Azure Machine Learning Compute.\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "print(ml_client.compute.get(cpu_compute_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Define components and jobs in pipeline\n",
        "\n",
        "## 2.1 Load existing command component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673860079582
        },
        "name": "load-from-yaml"
      },
      "outputs": [],
      "source": [
        "# load existing command component to partition the single csv data to mltable.\n",
        "partition_data = load_component(source=\"./src/partition_data/partition_data.yml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Declare parallel job by `parallel_run_function`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673860081288
        },
        "name": "parallel-job-for-file-data"
      },
      "outputs": [],
      "source": [
        "# Declare parallel job with run_function task\n",
        "many_model_training_with_partition_keys = parallel_run_function(\n",
        "    name=\"train_many_models_with_partition_keys\",\n",
        "    display_name=\"Train Many Models With Partition Keys\",\n",
        "    description=\"parallel job to train many models with partition_keys on mltable input\",\n",
        "    tags={\n",
        "        \"azureml_parallel_example\": \"oj_many-model_sdk\",\n",
        "    },\n",
        "    inputs=dict(\n",
        "        data_source=Input(\n",
        "            type=AssetTypes.MLTABLE,\n",
        "            description=\"Input mltable with predefined partition format.\",\n",
        "            mode=InputOutputModes.DIRECT,   # [Important] To use 'partition_keys', input MLTable is required to use 'direct' mode.\n",
        "        ),\n",
        "        drop_cols=Input(\n",
        "            type=\"string\",\n",
        "            description=\"Columns need to be dropped before training. Split by comma.\",\n",
        "        ),\n",
        "        target_col=Input(\n",
        "            type=\"string\",\n",
        "            description=\"The column name for label of the input data.\",\n",
        "        ),\n",
        "        date_col=Input(\n",
        "            type=\"string\",\n",
        "            description=\"The column name for datatime. This will be used for generating time-series lagging data.\",\n",
        "        ),\n",
        "        lagging_orders=Input(\n",
        "            type=\"string\",\n",
        "            description=\"List of int which indicate how to generate lagging data for time-series input. Split by comma.\",\n",
        "        ),\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        model_perf=Output(\n",
        "            type=AssetTypes.URI_FILE,\n",
        "            mode=InputOutputModes.RW_MOUNT,\n",
        "        ),\n",
        "        model_folder=Output(\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            mode=InputOutputModes.RW_MOUNT,\n",
        "        ),\n",
        "    ),\n",
        "    input_data=\"${{inputs.data_source}}\",   # Define which input data will be splitted into mini-batches\n",
        "    partition_keys=[\"Store\", \"Brand\"],      # Use 'partition_keys' as the data division method. This method requires MLTable input with partition setting pre-defined in MLTable artifact.\n",
        "    instance_count=2,                       # Use 2 nodes from compute cluster to run this parallel job.\n",
        "    max_concurrency_per_instance=1,         # Create 2 worker processors in each compute node to execute mini-batches.\n",
        "    error_threshold=-1,                     # Monitor the failures of item processed by the gap between mini-batch input count and returns. 'Many model training' scenario doesn't fit this setting and '-1' means ignore counting failure items by mini-batch returns.\n",
        "    mini_batch_error_threshold=5,           # Monitor the failed mini-batch by exception, time out, or null return. When failed mini-batch count is higher than this setting, the parallel job will be marked as 'failed'.\n",
        "    retry_settings=dict(\n",
        "        max_retries=2,                      # Define how many retries when mini-batch execution is failed by exception, time out, or null return.\n",
        "        timeout=60,                         # Define the timeout in second for each mini-batch execution.\n",
        "    ),\n",
        "    logging_level=\"DEBUG\",\n",
        "    environment_variables={\n",
        "      \"AZUREML_PARALLEL_EXAMPLE\": \"oj_many-model_sdk\",\n",
        "    },\n",
        "    task=RunFunction(\n",
        "        code=\"./src/parallel_train/\",\n",
        "        entry_script=\"parallel_train.py\",\n",
        "        environment=Environment(\n",
        "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "            conda_file=\"./src/parallel_train/conda.yml\",\n",
        "        ),\n",
        "        program_arguments=\"--drop_cols ${{inputs.drop_cols}} \"  # Passthrough input parameters into parallel_train script.\n",
        "        \"--target_col ${{inputs.target_col}} \"\n",
        "        \"--date_col ${{inputs.date_col}} \"\n",
        "        \"--lagging_orders ${{inputs.lagging_orders}} \"\n",
        "        \"--model_folder ${{outputs.model_folder}} \",\n",
        "        append_row_to=\"${{outputs.model_perf}}\",                # Define where to output the aggregated returns from each mini-batches.\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Build pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673860087207
        },
        "name": "build-pipeline"
      },
      "outputs": [],
      "source": [
        "# Declare the overall input of the job.\n",
        "input_oj_data = Input(\n",
        "    path=\"./oj_sales_data/oj_sales_data.csv\", type=AssetTypes.URI_FILE, mode=InputOutputModes.RO_MOUNT\n",
        ")\n",
        "\n",
        "# Declare pipeline structure.\n",
        "@pipeline(\n",
        "    display_name=\"parallel job for oj many model training\",\n",
        ")\n",
        "def partition_job_in_pipeline(\n",
        "    pipeline_input_data,\n",
        "):\n",
        "    # Declare 1st data partition command job. \n",
        "    partition_job = partition_data(\n",
        "        data_source=pipeline_input_data,\n",
        "        partition_keys=\"Store,Brand\",\n",
        "    )\n",
        "\n",
        "    # Declare 2nd parallel model training job. \n",
        "    parallel_train = many_model_training_with_partition_keys(\n",
        "        data_source=partition_job.outputs.tabular_output_data,\n",
        "        drop_cols=\"Revenue,Advert,Store,Brand\",\n",
        "        target_col=\"Quantity\",\n",
        "        date_col=\"WeekStarting\",\n",
        "        lagging_orders=\"1,2,3,4,5,6\",\n",
        "    )\n",
        "\n",
        "    # User could override parallel job run-level property when invoke that parallel job/component in pipeline.\n",
        "    parallel_train.resources.instance_count = 3\n",
        "    parallel_train.max_concurrency_per_instance = 2\n",
        "    parallel_train.mini_batch_error_threshold = 10\n",
        "    parallel_train.outputs.model_perf.path = f\"azureml://datastores/workspaceblobstore/paths/${{name}}/my_append_output.csv\"\n",
        "\n",
        "# Create pipeline instance\n",
        "my_job = partition_job_in_pipeline(\n",
        "    pipeline_input_data=input_oj_data,\n",
        ")\n",
        "\n",
        "# Set pipeline level compute\n",
        "my_job.tags.update\n",
        "my_job.settings.default_compute = \"cpu-cluster\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673860094391
        }
      },
      "outputs": [],
      "source": [
        "print(my_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Submit pipeline job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1673858666260
        },
        "name": "submit-pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    my_job, \n",
        "    experiment_name=\"parallel examples\",\n",
        ")\n",
        "pipeline_job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "stream-pipeline"
      },
      "outputs": [],
      "source": [
        "# wait until the job completes\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    }
  ],
  "metadata": {
    "description": {
      "description": "Create pipeline with parallel node to do batch inference"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5453941c8a5423dedcc1bbfe54c629c64419b823fd855fc23a2722efc7559a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
