{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist batch prediction example \\[Parallel job\\] \\[SDK example\\]\n",
    "## Key notes for this example\n",
    "- How to use **parallel job** for **batch inferencing** scenario.\n",
    "- How to use parallel job **run_function** task with predefined **entry_script**.\n",
    "- How to use **url_folder** with **files data** as the **input of parallel job**.\n",
    "- How to use **mini_batch_size** in parallel job to split input data by size. \n",
    "- How to use **append_row_to** to aggregate returns to **uri_file** output.\n",
    "\n",
    "To get the same example with CLI + Yaml experience, please refer to: [link](../../../../../cli/jobs/parallel/3a_mnist_batch_identification/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1675653935697
    },
    "name": "required-library"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input, Output, load_component\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Environment, ResourceConfiguration\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credential\n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1675653940684
    },
    "name": "credential"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get a handle to the workspace\n",
    "\n",
    "We use config file to connect to a workspace. The Azure ML workspace should be configured with computer cluster. [Check this notebook for configure a workspace](../../configuration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1675653943687
    },
    "name": "workspace"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "print(ml_client.compute.get(cpu_compute_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define components and jobs in pipeline\n",
    "## 2.1 Load existing component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1675654881745
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prepare_data_component = load_component(source=\"./script/prepare_data.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2.2 Declare parallel job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1675654195739
    },
    "name": "parallel-job-for-file-data"
   },
   "outputs": [],
   "source": [
    "# Declare parallel job with run_function task\n",
    "batch_inferencing_with_mini_batch_size = parallel_run_function(\n",
    "    name=\"batch_inferencing_with_mini_batch_size\",\n",
    "    display_name=\"Batch Inferencing with mini_batch_size\",\n",
    "    description=\"parallel job to do batch inferencing with mini_batch_size on url folder with files input\",\n",
    "    tags={\n",
    "        \"azureml_parallel_example\": \"3a_sdk\",\n",
    "    },\n",
    "    inputs=dict(\n",
    "        job_data_path=Input(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=\"Input tabular mltable data.\",\n",
    "            mode=InputOutputModes.RO_MOUNT,\n",
    "        ),\n",
    "        score_model=Input(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=\"Folder contains the model file.\",\n",
    "            mode=InputOutputModes.DOWNLOAD,\n",
    "        ),\n",
    "    ),\n",
    "    outputs=dict(\n",
    "        job_output_file=Output(\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "    ),\n",
    "    input_data=\"${{inputs.job_data_path}}\",  # Define which input data will be splitted into mini-batches\n",
    "    mini_batch_size=\"5\",  # Use 'mini_batch_size' as the data division method. For files input data, this number define the file count for each mini-batch.\n",
    "    instance_count=2,  # Use 2 nodes from compute cluster to run this parallel job.\n",
    "    max_concurrency_per_instance=2,  # Create 2 worker processors in each compute node to execute mini-batches.\n",
    "    error_threshold=5,  # Monitor the failures of item processed by the gap between mini-batch input count and returns. 'Batch inferencing' scenario should return a list, dataframe, or tuple with the successful items to try to meet this threshold.\n",
    "    mini_batch_error_threshold=5,  # Monitor the failed mini-batch by exception, time out, or null return. When failed mini-batch count is higher than this setting, the parallel job will be marked as 'failed'.\n",
    "    retry_settings=dict(\n",
    "        max_retries=2,  # Define how many retries when mini-batch execution is failed by exception, time out, or null return.\n",
    "        timeout=60,  # Define the timeout in second for each mini-batch execution.\n",
    "    ),\n",
    "    logging_level=\"DEBUG\",\n",
    "    environment_variables={\n",
    "        \"AZUREML_PARALLEL_EXAMPLE\": \"3a_sdk\",\n",
    "    },\n",
    "    task=RunFunction(\n",
    "        code=\"./script\",\n",
    "        entry_script=\"digit_identification.py\",\n",
    "        environment=Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "            conda_file=\"./script/environment_parallel.yml\",\n",
    "        ),\n",
    "        program_arguments=\"--model ${{inputs.score_model}} \",\n",
    "        append_row_to=\"${{outputs.job_output_file}}\",  # Define where to output the aggregated returns from each mini-batches.\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1675654935751
    },
    "name": "build-pipeline"
   },
   "outputs": [],
   "source": [
    "# Declare the inputs of the job.\n",
    "input_model_folder = Input(\n",
    "    path=\"./mnist_models\", type=AssetTypes.URI_FOLDER, mode=InputOutputModes.DOWNLOAD\n",
    ")\n",
    "\n",
    "# Declare pipeline structure.\n",
    "@pipeline(\n",
    "    display_name=\"parallel job for iris batch inferencing\",\n",
    ")\n",
    "def parallel_job_in_pipeline():\n",
    "    # Declare command job to prepare mnist data\n",
    "    prepare_data = prepare_data_component()\n",
    "\n",
    "    # Declare parallel inferencing job.\n",
    "    predict_digits_mnist = batch_inferencing_with_mini_batch_size(\n",
    "        job_data_path=prepare_data.outputs.mnist_png,\n",
    "        score_model=input_model_folder,\n",
    "    )\n",
    "\n",
    "    # User could override parallel job run-level property when invoke that parallel job/component in pipeline.\n",
    "    predict_digits_mnist.resources.instance_count = 2\n",
    "    predict_digits_mnist.max_concurrency_per_instance = 2\n",
    "    predict_digits_mnist.mini_batch_error_threshold = 10\n",
    "    predict_digits_mnist.outputs.job_output_file.path = f\"azureml://datastores/workspaceblobstore/paths/${{name}}/aggregated_returns.csv\"\n",
    "\n",
    "\n",
    "# Create pipeline instance\n",
    "my_job = parallel_job_in_pipeline()\n",
    "\n",
    "# Set pipeline level compute\n",
    "my_job.tags.update\n",
    "my_job.settings.default_compute = \"cpu-cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1675412007635
    }
   },
   "outputs": [],
   "source": [
    "print(my_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1675654944788
    },
    "name": "submit-pipeline"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>parallel examples</td><td>teal_star_wbf1qth7wy</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/teal_star_wbf1qth7wy?wsid=/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourcegroups/alainli-rg/workspaces/alainli-prs-eastus2&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {}, 'outputs': {}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa3ffef4250>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'parallel job for iris batch inferencing', 'is_deterministic': None, 'inputs': {}, 'outputs': {}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'prepare_data': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'prepare_data', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa414600e20>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {}, 'job_outputs': {}, 'inputs': {}, 'outputs': {}, 'component': 'azureml_anonymous:27291cf9-0b64-4925-9dfe-ec721602da50', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'a5d5f726-b20b-438d-9684-ef24ab64b3b0', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'swept': False}), 'predict_digits_mnist': Parallel({'init': False, 'type': 'parallel', 'status': None, 'log_files': None, 'name': 'predict_digits_mnist', 'description': None, 'tags': {'azureml_parallel_example': '3a_sdk'}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/alainli-v20/code/Users/alainli/prs2.0/examples/3a_mnist_batch_identification', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa3ffef44f0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'Batch Inferencing with mini_batch_size', 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'score_model': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceblobstore/paths/LocalUpload/751725aa7d71c5f0eb4ab96705f967d1/mnist_models/', 'mode': 'download'}, 'job_data_path': '${{parent.jobs.prepare_data.outputs.mnist_png}}'}, 'job_outputs': {'job_output_file': {'type': 'uri_file', 'path': 'azureml://datastores/workspaceblobstore/paths/${name}/aggregated_returns.csv', 'mode': 'rw_mount'}}, 'inputs': {'score_model': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa3ffef4580>, 'job_data_path': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa3ffef4400>}, 'outputs': {'job_output_file': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7fa3ffef4880>}, 'component': 'azureml_anonymous:438c8c51-2a84-4499-be50-96c3e485f7ef', 'referenced_control_flow_node_instance_id': None, 'kwargs': {}, 'instance_id': '2763992e-acad-42fc-bfee-cbf89d8e1121', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'task': {'type': 'run_function', 'code': '/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastus2/codes/0a2196a8-c05e-4add-b712-9fea1f090732/versions/1', 'entry_script': 'digit_identification.py', 'program_arguments': '--model ${{inputs.score_model}} ', 'append_row_to': '${{outputs.job_output_file}}', 'environment': '/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastus2/environments/CliV2AnonymousEnvironment/versions/383d54f309a26c2a2a4739a2f5b0d002'}, 'mini_batch_size': 5, 'partition_keys': None, 'input_data': '${{inputs.job_data_path}}', 'retry_settings': {'timeout': 60, 'max_retries': 2}, 'logging_level': 'DEBUG', 'max_concurrency_per_instance': 2, 'error_threshold': 5, 'mini_batch_error_threshold': 10, 'resources': {'instance_count': 2}, 'environment_variables': {'AZUREML_PARALLEL_EXAMPLE': '3a_sdk'}})}, 'job_types': {'command': 1, 'parallel': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'teal_star_wbf1qth7wy', 'description': None, 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.defaultComputeName': 'cpu-cluster', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'id': '/subscriptions/ee85ed72-2b26-48f6-a0e8-cb5bcf98fbd9/resourceGroups/alainli-rg/providers/Microsoft.MachineLearningServices/workspaces/alainli-prs-eastus2/jobs/teal_star_wbf1qth7wy', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/alainli-v20/code/Users/alainli/prs2.0/examples/3a_mnist_batch_identification', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fa3ffef4460>, 'serialize': <msrest.serialization.Serializer object at 0x7fa3ffef4d30>, 'display_name': 'parallel job for iris batch inferencing', 'experiment_name': 'parallel examples', 'compute': None, 'services': {'Tracking': <azure.ai.ml._restclient.v2022_10_01_preview.models._models_py3.JobService object at 0x7fa3ffee9b80>, 'Studio': <azure.ai.ml._restclient.v2022_10_01_preview.models._models_py3.JobService object at 0x7fa3ffee9bb0>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    my_job,\n",
    "    experiment_name=\"parallel examples\",\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "stream-pipeline"
   },
   "outputs": [],
   "source": [
    "# wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Create pipeline with parallel node to do batch inference"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ae07c4b99ac25738c7bfa6b1621bd60cdd0bc17be892292e1994851af7067b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
