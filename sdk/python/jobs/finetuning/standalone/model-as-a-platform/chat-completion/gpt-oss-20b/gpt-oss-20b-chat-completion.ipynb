{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion - Multilingual-Thinking\n",
    "\n",
    "This sample shows how use `chat-completion` components from the `azureml` system registry to fine tune a model to complete a conversation between 2 people using Multilingual-Thinking dataset. We then deploy the fine tuned model to an online endpoint for real time inference.\n",
    "\n",
    "### Training data\n",
    "We will use the [Multilingual-Thinking](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking) dataset. Multilingual-Thinking is a reasoning dataset where the chain-of-thought has been translated from English into one of 4 languages: Spanish, French, Italian, and German. The dataset was created by sampling 1k training samples from the SystemChat subset of SmolTalk2 and translating the reasoning traces with another language model.\n",
    "\n",
    "### Model\n",
    "We will use the `gpt-oss-20b` model to show how user can finetune a model for chat-completion task. \n",
    "\n",
    "### Outline\n",
    "* Setup pre-requisites such as compute.\n",
    "* Pick a model to fine tune.\n",
    "* Pick and explore training data.\n",
    "* Configure the fine tuning job.\n",
    "* Run the fine tuning job.\n",
    "* Review training and evaluation metrics. \n",
    "* Register the fine tuned model. \n",
    "* Deploy the fine tuned model for real time inference.\n",
    "* Clean up resources. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check or create compute. A single GPU node can have multiple GPU cards. For example, in one node of `Standard_NC96ads_A100_v4` there are 4 NVIDIA A100 GPUs, while in `Standard_ND96isr_H100_v5` there are 8 NVIDIA H100 GPUs. Refer to the [docs](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) for this information. The number of GPU cards per node is set in the param `gpus_per_node` below. Setting this value correctly will ensure utilization of all GPUs in the node. The recommended GPU compute SKUs can be found [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series), [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series), and [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv5-series).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies by running below cell. This is not an optional step if running in a new environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "%pip install datasets==4.0.0\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"\",\n",
    "        resource_group_name=\"\",\n",
    "        workspace_name=\"\",\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML registry, \"azureml-openai-oss\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml-openai-oss\")\n",
    "experiment_name = \"chat_completion_gpt_oss\"\n",
    "\n",
    "# generating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick gpt-oss model to fine tune\n",
    "\n",
    "`gpt-oss-20b` is a open source model from OpenAI for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters), we need to finetune the model for our specific purpose in order to use it. You can browse these models in the Model Catalog in the AzureML Studio, filtering by the `chat-completion` task. \n",
    "\n",
    "Note the model id property of the model. This will be passed as input to the fine tuning job. This is also available as the `Asset ID` field in model details page in AzureML Studio Model Catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-oss-20b\"\n",
    "foundation_model = registry_ml_client.models.get(model_name, version=\"6\")\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a compute to be used with the job\n",
    "\n",
    "The finetune job works `ONLY` with `GPU` compute. The size of the compute depends on how big the model is and in most cases it becomes tricky to identify the right compute for the job. In this cell, we guide the user to select the right compute for the job.\n",
    "\n",
    "`NOTE1` The computes listed below work with the most optimized configuration. Any changes to the configuration might lead to Cuda Out Of Memory error. In such cases, try to upgrade the compute to a bigger compute size.\n",
    "\n",
    "`NOTE2` While selecting the compute_cluster_size below, make sure the compute is available in your resource group. If a particular compute is not available you can make a request to get access to the compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "if \"finetune_compute_allow_list\" in foundation_model.tags:\n",
    "    computes_allow_list = ast.literal_eval(\n",
    "        foundation_model.tags[\"finetune_compute_allow_list\"]\n",
    "    )  # convert string to python list\n",
    "    print(f\"Please create a compute from the above list - {computes_allow_list}\")\n",
    "else:\n",
    "    computes_allow_list = None\n",
    "    print(\"`finetune_compute_allow_list` is not part of model tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a specific compute size to work with change it here. By default we use the 8 x V100 compute from the above list\n",
    "compute_cluster_size = \"Standard_NC96ads_A100_v4\"\n",
    "\n",
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-a100-nc-series'\n",
    "compute_cluster = \"gpu-a100-nc-series\"\n",
    "\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster,\n",
    "            size=compute_cluster_size,\n",
    "            tier=\"Dedicated\",\n",
    "            max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(\n",
    "                \"Attempt #2 - Trying to create a low priority compute. Since this is a low priority compute, the job could get pre-empted before completion.\"\n",
    "            )\n",
    "            compute = AmlCompute(\n",
    "                name=compute_cluster,\n",
    "                size=compute_cluster_size,\n",
    "                tier=\"LowPriority\",\n",
    "                max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "            )\n",
    "            workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise ValueError(\n",
    "                f\"WARNING! Compute size {compute_cluster_size} not available in workspace\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Sanity check on the created compute\n",
    "compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "if compute.provisioning_state.lower() == \"failed\":\n",
    "    raise ValueError(\n",
    "        f\"Provisioning failed, Compute '{compute_cluster}' is in failed state. \"\n",
    "        f\"please try creating a different compute\"\n",
    "    )\n",
    "\n",
    "if computes_allow_list is not None:\n",
    "    computes_allow_list_lower_case = [x.lower() for x in computes_allow_list]\n",
    "    if compute.size.lower() not in computes_allow_list_lower_case:\n",
    "        raise ValueError(\n",
    "            f\"VM size {compute.size} is not in the allow-listed computes for finetuning\"\n",
    "        )\n",
    "else:\n",
    "    # Computes with K80 GPUs are not supported\n",
    "    unsupported_gpu_vm_list = [\n",
    "        \"standard_nc6\",\n",
    "        \"standard_nc12\",\n",
    "        \"standard_nc24\",\n",
    "        \"standard_nc24r\",\n",
    "    ]\n",
    "    if compute.size.lower() in unsupported_gpu_vm_list:\n",
    "        raise ValueError(\n",
    "            f\"VM size {compute.size} is currently not supported for finetuning\"\n",
    "        )\n",
    "\n",
    "\n",
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute.\n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpu_count_found = False\n",
    "workspace_compute_sku_list = workspace_ml_client.compute.list_sizes()\n",
    "available_sku_sizes = []\n",
    "for compute_sku in workspace_compute_sku_list:\n",
    "    available_sku_sizes.append(compute_sku.name)\n",
    "    if compute_sku.name.lower() == compute.size.lower():\n",
    "        gpus_per_node = compute_sku.gpus\n",
    "        gpu_count_found = True\n",
    "# if gpu_count_found not found, then print an error\n",
    "if gpu_count_found:\n",
    "    print(f\"Number of GPU's in compute {compute.size}: {gpus_per_node}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Number of GPU's in compute {compute.size} not found. Available skus are: {available_sku_sizes}.\"\n",
    "        f\"This should not happen. Please check the selected compute cluster: {compute_cluster} and try again.\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pick the dataset for fine-tuning the model\n",
    "\n",
    "We use the [Multilingual-Thinking](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking) dataset. The dataset has train split, suitable for:\n",
    "* Supervised fine-tuning (sft).\n",
    "* Reasoning .\n",
    "**Key points:**\n",
    "- Only one split is present (e.g., `train`), with 1000 examples.\n",
    "- Each example contains fields such as `reasoning_language`, `developer`, `user`, `analysis`, `final`, and a list of `messages`.\n",
    "- The `messages` field is a list of message objects, each with a `content`, `role`, and optionally `thinking`.\n",
    "- Typical roles include: `system` (for instructions/context), `user`, and `assistant`.\n",
    "\n",
    "> The dataset is stored in jsonl. You can load and visualize it using pandas.\n",
    "\n",
    "#### Example data schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"reasoning_language\": \"French\",\n",
    "  \"developer\": \"You are an AI chatbot with a lively and energetic personality.\",\n",
    "  \"user\": \"Can you show me the latest trends on Twitter right now?\",\n",
    "  \"analysis\": \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\",\n",
    "  \"final\": \"Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \\\"Trending\\\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \\\"Distracted Boyfriend\\\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": \"reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.\",\n",
    "      \"role\": \"system\",\n",
    "      \"thinking\": null\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"Can you show me the latest trends on Twitter right now?\",\n",
    "      \"role\": \"user\",\n",
    "      \"thinking\": null\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \\\"Trending\\\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \\\"Distracted Boyfriend\\\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!\",\n",
    "      \"role\": \"assistant\",\n",
    "      \"thinking\": \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "> **Note:** The `developer` and `system` roles provide custom instructions or context for the model. The `user` and `assistant` roles represent the conversation turns. The `thinking` field can be used for intermediate reasoning or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def save_hf_dataset_to_jsonl(dataset, output_file):\n",
    "    \"\"\"Save HuggingFace dataset to JSONL file.\"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for record in dataset:\n",
    "            f.write(json.dumps(record, ensure_ascii=False, default=str) + \"\\n\")\n",
    "    print(f\"Saved {len(dataset)} records to {output_file}\")\n",
    "\n",
    "\n",
    "def convert_multilingual_thinking(input_file, output_file):\n",
    "    \"\"\"Convert multilingual thinking dataset to standard JSONL format with only messages field.\"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    json_objects = [json.loads(line) for line in lines if line.strip()]\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in json_objects:\n",
    "            cleaned_messages = []\n",
    "            for msg in obj.get(\"messages\", []):\n",
    "                cleaned_msg = {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "                if \"thinking\" in msg:\n",
    "                    cleaned_msg[\"thinking\"] = msg[\"thinking\"]\n",
    "                cleaned_messages.append(cleaned_msg)\n",
    "            output_obj = {\"messages\": cleaned_messages}\n",
    "            f.write(json.dumps(output_obj, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Converted {len(json_objects)} records to {output_file}\")\n",
    "\n",
    "\n",
    "def split_jsonl(\n",
    "    input_file,\n",
    "    train_file,\n",
    "    valid_file,\n",
    "    test_file,\n",
    "    train_size=800,\n",
    "    valid_size=100,\n",
    "    test_size=100,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"Split JSONL file into train/valid/test splits.\"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line for line in f if line.strip()]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "    train_lines = lines[:train_size]\n",
    "    valid_lines = lines[train_size : train_size + valid_size]\n",
    "    test_lines = lines[train_size + valid_size : train_size + valid_size + test_size]\n",
    "    with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(train_lines)\n",
    "    with open(valid_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(valid_lines)\n",
    "    with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(test_lines)\n",
    "    print(\n",
    "        f\"Train: {len(train_lines)}, Valid: {len(valid_lines)}, Test: {len(test_lines)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "save_hf_dataset_to_jsonl(dataset, \"multilingual_thinking_raw_data.json\")\n",
    "convert_multilingual_thinking(\n",
    "    \"multilingual_thinking_raw_data.json\", \"multilingual_thinking_formatted_data.jsonl\"\n",
    ")\n",
    "split_jsonl(\n",
    "    \"multilingual_thinking_formatted_data.jsonl\",\n",
    "    \"multilingual_thinking_train.jsonl\",\n",
    "    \"multilingual_thinking_valid.jsonl\",\n",
    "    \"multilingual_thinking_test.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ./multilingual_thinking_train.jsonl file into a pandas dataframe and show the first row\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(\"./multilingual_thinking_train.jsonl\", lines=True)\n",
    "df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Submit the fine tuning job using the the model and data as inputs\n",
    " \n",
    "Create the job that uses the `chat-completion` pipeline component. [Learn more](https://github.com/Azure/azureml-assets/blob/main/assets/training/finetune_acft_hf_nlp/components/pipeline_components/chat_completion/README.md) about all the parameters supported for fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define finetune parameters\n",
    "\n",
    "Finetune parameters can be grouped into 2 categories - training parameters, optimization parameters\n",
    "\n",
    "Training parameters define the training aspects such as - \n",
    "1. the optimizer, scheduler to use\n",
    "2. the metric to optimize the finetune\n",
    "3. number of training steps and the batch size\n",
    "and so on\n",
    "\n",
    "Optimization parameters help in optimizing the GPU memory and effectively using the compute resources. Below are few of the parameters that belong to this category. _The optimization parameters differs for each model and are packaged with the model to handle these variations._\n",
    "1. enable the deepspeed and LoRA\n",
    "2. enable mixed precision training\n",
    "2. enable multi-node training \n",
    "\n",
    "Note: Supervised finetuning may result in loosing alignment or catastrophic forgetting. We recommend checking for this issue and running an alignment stage after you finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default training parameters\n",
    "training_parameters = dict(\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "# Default optimization parameters\n",
    "optimization_parameters = dict(\n",
    "    apply_lora=\"true\",\n",
    "    apply_deepspeed=\"true\",\n",
    "    deepspeed_stage=3,\n",
    ")\n",
    "# Let's construct finetuning parameters using training and optimization paramters.\n",
    "finetune_parameters = {**training_parameters, **optimization_parameters}\n",
    "\n",
    "# Each model finetuning works best with certain finetuning parameters which are packed with model as `model_specific_defaults`.\n",
    "# Let's override the finetune_parameters in case the model has some custom defaults.\n",
    "if \"model_specific_defaults\" in foundation_model.tags:\n",
    "    print(\"Warning! Model specific defaults exist. The defaults could be overridden.\")\n",
    "    finetune_parameters.update(\n",
    "        ast.literal_eval(  # convert string to python dict\n",
    "            foundation_model.tags[\"model_specific_defaults\"]\n",
    "        )\n",
    "    )\n",
    "print(\n",
    "    f\"The following finetune parameters are going to be set for the run: {finetune_parameters}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pipeline display name for distinguishing different runs from the name\n",
    "def get_pipeline_display_name():\n",
    "    batch_size = (\n",
    "        int(finetune_parameters.get(\"per_device_train_batch_size\", 1))\n",
    "        * int(finetune_parameters.get(\"gradient_accumulation_steps\", 1))\n",
    "        * int(gpus_per_node)\n",
    "        * int(finetune_parameters.get(\"num_nodes_finetune\", 1))\n",
    "    )\n",
    "    scheduler = finetune_parameters.get(\"lr_scheduler_type\", \"linear\")\n",
    "    deepspeed = finetune_parameters.get(\"apply_deepspeed\", \"false\")\n",
    "    ds_stage = finetune_parameters.get(\"deepspeed_stage\", \"2\")\n",
    "    if deepspeed == \"true\":\n",
    "        ds_string = f\"ds{ds_stage}\"\n",
    "    else:\n",
    "        ds_string = \"nods\"\n",
    "    lora = finetune_parameters.get(\"apply_lora\", \"false\")\n",
    "    if lora == \"true\":\n",
    "        lora_string = \"lora\"\n",
    "    else:\n",
    "        lora_string = \"nolora\"\n",
    "    save_limit = finetune_parameters.get(\"save_total_limit\", -1)\n",
    "    seq_len = finetune_parameters.get(\"max_seq_length\", -1)\n",
    "    return (\n",
    "        model_name\n",
    "        + \"-\"\n",
    "        + \"reasoning\"\n",
    "        + \"-\"\n",
    "        + f\"bs{batch_size}\"\n",
    "        + \"-\"\n",
    "        + f\"{scheduler}\"\n",
    "        + \"-\"\n",
    "        + ds_string\n",
    "        + \"-\"\n",
    "        + lora_string\n",
    "        + f\"-save_limit{save_limit}\"\n",
    "        + f\"-seqlen{seq_len}\"\n",
    "    )\n",
    "\n",
    "\n",
    "pipeline_display_name = get_pipeline_display_name()\n",
    "print(f\"Display name used for the run: {pipeline_display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"chat_completion_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline(name=pipeline_display_name)\n",
    "def create_pipeline():\n",
    "    chat_completion_pipeline = pipeline_component_func(\n",
    "        pytorch_model_path=foundation_model.id,\n",
    "        compute_model_import=compute_cluster,\n",
    "        compute_preprocess=compute_cluster,\n",
    "        compute_finetune=compute_cluster,\n",
    "        compute_model_evaluation=compute_cluster,\n",
    "        # map the dataset splits to parameters\n",
    "        train_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./multilingual_thinking_train.jsonl\"\n",
    "        ),\n",
    "        validation_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./multilingual_thinking_valid.jsonl\"\n",
    "        ),\n",
    "        test_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./multilingual_thinking_test.jsonl\"\n",
    "        ),\n",
    "        # Training settings\n",
    "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
    "        **finetune_parameters\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": chat_completion_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "\n",
    "# set continue on step failure to False\n",
    "pipeline_object.settings.continue_on_step_failure = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Register the fine tuned model with the workspace\n",
    "\n",
    "We will register the model from the output of the fine tuning job. This will track lineage between the fine tuned model and the fine tuning job. The fine tuning job, further, tracks lineage to the foundation model, data and training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# check if the `trained_model` output is available\n",
    "print(\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job.name).outputs)\n",
    "\n",
    "# fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
    "    pipeline_job.name, \"trained_model\"\n",
    ")\n",
    "\n",
    "finetuned_model_name = model_name + \"multi-lingual-reasoning\"\n",
    "finetuned_model_name = finetuned_model_name.replace(\"/\", \"-\")\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=finetuned_model_name,\n",
    "    version=timestamp,  # use timestamp as version to avoid version conflict\n",
    "    description=model_name\n",
    "    + \" fine tuned model for multi-lingual-thinking chat-completion\",\n",
    ")\n",
    "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
    "# register the model from pipeline job output\n",
    "registered_model = workspace_ml_client.models.create_or_update(\n",
    "    prepare_to_register_model\n",
    ")\n",
    "print(\"registered model: \\n\", registered_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Deploy the fine tuned model to an online endpoint\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    OnlineRequestSettings,\n",
    ")\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "\n",
    "online_endpoint_name = \"multi-lingual-reasoning-chat-completion-\" + timestamp\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + registered_model.name\n",
    "    + \", fine tuned model for multi-lingual-reasoning-chat-completion\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find here the list of SKU's supported for deployment - [Managed online endpoints SKU list](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "instance_type = \"Standard_NC6s_v3\"\n",
    "\n",
    "# Inference compute allow list that supports deployment\n",
    "if \"inference_compute_allow_list\" in foundation_model.tags:\n",
    "    inference_computes_allow_list = ast.literal_eval(\n",
    "        foundation_model.tags[\"inference_compute_allow_list\"]\n",
    "    )  # convert string to python list\n",
    "    print(f\"Please create a compute from the above list - {computes_allow_list}\")\n",
    "else:\n",
    "    inference_computes_allow_list = None\n",
    "    print(\"`inference_compute_allow_list` is not part of model tags\")\n",
    "\n",
    "# Check if the compute is in the allow listed computes\n",
    "if (\n",
    "    inference_computes_allow_list is not None\n",
    "    and instance_type not in inference_computes_allow_list\n",
    "):\n",
    "    print(\n",
    "        f\"`instance_type` is not in the allow listed compute. Please select a value from {inference_computes_allow_list}\"\n",
    "    )\n",
    "\n",
    "# create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    liveness_probe=ProbeSettings(initial_delay=600),\n",
    "    request_settings=OnlineRequestSettings(request_timeout_ms=90000),\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test the endpoint with sample data\n",
    "\n",
    "We will fetch some sample data from the test dataset and submit to online endpoint for inference. We will then show the display the scored labels alongside the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ./multilingual_thinking_test.jsonl into a pandas dataframe\n",
    "test_df = pd.read_json(\"./multilingual_thinking_test.jsonl\", lines=True)\n",
    "# take few random samples\n",
    "test_df = test_df.sample(n=1)\n",
    "# rebuild index\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# create a json object with the key as \"input_data\" and value as a list of values from the text column of the test dataframe\n",
    "parameters = {\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 200,\n",
    "}\n",
    "test_json = {\n",
    "    \"input_data\": {\n",
    "        \"input_string\": [test_df[\"messages\"][0]],\n",
    "        \"parameters\": parameters,\n",
    "    },\n",
    "    \"params\": {},\n",
    "}\n",
    "# save the json object to a file named sample_score.json in the ./samsum-dataset folder\n",
    "with open(\"./sample_score.json\", \"w\") as f:\n",
    "    json.dump(test_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"demo\",\n",
    "    request_file=\"./sample_score.json\",\n",
    ")\n",
    "print(\"raw response: \\n\", response, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Delete the online endpoint\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
