{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Installing Azure cli and Azure SDK for Python.\n",
    "! pip install azure-core azure-ai-ml rich\n",
    "#! curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562e306",
   "metadata": {},
   "source": [
    "### Create AzureML Workspace connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in various AzureML system registries\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "experiment_name = \"grpo_chat_completion_qwen_2_5_7b_instruct\"\n",
    "# Get AzureML workspace object.\n",
    "workspace = workspace_ml_client._workspaces.get(workspace_ml_client.workspace_name)\n",
    "workspace.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb026c1",
   "metadata": {},
   "source": [
    "### 2. Pick a model to fine tune\n",
    "\n",
    "`Qwen2.5` is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. If you have opened this notebook for a different model, replace the model name and version accordingly.\n",
    "\n",
    "The pipeline which we are going to use for fine-tuning supports models which are open source and available from hugging face and open source models from Azure model catalog.\n",
    "Note the model id property of the model. This will be passed as input to the fine tuning job. This is also available as the `Asset ID` field in model details page in Azure AI Studio Model Catalog.\n",
    "\n",
    "For importing a model from AzureML catalogue refer to model import section of [chat_completion_with_model_as_platform.ipynb](..\\finetuning\\standalone\\model-as-a-platform\\chat-completion\\chat_completion_with_model_as_platform.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40190640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid HuggingFace ID of Open Source model to be used for fine-tuning\n",
    "model_name = f\"Qwen/Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd924e",
   "metadata": {},
   "source": [
    "The complete dataset can be found in datasets/med_mcqa. Here is a sample of the dataset :\n",
    "\n",
    "## <span style=\"font-size:0.8em;\"> Sample DataSet</span>\n",
    "\n",
    "ðŸ“˜ When solving a multiple-choice question, we ask the LLM to follow this structure:\n",
    "\n",
    "**Think out loud**: Explain your reasoning step by step. Wrap this part in tags.  \n",
    "**Give your final answer**: Clearly state your chosen option (A, B, C, or D) and explain why it's correct. Wrap this part in tags.<br>\n",
    "**Final Answer line**: On a new line, write Final Answer: followed by just one letter â€” A, B, C, or D. \n",
    "\n",
    "âœ… **Example Question**: \n",
    "\n",
    "```text\n",
    "CSF Rhinorrhea occurs due to damage of:\n",
    "\n",
    "Options:\n",
    "\n",
    "A. Roof of orbit \n",
    "B. Cribriform plate of ethmoidal bone \n",
    "C. Frontal sinus \n",
    "D. Sphenoid bone\n",
    "```\n",
    "**Ideal reasoning model response:**\n",
    "```text\n",
    "<think>\n",
    "\n",
    "Start by identifying the anatomical structure most commonly associated with CSF (cerebrospinal fluid) leakage. CSF rhinorrhea typically results from a breach in the skull base, especially the cribriform plate of the ethmoid bone, which is thin and located near the nasal cavity.\n",
    "\n",
    "</think>\n",
    "\n",
    "The cribriform plate of the ethmoid bone is the most common site of CSF leakage into the nasal cavity, leading to CSF rhinorrhea. This makes option B the correct answer. \n",
    "\n",
    "<answer>B</answer>\n",
    "\n",
    "Final Answer: B\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71ed6c",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497f1aa-513e-48ec-8c46-812f169c2975",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# <span style=\"font-size:0.8em;\">ðŸ§© Section 2: How to train a Reasoning Model on AML Using GRPO Trainer</span>\n",
    "\n",
    "<div style=\"display: flex; align-items: flex-start; gap: 32px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "    <p>The reasoning model training process typically includes three key components:</p>\n",
    "    <ul>\n",
    "      <li><strong>Sampler</strong> â€“ Generates multiple candidate responses from the model</li>\n",
    "      <li><strong>Reward Function</strong> â€“ Evaluates and scores each response based on criteria like accuracy or structure</li>\n",
    "      <li><strong>Trainer</strong> â€“ Updates the model to reinforce high-quality outputs</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "      In this example we use the <strong>GRPO Trainer</strong> for training Qwen2.5-7B-Instruct model into a reasoning model. We use the GRPO implementation from TRL library.\n",
    "    </p>\n",
    "    <br>\n",
    "    <p>\n",
    "      <strong>GRPO</strong> (<strong>G</strong>roup <strong>R</strong>elative <strong>P</strong>olicy <strong>O</strong>ptimization) is a reinforcement learning technique that:\n",
    "    </p>\n",
    "    <ul>\n",
    "      <li><em>Compares</em> multiple answers within a group</li>\n",
    "      <li><em>Rewards</em> the best-performing outputs</li>\n",
    "      <li><em>Penalizes</em> poor ones</li>\n",
    "      <li>Applies careful updates to <em>avoid sudden changes</em></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; justify-content: center;\">\n",
    "    <img src=\"images/training_loop.png\" alt=\"Training Loop\" style=\"max-width:100%; width: 600px;\"/>\n",
    "  </div>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25545006-ab16-4641-87d7-96128ad6da6a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <span style=\"font-size:0.8em;\">Why does training reasoning models become easy in Azure ML?</span>  \n",
    "\n",
    "\n",
    "- **AzureML natively supports reasoning model training**, with seamless integration of vLLM and scalable training workflows.\n",
    "\n",
    "- **DeepSpeed scales effortlessly on AML**, enabling multi-node training by sharding model states across GPUs.\n",
    "\n",
    "- **Robust tracking, metrics, and debugging tools** make experimentation on AML smooth and production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f94ada",
   "metadata": {},
   "source": [
    "#### 2.2 Create compute\n",
    "\n",
    "In order to finetune a model on Azure Machine Learning studio, you will need to create a compute resource first. **Creating a compute will take 3-4 minutes.** \n",
    "\n",
    "For additional references, see [Azure Machine Learning in a Day](https://github.com/Azure/azureml-examples/blob/main/tutorials/azureml-in-a-day/azureml-in-a-day.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7297533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "compute_cluster_name = \"compute-cluster-grpo-chat-completion\"\n",
    "\n",
    "try:\n",
    "    compute_cluster = workspace_ml_client.compute.get(compute_cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_cluster = AmlCompute(\n",
    "        name=compute_cluster_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_ND96amsr_A100_v4\",\n",
    "        idle_time_before_scale_down=120,\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "    )\n",
    "    workspace_ml_client.begin_create_or_update(compute_cluster).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93503de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default training parameters\n",
    "training_parameters = dict(\n",
    "    dataset_prompt_column=\"problem\",\n",
    "    epsilon=\"0.5\",\n",
    "    eval_strategy=\"no\",\n",
    "    gradient_accumulation_steps=\"4\",\n",
    "    learning_rate=\"1e-06\",\n",
    "    max_steps=\"10\",\n",
    "    num_generations=\"4\",\n",
    "    num_iterations=\"1\",\n",
    "    num_nodes_finetune=\"1\",\n",
    "    num_train_epochs=3,\n",
    "    number_of_gpu_to_use_finetuning=\"8\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    ")\n",
    "\n",
    "# Let's construct finetuning parameters using training and optimization paramters.\n",
    "finetune_parameters = {**training_parameters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pipeline display name for distinguishing different runs from the name\n",
    "def get_pipeline_display_name():\n",
    "    batch_size = (\n",
    "        int(finetune_parameters.get(\"per_device_train_batch_size\", 1))\n",
    "        * int(finetune_parameters.get(\"gradient_accumulation_steps\", 1))\n",
    "        * int(finetune_parameters.get(\"number_of_gpu_to_use_finetuning\"))\n",
    "        * int(finetune_parameters.get(\"num_nodes_finetune\", 1))\n",
    "    )\n",
    "    max_prompt_length = finetune_parameters.get(\"max_prompt_length\", -1)\n",
    "    return (\n",
    "        model_name\n",
    "        + \"-\"\n",
    "        + \"grpo\"\n",
    "        + \"-\"\n",
    "        + f\"bs{batch_size}\"\n",
    "        + \"-\"\n",
    "        + f\"{max_prompt_length}\"\n",
    "    )\n",
    "\n",
    "\n",
    "pipeline_display_name = get_pipeline_display_name()\n",
    "print(f\"Display name used for the run: {pipeline_display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"grpo_chat_completion_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline(name=pipeline_display_name)\n",
    "def create_pipeline():\n",
    "    grpo_chat_completion_pipeline = pipeline_component_func(\n",
    "        huggingface_id=model_name,\n",
    "        compute_model_import=compute_cluster_name,\n",
    "        compute_finetune=compute_cluster_name,\n",
    "        # map the dataset splits to parameters\n",
    "        dataset_train_split=Input(\n",
    "            type=\"uri_file\", path=\"./datasets/med_mcqa/train.jsonl\"\n",
    "        ),\n",
    "        dataset_validation_split=Input(\n",
    "            type=\"uri_file\", path=\"./datasets/med_mcqa/validation.jsonl\"\n",
    "        ),\n",
    "        deepspeed_config=Input(type=\"uri_file\", path=\"./config/zero3.json\"),\n",
    "        # Training settings\n",
    "        **finetune_parameters,\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": grpo_chat_completion_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "\n",
    "# set continue on step failure to False\n",
    "pipeline_object.settings.continue_on_step_failure = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a649ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "created_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6038d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the pipeline job to complete\n",
    "status = workspace_ml_client.jobs.get(created_job.name).status\n",
    "\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    status = workspace_ml_client.jobs.get(created_job.name).status\n",
    "    print(f\"Current job status: {status}\")\n",
    "    if status in [\"Failed\", \"Completed\", \"Canceled\"]:\n",
    "        print(\"Job has finished with status: {0}\".format(status))\n",
    "        break\n",
    "    else:\n",
    "        print(\"Job is still running. Checking again in 30 seconds.\")\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138ca1a-50fc-4847-8af2-dbcb9d152567",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Register and deploy the fine tuned model\n",
    "\n",
    "The output of the training is a set of files representing the weights of the trained model. To use it for inferencing, we will register the files as a model and then create an endpoint and a deployment for it. An endpoint provides security, url and traffic-splitting aspects of inferencing, whereas a deployment actually hosts and runs the registered model.\n",
    "\n",
    "You can find the assets registered in this section in the AzureML portal ([ml.azure.com](ml.azure.com)). Navigate to your resource group and workspace and click on the models or endpoints tab on the left panel. Deployments are sub-entities of endpoints and they can be found on the detailed view page of a particular endpoint.\n",
    "\n",
    "For detailed implementation of model registration and deployment, refer to the same section in [launch_grpo_command_job-med-mcqa-commented.ipynb](./launch_grpo_command_job-med-mcqa-commented.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
