{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access Azure Blob storage using Access Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "access_key = token_library.getSecret(\"<KEY_VAULT_NAME>\", \"<ACCESS_KEY_SECRET_NAME>\")\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.key.<STORAGE_ACCOUNT_NAME>.blob.core.windows.net\", access_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"wasbs://<BLOB_CONTAINER_NAME>@<STORAGE_ACCOUNT_NAME>.blob.core.windows.net/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"wasbs://<BLOB_CONTAINER_NAME>@<STORAGE_ACCOUNT_NAME>.blob.core.windows.net/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access Azure Blob storage using SAS token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "sas_token = token_library.getSecret(\"<KEY_VAULT_NAME>\", \"<SAS_TOKEN_SECRET_NAME>\")\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.azure.sas.<BLOB_CONTAINER_NAME>.<STORAGE_ACCOUNT_NAME>.blob.core.windows.net\", sas_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"wasbs://<BLOB_CONTAINER_NAME>@<STORAGE_ACCOUNT_NAME>.blob.core.windows.net/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"wasbs://<BLOB_CONTAINER_NAME>@<STORAGE_ACCOUNT_NAME>.blob.core.windows.net/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access ADLS Gen 2 using User Identity passthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access ADLS Gen 2 using Service Principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "# Set up service principal tenant ID, client ID and secret from Azure Key Vault\n",
        "client_id = token_library.getSecret(\"<KEY_VAULT_NAME>\", \"<CLIENT_ID_SECRET_NAME>\")\n",
        "tenant_id = token_library.getSecret(\"<KEY_VAULT_NAME>\", \"<TENANT_ID_SECRET_NAME>\")\n",
        "client_secret = token_library.getSecret(\"<KEY_VAULT_NAME>\", \"<CLIENT_SECRET_NAME>\")\n",
        "\n",
        "# Set up service principal which has access of the data\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.auth.type.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net\",\"OAuth\")\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth.provider.type.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth2.client.id.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net\", client_id)\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth2.client.secret.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net\", client_secret)\n",
        "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth2.client.endpoint.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenant_id + \"/oauth2/token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access credentialed AzureML Blob Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"azureml://datastores/workspaceblobstore/paths/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"azureml://datastores/workspaceblobstore/paths/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access credentialless AzureML Blob Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"azureml://datastores/credlessblobdatastore/paths/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"azureml://datastores/credlessblobdatastore/paths/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access credentialed AzureML ADLS Gen 2 Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"azureml://datastores/adlsg2datastore/paths/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"azureml://datastores/credadlsg2datastore/paths/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access credentialless AzureML ADLS Gen 2 Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as pd\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "df = pd.read_csv(\"azureml://datastores/credlessadlsg2datastore/paths/data/titanic.csv\", index_col=\"PassengerId\")\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Age\"],\n",
        "    outputCol=\"Age\").setStrategy(\"mean\") # Replace missing values in Age column with the mean value\n",
        "df.fillna(value={\"Cabin\" : \"None\"}, inplace=True) # Fill Cabin column with value \"None\" if missing\n",
        "df.dropna(inplace=True) # Drop the rows which still have any missing value\n",
        "df.to_csv(\"azureml://datastores/credlessadlsg2datastore/paths/data/wrangled\", index_col=\"PassengerId\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Access mounted File Share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pyspark.pandas as pd\n",
        "abspath = os.path.abspath('.')\n",
        "file = \"file://\" + abspath + \"/Users/<USER>/data/titanic.csv\"\n",
        "print(file)\n",
        "df = pd.read_csv(file)\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('vnext')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": "ipython",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython",
      "version": "3.7.10"
    },
    "microsoft": {
      "language": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6aeff17a1aa7735c2f7cb3a6d691fe1b4d4c3b8d2d650f644ad0f24e1b8e3f3f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
