{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9467c5",
   "metadata": {},
   "source": [
    "# Deploying models with MLflow\n",
    "\n",
    "Azure ML supports no-code deployment of models created and logged with MLflow. This means that you don't have to provide a scoring script or an environment. Those models can be deployed to ACI (Azure Container Instances), AKS (Azure Kubernetes Services) or our managed inference services (usually referred as MIR). \n",
    "\n",
    "For no-code-deployment, Azure Machine Learning\n",
    "\n",
    "- Dynamically installs Python packages provided in the conda.yaml file, this means the dependencies are installed during container runtime.\n",
    "- The base container image/curated environment used for dynamic installation is `mcr.microsoft.com/azureml/mlflow-ubuntu18.04-py37-cpu-inference` or `AzureML-mlflow-ubuntu18.04-py37-cpu-inference`.\n",
    "\n",
    "The following table shows the target support for MLflow models in Azure ML:\n",
    "\n",
    "\n",
    "| Feature | ACI | AKS | MIR |\n",
    "| :- | :-: | :-: | :-: |\n",
    "| Deploying models logged with MLflow to real time inference | ☑️* | ☑️* | ☑️* |\n",
    "| Deploying models logged with MLflow to batch inference | ☐** | ☐** | ☑️ |\n",
    "| Deploying models with ColSpec signatures | ☑️**** | ☑️**** | ☑️**** |\n",
    "| Deploying models with TensorSpec signatures | ☑️ | ☑️ | ☑️ |\n",
    "| Run models logged with MLflow in you local compute with Azure ML CLI v2 | ☑️ | ☑️ | ☐*** |\n",
    "| Debug online endpoints locally in Visual Studio Code (preview) | ☐ | ☐ | ☐ |\n",
    "\n",
    "**Notes:**\n",
    "- (*) Spark flavor is not supported at the moment.\n",
    "- (**) We suggest you to use Azure Machine Learning Pipelines with Parallel Run Step.\n",
    "- (***) For deploying MLflow models locally, use the command `mlflow models serve -m <MODEL_NAME>`. Configure the environment variable `MLFLOW_TRACKING_URI` with the URL of your tracking server.\n",
    "- (****) Data type `mlflow.types.DataType.Binary` is not supported as column type. For models that works with images, we suggest you to use Base64 encoding schemes with a `mlflow.types.DataType.String` column type, which is commonly used when there is a need to encode binary data that needs be stored and transferred over media. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20f351",
   "metadata": {},
   "source": [
    "There are two workflows for deploying models to Azure ML:\n",
    "\n",
    "- Using the MLflow plugin [azureml-mlflow](https://pypi.org/project/azureml-mlflow/).\n",
    "- Using Azure ML CLI/SDK v2.\n",
    "\n",
    "If you are familiar with MLflow are you wish to continue using the same set of methods, keep using the approach. If, on the other hand, you are more familiar with the Azure ML CLI, you want to automate deployments using CI/CD pipelines, or you want to keep deployments configuration in a git repository, we recommend you to use the Azure ML CLI v2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89167b93",
   "metadata": {},
   "source": [
    "## Deploying models using Azure ML plugin for MLflow\n",
    "\n",
    "The MLflow plugin azureml-mlflow can deploy models to Azure ML, either to Azure Kubernetes Service (AKS), Azure Container Instances (ACI) and Managed Inference Service (MIR) for real-time serving.\n",
    "\n",
    "> Note that deploying to Managed Inference Service - Batch endpoints is not supported in the MLflow plugin at the moment.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Install the `azureml-mlflow` package.\n",
    "- Configure the MLflow tracking URI or MLflow's registry URI to point to the workspace you are working on. See [MLflow Tracking URI to connect with Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow) for more details.\n",
    "\n",
    "### Deploying models to ACI/AKS\n",
    "\n",
    "Deployments can be generated using both the Python API for MLflow or MLflow CLI. In both cases, a JSON configuration file can be indicated with the details of the deployment you want to achieve. If not indicated, then a default deployment is done using Azure Container Instances (ACI) and a minimal configuration. The full specification of this configuration for ACI and AKS file can be checked at [Deployment configuration schema](https://docs.microsoft.com/en-us/azure/machine-learning/reference-azure-machine-learning-cli#deployment-configuration-schema).\n",
    "\n",
    "#### Configuration example for ACI deployment\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"computeType\": \"aci\",\n",
    "  \"containerResourceRequirements\":\n",
    "  {\n",
    "    \"cpu\": 1,\n",
    "    \"memoryInGB\": 1\n",
    "  },\n",
    "  \"location\": \"eastus2\",\n",
    "}\n",
    "```\n",
    "\n",
    "Remarks:\n",
    "- If `containerResourceRequirements` is not indicated, a deployment with minimal compute configuration is applied (cpu: 0.1 and memory: 0.5).\n",
    "- If `location` is not indicated, it defaults to the location of the workspace.\n",
    "\n",
    "#### Configuration example for an AKS deployment\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"computeType\": \"aks\",\n",
    "  \"computeTargetName\": \"aks-mlflow\"\n",
    "}\n",
    "```\n",
    "\n",
    "Remarks:\n",
    "- In above exmaple, `aks-mlflow` is the name of an Azure Kubernetes Cluster registered/created in Azure Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2383f",
   "metadata": {},
   "source": [
    "The following sample creates a deployment using an ACI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb03e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# Create the deployment configuration.\n",
    "# If no deployment configuration is provided, then the deployment happens on ACI.\n",
    "deploy_config = {\"computeType\": \"aci\"}\n",
    "\n",
    "# Write the deployment configuration into a file.\n",
    "deployment_config_path = \"deployment_config.json\"\n",
    "with open(deployment_config_path, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(deploy_config))\n",
    "\n",
    "# Set the tracking uri in the deployment client.\n",
    "client = get_deploy_client(\"<azureml-mlflow-tracking-url>\")\n",
    "\n",
    "# MLflow requires the deployment configuration to be passed as a dictionary.\n",
    "config = {\"deploy-config-file\": deployment_config_path}\n",
    "model_name = \"mymodel\"\n",
    "model_version = 1\n",
    "\n",
    "# define the model path and the name is the service name\n",
    "# if model is not registered, it gets registered automatically and a name is autogenerated using the \"name\" parameter below\n",
    "client.create_deployment(\n",
    "    model_uri=f\"models:/{model_name}/{model_version}\",\n",
    "    config=config,\n",
    "    name=\"mymodel-aci-deployment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57386544",
   "metadata": {},
   "source": [
    "### Deploying models to Managed Inference\n",
    "\n",
    "Deployments can be generated using both the Python API for MLflow or MLflow CLI. In both cases, a JSON configuration file needs to be indicated with the details of the deployment you want to achieve. The full specification of this configuration can be found at [Managed online deployment schema (v2)](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-deployment-managed-online).\n",
    "\n",
    "#### Configuration example for an Managed Inference Service deployment (real time)\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instance_type\": \"Standard_DS2_v2\",\n",
    "    \"instance_count\": 1,\n",
    "}\n",
    "```\n",
    "\n",
    "Remarks:\n",
    "- We recommend `instance_count` to be at least 3 to ensure High Availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22edac",
   "metadata": {},
   "source": [
    "The following sample deploys a model to a real time Managed Inference Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# Create the deployment configuration.\n",
    "deploy_config = {\n",
    "    \"instance_type\": \"Standard_DS2_v2\",\n",
    "    \"instance_count\": 1,\n",
    "}\n",
    "\n",
    "# Write the deployment configuration into a file.\n",
    "deployment_config_path = \"deployment_config.json\"\n",
    "with open(deployment_config_path, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(deploy_config))\n",
    "\n",
    "# Set the tracking uri in the deployment client.\n",
    "client = get_deploy_client(\"<azureml-mlflow-tracking-url>\")\n",
    "\n",
    "# MLflow requires the deployment configuration to be passed as a dictionary.\n",
    "config = {\"deploy-config-file\": deployment_config_path}\n",
    "model_name = \"mymodel\"\n",
    "model_version = 1\n",
    "\n",
    "# define the model path and the name is the service name\n",
    "# if model is not registered, it gets registered automatically and a name is autogenerated using the \"name\" parameter below\n",
    "client.create_deployment(\n",
    "    model_uri=f\"models:/{model_name}/{model_version}\",\n",
    "    config=config,\n",
    "    name=\"mymodel-mir-deployment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa4635",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "- Notice how `endpoint` has been specified for Managed Inference endpoints, which support multiple deployment being deployed to the same endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb7e5e",
   "metadata": {},
   "source": [
    "## Deploying models using Azure ML CLI v2 to Managed Inference\n",
    "\n",
    "You can use Azure ML CLI v2 to deploy models trained and logged with MLflow to Managed Inference. When you deploy your MLflow model using the Azure ML CLI v2, it's a no-code-deployment so you don't have to provide a scoring script or an environment, but you can if needed.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before following the steps in this article, make sure you have the following prerequisites:\n",
    "\n",
    "- The [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/) and the `ml` extension to the Azure CLI. For more information, see [Install, set up, and use the CLI (v2) (preview)](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli).\n",
    "- A MLflow model. You can deploy models inside a run, models registered in Azure Machine Learning Registry, or a MLflow model's folder stored in the local file system.\n",
    "\n",
    "### Deploying models to Managed Inference\n",
    "\n",
    "To deploy models using the Azure ML CLI v2:\n",
    "\n",
    "1. Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint:\n",
    "\n",
    "```yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\n",
    "name: mymodel-mir-deployment\n",
    "auth_mode: key\n",
    "```\n",
    "\n",
    "2. To create a new endpoint using the YAML configuration, use the following command:\n",
    "\n",
    "```bash\n",
    "ENDPOINT_NAME=\"mymodel-mir-deployment\"\n",
    "az ml online-endpoint create -n $ENDPOINT_NAME -f mlflow-endpoint.yaml\n",
    "```\n",
    "\n",
    "3. Create a YAML configuration file for the deployment. \n",
    "\n",
    "```yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: default\n",
    "endpoint_name: mymodel-mir-endpoint\n",
    "model: azureml:model_name:1\n",
    "instance_type: Standard_DS2_v2\n",
    "instance_count: 1\n",
    "```\n",
    "\n",
    "4. To create the deployment using the YAML configuration, use the following command:\n",
    "\n",
    "```bash\n",
    "az ml online-deployment create --endpoint $ENDPOINT_NAME -f mlflow-deployment.yaml --all-traffic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f9582",
   "metadata": {},
   "source": [
    "## Considerations when deploying to real time inference\n",
    "\n",
    "When deploying to any of the real time services, take the following into consideration:\n",
    "\n",
    "\n",
    "### Input's format\n",
    "\n",
    "The following input's types are supported in Azure ML. Take a look at *Notes* in the bottom of the table for additional considerations.\n",
    "\n",
    "| Input type | Support in MLflow models (serve) | Support in Azure ML|\n",
    "| :- | :-: | :-: |\n",
    "| JSON-serialized pandas DataFrames in the split orientation | ☑️ | ☑️ |\n",
    "| JSON-serialized pandas DataFrames in the records orientation | ☑️ | ☐* |\n",
    "| CSV-serialized pandas DataFrames | ☑️ | ☐** |\n",
    "| Tensor input format as JSON-serialized lists (tensors) and dictionary of lists (named tensors) | ☐ | ☑️ |\n",
    "| Tensor input formatted as in TF Serving’s API | ☑️ | ☐ |\n",
    "\n",
    "Notes:\n",
    "\n",
    "- (*) We suggest you to use split orientation instead. Records orientation doesn't guarante column ordering preservation.\n",
    "- (**) We suggest you to explore batch inference for processing files.\n",
    "\n",
    "Your inputs should be submitted inside the a JSON payload containing a dictionary with key `input_data`. The following shows a valid example for the heart classifier model we were working on in JSON-serialized pandas DataFrames in the split orientation:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"input_data\": {\n",
    "        \"columns\": [\n",
    "            \"age\", \"sex\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"\n",
    "        ],\n",
    "        \"index\": [1],\n",
    "        \"data\": [\n",
    "            [1, 1, 145, 233, 1, 2, 150, 0, 2.3, 3, 0, 2]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Using your own scoring script or environment with MLflow models\n",
    "\n",
    "Although Azure ML supports deployment without indicating an scoring script or an environment, you can still indicate such configurations when deploying models using the Azure ML CLI v2 in the endpoint configuration:\n",
    "\n",
    "```yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: main\n",
    "endpoint_name: heart-classifier-endpoint\n",
    "model: azureml:heart-classifier:1\n",
    "code_configuration:\n",
    "  code:\n",
    "    local_path: score.py\n",
    "instance_type: Standard_DS3_v2\n",
    "instance_count: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96ccb75",
   "metadata": {},
   "source": [
    "The corresponding `score.py` file would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed810aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "\"\"\"\n",
    "Scoring routine\n",
    "\"\"\"\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "\n",
    "input_sample = pd.DataFrame(data=[{\n",
    "    \"age\":63,\n",
    "    \"sex\":1,\n",
    "    \"cp\":1,\n",
    "    \"trestbps\":145,\n",
    "    \"chol\":233,\n",
    "    \"fbs\":1,\n",
    "    \"restecg\":2,\n",
    "    \"thalach\":150,\n",
    "    \"exang\":0,\n",
    "    \"oldpeak\":2.3,\n",
    "    \"slope\":3,\n",
    "    \"ca\":0,\n",
    "    \"thal\":2\n",
    "},])\n",
    "\n",
    "output_sample = np.ndarray([1,])\n",
    "\n",
    "MODEL = None\n",
    "\n",
    "def init():\n",
    "    model_path = os.getenv(\"AZUREML_MODEL_DIR\")\n",
    "    logging.info(f\"[INFO] Loading model from package {model_path}\")\n",
    "\n",
    "    global MODEL\n",
    "    # You can use the specific flavor (`xgboost`) or the generic one `pyfunc`\n",
    "    MODEL = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "\n",
    "@input_schema('data', PandasParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    logging.info(\"Request received\")\n",
    "\n",
    "    try:\n",
    "        results = MODEL.predict(data)\n",
    "        if isinstance(results, pd.DataFrame):\n",
    "            results = results.values\n",
    "        return json.dumps({\"result\": results.tolist()})\n",
    "\n",
    "    except RuntimeError as E:\n",
    "        logging.error(f'[ERR] Exception happened: {str(E)}')\n",
    "        return f'Input {str(data)}. Exception was: {str(E)}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c897a5f",
   "metadata": {},
   "source": [
    "## Considerations when deploying to batch inference\n",
    "\n",
    "Azure ML supports no-code deployment for batch inference in Managed Inference service. This represents a convenient way to deploy models that require processing of big amounts of data in a batch-fashion.\n",
    "\n",
    "### How work is distributed on workers\n",
    "\n",
    "Work is distributed at the file level, for both structured and unstructured data. As a consequence, only [file datasets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#filedataset) or [URI folders](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-data) are supported for this feature. Each worker processes batches of `Mini batch size` files at a time. Further parallelism can be achieved if `Max concurrency per instance` is increased. \n",
    "\n",
    "> Nested folder structures are not explored during inference. If you are partitioning your data using folders, make sure to flatten the structure beforehand.\n",
    "\n",
    "\n",
    "### File's types support\n",
    "\n",
    "The following data types are supported for batch inference.\n",
    "\n",
    "| File extension | Type returned as model's input | Signature requirement |\n",
    "| :- | :- | :- |\n",
    "| `.csv` | `pd.DataFrame` | `ColSpec`. If not provided, columns typing is not enforced. |\n",
    "| `.png`, `.jpg`, `.jpeg`, `.tiff`, `.bmp`, `.gif` | `np.ndarray` | `TensorSpec`. Input is reshaped to match tensors shape if available. If no signature is available, tensors of type `np.uint8` are inferred. |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b8a1be1e2403238145698c0f44c56932b9f9ce2a9391696719cccf625cf1ff9"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
