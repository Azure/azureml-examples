{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating models with an scoring script to MLflow\n",
    "\n",
    "Azure ML supports no-code deployment for any model registered in the registry using MLFlow. That means that scoring scripts are not necessary for running a given model. The question then would be: where can I place all the custom logic I used to have in my scoring script?\n",
    "\n",
    "The MLFlow format dictates not just how the model is stored, but also how it is run. Hence, any specific instructions about how to run a given model needs to be provided at the time you log the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "\n",
    "Let's consider the [Heart Disease UCI](https://archive.ics.uci.edu/ml/datasets/heart+disease) problem where the \"goal\" refers to the predict a heart disease in the patient. A simple code to solve the problem would be like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_url = \"https://azuremlexampledata.blob.core.windows.net/data/heart-disease-uci/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "df[\"thal\"] = df[\"thal\"].astype(\"category\").cat.codes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"target\", axis=1), df[\"target\"], test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlflow.set_experiment(\"heart-disease-classifier\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Were you using Azure ML SDK v1 before? If so, this notebook stills apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's image that we deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "\n",
    "# Sample input for the service\n",
    "input_sample = pd.DataFrame(data=[{\n",
    "    \"age\":63,\n",
    "    \"sex\":1,\n",
    "    \"cp\":1,\n",
    "    \"trestbps\":145,\n",
    "    \"chol\":233,\n",
    "    \"fbs\":1,\n",
    "    \"restecg\":2,\n",
    "    \"thalach\":150,\n",
    "    \"exang\":0,\n",
    "    \"oldpeak\":2.3,\n",
    "    \"slope\":3,\n",
    "    \"ca\":0,\n",
    "    \"thal\":2\n",
    "}])\n",
    "\n",
    "# Sample output for the service\n",
    "output_sample = pd.DataFrame(data=[{\n",
    "    \"class\": 0,\n",
    "    \"confidence\": 0.25\n",
    "}])\n",
    "\n",
    "MODEL = None\n",
    "\n",
    "def init():\n",
    "    global MODEL\n",
    "\n",
    "    model_path = os.getenv(\"AZUREML_MODEL_DIR\")\n",
    "    MODEL = XGBClassifier(use_label_encoder=False)\n",
    "    MODEL.load_model(model_path)\n",
    "\n",
    "@input_schema('data', PandasParameterType(input_sample))\n",
    "@output_schema(PandasParameterType(output_sample))\n",
    "def run(data: pd.DataFrame):\n",
    "    try:\n",
    "        predictions = MODEL.predict_proba(data)\n",
    "        classes = predictions.argmax(axis=1)\n",
    "        confidence = predictions.max(axis=1)\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"class\": classes.tolist(),\n",
    "            \"confidence\": confidence.tolist()\n",
    "        })\n",
    "\n",
    "    except RuntimeError as E:\n",
    "        logging.error(f'[ERR] Exception happened: {str(E)}')\n",
    "        return f'Input {str(data)}. Exception was: {str(E)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the scoring script, this script returns both the class and the probability predicted for that class. If we directly try to deploy this model using `mlflow model serve` or Azure ML No-code deployment, then the model would return only the predicted class. This is the default behavior of a model logged with XGBoost. However, we can change that implementation in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Indicate to MLflow how we want to run the model\n",
    "\n",
    "We need to indicate how MLflow has to run the model. According to MLflow, this needs to be done at the moment of logging the model, as in MLflow a model is not just the artifact but also the configuration about how to run it. In this particular case we are going to create a wrapper for our model object, which will be an instance of `PythonModel`.\n",
    "\n",
    "```python\n",
    "from mlflow.pyfunc import PythonModel, PythonModelContext\n",
    "\n",
    "class ModelWrapper(PythonModel):\n",
    "    def load_context(self, context: PythonModelContext):\n",
    "        pass\n",
    "        \n",
    "    def predict(self, context: PythonModelContext, data):\n",
    "        pass\n",
    "```\n",
    "\n",
    "This class has to be constructed in the following way:\n",
    "- Put all the logic from the `int()` method inside the `load_context` method.\n",
    "   - Replace the global variable `MODEL` for a local variable inside the instance, `self.model`.\n",
    "   - Replace `os.getenv(\"AZUREML_MODEL_DIR\")` for `context.artifacts[\"model\"]`\n",
    "- Put all the logic from the `run` method inside the `predict` method.\n",
    "   - Replace the global variable `MODEL` for `self.model`\n",
    "   - Input will be provided as a parameters, and it will always be either `pd.DataFrame` or `numpy.ndarray` (or a dictionary of `numpy.ndarray`, depending on the signature you are indicating. Columnar signatures always translate to `pd.DataFrame` while Tensor inputs always translates to `np.ndarray`. If named tensors are used, then it will translate to dictionary of `ndarray`\n",
    "   - Returns typing should be equivalent to inputs in terms of the mappings.\n",
    "- **Any importing of libraries should be done locally inside the method that is using it**. (besides the imports related to `mlflow`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting class would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.pyfunc import PythonModel, PythonModelContext\n",
    "\n",
    "\n",
    "class ModelWrapper(PythonModel):\n",
    "    def load_context(self, context: PythonModelContext):\n",
    "        from xgboost import XGBClassifier\n",
    "\n",
    "        model_path = context.artifacts[\"model\"]\n",
    "        self.model = XGBClassifier(use_label_encoder=False)\n",
    "        self.model.load_model(model_path)\n",
    "\n",
    "    def predict(self, context: PythonModelContext, data):\n",
    "        import pandas as pd\n",
    "\n",
    "        predictions = self.model.predict_proba(data)\n",
    "        classes = predictions.argmax(axis=1)\n",
    "        confidence = predictions.max(axis=1)\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            data={\"class\": classes.tolist(), \"confidence\": confidence.tolist()}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Use you sample inputs and outputs to infer the signature of the model:\n",
    "\n",
    "MLflow uses the signature of a model to infer the types of the expected data. This will get translate into Azure ML into the service contract which may be useful for users that are not familiar with the model itself and want to know what the data they should send looks like. We can use the sample data to infer the signature types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = pd.DataFrame(\n",
    "    data=[\n",
    "        {\n",
    "            \"age\": 63,\n",
    "            \"sex\": 1,\n",
    "            \"cp\": 1,\n",
    "            \"trestbps\": 145,\n",
    "            \"chol\": 233,\n",
    "            \"fbs\": 1,\n",
    "            \"restecg\": 2,\n",
    "            \"thalach\": 150,\n",
    "            \"exang\": 0,\n",
    "            \"oldpeak\": 2.3,\n",
    "            \"slope\": 3,\n",
    "            \"ca\": 0,\n",
    "            \"thal\": 2,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_sample = pd.DataFrame(data=[{\"class\": 0, \"confidence\": 0.25}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "signature = infer_signature(input_sample, output_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Log the model inside the training script:\n",
    "\n",
    "As usual we log the model inside the training routine, however, this time we will do it in a different way. If you were relying on `autolog()` then we need to turn it off because otherwise it will log the default behavior of the model's output (the one we want to change). You don't need to disable it completely, just tell autolog to do not log models. You can do that like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "mlflow.xgboost.autolog(log_models=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's log the model ourselves. First, let's save it to disk:\n",
    "\n",
    "```python\n",
    "model_path = 'xgb.model'\n",
    "model.save_model(model_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's log the model. The complete routine would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.xgboost.autolog(log_models=False)\n",
    "\n",
    "    model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    model_path = \"xgb.model\"\n",
    "    model.save_model(model_path)\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"classifier\",\n",
    "        python_model=ModelWrapper(),\n",
    "        artifacts={\"model\": model_path},\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the model\n",
    "\n",
    "You can now deploy this model using the regular approach. First, let's register the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = mlflow.search_runs(\n",
    "    experiment_names=\"heart-disease-classifier\", output_format=\"list\"\n",
    ")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run.info.run_id}/classifier\", name=\"heart-disease-classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the MLflow tracking URI. If you are running in a compute instance, you can get it from the environment variable `MLFLOW_TRACKING_URI`. If you are running locally on your laptop you can get it from the Azure ML CLI v2 or from the Azure Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_uri = os.environ[\"MLFLOW_TRACKING_URI\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the deployment client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_deploy_client(target_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "deploy_config = {\n",
    "    \"instance_type\": \"Standard_DS2_v2\",\n",
    "    \"instance_count\": 1,\n",
    "}\n",
    "\n",
    "deployment_config_path = \"deployment_config.json\"\n",
    "with open(deployment_config_path, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(deploy_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice = client.create_deployment(\n",
    "    model_uri=\"models:/heart-disease-classifier/1\",\n",
    "    name=\"heart-disease-classifier-svc\",\n",
    "    config={\"deploy-config-file\": deployment_config_path},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
