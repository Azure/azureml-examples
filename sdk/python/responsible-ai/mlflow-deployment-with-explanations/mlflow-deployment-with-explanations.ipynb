{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow Deployment with Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to implement scoring-time explanations for [MLflow](https://www.mlflow.org/) model. This tutorial produces (1) a new model whose `predict()` method returns both predictions and explanations and (2) a custom deployment of that model to an [online endpoint](https://docs.microsoft.com/azure/machine-learning/concept-endpoints) for that model.\n",
    "\n",
    "The \"explanations\" for this model come in the form of local feature importance values. Local feature importance measures the contribution of features for a specific prediction. This tutorial leverages Microsoft's [Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox) to generate these values, which uses a Mimic explainer, also known as a global surrogate model. You can learn more in the Interpret ML Book's [chapter on global surrogates](https://christophm.github.io/interpretable-ml-book/global.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example of deployment with explanations](assets/DeploymentExample.png \"Example of deployment with explanations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with computer cluster - [Learn about workspaces](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace)\n",
    "- The original model (the one you want predictions for) in your Azure ML workspace - [Learn about models](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-train-model?view=azureml-api-2)\n",
    "- The baseline data (the data that will be used to initialize the explainer, this can be the same as the training data for the original model) as a data asset in your Azure ML workspace in MLTable format - [Learn more about data](https://learn.microsoft.com/en-us/azure/machine-learning/concept-data?view=azureml-api-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook relies on packages in the `requirements.txt` file. Install them or run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [workspace](https://docs.microsoft.com/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure workspace details and connect **(user input required)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial.\n",
    "\n",
    "Below, enter the information for the Azure ML workspace where the original model and baseline data assets exist. The model with explanations will be created and deployed in this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the details of your AML workspace\n",
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")\n",
    "print(f\"ML Client: {ml_client}\")\n",
    "\n",
    "\n",
    "# If you are running on a Compute Instance or Compute Cluster in Azure Machine Learning \n",
    "# skip the following lines as MLflow is already configured and ready to be used. \n",
    "azureml_tracking_uri = ml_client.workspaces.get(\n",
    "    ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(azureml_tracking_uri)\n",
    "print(f\"Tracking URI: {azureml_tracking_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Wrapper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses a wrapper model to provide explanations along with predictions of the original model. The code for this wrapper can be reviewed in `explanation_wrapper.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model information **(user input required)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the information about the original model, baseline data, and new model with explanations. The information about the original model and baseline data must match the assets in your Azure ML workspace. the `wrapper_model_name` will be the name of your model with explanations, you may choose any name (e.g. \"iris_model_with_explanations\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter original model information (must match Azure ML asset)\n",
    "model_name = \"<MODEL_NAME>\"  # Name of the model for which explanations are desired\n",
    "model_version = \"<MODEL_VERSION>\"  # Version of that model\n",
    "\n",
    "# Enter original model information about features and task\n",
    "target_column = \"<TARGET_COLUMN>\"  # The target (aka predicted) column of the data\n",
    "task_type = \"<'classification' | 'regression'>\"  # Task type of the model, either 'classification' or 'regression'\n",
    "categorical_features = []  # Optional, will be calculated from baseline data if None\n",
    "\n",
    "# Enter baseline data information (must match Azure ML asset)\n",
    "baseline_data_name = \"<BASELINE_DATA_NAME>\"  # Name of the baseline data\n",
    "baseline_data_version = \"<BASELINE_DATA_VERSION>\"  # Version of that data\n",
    "\n",
    "# Enter wrapper model name\n",
    "wrapper_model_name = \"<WRAPPER_MODEL_NAME>\"  # Name for the model with explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"models:/{model_name}/{model_version}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load baseline data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used to create the explanation wrapper must match the data used to train the original model. Be sure to drop any columns that were dropped during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mltable\n",
    "\n",
    "data_asset = ml_client.data.get(name=baseline_data_name, version=baseline_data_version)\n",
    "baseline_df = mltable.load(f\"azureml:/{data_asset.id}\").to_pandas_dataframe()\n",
    "# Drop any columns that were dropped when training the model. Uncomment and fill in first parameter\n",
    "# baseline_df = baseline_df.drop([], axis=\"columns\")\n",
    "\n",
    "print(\"Baseline Data (first 5 rows):\")\n",
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create instance of ExplanationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explanation_wrapper import ExplanationWrapper\n",
    "\n",
    "explanation_wrapper = ExplanationWrapper(\n",
    "    model_uri=model_uri,\n",
    "    baseline_df=baseline_df,\n",
    "    target_column=target_column,\n",
    "    task_type=task_type,\n",
    "    categorical_features=categorical_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Register Wrapper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Download original model **(user input required)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local folder\n",
    "import os\n",
    "\n",
    "local_path = \"./artifact/original_model\"\n",
    "if not os.path.exists(local_path):\n",
    "    os.makedirs(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "# Initialize MLFlow client\n",
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download run's artifacts/outputs\n",
    "mlflow.artifacts.download_artifacts(artifact_uri=f\"models:/{model_name}/{model_version}\", dst_path=local_path)\n",
    "print(\"Artifacts downloaded in: {}\".format(local_path))\n",
    "print(\"Artifacts: {}\".format(os.listdir(local_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_model_dir = os.path.join(local_path, model_name)\n",
    "\n",
    "# Show the contents of the MLFlow model folder\n",
    "os.listdir(mlflow_model_dir)\n",
    "\n",
    "# You should see a list of files such as the following:\n",
    "# ['artifacts', 'conda.yaml', 'MLmodel', 'python_env.yaml', 'python_model.pkl', 'requirements.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the variable below to be the the file path of the folder that contains the MLFlow model (the `MLmodel` file). To determine this, either (1) use the path provided in the above cell's output or (2) look inside the `artifacts/original_model` folder and then the folder named after the original model. For example, under the following file structure:\n",
    "\n",
    "![Example downloaded model file structure](assets/ModelFilePathExample.png \"Example downloaded model file structure\")\n",
    "\n",
    "You would update `model_file_path` to be './original_model/{model_name}/**model**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = (\n",
    "    f\"./artifacts/original_model/{model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get original model signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is one, load the model signature from Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import Model\n",
    "\n",
    "mlflow_model = Model.load(model_uri)\n",
    "model_signature = mlflow_model.signature\n",
    "print(f\"Model Signature: {model_signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Register wrapper model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and then register the model to your workspace. If you are re-running this step, you may need to delete the folder containing the last version of the saved wrapper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = {\"model\": model_file_path, \"RAI insights\": \"./artifacts/RAI_Insights\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pyfunc.save_model(\n",
    "    path=f\"artifacts/{wrapper_model_name}\",\n",
    "    code_path=[\"./explanation_wrapper.py\"],\n",
    "    conda_env=\"./env.yml\",\n",
    "    python_model=explanation_wrapper,\n",
    "    artifacts=artifacts,\n",
    "    signature=model_signature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.register_model(f\"file://artifacts/{wrapper_model_name}\", wrapper_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Wrapper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Deployment Information **(user input required)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the name of an **existing** endpoint and the deployment name of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"<AML_ENDPOINT_NAME>\"\n",
    "deployment_name = \"<DEPLOYMENT_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Get wrapper model ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the model name and version are correct for the newly registered wrapper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_list = list(ml_client.models.list(wrapper_model_name))\n",
    "wrapper_model_version = version_list[0].version\n",
    "wrapper_model = ml_client.models.get(wrapper_model_name, wrapper_model_version)\n",
    "print(f\"Using model name: {wrapper_model_name}, version: {wrapper_model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Create and deploy endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    OnlineRequestSettings,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    ")\n",
    "\n",
    "# Define the deployment\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=wrapper_model.id,\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(request_timeout_ms=90000),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        period=100,\n",
    "        initial_delay=500,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        period=100,\n",
    "        initial_delay=500,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Trigger the deployment creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(deployment).wait()\n",
    "    print(\"\\n---Deployment created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Assign all traffic to the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the traffic configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "traffic_config = {\"traffic\": {deployment_name: 100}}\n",
    "traffic_config_path = \"artifacts/traffic_config.json\"\n",
    "with open(traffic_config_path, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(traffic_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "deployment_client = get_deploy_client(mlflow.get_tracking_uri())\n",
    "deployment_client.update_endpoint(\n",
    "    endpoint=endpoint_name,\n",
    "    config={\"endpoint-config-file\": traffic_config_path},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the deployment can be done through the following steps or the UI in Azure ML portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prepare test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Import from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the number of samples you plan to use to test the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = \"<NUMBER_OF_SAMPLES>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a csv file with data for your model into this folder and read it in. Again, make sure the column data matches the training data for both the original model and wrapped model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    pd.read_csv(\"<CSV_NAME>\")\n",
    "    .sample(n=1)\n",
    "    .drop(columns=[target_column])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data directly into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    '<COLUMN_1_NAME>': <COLUMN_1_VALUE>,\n",
    "    '<COLUMN_2_NAME>': <COLUMN_2_VALUE>,\n",
    "    '<COLUMN_3_NAME>': <COLUMN_3_VALUE>,\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n == 1:\n",
    "    sample = pd.DataFrame(data=data, index=[0])\n",
    "else:\n",
    "    sample = pd.DataFrame(data=data)\n",
    "print(f\"Sample Data: {sample.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get payload of `predict()` from the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = deployment_client.predict(endpoint=endpoint_name, df=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract predictions and explanations from payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if isinstance(payload, pd.DataFrame):\n",
    "    print(\"Return type is DataFrame\")\n",
    "    predictions = payload[\"predictions\"].values\n",
    "    explanations = payload[\"explanations\"].values\n",
    "elif isinstance(payload, np.ndarray):\n",
    "    print(\"Return type is ndarray\")\n",
    "    predictions = payload.item()[\"predictions\"]\n",
    "    explanations = payload.item()[\"explanations\"]\n",
    "else:\n",
    "    print(\n",
    "        \"Return type not supported - either skip the rest of this notebook or write your own code to extract the predictions and explanations lists\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(baseline_df.drop(columns=[target_column]).columns)\n",
    "if task_type == \"classification\":\n",
    "    classes = np.array(baseline_df[target_column].unique())\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"For data point {i}:\")\n",
    "    print(f\"{sample.loc[i]}\\n\")\n",
    "\n",
    "    print(f\"Prediction: {predictions[i]}\\n\")\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        for j in range(len(classes)):\n",
    "            importances = np.array(explanations[i][j][0])\n",
    "            explanations_df = pd.DataFrame(\n",
    "                data={\"feature\": features, \"local importance\": importances}\n",
    "            )\n",
    "            print(f\"Feature importances for class: {classes[j]}\")\n",
    "            if predictions[i] == classes[j]:\n",
    "                print(\"This is the predicted class for this row of data\")\n",
    "            else:\n",
    "                print(\"This is NOT the predicted class for this row of data\")\n",
    "            print(f\"{explanations_df}\\n\")\n",
    "    else:\n",
    "        importances = np.array(explanations[i][0])\n",
    "        explanations_df = pd.DataFrame(\n",
    "            data={\"feature\": features, \"local importance\": importances}\n",
    "        )\n",
    "        print(f\"Feature importances: {explanations_df}\\n\")\n",
    "\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore data (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature importances for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_type == \"regression\":\n",
    "    for i in range(len(explanations)):\n",
    "        explanations[i] = explanations[i][0]\n",
    "\n",
    "    features = np.array(baseline_df.drop(columns=[target_column]).columns)\n",
    "    for i in range(len(predictions)):\n",
    "        print(f\"For data point {i}:\")\n",
    "        print(f\"{sample.loc[i]}\\n\")\n",
    "\n",
    "        importances = np.array(explanations[i])\n",
    "        explanations_df = pd.DataFrame(\n",
    "            data={\"feature\": features, \"local importance\": importances}\n",
    "        )\n",
    "\n",
    "        plot = px.bar(\n",
    "            data_frame=explanations_df,\n",
    "            x=\"feature\",\n",
    "            y=\"local importance\",\n",
    "            title=\"Local Feature Importance\",\n",
    "        )\n",
    "        plot.show()\n",
    "\n",
    "        print(\"\\n\\n\\n\")\n",
    "else:\n",
    "    print(\"Task type is not regression - skip this section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Feature importances for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_type == \"classification\":\n",
    "    features = np.array(baseline_df.drop(columns=[target_column]).columns)\n",
    "    classes = np.array(baseline_df[target_column].unique())\n",
    "    for i in range(len(predictions)):\n",
    "        print(f\"For data point {i}:\")\n",
    "        print(f\"{sample.loc[i]}\\n\")\n",
    "\n",
    "        aggregated_df = pd.DataFrame(columns=[\"class\", \"feature\", \"local importance\"])\n",
    "        for j in range(len(classes)):\n",
    "            importances = explanations[i][j][0]\n",
    "            explanations_df = pd.DataFrame(\n",
    "                data={\"feature\": features, \"local importance\": importances}\n",
    "            )\n",
    "\n",
    "            for k in range(len(features)):\n",
    "                new_row = pd.DataFrame(\n",
    "                    data=[[classes[j], features[k], explanations[i][j][0][k]]],\n",
    "                    columns=[\"class\", \"feature\", \"local importance\"],\n",
    "                )\n",
    "                aggregated_df = pd.concat([aggregated_df, new_row])\n",
    "\n",
    "            title = f\"Local Importance for Class {classes[j]}\"\n",
    "            if predictions[i] == classes[j]:\n",
    "                title += \" (Predicted Class)\"\n",
    "            else:\n",
    "                title += \" (Not the Predicted Class)\"\n",
    "            plot = px.bar(\n",
    "                data_frame=explanations_df,\n",
    "                x=\"feature\",\n",
    "                y=\"local importance\",\n",
    "                title=title,\n",
    "            )\n",
    "            plot.show()\n",
    "\n",
    "        plot = px.bar(\n",
    "            data_frame=aggregated_df,\n",
    "            x=\"feature\",\n",
    "            y=\"local importance\",\n",
    "            color=\"class\",\n",
    "            title=\"Aggregated Local Importance\",\n",
    "        )\n",
    "        plot.show()\n",
    "\n",
    "        print(\"\\n\\n\\n\")\n",
    "else:\n",
    "    print(\"Task type is not classification - skip this section\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-dep-exp-bug-bash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
