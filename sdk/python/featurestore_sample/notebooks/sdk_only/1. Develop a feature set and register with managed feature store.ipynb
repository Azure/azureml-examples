{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Tutorial #1: Develop a feature set and register with managed feature store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Azure ML managed feature store lets you discover, create and operationalize features. Features are the connective tissue in ML lifecycle, starting from prototyping where you experiment with various features to operationalization where models are deployed and feature data is looked up during inference. For information on basics concept of feature store, see [feature store concepts](fs-concepts).\n",
    "\n",
    "In this tutorial series you will experience how features seamlessly integrates all the phases of ML lifecycle:\n",
    "Part 1 (this tutorial): Create\n",
    "\n",
    "This tutorial is the first part of a three part series. In this tutorial you will:\n",
    "- Create a new minimal feature store resource\n",
    "- Develop and test featureset locally with feature transformation capability\n",
    "- Register a feature-store entity with the feature store\n",
    "- Register the featureset that you developed with the feature store\n",
    "- Generate sample training data dataframe using the features you created\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Important\n",
    "\n",
    "This feature is currently in public preview. This preview version is provided without a service-level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prerequisites\n",
    "Before following the steps in this article, make sure you have the following prerequisites:\n",
    "\n",
    "* An Azure Machine Learning workspace. If you don't have one, use the steps in the [Quickstart: Create workspace resources](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2) article to create one.\n",
    "* To perform the steps in this article, your user account must be assigned the owner or contributor role to a resource group where the feature store will be created"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Prepare the notebook environment for development\n",
    "Note: This tutorial uses AzureML spark notebook for development. (placeholder: link to ADB document once ready)\n",
    "\n",
    "1. Clone the examples repository to your local machine: To run the tutorial, first clone the [examples repository (azureml-examples)](https://github.com/azure/azureml-examples)\n",
    "\n",
    "```bash\n",
    "git clone --depth 1 https://github.com/Azure/azureml-examples\n",
    "```\n",
    "\n",
    "Alternatively you can download a zip file from the [examples repository (azureml-examples)](https://github.com/azure/azureml-examples): click on the `code` dropdown and click `Download ZIP`. Then unzip the contents into a folder in your local machine.\n",
    "\n",
    "2. Upload the feature store samples directory to project workspace: Open Azure ML studio UI of your Azure ML workspace -> click on \"Notebooks\" in left nav -> right click on your user name in the directory listing -> click \"upload folder\" -> select the feature store samples folder from the cloned directory path: `azureml-examples/sdk/python/featurestore-sample`\n",
    "\n",
    "3. You can either create a new notebook and paste the instructions in this document step by step and execute OR open the existing notebook titled `1.Develop a feature set and register with managed feature store.ipynb`. You can execute step by step. Keep this document open and refer to it for detailed explanation of the steps. The notebooks are available in the folder: `featurestore_sample/notebooks`. Select either `sdk_only` folder or the `sdk_and_cli` folder. The latter has CLI commands mixed with python sdk useful in ci/cd scenarios.\n",
    "\n",
    "4. In the \"Compute\" dropdown in the top nav, select \"Serverless Spark Compute\". It may take 1-2 minutes for this activity to complete. Wait for a status bar in the top to display `configure session`\n",
    "\n",
    "5. Click on \"configure session\" -> click on \"Python packages\" -> click on \"upload conda file\" -> select the file `azureml-examples/sdk/python/featurestore-sample/project/env/conda.yml` from your local machine; Also increase the session time out (idle time) if you want to reduce serverless spark cluster startup time.\n",
    "\n",
    "__Important:__ Except for this step, you need to run all the other steps every time you have a new spark session/session time out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415277710
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "start-spark-session",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this cell to start the spark session (any code block will start the session ). This can take around 10 mins.\n",
    "print(\"start spark session\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Setup root directory for the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415282100
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "root-dir",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Please update your alias belpw (or any custom directory you uploaded the samples to).\n",
    "# You can find the name from the directory structure in the left nav\n",
    "root_dir = \"./Users/<your_user_alias>/featurestore_sample\"\n",
    "\n",
    "if os.path.isdir(root_dir):\n",
    "    print(\"The folder exists.\")\n",
    "else:\n",
    "    print(\"The folder does not exist. Please create or fix the path\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Note\n",
    "Feature store Vs Project workspace: You will use a featurestore to reuse features across projects. You will use a project workspace(the current workspace) to train and inference models, by leveraging features from feature stores. Many project workspaces can share and reuse a same feature store.\n",
    "\n",
    "## Note\n",
    "In this tutorial you will be using two SDK's:\n",
    "\n",
    "1. Feature store CRUD sdk:  You will use the same SDK, MLClient (package name `azure-ai-ml`), that you use with Azure ML workpace. This will be used for feature store CRUD operations (Create, Update and Delete) for featurestore, featureset and featurestore-entity. This is because feature store is implemented as a type of workspace. \n",
    "2. Feature store core sdk: This sdk (`azureml-featurestore`) is meant to be used for feature set development and consumption (you will learn more about these operations later):\n",
    "- List/Get registered feature set\n",
    "- Generate/resolve feature retrieval spec\n",
    "- Execute featureset definition to generate Spark dataframe\n",
    "- Generate training using a point-in-time join\n",
    "\n",
    "For this tutorial so you do not need to install any of these explicitly, since the instructions already cover them (conda yaml in the above step include these)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Create a minimal feature store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 1a: Set feature store parameters\n",
    "Set name, location and other values for the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415347313
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "fs-params",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# We use the subscription, resource group, region of this active project workspace.\n",
    "# You can optionally replace them to create the resources in a different subsciprtion/resourceGroup, or use existing resources\n",
    "import os\n",
    "\n",
    "featurestore_name = \"my-featurestore\"\n",
    "featurestore_location = \"eastus\"\n",
    "featurestore_subscription_id = os.environ[\"AZUREML_ARM_SUBSCRIPTION\"]\n",
    "featurestore_resource_group_name = os.environ[\"AZUREML_ARM_RESOURCEGROUP\"]\n",
    "version = \"<VERSION>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 1b: Create the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415482376
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "create-fs",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    FeatureStore,\n",
    "    FeatureStoreEntity,\n",
    "    FeatureSet,\n",
    ")\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "ml_client = MLClient(\n",
    "    AzureMLOnBehalfOfCredential(),\n",
    "    subscription_id=featurestore_subscription_id,\n",
    "    resource_group_name=featurestore_resource_group_name,\n",
    ")\n",
    "\n",
    "\n",
    "fs = FeatureStore(name=featurestore_name, location=featurestore_location)\n",
    "# wait for featurestore creation\n",
    "fs_poller = ml_client.feature_stores.begin_create(fs, update_dependent_resources=True)\n",
    "print(fs_poller.result())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 1c: Initialize AzureML feature store core SDK client\n",
    "As explained above, this is used to develop and consume features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415633739
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-fs-core-sdk",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# feature store client\n",
    "from azureml.featurestore import FeatureStoreClient\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "featurestore = FeatureStoreClient(\n",
    "    credential=AzureMLOnBehalfOfCredential(),\n",
    "    subscription_id=featurestore_subscription_id,\n",
    "    resource_group_name=featurestore_resource_group_name,\n",
    "    name=featurestore_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Prototype and develop a transaction rolling aggregation featureset in this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 2a: Explore the transactions source data\n",
    "\n",
    "#### Note\n",
    "The sample data used in this notebook is hosted in a public accessible blob container. It can only be read in Spark via `wasbs` driver. When you create feature sets using your own source data, please host them in adls gen2 account and use `abfss` driver in the data path.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415673106
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "explore-txn-src-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# remove the \".\" in the roor directory path as we need to generate absolute path to read from spark\n",
    "transactions_source_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/datasources/transactions-source/*.parquet\"\n",
    "transactions_src_df = spark.read.parquet(transactions_source_data_path)\n",
    "\n",
    "display(transactions_src_df.head(5))\n",
    "# Note: display(training_df.head(5)) displays the timestamp column in a different format. You can can call transactions_src_df.show() to see correctly formatted value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 2b: Develop a transactions featureset locally\n",
    "\n",
    "Featureset specification is a self-contained definition of feature set that can be developed and tested locally.\n",
    "\n",
    "Lets create the following rolling window aggregate features:\n",
    "- transactions 3-day count\n",
    "- transactions amount 3-day sum\n",
    "- transactions amount 3-day avg\n",
    "- transactions 7-day count\n",
    "- transactions amount 7-day sum\n",
    "- transactions amount 7-day avg\n",
    "\n",
    "__Action__:\n",
    "- Inspect the feature transformation code file: `featurestore/featuresets/transactions/spec/transformation_code/transaction_transform.py`. You will see how is the rolling aggregation defined for the features. This is a spark transformer.\n",
    "\n",
    "To understand the feature set and transformations in more detail, see [feature store concepts](fs-concepts-url-todo) and [transformation concepts](fs-transformation-concepts-todo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415951330
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "develop-txn-fset-locally",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.featurestore import create_feature_set_spec, FeatureSetSpec\n",
    "from azureml.featurestore.contracts import (\n",
    "    DateTimeOffset,\n",
    "    FeatureSource,\n",
    "    TransformationCode,\n",
    "    Column,\n",
    "    ColumnType,\n",
    "    SourceType,\n",
    "    TimestampColumn,\n",
    ")\n",
    "\n",
    "\n",
    "transactions_featureset_code_path = (\n",
    "    root_dir + \"/featurestore/featuresets/transactions/transformation_code\"\n",
    ")\n",
    "\n",
    "transactions_featureset_spec = create_feature_set_spec(\n",
    "    source=FeatureSource(\n",
    "        type=SourceType.parquet,\n",
    "        path=\"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/datasources/transactions-source/*.parquet\",\n",
    "        timestamp_column=TimestampColumn(name=\"timestamp\"),\n",
    "        source_delay=DateTimeOffset(days=0, hours=0, minutes=20),\n",
    "    ),\n",
    "    transformation_code=TransformationCode(\n",
    "        path=transactions_featureset_code_path,\n",
    "        transformer_class=\"transaction_transform.TransactionFeatureTransformer\",\n",
    "    ),\n",
    "    index_columns=[Column(name=\"accountID\", type=ColumnType.string)],\n",
    "    source_lookback=DateTimeOffset(days=7, hours=0, minutes=0),\n",
    "    temporal_join_lookback=DateTimeOffset(days=1, hours=0, minutes=0),\n",
    "    infer_schema=True,\n",
    ")\n",
    "# Generate a spark dataframe from the feature set specification\n",
    "transactions_fset_df = transactions_featureset_spec.to_spark_dataframe()\n",
    "# display few records\n",
    "display(transactions_fset_df.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 2c:  Export as feature set spec\n",
    "Inorder to register the feature set spec with the feature store, it needs to be saved in a specific format. \n",
    "Action: Please inspect the generated `transactions` FeaturesetSpec: Open this file from the file tree to see the spec: `featurestore/featuresets/accounts/spec/FeaturesetSpec.yaml`\n",
    "\n",
    "Spec contains these important elements:\n",
    "\n",
    "1. `source`: reference to a storage. In this case a parquet file in a blob storage.\n",
    "1. `features`: list of features and their datatypes. If you provide transformation code (see Day 2 section), the code has to return a dataframe that maps to the features and datatypes.\n",
    "1. `index_columns`: the join keys required to access values from the feature set\n",
    "\n",
    "Learn more about it in the [top level feature store entities document](fs-concepts-todo) and the [feature set spec yaml reference](reference-yaml-featureset-spec.md).\n",
    "\n",
    "The additional benefit of persisting it is that it can be source controlled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683416345890
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "dump-transactions-fs-spec",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a new folder to dump the feature set spec\n",
    "transactions_featureset_spec_folder = (\n",
    "    root_dir + \"/featurestore/featuresets/transactions/spec\"\n",
    ")\n",
    "\n",
    "# check if the folder exists, create one if not\n",
    "if not os.path.exists(transactions_featureset_spec_folder):\n",
    "    os.makedirs(transactions_featureset_spec_folder)\n",
    "\n",
    "transactions_featureset_spec.dump(transactions_featureset_spec_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Register a feature-store entity\n",
    "Entity helps enforce best practice that same join key definitions are used across featuresets which uses the same logical entities. Examples of entities are account entity, customer entity etc. Entities are typically created once and reused across feature-sets. For information on basics concept of feature store, see [feature store concepts](fs-concepts)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 3a: Initialize the Feature Store CRUD client\n",
    "\n",
    "As explained in the beginning of this tutorial, MLClient is used for CRUD of assets in feature store. The below code looks up the feature store we created in an earlier step. We cannot reuse the same ml_client used above here because the former is scoped at resource group level, which is a prerequisite for creation of feature store. The below one is scoped at feature store level.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683416010696
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-fset-crud-client",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# mlclient on feature store\n",
    "fs_client = MLClient(\n",
    "    AzureMLOnBehalfOfCredential(),\n",
    "    featurestore_subscription_id,\n",
    "    featurestore_resource_group_name,\n",
    "    featurestore_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 3b: Register `account` entity with the feature store\n",
    "Create account entity that has join key `accountID` of `string` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683416051121
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "register-acct-entity",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import DataColumn, DataColumnType\n",
    "\n",
    "account_entity_config = FeatureStoreEntity(\n",
    "    name=\"account\",\n",
    "    version=version,\n",
    "    index_columns=[DataColumn(name=\"accountID\", type=DataColumnType.STRING)],\n",
    "    stage=\"Development\",\n",
    "    description=\"This entity represents user account index key accountID.\",\n",
    "    tags={\"data_typ\": \"nonPII\"},\n",
    ")\n",
    "\n",
    "poller = fs_client.feature_store_entities.begin_create_or_update(account_entity_config)\n",
    "print(poller.result())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Register the transaction featureset with the featurestore\n",
    "You register a feature set asset with the feature store so that you can share and reuse with others. You also get managed capabilities like versioning and materialization (we will learn in this tutorial series).\n",
    "\n",
    "The feature set asset has reference to the feature set spec that you created earlier and additional properties like version and materialization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683416442205
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "register-txn-fset",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import FeatureSetSpecification\n",
    "\n",
    "transaction_fset_config = FeatureSet(\n",
    "    name=\"transactions\",\n",
    "    version=version,\n",
    "    description=\"7-day and 3-day rolling aggregation of transactions featureset\",\n",
    "    entities=[f\"azureml:account:{version}\"],\n",
    "    stage=\"Development\",\n",
    "    specification=FeatureSetSpecification(path=transactions_featureset_spec_folder),\n",
    "    tags={\"data_type\": \"nonPII\"},\n",
    ")\n",
    "\n",
    "poller = fs_client.feature_sets.begin_create_or_update(transaction_fset_config)\n",
    "print(poller.result())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Explore the FeatureStore UI\n",
    "* Goto the [Azure ML global landing page](https://ml.azure.com/home?flight=FeatureStores).\n",
    "* Click on `Feature stores` in the left nav\n",
    "* You will see the list of feature stores that you have access to. Click on the feature store that you created above.\n",
    "\n",
    "You can see the feature set and entity that you created.\n",
    "\n",
    "Note: Creating and updating feature store assets are possible only through SDK and CLI. You can use the UI to search/browse the feature store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Generate a training data dataframe using the registered features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 5a: Load observation data\n",
    "\n",
    "We start by exploring the observation data. Observation data is typically the core data used in training and inference data. This is then joined with feature data to create the full training data. Observation data is the data captured during the time of the event: in this case it has core transaction data including transaction id, account id, transaction amount. In this case, since it is for training, it also has the target variable appended (is_fraud).\n",
    "\n",
    "To learn more core concepts including observation data, refer to the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683417449378
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "load-obs-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "observation_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/observation_data/train/*.parquet\"\n",
    "observation_data_df = spark.read.parquet(observation_data_path)\n",
    "obs_data_timestamp_column = \"timestamp\"\n",
    "\n",
    "display(observation_data_df)\n",
    "# Note: the timestamp column is displayed in a different format. Optionally, you can can call training_df.show() to see correctly formatted value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 5c: Get the registered featureset and list its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683416499081
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "get-txn-fset",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# look up the featureset by providing name and version\n",
    "transactions_featureset = featurestore.feature_sets.get(\"transactions\", version)\n",
    "# list its features\n",
    "transactions_featureset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683416508419
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "print-txn-fset-sample-values",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# print sample values\n",
    "display(transactions_featureset.to_spark_dataframe().head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 5d: Select features and generate training data\n",
    "In this step we will select features that we would like to be part of training data and use the feature store sdk to generate the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683417499373
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "select-features-and-gen-training-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.featurestore import get_offline_features\n",
    "\n",
    "# you can select features in pythonic way\n",
    "features = [\n",
    "    transactions_featureset.get_feature(\"transaction_amount_7d_sum\"),\n",
    "    transactions_featureset.get_feature(\"transaction_amount_7d_avg\"),\n",
    "]\n",
    "\n",
    "# you can also specify features in string form: featurestore:featureset:version:feature\n",
    "more_features = [\n",
    "    f\"transactions:{version}:transaction_3d_count\",\n",
    "    f\"transactions:{version}:transaction_amount_3d_avg\",\n",
    "]\n",
    "\n",
    "more_features = featurestore.resolve_feature_uri(more_features)\n",
    "features.extend(more_features)\n",
    "\n",
    "# generate training dataframe by using feature data and observation data\n",
    "training_df = get_offline_features(\n",
    "    features=features,\n",
    "    observation_data=observation_data_df,\n",
    "    timestamp_column=obs_data_timestamp_column,\n",
    ")\n",
    "\n",
    "# Ignore the message that says feature set is not materialized (materialization is optional). We will enable materialization in the next part of the tutorial.\n",
    "display(training_df)\n",
    "# Note: the timestamp column is displayed in a different format. Optionally, you can can call training_df.show() to see correctly formatted value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You can see how the features are appended to the training data using a point-in-time join.\n",
    "\n",
    "We have reached the end of the tutorial. Now you have your training data using features from feature store. You can either save it to storage for later use, or run model training on it directly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Part 4 of the tutorial has instructions deleting the resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Next steps\n",
    "* Part 2 of tutorial: Enable materialization and backfill feature data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_ignore_dictionary": [
     "dataframe",
     "featureset",
     "operationalization",
     "operationalize"
    ],
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
