{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Tutorial #1: Develop a feature set and register with managed feature store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Important\n",
    "\n",
    "This feature is currently in public preview. This preview version is provided without a service-level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Azure ML managed feature store lets you discover, create and operationalize features. Features are the connective tissue in ML lifecycle, starting from prototyping where you experiment with various features to operationalization where models are deployed and feature data is looked up during inference. For information on basics concept of feature store, see [feature store concepts](fs-concepts).\n",
    "\n",
    "In this tutorial series you will experience how features seamlessly integrates all the phases of ML lifecycle:\n",
    "Part 1 (this tutorial): Create\n",
    "\n",
    "This tutorial is the first part of a three part series. In this tutorial you will:\n",
    "- Create a new minimal feature store resource\n",
    "- Develop and test featureset locally with feature transformation capability\n",
    "- Register a feature-store entity with the feature store\n",
    "- Register the featureset that you developed with the feature store\n",
    "- Generate sample training data dataframe using the features you created\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prerequisites\n",
    "Before following the steps in this article, make sure you have the following prerequisites:\n",
    "\n",
    "* An Azure Machine Learning workspace. If you don't have one, use the steps in the [Quickstart: Create workspace resources](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2) article to create one.\n",
    "* To perform the steps in this article, your user account must be assigned the owner or contributor role to a resource group where the feature store will be created"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (new for sdk/cli track) Note\n",
    "This tutorial series has two tracks:\n",
    "1. SDK only track: Uses only python SDKs. Suitable if pure python based development and deployment is preferred.\n",
    "1. SDK & CLI track: Uses CLI for CRUD operations (create/update/delete) and python SDK for feature set development and testing only. This is useful in ci/cd or GitOps scenarios where CLI/yaml is preferred."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Setup "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### (updated: common) Prepare the notebook environment for development\n",
    "Note: This tutorial uses Azure Machine Learning spark notebook for development.\n",
    "\n",
    "1. Clone the examples repository to your local machine: To run the tutorial, first clone the [examples repository - (azureml-examples)](https://github.com/azure/azureml-examples) with this command:\n",
    "\n",
    "   `git clone --depth 1 https://github.com/Azure/azureml-examples`\n",
    "\n",
    "   You can also download a zip file from the [examples repository (azureml-examples)](https://github.com/azure/azureml-examples). At this page, first select the `code` dropdown, and then select `Download ZIP`. Then, unzip the contents into a folder on your local device.\n",
    "\n",
    "1. Upload the feature store samples directory to project workspace.\n",
    "      * Open Azure Machine Learning studio UI of your Azure Machine Learning workspace\n",
    "      * Select **Notebooks** in left nav\n",
    "      * Select your user name in the directory listing\n",
    "      * Select **upload folder**\n",
    "      * Select the feature store samples folder from the cloned directory path: `azureml-examples/sdk/python/featurestore-sample`\n",
    "\n",
    "1. Running the tutorial:\n",
    "* Option 1: Create a new notebook, and execute the instructions in this document step by step. \n",
    "* Option 2: Open the existing notebook named `1. Develop a feature set and register with managed feature store.ipynb`, and run it step by step. The notebooks are available in `featurestore_sample/notebooks` directory. You can select from `sdk_only` or `sdk_and_cli`. You may keep this document open and refer to it for additional explanation and documentation links.\n",
    "\n",
    "1. Select **Serverless Spark compute** in the top nav \"Compute\" dropdown. This operation might take one to two minutes. Wait for a status bar in the top to display **configure session**.\n",
    "\n",
    "1. Select \"configure session\" from the top nav (this could take one to two minutes to display):\n",
    "\n",
    "      1. Select **configure session** in the top status bar\n",
    "      1. Select **Python packages**\n",
    "      1. Select **Upload conda file**\n",
    "      1. Select file `azureml-examples/sdk/python/featurestore-sample/project/env/conda.yml` located on your local device\n",
    "      1. (Optional) Increase the session time-out (idle time) to reduce the serverless spark cluster startup time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684196710350
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "start-spark-session",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this cell to start the spark session (any code block will start the session ). This can take around 10 mins.\n",
    "print(\"start spark session\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Setup root directory for the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684196736758
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "root-dir",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Please update your alias belpw (or any custom directory you uploaded the samples to).\n",
    "# You can find the name from the directory structure in the left nav\n",
    "root_dir = \"./Users/<your_user_alias>/featurestore_sample\"\n",
    "\n",
    "if os.path.isdir(root_dir):\n",
    "    print(\"The folder exists.\")\n",
    "else:\n",
    "    print(\"The folder does not exist. Please create or fix the path\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### (new for sdk/cki track) Setup CLI\n",
    "\n",
    "1. Install azure ml cli extention\n",
    "1. Authenticate\n",
    "1. Set the default subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684196793636
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "install-ml-ext-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# install azure ml cli extension\n",
    "!az extension add --name ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684196876106
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "auth-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# authenticate\n",
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684196882653
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "set-default-subs-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set default subscription\n",
    "import os\n",
    "\n",
    "subscription_id = os.environ[\"AZUREML_ARM_SUBSCRIPTION\"]\n",
    "\n",
    "!az account set -s $subscription_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Note\n",
    "Feature store Vs Project workspace: You will use a featurestore to reuse features across projects. You will use a project workspace(the current workspace) to train and inference models, by leveraging features from feature stores. Many project workspaces can share and reuse a same feature store.\n",
    "\n",
    "## (updated for sdk/cki track) Note\n",
    "In this tutorial you will be using cli and feature store core SDK:\n",
    "Uses CLI for CRUD operations (create/update/delete) and python SDK for feature set development and testing only. This is useful in ci/cd or GitOps scenarios where CLI/yaml is preferred.\n",
    "\n",
    "1. CLI:  You will use CLI for CRUD operations (create/update/delete) on feature-store, feature-set and feature-store-entity.\n",
    "2. Feature store core sdk: This sdk (`azureml-featurestore`) is meant to be used for feature set development and consumption (you will learn more about these operations later):\n",
    "- List/Get registered feature set\n",
    "- Generate/resolve feature retrieval spec\n",
    "- Execute featureset definition to generate Spark dataframe\n",
    "- Generate training using a point-in-time join\n",
    "\n",
    "For this tutorial so you do not need to install any of these explicitly, since the instructions already cover them (conda yaml in the above step include these)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Create a minimal feature store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 1a: Set feature store parameters\n",
    "Set name, location and other values for the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684196896581
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "fs-params",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# We use the subscription, resource group, region of this active project workspace.\n",
    "# You can optionally replace them to create the resources in a different subsciprtion/resourceGroup, or use existing resources\n",
    "import os\n",
    "\n",
    "featurestore_name = \"<FEATURESTORE_NAME>\"\n",
    "featurestore_location = \"eastus\"\n",
    "featurestore_subscription_id = os.environ[\"AZUREML_ARM_SUBSCRIPTION\"]\n",
    "featurestore_resource_group_name = os.environ[\"AZUREML_ARM_RESOURCEGROUP\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 1b: Create the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1683415482376
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "create-fs-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "!az ml feature-store create --subscription $featurestore_subscription_id --resource-group $featurestore_resource_group_name --location $featurestore_location --name $featurestore_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 1c: Initialize AzureML feature store core SDK client\n",
    "As explained above, this is used to develop and consume features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684196999253
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-fs-core-sdk",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# feature store client\n",
    "from azureml.featurestore import FeatureStoreClient\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "featurestore = FeatureStoreClient(\n",
    "    credential=AzureMLOnBehalfOfCredential(),\n",
    "    subscription_id=featurestore_subscription_id,\n",
    "    resource_group_name=featurestore_resource_group_name,\n",
    "    name=featurestore_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Prototype and develop a transaction rolling aggregation featureset in this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 2a: Explore the transactions source data\n",
    "\n",
    "#### Note\n",
    "The sample data used in this notebook is hosted in a public accessible blob container. It can only be read in Spark via `wasbs` driver. When you create feature sets using your own source data, please host them in adls gen2 account and use `abfss` driver in the data path.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197039351
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "explore-txn-src-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# remove the \".\" in the roor directory path as we need to generate absolute path to read from spark\n",
    "transactions_source_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/datasources/transactions-source/*.parquet\"\n",
    "transactions_src_df = spark.read.parquet(transactions_source_data_path)\n",
    "\n",
    "display(transactions_src_df.head(5))\n",
    "# Note: display(training_df.head(5)) displays the timestamp column in a different format. You can can call transactions_src_df.show() to see correctly formatted value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 2b: Develop a transactions featureset locally\n",
    "\n",
    "Featureset specification is a self-contained definition of feature set that can be developed and tested locally.\n",
    "\n",
    "Lets create the following rolling window aggregate features:\n",
    "- transactions 3-day count\n",
    "- transactions amount 3-day sum\n",
    "- transactions amount 3-day avg\n",
    "- transactions 7-day count\n",
    "- transactions amount 7-day sum\n",
    "- transactions amount 7-day avg\n",
    "\n",
    "__Action__:\n",
    "- Inspect the feature transformation code file: `featurestore/featuresets/transactions/spec/transformation_code/transaction_transform.py`. You will see how is the rolling aggregation defined for the features. This is a spark transformer.\n",
    "\n",
    "To understand the feature set and transformations in more detail, see [feature store concepts](fs-concepts-url-todo) and [transformation concepts](fs-transformation-concepts-todo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197090968
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "develop-txn-fset-locally",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.featurestore import create_feature_set_spec, FeatureSetSpec\n",
    "from azureml.featurestore.contracts import (\n",
    "    DateTimeOffset,\n",
    "    FeatureSource,\n",
    "    TransformationCode,\n",
    "    Column,\n",
    "    ColumnType,\n",
    "    SourceType,\n",
    "    TimestampColumn,\n",
    ")\n",
    "\n",
    "\n",
    "transactions_featureset_code_path = (\n",
    "    root_dir + \"/featurestore/featuresets/transactions/transformation_code\"\n",
    ")\n",
    "\n",
    "transactions_featureset_spec = create_feature_set_spec(\n",
    "    source=FeatureSource(\n",
    "        type=SourceType.parquet,\n",
    "        path=\"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/datasources/transactions-source/*.parquet\",\n",
    "        timestamp_column=TimestampColumn(name=\"timestamp\"),\n",
    "        source_delay=DateTimeOffset(days=0, hours=0, minutes=20),\n",
    "    ),\n",
    "    transformation_code=TransformationCode(\n",
    "        path=transactions_featureset_code_path,\n",
    "        transformer_class=\"transaction_transform.TransactionFeatureTransformer\",\n",
    "    ),\n",
    "    index_columns=[Column(name=\"accountID\", type=ColumnType.string)],\n",
    "    source_lookback=DateTimeOffset(days=7, hours=0, minutes=0),\n",
    "    temporal_join_lookback=DateTimeOffset(days=1, hours=0, minutes=0),\n",
    "    infer_schema=True,\n",
    ")\n",
    "# Generate a spark dataframe from the feature set specification\n",
    "transactions_fset_df = transactions_featureset_spec.to_spark_dataframe()\n",
    "# display few records\n",
    "display(transactions_fset_df.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 2c:  Export as feature set spec\n",
    "Inorder to register the feature set spec with the feature store, it needs to be saved in a specific format. \n",
    "Action: Please inspect the generated `transactions` FeaturesetSpec: Open this file from the file tree to see the spec: `featurestore/featuresets/accounts/spec/FeaturesetSpec.yaml`\n",
    "\n",
    "Spec contains these important elements:\n",
    "\n",
    "1. `source`: reference to a storage. In this case a parquet file in a blob storage.\n",
    "1. `features`: list of features and their datatypes. If you provide transformation code (see Day 2 section), the code has to return a dataframe that maps to the features and datatypes.\n",
    "1. `index_columns`: the join keys required to access values from the feature set\n",
    "\n",
    "Learn more about it in the [top level feature store entities document](fs-concepts-todo) and the [feature set spec yaml reference](reference-yaml-featureset-spec.md).\n",
    "\n",
    "The additional benefit of persisting it is that it can be source controlled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197124310
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "dump-transactions-fs-spec",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a new folder to dump the feature set spec\n",
    "transactions_featureset_spec_folder = (\n",
    "    root_dir + \"/featurestore/featuresets/transactions/spec\"\n",
    ")\n",
    "\n",
    "# check if the folder exists, create one if not\n",
    "if not os.path.exists(transactions_featureset_spec_folder):\n",
    "    os.makedirs(transactions_featureset_spec_folder)\n",
    "\n",
    "transactions_featureset_spec.dump(transactions_featureset_spec_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Register a feature-store entity\n",
    "Entity helps enforce best practice that same join key definitions are used across featuresets which uses the same logical entities. Examples of entities are account entity, customer entity etc. Entities are typically created once and reused across feature-sets. For information on basics concept of feature store, see [feature store concepts](fs-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197149800
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "register-acct-entity-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "account_entity_path = root_dir + \"/featurestore/entities/account.yaml\"\n",
    "!az ml feature-store-entity create --file $account_entity_path --resource-group $featurestore_resource_group_name --workspace-name $featurestore_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Register the transaction featureset with the featurestore\n",
    "You register a feature set asset with the feature store so that you can share and reuse with others. You also get managed capabilities like versioning and materialization (we will learn in this tutorial series).\n",
    "\n",
    "The feature set asset has reference to the feature set spec that you created earlier and additional properties like version and materialization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197175527
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "register-txn-fset-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "account_featureset_path = (\n",
    "    root_dir + \"/featurestore/featuresets/transactions/featureset_asset.yaml\"\n",
    ")\n",
    "!az ml feature-set create --file $account_featureset_path --resource-group $featurestore_resource_group_name --workspace-name $featurestore_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Explore the FeatureStore UI\n",
    "* Goto the [Azure ML global landing page](https://ml.azure.com/home?flight=FeatureStores). \n",
    "* Click on `Feature stores` in the left nav\n",
    "* You will see the list of feature stores that you have access to. Click on the feature store that you created above.\n",
    "\n",
    "You can see the feature set and entity that you created.\n",
    "\n",
    "Note: Creating and updating feature store assets are possible only through SDK and CLI. You can use the UI to search/browse the feature store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Generate a training data dataframe using the registered features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 5a: Load observation data\n",
    "\n",
    "We start by exploring the observation data. Observation data is typically the core data used in training and inference data. This is then joined with feature data to create the full training data. Observation data is the data captured during the time of the event: in this case it has core transaction data including transaction id, account id, transaction amount. In this case, since it is for training, it also has the target variable appended (is_fraud).\n",
    "\n",
    "To learn more core concepts including observation data, refer to the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197186172
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "load-obs-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "observation_data_path = \"wasbs://data@azuremlexampledata.blob.core.windows.net/feature-store-prp/observation_data/train/*.parquet\"\n",
    "observation_data_df = spark.read.parquet(observation_data_path)\n",
    "obs_data_timestamp_column = \"timestamp\"\n",
    "\n",
    "display(observation_data_df)\n",
    "# Note: the timestamp column is displayed in a different format. Optionally, you can can call training_df.show() to see correctly formatted value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 5c: Get the registered featureset and list its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197204532
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "get-txn-fset",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# look up the featureset by providing name and version\n",
    "transactions_featureset = featurestore.feature_sets.get(\"transactions\", \"1\")\n",
    "# list its features\n",
    "transactions_featureset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684197215222
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "print-txn-fset-sample-values",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# print sample values\n",
    "display(transactions_featureset.to_spark_dataframe().head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Step 5d: Select features and generate training data\n",
    "In this step we will select features that we would like to be part of training data and use the feature store sdk to generate the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684170679594
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "select-features-and-gen-training-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.featurestore import get_offline_features\n",
    "\n",
    "# you can select features in pythonic way\n",
    "features = [\n",
    "    transactions_featureset.get_feature(\"transaction_amount_7d_sum\"),\n",
    "    transactions_featureset.get_feature(\"transaction_amount_7d_avg\"),\n",
    "]\n",
    "\n",
    "# you can also specify features in string form: featurestore:featureset:version:feature\n",
    "more_features = [\n",
    "    \"transactions:1:transaction_3d_count\",\n",
    "    \"transactions:1:transaction_amount_3d_avg\",\n",
    "]\n",
    "\n",
    "more_features = featurestore.resolve_feature_uri(more_features)\n",
    "features.extend(more_features)\n",
    "\n",
    "# generate training dataframe by using feature data and observation data\n",
    "training_df = get_offline_features(\n",
    "    features=features,\n",
    "    observation_data=observation_data_df,\n",
    "    timestamp_column=obs_data_timestamp_column,\n",
    ")\n",
    "\n",
    "# Ignore the message that says feature set is not materialized (materialization is optional). We will enable materialization in the next part of the tutorial.\n",
    "display(training_df)\n",
    "# Note: the timestamp column is displayed in a different format. Optionally, you can can call training_df.show() to see correctly formatted value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "You can see how the features are appended to the training data using a point-in-time join.\n",
    "\n",
    "We have reached the end of the tutorial. Now you have your training data using features from feature store. You can either save it to storage for later use, or run model training on it directly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Part 4 of the tutorial has instructions deleting the resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Next steps\n",
    "* Part 2 of tutorial: Enable materialization and backfill feature data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_ignore_dictionary": [
     "dataframe",
     "featureset",
     "operationalization",
     "operationalize"
    ],
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
