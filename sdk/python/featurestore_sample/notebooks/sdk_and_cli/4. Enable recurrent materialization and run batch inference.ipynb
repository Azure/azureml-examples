{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Tutorial #4: Enable recurrent materialization and run batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "_Managed feature store is in private preview, which is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In this tutorial series you will experience how features seamlessly integrates all the phases of ML lifecycle: Prototyping features, training and operationalizing.\n",
    "\n",
    "In the previous part of the tutorial you learnt to experiment with features, train the model and register the model along with the feature-retrieval spec. In this tutorial you will learn how to run batch inference for the registered model.\n",
    "\n",
    "You will perform the following:\n",
    "- Enable recurrent materialization for the `transactions` feature set\n",
    "- Run batch inference pipeline on the registered model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Prerequisites\n",
    "1. Please ensure you have executed the previous parts of this tutorial series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### (updated)  Configure Azure ML spark notebook\n",
    "\n",
    "1. Running the tutorial: You can either create a new notebook, and execute the instructions in this document step by step or open the existing notebook named `4. Enable recurrent materialization and run batch inference`, and run it. The notebooks are available in `featurestore_sample/notebooks` directory. You can select from `sdk_only` or `sdk_and_cli`. You may keep this document open and refer to it for additional explanation and documentation links.\n",
    "1. In the \"Compute\" dropdown in the top nav, select \"AzureML Spark Compute\". \n",
    "1. Click on \"configure session\" in bottom nav -> click on \"upload conda file\" -> select the file azureml-examples/sdk/python/featurestore-sample/project/env/conda.yml from your local machine; Also increase the session time out (idle time) if you want to avoid running the prerequisites frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1684255149038
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "start-spark-session",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-05-16T16:39:07.6808662Z",
       "execution_start_time": "2023-05-16T16:39:07.2559521Z",
       "livy_statement_state": "available",
       "parent_msg_id": "d387f528-76b5-49cd-a6e4-bc7056c4db68",
       "queued_time": "2023-05-16T16:38:05.630714Z",
       "session_id": "324",
       "session_start_time": "2023-05-16T16:38:05.6321665Z",
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "02c82d68-70da-4f10-89b1-7d661232d9b7",
       "state": "finished",
       "statement_id": 6
      },
      "text/plain": [
       "StatementMeta(02c82d68-70da-4f10-89b1-7d661232d9b7, 324, 6, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start spark session\n"
     ]
    }
   ],
   "source": [
    "# run this cell to start the spark session (any code block will start the session ). This can take around 10 mins.\n",
    "print(\"start spark session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Setup root directory for the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684250013051
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "root-dir",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# please update the dir to ./Users/{your-alias} (or any custom directory you uploaded the samples to).\n",
    "# You can find the name from the directory structure inm the left nav\n",
    "root_dir = \"./Users/<your user alias>/featurestore_sample\"\n",
    "\n",
    "if os.path.isdir(root_dir):\n",
    "    print(\"The folder exists.\")\n",
    "else:\n",
    "    print(\"The folder does not exist. Please create or fix the path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### (new for sdk/cki track) Setup CLI\n",
    "\n",
    "1. Install azure ml cli extention\n",
    "1. Authenticate\n",
    "1. Set the default subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "install-ml-ext-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!az extension add --name ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "auth-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684250191238
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "set-default-subs-cli",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "subscription_id = os.environ[\"AZUREML_ARM_SUBSCRIPTION\"]\n",
    "\n",
    "!az account set -s $subscription_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Initialize the project workspace CRUD client\n",
    "This is the current workspace where you will be running the tutorial notebook from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1684255170922
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-ws-crud-client",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-05-16T16:39:29.5549661Z",
       "execution_start_time": "2023-05-16T16:39:19.4825303Z",
       "livy_statement_state": "available",
       "parent_msg_id": "c419e4f6-ee80-41d3-a3ae-ff45ad0457d4",
       "queued_time": "2023-05-16T16:39:19.4187034Z",
       "session_id": "324",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "02c82d68-70da-4f10-89b1-7d661232d9b7",
       "state": "finished",
       "statement_id": 7
      },
      "text/plain": [
       "StatementMeta(02c82d68-70da-4f10-89b1-7d661232d9b7, 324, 7, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Initialize the MLClient of this project workspace\n",
    "import os\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "project_ws_sub_id = os.environ[\"AZUREML_ARM_SUBSCRIPTION\"]\n",
    "project_ws_rg = os.environ[\"AZUREML_ARM_RESOURCEGROUP\"]\n",
    "project_ws_name = os.environ[\"AZUREML_ARM_WORKSPACE_NAME\"]\n",
    "\n",
    "# connect to the project workspace\n",
    "ws_client = MLClient(\n",
    "    AzureMLOnBehalfOfCredential(), project_ws_sub_id, project_ws_rg, project_ws_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Initialize the feature store variables\n",
    "Ensure you update the `featurestore_name` to reflect what you created in part 1 of this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684250478576
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-fs-crud-client",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "featurestore_name = \"my-featurestore\"  # use the same name from part #1 of the tutorial\n",
    "featurestore_subscription_id = os.environ[\"AZUREML_ARM_SUBSCRIPTION\"]\n",
    "featurestore_resource_group_name = os.environ[\"AZUREML_ARM_RESOURCEGROUP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Initialize the feature store core sdk client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684250496055
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "init-fs-core-sdk",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# feature store client\n",
    "from azureml.featurestore import FeatureStoreClient\n",
    "from azure.ai.ml.identity import AzureMLOnBehalfOfCredential\n",
    "\n",
    "featurestore = FeatureStoreClient(\n",
    "    credential=AzureMLOnBehalfOfCredential(),\n",
    "    subscription_id=featurestore_subscription_id,\n",
    "    resource_group_name=featurestore_resource_group_name,\n",
    "    name=featurestore_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Enable recurrent materialization on the `transactions` featureset\n",
    "\n",
    "In part 2 of this tutorial you enabled materialization and performed backfill on the transactions feature set. Backfill is an ondemand one-time operation to compute and store feature values in the materialization store. However when you want to perform inference of the model in production, you might want to keep the materilization store upto date by setting up recurrent materialization jobs. These jobs run on user defined schedule\n",
    "The recurrent job schedule works in the following way: \n",
    "- A window is defined by the interval and frequency. E.g., interval = 3 and frequency = Hour define a 3-hour window\n",
    "- The first window starts at the start_time defined in the RecurenceTrigger, and so on.\n",
    "- The first recurrent job will be submitted at the begining of the next window after the update time.\n",
    "- Later recurrent jobs will be submitted at every window after the first job.\n",
    "\n",
    "As explained in the previous parts of the tutorials, once data is materialized (backfill/recurrent materialization), feature retrieval will use the materialized data by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684250967027
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "enable-recurrent-mat-txns-fset",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "feature_set_schedule_yaml = (\n",
    "    root_dir\n",
    "    + \"/featurestore/featuresets/transactions/featureset_asset_offline_enabled_with_schedule.yaml\"\n",
    ")\n",
    "\n",
    "!az ml feature-set update --file $feature_set_schedule_yaml --resource-group $featurestore_resource_group_name --workspace-name $featurestore_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Track status of the recurrent materialization jobs in the feature store studio UI\n",
    "This job will every three hours. \n",
    "\n",
    "__Action__:\n",
    "\n",
    "1. Feel free to execute the next step for now (batch inference).\n",
    "1. In three hours check the recurrent job status via the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Run the batch-inference pipeline\n",
    "\n",
    "In this step you will manually trigger the batch inference pipeline. In a production scenario, this could be trigerred by a ci/cd pipeline based on model registration/approval.\n",
    "\n",
    "The batch-inference has the following steps:\n",
    "\n",
    "1. Feature retrieval step: This use the same built-in feature retrieval component that we used in the training pipeline in the part 3 of the tutorial. Incase of training pipeline, we provided feature retreival spec as an input to the component. However in case of batch inference we will pass the registered model as the input and the component will look for feature retrieval spec in the model artifact. Another difference is that in case of training, the observation data had the target variable, however incase of batch inference it will not be present. The feature retrieval step will join the observation data with the features and output the data for batch inference.\n",
    "1. Batch inference: This step uses the batch inference input data from previous step, runs inference on the model and outputs the data by appending the predicted value.\n",
    "\n",
    "__Note:__ In this example we use a job for batch inference. You can also use Azure ML's batch endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684252793076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "run-batch-inf-pipeline",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# set the batch inference pipeline path\n",
    "batch_inference_pipeline_path = (\n",
    "    root_dir + \"/project/fraud_model/pipelines/batch_inference_pipeline.yaml\"\n",
    ")\n",
    "\n",
    "!az ml job create --file $batch_inference_pipeline_path --resource-group $project_ws_rg --workspace-name $project_ws_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Inspect the batch inference output data\n",
    "1. In the pipeline view, double click on `inference_step` -> in `outputs` card, copy the `Data` field. It will be something like `azureml_995abbc2-3171-461e-8214-c3c5d17ede83_output_data_data_with_prediction:1`. \n",
    "1. Paste it in the below cell with name and version separately (notice that the last character is the version, separated by a `:`).\n",
    "1. You will see the `predict_is_fraud` column generated by the batch inference pipeline\n",
    "\n",
    "Explanation: Since we did not provide a `name` and `version` in the `outputs` of the `inference_step` in the batch inference pipeline (`/project/fraud_mode/pipelines/batch_inference_pipeline.yaml`), the system created an untracked data asset with a guid as name and version as 1. In the next cell we will be getting the data path from the asset and displaying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1684255304980
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "inspect-batch-inf-output-data",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-05-16T16:41:43.5604607Z",
       "execution_start_time": "2023-05-16T16:41:24.2920778Z",
       "livy_statement_state": "available",
       "parent_msg_id": "1749de2e-a0b4-4011-9ec9-cd277fde0c1f",
       "queued_time": "2023-05-16T16:41:24.2240828Z",
       "session_id": "324",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-05-16T16:41:41.028GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n#inf_data_output = ws_client.data.get(name=\"azureml_1c106662-aa5e-4354-b5f9-57c1b0fdb3a7_output_data_data_with_prediction\", version=\"1\")\ninf_data_output = ws_client.data.get(name=\"azureml_0a7417c8-409a-4536-a069-4ea23a08ebfe_output_data_data_with_prediction\", version=\"1\")\ninf_output_df = spark.read.parquet(inf_data_output.path)\ndisplay(inf_output_df.head(5))",
          "jobGroup": "9",
          "jobId": 4,
          "killedTasksSummary": {},
          "name": "getRowsInJsonString at Display.scala:403",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 0,
          "stageIds": [
           4
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-05-16T16:41:40.923GMT"
         },
         {
          "completionTime": "2023-05-16T16:41:40.908GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n#inf_data_output = ws_client.data.get(name=\"azureml_1c106662-aa5e-4354-b5f9-57c1b0fdb3a7_output_data_data_with_prediction\", version=\"1\")\ninf_data_output = ws_client.data.get(name=\"azureml_0a7417c8-409a-4536-a069-4ea23a08ebfe_output_data_data_with_prediction\", version=\"1\")\ninf_output_df = spark.read.parquet(inf_data_output.path)\ndisplay(inf_output_df.head(5))",
          "jobGroup": "9",
          "jobId": 3,
          "killedTasksSummary": {},
          "name": "getRowsInJsonString at Display.scala:403",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 4,
          "numCompletedStages": 1,
          "numCompletedTasks": 4,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 4,
          "rowCount": 0,
          "stageIds": [
           3
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-05-16T16:41:40.022GMT"
         },
         {
          "completionTime": "2023-05-16T16:41:40.009GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n#inf_data_output = ws_client.data.get(name=\"azureml_1c106662-aa5e-4354-b5f9-57c1b0fdb3a7_output_data_data_with_prediction\", version=\"1\")\ninf_data_output = ws_client.data.get(name=\"azureml_0a7417c8-409a-4536-a069-4ea23a08ebfe_output_data_data_with_prediction\", version=\"1\")\ninf_output_df = spark.read.parquet(inf_data_output.path)\ndisplay(inf_output_df.head(5))",
          "jobGroup": "9",
          "jobId": 2,
          "killedTasksSummary": {},
          "name": "getRowsInJsonString at Display.scala:403",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 0,
          "stageIds": [
           2
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-05-16T16:41:36.489GMT"
         },
         {
          "completionTime": "2023-05-16T16:41:34.462GMT",
          "dataRead": 689540,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n#inf_data_output = ws_client.data.get(name=\"azureml_1c106662-aa5e-4354-b5f9-57c1b0fdb3a7_output_data_data_with_prediction\", version=\"1\")\ninf_data_output = ws_client.data.get(name=\"azureml_0a7417c8-409a-4536-a069-4ea23a08ebfe_output_data_data_with_prediction\", version=\"1\")\ninf_output_df = spark.read.parquet(inf_data_output.path)\ndisplay(inf_output_df.head(5))",
          "jobGroup": "9",
          "jobId": 1,
          "killedTasksSummary": {},
          "name": "head at /tmp/ipykernel_16337/1529396778.py:4",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 4096,
          "stageIds": [
           1
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-05-16T16:41:32.763GMT"
         },
         {
          "completionTime": "2023-05-16T16:41:30.531GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n#inf_data_output = ws_client.data.get(name=\"azureml_1c106662-aa5e-4354-b5f9-57c1b0fdb3a7_output_data_data_with_prediction\", version=\"1\")\ninf_data_output = ws_client.data.get(name=\"azureml_0a7417c8-409a-4536-a069-4ea23a08ebfe_output_data_data_with_prediction\", version=\"1\")\ninf_output_df = spark.read.parquet(inf_data_output.path)\ndisplay(inf_output_df.head(5))",
          "jobGroup": "9",
          "jobId": 0,
          "killedTasksSummary": {},
          "name": "parquet at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 0,
          "stageIds": [
           0
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-05-16T16:41:25.563GMT"
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 5,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "02c82d68-70da-4f10-89b1-7d661232d9b7",
       "state": "finished",
       "statement_id": 9
      },
      "text/plain": [
       "StatementMeta(02c82d68-70da-4f10-89b1-7d661232d9b7, 324, 9, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "e0a8aa54-ece6-49e3-ae90-2b4559c41ecc",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, e0a8aa54-ece6-49e3-ae90-2b4559c41ecc)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inf_data_output = ws_client.data.get(name=\"azureml_1c106662-aa5e-4354-b5f9-57c1b0fdb3a7_output_data_data_with_prediction\", version=\"1\")\n",
    "inf_data_output = ws_client.data.get(\n",
    "    name=\"azureml_0a7417c8-409a-4536-a069-4ea23a08ebfe_output_data_data_with_prediction\",\n",
    "    version=\"1\",\n",
    ")\n",
    "inf_output_df = spark.read.parquet(inf_data_output.path)\n",
    "display(inf_output_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Cleanup\n",
    "If you created a resource group for the tutorial, you can delete the resource group to delete all the resources associated with this tutorial.\n",
    "\n",
    "Otherwise, you can delete the resources individually:\n",
    "\n",
    "* Delete the feature store: Go to the resource group in the azure portal, select the feature store and delete it\n",
    "* Follow the instructions [here](https://review.learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azp&view=azureml-api-2#delete-a-user-assigned-managed-identity) to delete the user assigned managed identity\n",
    "* Delete the offline store (storage account): Go to the resource group in the azure portal, select the storage you created and delete it"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
