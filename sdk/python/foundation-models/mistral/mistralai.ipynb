{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Mistral in Azure AI and Azure ML\n",
    "\n",
    "Use `mistralai` client to consume Mistral deployments in Azure AI and Azure ML. Notice that Mistral supports only chat completions API.\n",
    "\n",
    "> Review the [documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral) for the Mistral family of models at for AI Studio and for ML Studio for details on how to provision inference endpoints, regional availability, pricing and inference schema reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start, there are certain steps we need to take to deploy the models:\n",
    "\n",
    "* Follow the steps listed in [this](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral?tabs=mistral-large#prerequisites) article to set up resources.\n",
    "* Go to Azure AI Studio and select the model on Model Catalog.\n",
    "\n",
    "    > Notice that some models may not be available in all the regions in Azure AI and Azure Machine Learning. On those cases, you can create a workspace or project in the region where the models are available and then consume it with a connection from a different one. To learn more about using connections see [Consume models with connections](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deployments-connections)\n",
    "\n",
    "* Create a Serverless deployment using the steps listed [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral?tabs=mistral-large#create-a-new-deployment).\n",
    "\n",
    "Once deployed successfully, you should be assigned for an API endpoint and a security key for inference.\n",
    "\n",
    "For more information, you should consult Azure's official documentation [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral?tabs=mistral-large) for model deployment and inference.\n",
    "\n",
    "To complete this tutorial, you will need to:\n",
    "\n",
    "* Install `mistralai`:\n",
    "\n",
    "    ```bash\n",
    "    pip install mistralai\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following is an example about how to use `mistralai` with a Mistral model deployed in Azure AI and Azure ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `mistralai`, create a client and configure it as follows:\n",
    "\n",
    "- `endpoint`: Use the endpoint URL from your deployment. Do not include either `/chat/completions` as this is included automatically by the client.\n",
    "- `api_key`: Use your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "chat_client"
   },
   "outputs": [],
   "source": [
    "client = MistralClient(\n",
    "    endpoint=\"https://<endpoint>.<region>.inference.ai.azure.com\", api_key=\"<secret>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: Alternatively, you can configure your API key in the environment variables `MISTRAL_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the client to create chat completions requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "chat_invoke"
   },
   "outputs": [],
   "source": [
    "chat_response = client.chat(\n",
    "    model=\"azureai\",\n",
    "    messages=[\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=\"Who is the most renowned French painter? Provide a short answer.\",\n",
    "        )\n",
    "    ],\n",
    "    max_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "chat_response"
   },
   "outputs": [],
   "source": [
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral also support the parameter `safe_prompt`. Toggling the safe prompt will prepend your messages with the following system prompt:\n",
    "\n",
    "> Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "chat_invoke_safe"
   },
   "outputs": [],
   "source": [
    "chat_response = client.chat(\n",
    "    model=\"azureai\",\n",
    "    messages=[\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=\"Who is the most renowned French painter? Provide a short answer.\",\n",
    "        )\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    safe_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "Certain versions of the Mistral model family support function calling using Tools. `Mistral Large` for instance has such capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows you can use function calling with `Mistral Large` to get flight information between cities. Let's define the tools available for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_definition"
   },
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import Function, ToolChoice\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": Function(\n",
    "            name=\"get_flight_info\",\n",
    "            description=\"Get flight information between two cities or airports\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"loc_origin\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The departure airport, e.g. MIA\",\n",
    "                    },\n",
    "                    \"loc_destionation\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The destination airport, e.g. NYC\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"loc_origin\", \"loc_destination\"],\n",
    "            },\n",
    "        ),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the toolkit is designed, we can just past it to the LLM call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_chat_invoke"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=\"You are a helpful assistant that help users to find information about traveling, how to get to places and the different transportations options. You care about the environment and you always have that in mind when answering inqueries.\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=\"When is the next flight from Miami to Seattle?\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_response = client.chat(\n",
    "    model=\"azureai\", messages=messages, tools=tools, tool_choice=ToolChoice.auto\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out if a tool needs to be called by inspecting the response. When a tool needs to be called, you will see `finish_reason` is `FinishReason.tool_calls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_chat_invoke_output"
   },
   "outputs": [],
   "source": [
    "print(chat_response.choices[0].finish_reason)\n",
    "\n",
    "response_message = chat_response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "print(tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the details of the tool that needs to be called, you can inspect the repsonse. The following example gets and prints all the tools being needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_chat_invoke_output_toolcalls"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    function_name = tool_call.function.name\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    tool_call_id = tool_call.id\n",
    "\n",
    "    print(f\"Calling function `{function_name}` with arguments {function_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demostrate usage, let's define this function to be implemented in Python directly as a `function`. We are going to mock a simple response but you can call an external service, or provide any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_definition_python"
   },
   "outputs": [],
   "source": [
    "def get_flight_info(loc_origin: str, loc_destination: str):\n",
    "    return \"No flights available. You should take a train, specially if it helps to reduce CO2 emissions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this function is defined in the current notebook, we can get the function by anme using `locals()` dictionary. Notice that this is just done as a simple way to get the function callable from its string name. Then we can call it with the corresponding arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_definition_python_run"
   },
   "outputs": [],
   "source": [
    "callable_func = locals()[function_name]\n",
    "function_response = callable_func(**function_args)\n",
    "print(function_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a response from the function and its arguments, we can append a new message to the chat history. Notice how we are indicating the model that this chat message came from a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_chat_response_message"
   },
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"tool_call_id\": tool_call_id,\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": function_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the response from the model now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "tool_chat_response"
   },
   "outputs": [],
   "source": [
    "chat_response = client.chat(\n",
    "    model=\"azureai\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional resources\n",
    "\n",
    "Here are some additional reference:  \n",
    "\n",
    "* [Plan and manage costs (marketplace)](https://learn.microsoft.com/azure/ai-studio/how-to/costs-plan-manage#monitor-costs-for-models-offered-through-the-azure-marketplace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
