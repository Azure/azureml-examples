{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cohere Command R or Cohere Command R+ with LangChain and Azure AI Search to Answer Questions Using Your Data\n",
    "\n",
    "You can use Cohere Command R or Cohere Command R+ models deployed in Azure AI and Azure ML with `langchain` to create advanced retrieval augmented generation (RAG) pipelines.\n",
    "\n",
    "> Review the [documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command) for the Cohere family of models at for AI Studio and for ML Studio for details on how to provision inference endpoints, regional availability, pricing and inference schema reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start, there are certain steps we need to take to deploy the models:\n",
    "\n",
    "* Register for a valid Azure account with subscription \n",
    "* Make sure you have access to [Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio?tabs=home)\n",
    "* Create a project and resource group\n",
    "* Select `Cohere Command R` or `Cohere Command R+`.\n",
    "\n",
    "    > Notice that some models may not be available in all the regions in Azure AI and Azure Machine Learning. On those cases, you can create a workspace or project in the region where the models are available and then consume it with a connection from a different one. To learn more about using connections see [Consume models with connections](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/connections)\n",
    "\n",
    "* Deploy with \"Pay-as-you-go\"\n",
    "* Follow the same steps for `Cohere-embed-v3-english`\n",
    "\n",
    "Once deployed successfully, you should be assigned for an API endpoint and a security key for inference.\n",
    "\n",
    "For more information, you should consult Azure's official documentation [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command) for model deployment and inference.\n",
    "\n",
    "### Services\n",
    "\n",
    "You will need to ensure the following services have been created in your Azure environment:\n",
    "* Ensure you have created a search service. This can be done in the `Azure Portal` and more instructions can be found here: https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search **NOTE: You do not need to create the index, this will be done below**\n",
    "\n",
    "* Create a Cohere `Embed` and `Command` endpoint in the `Azure AI Studio`. Instructions can be found here: https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following example demonstrate how to create a RAG workflow that uses a `Cohere Command R` or `Cohere Command R+` model deployed in Azure AI and Azure ML. We will also leverage Azure AI Search to store our documents along with LangChain to orchestrate the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet langchain langchain-cohere azure-search-documents azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Constants\n",
    "We will set the values for the keys and models that we will use in our RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create instances of our Chat and Embed models deployed in Azure AI or Azure ML. Use the `langchain_cohere` package and configure it as follows:\n",
    "\n",
    "- `embed_endpoint` and `command_endpoint`: Use the endpoint URL from your deployment. Include `/v1` at the end of the endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "search_service_endpoint = \"https://<resource>.search.windows.net\"\n",
    "key_credential = AzureKeyCredential(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "index_name = \"AZURE_SEARCH_INDEX\"  # this index does not have to be created yet\n",
    "\n",
    "# Embed\n",
    "azure_cohere_embed_endpoint = \"https://<endpoint>.<region>.inference.ai.azure.com/v1\"\n",
    "azure_cohere_embed_key = \"<key>\"\n",
    "\n",
    "# Command\n",
    "azure_cohere_command_endpoint = \"https://<endpoint>.<region>.inference.ai.azure.com/v1\"\n",
    "azure_cohere_command_key = \"<key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Search Index\n",
    "\n",
    "We need to set up the index in our `Azure AI Search`\n",
    "\n",
    "We will use the `LangChain` `AzureSearch` package as well as the `CohereEmbeddings` package, which will serve as the embedding model for our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model to be used in the vector index\n",
    "embed_model = CohereEmbeddings(\n",
    "    base_url=azure_cohere_embed_endpoint,\n",
    "    cohere_api_key=azure_cohere_embed_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector store using AzureSearch\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=search_service_endpoint,\n",
    "    azure_search_key=key_credential,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embed_model,\n",
    "    semantic_configuration_name=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some sample data to add to our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_index = [\n",
    "    {\n",
    "        \"hotelId\": \"1\",\n",
    "        \"hotelName\": \"Fancy Stay\",\n",
    "        \"description\": \"Best hotel in town if you like luxury hotels.\",\n",
    "        \"category\": \"Luxury\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"2\",\n",
    "        \"hotelName\": \"Roach Motel\",\n",
    "        \"description\": \"Cheapest hotel in town. Infact, a motel.\",\n",
    "        \"category\": \"Budget\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"3\",\n",
    "        \"hotelName\": \"EconoStay\",\n",
    "        \"description\": \"Very popular hotel in town.\",\n",
    "        \"category\": \"Budget\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"4\",\n",
    "        \"hotelName\": \"Modern Stay\",\n",
    "        \"description\": \"Modern architecture, very polite staff and very clean. Also very affordable.\",\n",
    "        \"category\": \"Luxury\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"5\",\n",
    "        \"hotelName\": \"Secret Point\",\n",
    "        \"description\": \"One of the best hotel in town. The hotel is ideally located on the main commercial artery of the city in the heart of New York.\",\n",
    "        \"category\": \"Boutique\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add the texts into our vector index. This will automatically embed the field we choose (description in this case) and attach the associated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_texts(\n",
    "    texts=[d[\"description\"] for d in docs_to_index], metadatas=docs_to_index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a langchain retriever with our newly-populated Azure AI vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"Best luxury hotel in town\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our Chat function\n",
    "Next, we will create a basic chat function using the `ChatCohere` class\n",
    "\n",
    "Cohere's `chat endpoint` can accept documents directly and will return a grounded answer that includes citations against those documents.\n",
    "\n",
    "No prompt is needed since the `Cohere` model will automatically use a RAG prompt when documents are passed in.\n",
    "\n",
    "Because we are using `LangChain's expression language (LCEL)`, we will also wrap our function at the end with a `RunableLambda` function. Learn more about LCEL here: https://python.langchain.com/docs/expression_language/get_started/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(inputs):\n",
    "    \"\"\"\n",
    "    Ask a question to the chatbot, expecting a dictionary with 'question' and 'documents'.\n",
    "\n",
    "    Args:\n",
    "        inputs (dict): A dictionary containing 'question' and 'documents'.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the chatbot.\n",
    "    \"\"\"\n",
    "    question = inputs[\"question\"]\n",
    "    documents = inputs[\"documents\"]\n",
    "\n",
    "    documents = [d.metadata for d in documents]\n",
    "\n",
    "    chatbot = ChatCohere(\n",
    "        base_url=azure_cohere_command_endpoint,\n",
    "        cohere_api_key=azure_cohere_command_key,\n",
    "    )\n",
    "\n",
    "    response = chatbot(messages=[HumanMessage(content=question)], documents=documents)\n",
    "    return response\n",
    "\n",
    "\n",
    "ask = RunnableLambda(ask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Chain\n",
    "We can now create the chain by chaining the retriever and the chat function together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the answer chain using our search function and chat function\n",
    "answer_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | ask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our chain\n",
    "You can now run the full chain and see the response as an `AIMessage` object. This will contain the models answer along with citations from the documents retrieved from `Azure AI Search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_chain.invoke(\"luxury hotel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional resources\n",
    "\n",
    "Here are some additional reference:\n",
    "\n",
    "* [Plan and manage costs (marketplace)](https://learn.microsoft.com/azure/ai-studio/how-to/costs-plan-manage#monitor-costs-for-models-offered-through-the-azure-marketplace)\n",
    "                                        \n",
    "* [Cohere examples with LangChain](https://docs.cohere.com/docs/cohere-and-langchain)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
