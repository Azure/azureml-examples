{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cohere Command R or Cohere Command R+ with Azure AI Search to Answer Questions Using Your Data\n",
    "\n",
    "You can use Cohere `Command R` or Cohere `Command R+` models deployed in `Azure AI`and Azure ML to create advanced retrieval augmented generation (RAG) pipelines.\n",
    "\n",
    "> Review the [documentation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command) for the Cohere family of models at for AI Studio and for ML Studio for details on how to provision inference endpoints, regional availability, pricing and inference schema reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start, there are certain steps we need to take to deploy the models:\n",
    "\n",
    "* Register for a valid Azure account with subscription \n",
    "* Make sure you have access to [Azure AI Studio](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio?tabs=home)\n",
    "* Create a project and resource group\n",
    "* Select `Cohere Command R` or `Cohere Command R+`.\n",
    "\n",
    "    > Notice that some models may not be available in all the regions in Azure AI and Azure Machine Learning. On those cases, you can create a workspace or project in the region where the models are available and then consume it with a connection from a different one. To learn more about using connections see [Consume models with connections](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deployments-connections)\n",
    "\n",
    "* Deploy with \"Pay-as-you-go\"\n",
    "* Follow the same steps for `Cohere-embed-v3-english`\n",
    "\n",
    "Once deployed successfully, you should be assigned for an API endpoint and a security key for inference.\n",
    "\n",
    "For more information, you should consult Azure's official documentation [here](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command) for model deployment and inference.\n",
    "\n",
    "### Services\n",
    "\n",
    "You will need to ensure the following services have been created in your Azure environment:\n",
    "* Ensure you have created a search service. This can be done in the `Azure Portal` and more instructions can be found here: https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search **NOTE: You do not need to create the index, this will be done below**\n",
    "\n",
    "* Create a Cohere `Embed` and `Command` endpoint in the `Azure AI Studio`. Instructions can be found here: https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following example demonstrate how to create a RAG workflow that uses a `Cohere Command R` or `Cohere Command R+` model deployed in Azure AI and Azure ML. We will also leverage Azure AI Search to store our documents along with LangChain to orchestrate the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet cohere azure-search-documents azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Constants\n",
    "We will set the values for the keys and models that we will use in our RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create instances of our Chat and Embed models deployed in Azure AI or Azure ML. Use the `langchain_cohere` package and configure it as follows:\n",
    "\n",
    "- `embed_endpoint` and `command_endpoint`: Use the endpoint URL from your deployment. Include `/v1` at the end of the endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "search_service_endpoint = \"https://<resource>.search.windows.net\"\n",
    "key_credential = AzureKeyCredential(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "index_name = \"AZURE_SEARCH_INDEX\"  # this index does not have to be created yet\n",
    "\n",
    "# Embed\n",
    "azure_cohere_embed_endpoint = \"https://<endpoint>.<region>.inference.ai.azure.com/v1\"\n",
    "azure_cohere_embed_key = \"<key>\"\n",
    "\n",
    "# Command\n",
    "azure_cohere_command_endpoint = \"https://<endpoint>.<region>.inference.ai.azure.com/v1\"\n",
    "azure_cohere_command_key = \"<key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Search Index\n",
    "\n",
    "We need to set up the index in our `Azure AI Search`\n",
    "\n",
    "We will use the `Cohere` package as well and set the URL to be our Azure AI endpoint, which will serve as the embedding model for our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model to be used in the vector index\n",
    "co_embed = cohere.ClientV2(\n",
    "    base_url=azure_cohere_embed_endpoint, api_key=azure_cohere_embed_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some sample data to add to our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_index = [\n",
    "    {\n",
    "        \"hotelId\": \"1\",\n",
    "        \"hotelName\": \"Fancy Stay\",\n",
    "        \"description\": \"Best hotel in town if you like luxury hotels.\",\n",
    "        \"category\": \"Luxury\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"2\",\n",
    "        \"hotelName\": \"Roach Motel\",\n",
    "        \"description\": \"Cheapest hotel in town. Infact, a motel.\",\n",
    "        \"category\": \"Budget\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"3\",\n",
    "        \"hotelName\": \"EconoStay\",\n",
    "        \"description\": \"Very popular hotel in town.\",\n",
    "        \"category\": \"Budget\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"4\",\n",
    "        \"hotelName\": \"Modern Stay\",\n",
    "        \"description\": \"Modern architecture, very polite staff and very clean. Also very affordable.\",\n",
    "        \"category\": \"Luxury\",\n",
    "    },\n",
    "    {\n",
    "        \"hotelId\": \"5\",\n",
    "        \"hotelName\": \"Secret Point\",\n",
    "        \"description\": \"One of the best hotel in town. The hotel is ideally located on the main commercial artery of the city in the heart of New York.\",\n",
    "        \"category\": \"Boutique\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings\n",
    "We will create a vector field for each of the hotel descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed each of the descriptions\n",
    "# you will notice that Cohere has a field called \"input_type\" which can be set to \"search_document\", \"search_query\", \"classification\", or \"clustering\" depedning on the text you are embedding\n",
    "for doc in docs_to_index:\n",
    "    doc[\"descriptionVector\"] = co_embed.embed(\n",
    "        model=\"azureai\",\n",
    "        texts=[doc[\"description\"]],\n",
    "        input_type=\"search_document\",  # the type of content being embedded. Can be one of \"search_document, \"search_query\", \"classification\", \"clustering\", or \"image\"\n",
    "        embedding_types=[\n",
    "            \"float\"\n",
    "        ],  # the format of the embeddings. Can be one or more of \"float\", \"int8\", \"uint8\", \"binary\",\n",
    "    ).embeddings[\"float\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the index\n",
    "Next we will create the index using the Azure SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the search index in azure\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_service_endpoint, credential=key_credential\n",
    ")\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"hotelId\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "        facetable=True,\n",
    "    ),\n",
    "    SearchableField(name=\"hotelName\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"description\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String, filterable=True),\n",
    "    SearchField(\n",
    "        name=\"descriptionVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1024,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"myHnsw\")],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f\" {result.name} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert the embeddings\n",
    "Finally, we will add the data into our vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(\n",
    "    endpoint=search_service_endpoint, index_name=index_name, credential=key_credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client.upload_documents(documents=docs_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search\n",
    "We will create a small wrapper function to embed our query and search using Azure AI similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search function to encode the query and search the index\n",
    "def search(query):\n",
    "    \"\"\"\n",
    "    Searches for documents based on the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of search results.\n",
    "    \"\"\"\n",
    "    query_embedding = co_embed.embed(\n",
    "        model=\"azureai\",\n",
    "        texts=[query],\n",
    "        input_type=\"search_query\",  # the type of content being embedded. Can be one of \"search_document, \"search_query\", \"classification\", \"clustering\", or \"image\"\n",
    "        embedding_types=[\n",
    "            \"float\"\n",
    "        ],  # the format of the embeddings. Can be one or more of \"float\", \"int8\", \"uint8\", \"binary\",\n",
    "    ).embeddings[\"float\"][0]\n",
    "\n",
    "    # Azure AI search requires a vector query\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_embedding, k_nearest_neighbors=3, fields=\"descriptionVector\"\n",
    "    )\n",
    "\n",
    "    search_results = search_client.search(\n",
    "        search_text=None,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"hotelName\", \"description\", \"category\"],\n",
    "    )\n",
    "\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our search function\n",
    "for r in search(\"luxury hotel\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our Chat function\n",
    "Next, we will create a basic chat function using the `Chat` Cohere endpoint \n",
    "\n",
    "Cohere's `chat endpoint` can accept documents directly and will return a grounded answer that includes citations against those documents.\n",
    "\n",
    "No prompt is needed since the `Cohere` model will automatically use a RAG prompt when documents are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_chat = cohere.ClientV2(\n",
    "    base_url=azure_cohere_command_endpoint, api_key=azure_cohere_command_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    \"\"\"\n",
    "    Ask a question to the chatbot, expecting a dictionary with 'question' and 'documents'.\n",
    "\n",
    "    Args:\n",
    "        inputs (dict): A dictionary containing 'question' and 'documents'.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the chatbot.\n",
    "    \"\"\"\n",
    "    search_results = search(question)\n",
    "\n",
    "    # select category, description, and hotelName from the search results\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": f\"{index}\",\n",
    "            \"data\": {\n",
    "                \"category\": result[\"category\"],\n",
    "                \"description\": result[\"description\"],\n",
    "                \"hotelName\": result[\"hotelName\"],\n",
    "            },\n",
    "        }\n",
    "        for index, result in enumerate(search_results)\n",
    "    ]\n",
    "\n",
    "    response = co_chat.chat(\n",
    "        model=\"azureai\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        documents=documents,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ask(\"Whats a good hotel close to city center?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the results\n",
    "We can retrieve the answer and the citations from the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_text(text, citations):\n",
    "\n",
    "    # Process each citation in reverse order to prevent index shifting\n",
    "    text_with_citations = \"\"\n",
    "    text_start_index = 0\n",
    "    for citation in citations:\n",
    "        doc_ids_str = \", \".join([source.id for source in citation.sources])\n",
    "        citated_text = text[citation.start : citation.end]\n",
    "        # Bold the citation text and add document ids as superscript\n",
    "        cited_text_with_ids = f\"**{citated_text}**^({doc_ids_str})\"\n",
    "        text_with_citations = (\n",
    "            text[text_start_index : citation.start] + cited_text_with_ids\n",
    "        )\n",
    "        text_start_index = citation.end\n",
    "\n",
    "    text_with_citations += text[text_start_index:]\n",
    "\n",
    "    return text_with_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_text_output = pretty_text(res.message.content[0].text, res.message.citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretty_text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional resources\n",
    "\n",
    "Here are some additional reference:\n",
    "\n",
    "* [Plan and manage costs](https://learn.microsoft.com/azure/ai-studio/how-to/costs-plan-manage#monitor-costs-for-models-offered-through-the-azure-marketplace)\n",
    "\n",
    "* [Learn more about the Cohere SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-cohere-command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
