{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Math with Large Language Models\n",
    " \n",
    "### Notebook details\n",
    " \n",
    "This sample demonstrates how to train the selected student model using the teacher model, resulting in the creation of the distilled model.\n",
    " \n",
    "We will use the Meta Llama 3.1 405B Instruct as the teacher model and the Meta Llama 3.1 8B Instruct as the student model.\n",
    " \n",
    "**Note :**\n",
    " \n",
    "- Distillation offering is only available in **West US 3** regions.\n",
    "- Distillation should only be used for single turn chat completion format.\n",
    "- The Meta Llama 3.1 405B Instruct model can only be used as a teacher model.\n",
    "- The Meta Llama 3.1 8B Instruct can only be used as a student (target) model.\n",
    "\n",
    "**Prerequisites :**\n",
    "- Subscribe to the Meta Llama 3.1 405B Instruct and Meta Llama 3.1 8B Instruct, see [how to subscribe your project to the model offering in MS Learn](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio#subscribe-your-project-to-the-model-offering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Install the SDK v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import json\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes, DataGenerationTaskType, DataGenerationType\n",
    "from azure.ai.ml.model_customization import distillation, EndpointRequestSettings, PromptSettings\n",
    "from azure.ai.ml.entities import Data, ServerlessConnection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required workspace. We use the [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../configuration.ipynb) for more details on how to configure credentials and connect to a workspace.\n",
    "\n",
    "\n",
    "### 1.3.1 Prerequisites\n",
    "\n",
    "An AI Studio project in **West US 3** is required. Please follow [this](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?tabs=llama-two%2Cchatcompletion#prerequisites) document to setup your AI Studio project\n",
    "\n",
    "### 1.3.2 AI Studio project settings\n",
    "\n",
    "Update following cell with the information of the AI Studio project just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
    "RESOURCE_GROUP = \"<RESOURCE_GROUP>\"\n",
    "WORKSPACE_NAME = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Get handle to AI Studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(credential, SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)\n",
    "\n",
    "ai_project = ml_client._workspaces.get(ml_client.workspace_name)\n",
    "ai_project._workspace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "\n",
    "### 2.1 Download the dataset from HuggingFace repo\n",
    "\n",
    "For our example, we download and use the MultiArith dataset (https://huggingface.co/datasets/ChilleD/MultiArith) from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "class InputDataset(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.train_data_file_name,\n",
    "            self.test_data_file_name,\n",
    "            self.eval_data_file_name,\n",
    "        ) = (None, None, None)\n",
    "\n",
    "\n",
    "class NLIHuggingFaceInputDataset(InputDataset):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load_hf_dataset(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        train_sample_size=10,\n",
    "        val_sample_size=10,\n",
    "        test_sample_size=10,\n",
    "        train_split_name=\"train\",\n",
    "        val_split_name=\"validation\",\n",
    "        test_split_name=\"test\",\n",
    "    ):\n",
    "        full_dataset = load_dataset(dataset_name)\n",
    "\n",
    "        if val_split_name is not None:\n",
    "            train_data = full_dataset[train_split_name].select(range(train_sample_size))\n",
    "            val_data = full_dataset[val_split_name].select(range(val_sample_size))\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "        else:\n",
    "            train_val_data = full_dataset[train_split_name].select(\n",
    "                range(train_sample_size + val_sample_size)\n",
    "            )\n",
    "            train_data = train_val_data.select(range(train_sample_size))\n",
    "            val_data = train_val_data.select(\n",
    "                range(train_sample_size, train_sample_size + val_sample_size)\n",
    "            )\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define train and test sample sizes here. We use a 90-10 split of the training data for validation since there is no validation.\n",
    "# Note: For math task, no less than 40 entries is the allowed size for training or validation\n",
    "train_sample_size = 378\n",
    "val_sample_size = 42\n",
    "\n",
    "# Sample notebook using the dataset: https://huggingface.co/datasets/ChilleD/MultiArith\n",
    "dataset_name = \"ChilleD/MultiArith\"\n",
    "input_dataset = NLIHuggingFaceInputDataset()\n",
    "\n",
    "# Note: train_split_name and test_split_name can vary by dataset. They are passed as arguments in load_hf_dataset.\n",
    "# If val_split_name is None, the below function will split the train set to create the specified sized validation set.\n",
    "train, val, _ = input_dataset.load_hf_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    train_sample_size=train_sample_size,\n",
    "    val_sample_size=val_sample_size,\n",
    "    train_split_name=\"train\",\n",
    "    val_split_name=None,\n",
    ")\n",
    "\n",
    "print(\"Len of train data sample is \" + str(len(train)))\n",
    "print(\"Len of validation data sample is \" + str(len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare data to submit for inferencing\n",
    "The data has now been downloaded and processed in the case that only training data was available and not validation data. In this section we will format the downloaded data to match what is expected in an inferencing request. We will also add a system prompt to instruct the teacher model what kind of labels to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train_multiarith_378.jsonl\"\n",
    "valid_data_path = \"data/valid_multiarith_42.jsonl\"\n",
    "\n",
    "system_prompt = \"You are an AI assistant that only provides numerical answer to the given math question. \\\n",
    "Do not include reasoning, calculations, answer unit, mathematical operators (+, -, *, /, =), or any other extra words \\\n",
    "in your response. Please ensure your response is solely an integer that answers the question. If the answer is negative, \\\n",
    "include the negative sign; otherwise, do not use any sign.\"\n",
    "\n",
    "user_prompt_template = \"Question: {question}\"\n",
    "\n",
    "for row in train:\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt_template.format(question=row[\"question\"]),\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(train_data_path, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "for row in val:\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt_template.format(question=row[\"question\"]),\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(valid_data_path, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data defined locally, with local data to be uploaded\n",
    "train_data = Input(type=AssetTypes.URI_FILE, path=train_data_path)\n",
    "\n",
    "# If training data was registered to workspace already, navigate to the Data tab, select the data to use and use the 'Named asset URI'\n",
    "# train_data = \"azureml:math_train_multi_arith:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data defined locally, with local data to be uploaded\n",
    "valid_data = Input(type=AssetTypes.URI_FILE, path=valid_data_path)\n",
    "\n",
    "# If validation data was registered to workspace already, navigate to the Data tab, select the data to use and use the 'Named asset URI'\n",
    "# train_data = \"azureml:math_valid_multi_arith:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure and Run the Distillation Job\n",
    "In this section we will configure and run a Distillation job.\n",
    "\n",
    "### 3.1 Configure the job through the distillation() factory function\n",
    "\n",
    "#### distillation() parameters:\n",
    "\n",
    "The `distillation()` factory function allows user to configure Distillation for the label generation task for the most common scenarios with the following properties.\n",
    "\n",
    "- `experiment_name` - The name of the Experiment. An Experiment is like a folder with multiple runs in Azure ML Workspace that should be related to the same logical machine learning experiment.\n",
    "- `data_generation_type` - The type of data generation to perform. Valid options are 'label_generation'.\n",
    "- `data_generation_task_type` - The kind of data to generation. Valid options include 'NLI', 'NLI_QA', 'CONVERSATION', 'MATH', and 'SUMMARIZATION'.\n",
    "- `teacher_model_endpoint_connection` - A ServerlessConnection geared towards a MaaS endpoint. Requires the name of the endpoint, the endopoint url, and the api key for the endpoint.\n",
    "- `student_model` - The student model to train with the synthetic data generated from the teacher model.\n",
    "- `training_data` - The data to be used for training.\n",
    "- `validation_data` - The data to be used for validation.\n",
    "- `name` - The name of the Job/Run. This is an optional property. If not specified, a random name will be generated.\n",
    "\n",
    "\n",
    "##### Teacher Model Connection\n",
    "Select the teacher model to use. This requires a MaaS endpoint.\n",
    "Supported teacher models:\n",
    "1. Meta-Llama-3.1-405B-Instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_endpoint_name = \"Llama-3-1-405B-Instruct-vum\"\n",
    "teacher_model_endpoint_url = \"https://Meta-Llama-3-1-405B-Instruct-vum.westus3.models.ai.azure.com/chat/completions\"\n",
    "teacher_model_api_key = \"EXAMPLE_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Model\n",
    "Select the student model to use. Supported student models:\n",
    "1. Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model id\n",
    "student_model = \"azureml://registries/azureml-meta/models/Meta-Llama-3.1-8B-Instruct/versions/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillation_job = (\n",
    "    experiment_name=\"llama-distillation\",\n",
    "    data_generation_type=DataGenerationType.LABEL_GENERATION,\n",
    "    data_generation_task_type=DataGenerationTaskType.MATH,\n",
    "    teacher_model_endpoint_connection=ServerlessConnection(\n",
    "        name=teacher_model_endpoint_name,\n",
    "        endpoint=teacher_model_endpoint_url,\n",
    "        api_key=teacher_model_api_key\n",
    "    ),\n",
    "    student_model=student_model,\n",
    "    training_data=train_data,\n",
    "    validation_data=valid_data,\n",
    "    outputs={\"registered_model\": Output(type=\"mlflow_model\", name=\"llama-distilled\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Configure the distillation settings\n",
    "\n",
    "#### set_teacher_model_settings() function parameters:\n",
    "This is an optional configuration method to configure the settings inference requests will have when submitted to the teacher model endpoint.     \n",
    "    \n",
    "- `inference_parameters` - Inference parameters that are applied to inferencing requests. These inference parameters are aligned with parameters allowed by vllm. Currently, the inference parameters that are used by distillation are 'max_tokens', 'temperature', 'top_p', 'frequency_penalty', 'presence_penalty', and 'stop'.\n",
    "\n",
    "- `endpoint_request_settings` - An EndpointRequestSettings object that adds settings for the inferencing requests sent to the endpoint. Valid endpoint settings include 'min_endpoint_success_ratio' and 'request_batch_size'.\n",
    "    - `min_endpoint_success_ratio` - The minimum ratio of successful/total inferencing request needed for data generation to be considered successful. Will not proceed if the number of successful/total inferencing requests is below the ratio. Should be between 0 and 1, inclusive. Defaults to 0.7.\n",
    "    - `request_batch_size` - The number of inferencing requests to send at once to the teacher model endpoint. Defaults to 10.\n",
    "\n",
    "\n",
    "#### set_prompt_settings() function parameters:\n",
    "This is an optional configuration method to configure the settings for the system prompt used for the teacher model.\n",
    "\n",
    "- `prompt_setting` - A PromptSettings object that adds settings that determine what system prompt to use for the teacher model. Valid prompt settings for `MATH` task include 'enable_chain_of_thought'.\n",
    "    - `enable_chain_of_thought` - The option to leverage Chain of Thought (CoT) reasoning for distillation. CoT leverages step by step reasoning ability of the teacher model to generate more accurate labels.\n",
    "\n",
    "\n",
    "#### set_finetuning_settings() function parameters:\n",
    "This is an optional configuration method to configure the settings for finetuning the student model.\n",
    "\n",
    "- `hyperparameters` - The hyperparameters to use for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional settings to use for inferencing requests\n",
    "distillation_job.set_teacher_model_settings(\n",
    "    inference_parameters={\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 0.8\n",
    "    },\n",
    "    endpoint_request_settings=EndpointRequestSettings(\n",
    "        min_endpoint_success_ratio=0.7,\n",
    "        request_batch_size=10\n",
    "    )\n",
    ")\n",
    "\n",
    "# Optional settings to use for the system prompt\n",
    "distillation_job.set_prompt_settings(\n",
    "    prompt_settings=PromptSettings(\n",
    "        enable_chain_of_thought=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Optional settings to use for finetuning the student model\n",
    "distillation_job.set_finetuning_settings(\n",
    "    hyperparameters={\n",
    "        \"learning_rate\": \"0.00002\",\n",
    "        \"per_device_train_batch_size\": \"1\",\n",
    "        \"num_train_epochs\": \"3\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Submit the Job\n",
    "Using the `MLClient` created earlier, we will now run this Command in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_job = ml_client.jobs.create_or_update(distillation_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait Until the Distillation Job Finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(created_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consuming the Distilled Model\n",
    "\n",
    "Once the above job completes, you should be able to deploy the model and use it for inferencing. To deploy this model, do the following:\n",
    "\n",
    "1. Go to AI Studio\n",
    "2. Navigate to the Fine-tuning tab on the left menu\n",
    "3. In the list of models you see, click on the model which got created from the distillation\n",
    "4. This should take you to the details page where you can see the model attributes and other details\n",
    "5. Click on the Deploy button on top of the page\n",
    "6. Follow the steps to deploy the model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
