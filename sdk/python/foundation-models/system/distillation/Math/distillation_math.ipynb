{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Math with Large Language Models\n",
    " \n",
    "### Notebook details\n",
    " \n",
    "This sample demonstrates how to train the selected student model using the teacher model, resulting in the creation of the distilled model.\n",
    " \n",
    "We will use the Meta Llama 3.1 405B Instruct as the teacher model and the Meta Llama 3.1 8B Instruct as the student model.\n",
    " \n",
    "**Note :**\n",
    " \n",
    "- Distillation offering is only available in **West US 3** regions.\n",
    "- Distillation should only be used for single turn chat completion format.\n",
    "- The Meta Llama 3.1 405B Instruct model can only be used as a teacher model.\n",
    "- The Meta Llama 3.1 8B Instruct can only be used as a student (target) model.\n",
    "- Distllation is currently supported only for Natural Language Inference (NLI) task, which is a standard task in benchmarking for Natural Language Understanding.\n",
    "\n",
    "**Prerequisites :**\n",
    "- Subscribe to the Meta Llama 3.1 405B Instruct and Meta Llama 3.1 8B Instruct, see [how to subscribe your project to the model offering in MS Learn](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio#subscribe-your-project-to-the-model-offering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the SDK v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "An AI Studio project in **West US 3** is required. Please follow [this](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?tabs=llama-two%2Cchatcompletion#prerequisites) document to setup your AI Studio project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Studio project settings\n",
    "\n",
    "Update following cell with the information of the AI Studio project just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
    "RESOURCE_GROUP = \"<RESOURCE_GROUP>\"\n",
    "WORKSPACE_NAME = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get handle to AI Studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(credential, SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)\n",
    "\n",
    "ai_project = ml_client._workspaces.get(ml_client.workspace_name)\n",
    "ai_project._workspace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a teacher model\n",
    "\n",
    "We support **Meta-Llama-3.1-405B-Instruct** as the teacher model. \n",
    "### First deploy the teacher model in Azure AI Studio\n",
    "* Go to [Azure AI Studio](https://ai.azure.com)\n",
    "* Select Meta-Llama-3.1-405B-Instruct model from Model catalog.\n",
    "* Deploy with \"Pay-as-you-go\"\n",
    "* Once deployed successfully, you should be assigned for an API endpoint and a security key for inference.\n",
    "\n",
    "Update the following cell with the information of the deployment you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-3-405B Teacher model endpoint name\n",
    "# The serverless model name is the name found in ML Studio > Endpoints > Serverless endpoints > Model column\n",
    "TEACHER_MODEL_NAME = \"Meta-Llama-3.1-405B-Instruct\"\n",
    "\n",
    "# The serverless model endpoint name is the name found in ML Studio > Endpoints > Serverless endpoints > Name column\n",
    "# The endpoint URL will be resolved from this name by the MLFlow component\n",
    "TEACHER_MODEL_ENDPOINT_NAME = \"Meta-Llama-3-1-405B-Instruct-vum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a student model\n",
    "\n",
    "We will use **Meta-Llama-3.1-8B-Instruct** as student model. We only support chat completion models that are available for PayGo finetuning in Azure AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "STUDENT_MODEL_VERSION = 1\n",
    "\n",
    "# retrieve student model from model registry\n",
    "mlclient_azureml_meta = MLClient(credential, registry_name=\"azureml-meta\")\n",
    "student_model = mlclient_azureml_meta.models.get(\n",
    "    STUDENT_MODEL_NAME, version=STUDENT_MODEL_VERSION\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        student_model.name, student_model.version, student_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset from HuggingFace repo\n",
    "\n",
    "For our example, we download and use the MultiArith dataset (https://huggingface.co/datasets/ChilleD/MultiArith) from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "class InputDataset(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.train_data_file_name,\n",
    "            self.test_data_file_name,\n",
    "            self.eval_data_file_name,\n",
    "        ) = (None, None, None)\n",
    "\n",
    "\n",
    "class NLIHuggingFaceInputDataset(InputDataset):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load_hf_dataset(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        train_sample_size=10,\n",
    "        val_sample_size=10,\n",
    "        test_sample_size=10,\n",
    "        train_split_name=\"train\",\n",
    "        val_split_name=\"validation\",\n",
    "        test_split_name=\"test\",\n",
    "    ):\n",
    "        full_dataset = load_dataset(dataset_name)\n",
    "\n",
    "        if val_split_name is not None:\n",
    "            train_data = full_dataset[train_split_name].select(range(train_sample_size))\n",
    "            val_data = full_dataset[val_split_name].select(range(val_sample_size))\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "        else:\n",
    "            train_val_data = full_dataset[train_split_name].select(\n",
    "                range(train_sample_size + val_sample_size)\n",
    "            )\n",
    "            train_data = train_val_data.select(range(train_sample_size))\n",
    "            val_data = train_val_data.select(\n",
    "                range(train_sample_size, train_sample_size + val_sample_size)\n",
    "            )\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define train and test sample sizes here. We use a 90-10 split of the training data for validation since there is no validation.\n",
    "# Note: For math task, no less than 40 entries is the allowed size for training or validation\n",
    "train_sample_size = 378\n",
    "val_sample_size = 42\n",
    "\n",
    "# Sample notebook using the dataset: https://huggingface.co/datasets/ChilleD/MultiArith\n",
    "dataset_name = \"ChilleD/MultiArith\"\n",
    "input_dataset = NLIHuggingFaceInputDataset()\n",
    "\n",
    "# Note: train_split_name and test_split_name can vary by dataset. They are passed as arguments in load_hf_dataset.\n",
    "# If val_split_name is None, the below function will split the train set to create the specified sized validation set.\n",
    "train, val, _ = input_dataset.load_hf_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    train_sample_size=train_sample_size,\n",
    "    val_sample_size=val_sample_size,\n",
    "    train_split_name=\"train\",\n",
    "    val_split_name=None,\n",
    ")\n",
    "\n",
    "print(\"Len of train data sample is \" + str(len(train)))\n",
    "print(\"Len of validation data sample is \" + str(len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train_multiarith_378.jsonl\"\n",
    "valid_data_path = \"data/valid_multiarith_42.jsonl\"\n",
    "\n",
    "system_prompt = \"You are an AI assistant that only provides numerical answer to the given math question. \\\n",
    "Do not include reasoning, calculations, answer unit, mathematical operators (+, -, *, /, =), or any other extra words \\\n",
    "in your response. Please ensure your response is solely an integer that answers the question. If the answer is negative, \\\n",
    "include the negative sign; otherwise, do not use any sign.\"\n",
    "\n",
    "user_prompt_template = \"Question: {question}\"\n",
    "\n",
    "for row in train:\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt_template.format(question=row[\"question\"]),\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(train_data_path, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "for row in val:\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt_template.format(question=row[\"question\"]),\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(valid_data_path, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "train_data_name = \"math_train_multi_arith\"\n",
    "\n",
    "train_data = ml_client.data.create_or_update(\n",
    "    Data(\n",
    "        path=train_data_path,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Training dataset\",\n",
    "        name=train_data_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "train_data_asset_id = f\"azureml://locations/{ai_project.location}/workspaces/{ai_project._workspace_id}/data/{train_data.name}/versions/{train_data.version}\"\n",
    "train_data_asset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = None\n",
    "valid_data_name = \"math_valid_multi_arith\"\n",
    "\n",
    "valid_data = ml_client.data.create_or_update(\n",
    "    Data(\n",
    "        path=valid_data_path,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"validation dataset\",\n",
    "        name=valid_data_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_data_asset_id = f\"azureml://locations/{ai_project.location}/workspaces/{ai_project._workspace_id}/data/{valid_data.name}/versions/{valid_data.version}\"\n",
    "valid_data_asset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation strategy settings\n",
    "\n",
    "We provide the option to leverage Chain of Thought (CoT) reasoning for distillation. CoT leverages step by step reasoning ability of the teacher model to generate more accurate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_CHAIN_OF_THOUGHT = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlclient_azureml = MLClient(credential, registry_name=\"azureml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillation_pipeline_name = \"oss_distillation_pipeline\"\n",
    "distillation_pipeline_component = mlclient_azureml.components.get(\n",
    "    name=distillation_pipeline_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Task Type\n",
    "\n",
    "For math datasets, such as MultiArith (current dataset), where the answer is numeric, select `MATH` as the `data_generation_task_type`. \n",
    "\n",
    "There exists math datasets where the answer is expected to be a letter. For these datasets, use the [Math Q&A Notebook](../nlu_qa/distillation_qa_math.ipynb) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def distillation_pipeline(\n",
    "    teacher_model_endpoint_name: str,\n",
    "    enable_chain_of_thought: str,\n",
    "    system_properties: str,\n",
    "    input_finetune_model: Input,\n",
    "    train_file_path: Input,\n",
    "    validation_file_path: Input = None,\n",
    "):\n",
    "    oss_distillation = distillation_pipeline_component(\n",
    "        teacher_model_endpoint_name=teacher_model_endpoint_name,\n",
    "        enable_chain_of_thought=enable_chain_of_thought,\n",
    "        train_file_path=train_file_path,\n",
    "        validation_file_path=validation_file_path,\n",
    "        data_generation_task_type=\"MATH\",\n",
    "        # Finetune\n",
    "        mlflow_model_path=input_finetune_model,\n",
    "        model_asset_id=student_model.id,\n",
    "        system_properties=system_properties,\n",
    "        ## hyperparams\n",
    "        learning_rate=0.00002,\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=3,\n",
    "    )\n",
    "\n",
    "    return {\"output_model\": oss_distillation.outputs.output_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_properties = {\n",
    "    \"finetune_oss\": \"True\",\n",
    "    \"model_asset_id\": student_model.id,\n",
    "    \"PipelineType\": \"Finetune\",\n",
    "    \"azureml.PipelineType\": \"Finetune\",\n",
    "    \"azureml.ModelName\": student_model.name,\n",
    "    \"azureml.original_model_id\": student_model.id,\n",
    "    \"azureml.trainingData.assetId\": train_data_asset_id,\n",
    "}\n",
    "\n",
    "json_str = json.dumps(system_properties).replace(\" \", \"\")\n",
    "\n",
    "system_properties_b64_encoded = base64.b64encode(json_str.encode(\"utf-8\")).decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(f\"System properties => {system_properties_b64_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path_input = Input(type=\"uri_file\", path=train_data.path)\n",
    "validation_file_path_input = Input(type=\"uri_file\", path=valid_data.path)\n",
    "input_finetune_model = Input(type=\"mlflow_model\", path=student_model.id)\n",
    "experiment_name = f\"distillation-{TEACHER_MODEL_NAME}\".replace(\".\", \"-\")\n",
    "\n",
    "finetuning_job = distillation_pipeline(\n",
    "    teacher_model_endpoint_name=TEACHER_MODEL_ENDPOINT_NAME,\n",
    "    enable_chain_of_thought=ENABLE_CHAIN_OF_THOUGHT,\n",
    "    system_properties=system_properties_b64_encoded,\n",
    "    input_finetune_model=input_finetune_model,\n",
    "    train_file_path=train_file_path_input,\n",
    "    validation_file_path=validation_file_path_input,\n",
    ")\n",
    "\n",
    "finetuning_job.properties.update(system_properties)\n",
    "print(f\"job property: {finetuning_job.properties}\")\n",
    "\n",
    "finetuning_job.display_name = f\"finetune-{student_model.name}\"\n",
    "finetuning_job.experiment_name = experiment_name\n",
    "finetuning_job.settings.default_compute_type = \"serverless\"\n",
    "finetuning_job.continue_on_step_failure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit pipeline job to workspace\n",
    "ft_job = ml_client.jobs.create_or_update(finetuning_job)\n",
    "print(f\"Submitted job, progress available at {ft_job.studio_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming the distilled model\n",
    "\n",
    "Once the above job completes, you should be able to deploy the model and use it for inferencing. To deploy this model, do the following:\n",
    "\n",
    "* Go to AI Studio\n",
    "* Navigate to the Fine-tuning tab on the left menu\n",
    "* In the list of models you see, click on the model which got created from the distillation\n",
    "* This should take you to the details page where you can see the model attributes and other details\n",
    "* Click on the Deploy button on top of the page\n",
    "* Follow the steps to deploy the model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
