{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation with Large Language Models\n",
    " \n",
    "### Notebook details\n",
    " \n",
    "This sample demonstrates how to train the selected student model using the teacher model, resulting in the creation of the distilled model.\n",
    " \n",
    "We will use the Meta Llama 3.1 405B Instruct as the teacher model and the Meta Llama 3.1 8B Instruct as the student model.\n",
    " \n",
    "**Note :**\n",
    " \n",
    "- Distillation offering is only available in **West US 3** regions.\n",
    "- Distillation should only be used for single turn chat completion format.\n",
    "- The Meta Llama 3.1 405B Instruct model can only be used as a teacher model.\n",
    "- The Meta Llama 3.1 8B Instruct can only be used as a student (target) model.\n",
    "- Distllation is currently supported only for Natural Language Inference (NLI) task, which is a standard task in benchmarking for Natural Language Understanding.\n",
    "\n",
    "**Prerequisites :**\n",
    "- Subscribe to the Meta Llama 3.1 405B Instruct and Meta Llama 3.1 8B Instruct, see [how to subscribe your project to the model offering in MS Learn](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio#subscribe-your-project-to-the-model-offering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the SDK v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "%pip install azure-core\n",
    "%pip install azure-ai-inference\n",
    "\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.ai.ml import Input, MLClient\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Data, ServerlessEndpoint\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "An AI Studio project in **West US 3** is required. Please follow [this](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?tabs=llama-two%2Cchatcompletion#prerequisites) document to setup your AI Studio project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Studio project settings\n",
    "\n",
    "Update following cell with the information of the AI Studio project just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
    "RESOURCE_GROUP = \"<RESOURCE_GROUP>\"\n",
    "WORKSPACE_NAME = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get handle to AI Studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(credential, SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)\n",
    "\n",
    "ai_project = ml_client._workspaces.get(ml_client.workspace_name)\n",
    "ai_project._workspace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a teacher model\n",
    "\n",
    "We support **Meta-Llama-3.1-405B-Instruct** as the teacher model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reuse or create a serverless endpoint\n",
    "TEACHER_MODEL_NAME = \"Meta-Llama-3.1-405B-Instruct\"\n",
    "TEACHER_MODEL_ENDPOINT_NAME = \"Meta-Llama-3-1-405B-Instruct-vum\"\n",
    "\n",
    "mlclient_azureml_meta = MLClient(credential, registry_name=\"azureml-meta\")\n",
    "try:\n",
    "    ml_client.serverless_endpoints.get(TEACHER_MODEL_ENDPOINT_NAME)\n",
    "except ResourceNotFoundError:\n",
    "    # create the endpoint\n",
    "    teacher_model_id = (\n",
    "        \"azureml://registries/azureml-meta/models/Meta-Llama-3.1-405B-Instruct\"\n",
    "    )\n",
    "    teacher_endpoint = ServerlessEndpoint(\n",
    "        name=TEACHER_MODEL_ENDPOINT_NAME,\n",
    "        model_id=teacher_model_id,\n",
    "    )\n",
    "    ml_client.begin_create_or_update(teacher_endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a student model\n",
    "\n",
    "We will use **Meta-Llama-3.1-8B-Instruct** as student model. We only support chat completion models that are available for PayGo finetuning in Azure AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "STUDENT_MODEL_VERSION = 1\n",
    "\n",
    "# retrieve student model from model registry\n",
    "student_model = mlclient_azureml_meta.models.get(\n",
    "    STUDENT_MODEL_NAME, version=STUDENT_MODEL_VERSION\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        student_model.name, student_model.version, student_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset from HuggingFace repo\n",
    "\n",
    "For our example, we download and use the ConjNLI dataset (https://huggingface.co/datasets/cestwc/conjnli) from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "class InputDataset(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.train_data_file_name,\n",
    "            self.test_data_file_name,\n",
    "            self.eval_data_file_name,\n",
    "        ) = (None, None, None)\n",
    "\n",
    "\n",
    "class NLIHuggingFaceInputDataset(InputDataset):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load_hf_dataset(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        train_sample_size=10,\n",
    "        val_sample_size=10,\n",
    "        test_sample_size=10,\n",
    "        train_split_name=\"train\",\n",
    "        val_split_name=\"validation\",\n",
    "        test_split_name=\"test\",\n",
    "    ):\n",
    "        full_dataset = load_dataset(dataset_name)\n",
    "\n",
    "        if val_split_name is not None:\n",
    "            train_data = full_dataset[train_split_name].select(range(train_sample_size))\n",
    "            val_data = full_dataset[val_split_name].select(range(val_sample_size))\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "        else:\n",
    "            train_val_data = full_dataset[train_split_name].select(\n",
    "                range(train_sample_size + val_sample_size)\n",
    "            )\n",
    "            train_data = train_val_data.select(range(train_sample_size))\n",
    "            val_data = train_val_data.select(\n",
    "                range(train_sample_size, train_sample_size + val_sample_size)\n",
    "            )\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define train and test sample sizes here. Validation size is kept same as test sample size\n",
    "train_sample_size = 512\n",
    "val_sample_size = 256\n",
    "\n",
    "# Sample notebook using the dataset: https://huggingface.co/datasets/cestwc/conjnli\n",
    "dataset_name = \"cestwc/conjnli\"\n",
    "input_dataset = NLIHuggingFaceInputDataset()\n",
    "\n",
    "# Note: train_split_name and test_split_name can vary by dataset. They are passed as arguments in load_hf_dataset.\n",
    "# If val_split_name is None, the below function will split the train set to create the specified sized validation set.\n",
    "train, val, _ = input_dataset.load_hf_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    train_sample_size=train_sample_size,\n",
    "    val_sample_size=val_sample_size,\n",
    "    train_split_name=\"adversarial\",\n",
    "    val_split_name=None,\n",
    ")\n",
    "\n",
    "print(\"Len of train data sample is \" + str(len(train)))\n",
    "print(\"Len of validation data sample is \" + str(len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train_conjnli_512.jsonl\"\n",
    "valid_data_path = \"data/valid_conjnli_256.jsonl\"\n",
    "\n",
    "with open(train_data_path, \"w+\") as f:\n",
    "    for row in train:\n",
    "        data = {\"messages\": []}\n",
    "        data[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Your output should only be one of the three labels: 'entailment', 'contradiction', or 'neutral'.\",\n",
    "            }\n",
    "        )\n",
    "        data[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Given the following two texts, your task is to determine the logical relationship between them. The first text is the 'premise' and the second text is the 'hypothesis'. The relationship should be labeled as one of the following: 'entailment' if the premise entails the hypothesis, 'contradiction' if the premise contradicts the hypothesis, or 'neutral' if the premise neither entails nor contradicts the hypothesis.\\n\\nPremise: \"\n",
    "                + row[\"premise\"]\n",
    "                + \"\\nHypothesis: \"\n",
    "                + row[\"hypothesis\"],\n",
    "            }\n",
    "        )\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "with open(valid_data_path, \"w+\") as f:\n",
    "    for row in val:\n",
    "        data = {\"messages\": []}\n",
    "        data[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Your output should only be one of the three labels: 'entailment', 'contradiction', or 'neutral'.\",\n",
    "            }\n",
    "        )\n",
    "        data[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Given the following two texts, your task is to determine the logical relationship between them. The first text is the 'premise' and the second text is the 'hypothesis'. The relationship should be labeled as one of the following: 'entailment' if the premise entails the hypothesis, 'contradiction' if the premise contradicts the hypothesis, or 'neutral' if the premise neither entails nor contradicts the hypothesis.\\n\\nPremise: \"\n",
    "                + row[\"premise\"]\n",
    "                + \"\\nHypothesis: \"\n",
    "                + row[\"hypothesis\"],\n",
    "            }\n",
    "        )\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "train_data_name = \"nli_train_70\"\n",
    "\n",
    "train_data = ml_client.data.create_or_update(\n",
    "    Data(\n",
    "        path=train_data_path,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Training dataset\",\n",
    "        name=train_data_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "train_data_asset_id = f\"azureml://locations/{ai_project.location}/workspaces/{ai_project._workspace_id}/data/{train_data.name}/versions/{train_data.version}\"\n",
    "train_data_asset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = None\n",
    "valid_data_name = \"nli_valid_70\"\n",
    "\n",
    "valid_data = ml_client.data.create_or_update(\n",
    "    Data(\n",
    "        path=valid_data_path,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"validation dataset\",\n",
    "        name=valid_data_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_data_asset_id = f\"azureml://locations/{ai_project.location}/workspaces/{ai_project._workspace_id}/data/{valid_data.name}/versions/{valid_data.version}\"\n",
    "valid_data_asset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation strategy settings\n",
    "\n",
    "We provide the option to leverage Chain of Thought (CoT) reasoning for distillation. CoT leverages step by step reasoning ability of the teacher model to generate more accurate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_CHAIN_OF_THOUGHT = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlclient_azureml = MLClient(credential, registry_name=\"azureml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillation_pipeline_name = \"oss_distillation_pipeline\"\n",
    "distillation_pipeline_component = mlclient_azureml.components.get(\n",
    "    name=distillation_pipeline_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def distillation_pipeline(\n",
    "    teacher_model_endpoint_name: str,\n",
    "    enable_chain_of_thought: str,\n",
    "    system_properties: str,\n",
    "    input_finetune_model: Input,\n",
    "    train_file_path: Input,\n",
    "    validation_file_path: Input = None,\n",
    "):\n",
    "    oss_distillation = distillation_pipeline_component(\n",
    "        teacher_model_endpoint_name=teacher_model_endpoint_name,\n",
    "        enable_chain_of_thought=enable_chain_of_thought,\n",
    "        train_file_path=train_file_path,\n",
    "        validation_file_path=validation_file_path,\n",
    "        # Finetune\n",
    "        mlflow_model_path=input_finetune_model,\n",
    "        model_asset_id=student_model.id,\n",
    "        system_properties=system_properties,\n",
    "        ## hyperparams\n",
    "        learning_rate=0.00002,\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=3,\n",
    "        data_generation_task_type=\"NLI\",\n",
    "    )\n",
    "\n",
    "    return {\"output_model\": oss_distillation.outputs.output_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_properties = {\n",
    "    \"finetune_oss\": \"True\",\n",
    "    \"model_asset_id\": student_model.id,\n",
    "    \"PipelineType\": \"Finetune\",\n",
    "    \"azureml.PipelineType\": \"Finetune\",\n",
    "    \"azureml.ModelName\": student_model.name,\n",
    "    \"azureml.original_model_id\": student_model.id,\n",
    "    \"azureml.trainingData.assetId\": train_data_asset_id,\n",
    "}\n",
    "\n",
    "json_str = json.dumps(system_properties).replace(\" \", \"\")\n",
    "\n",
    "system_properties_b64_encoded = base64.b64encode(json_str.encode(\"utf-8\")).decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(f\"System properties => {system_properties_b64_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path_input = Input(type=\"uri_file\", path=train_data.path)\n",
    "validation_file_path_input = Input(type=\"uri_file\", path=valid_data.path)\n",
    "input_finetune_model = Input(type=\"mlflow_model\", path=student_model.id)\n",
    "experiment_name = f\"distillation-{TEACHER_MODEL_NAME}\".replace(\".\", \"-\")\n",
    "\n",
    "finetuning_job = distillation_pipeline(\n",
    "    teacher_model_endpoint_name=TEACHER_MODEL_ENDPOINT_NAME,\n",
    "    enable_chain_of_thought=ENABLE_CHAIN_OF_THOUGHT,\n",
    "    system_properties=system_properties_b64_encoded,\n",
    "    input_finetune_model=input_finetune_model,\n",
    "    train_file_path=train_file_path_input,\n",
    "    validation_file_path=validation_file_path_input,\n",
    ")\n",
    "\n",
    "finetuning_job.properties.update(system_properties)\n",
    "print(f\"job property: {finetuning_job.properties}\")\n",
    "\n",
    "# pipeline_job.identity = UserIdentityConfiguration()\n",
    "finetuning_job.display_name = f\"finetune-{student_model.name}\"\n",
    "finetuning_job.experiment_name = experiment_name\n",
    "finetuning_job.settings.default_compute_type = \"serverless\"\n",
    "finetuning_job.continue_on_step_failure = False\n",
    "# pipeline_job.settings.force_rerun = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit pipeline job to workspace\n",
    "ft_job = ml_client.jobs.create_or_update(finetuning_job)\n",
    "print(f\"Submitted job, progress available at {ft_job.studio_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a serverless endpoint to consume the model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to complete\n",
    "ml_client.jobs.stream(ft_job.name)\n",
    "registered_model_name = ml_client.jobs.get(ft_job.name).properties[\n",
    "    \"registered_ft_model_name\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model url for registered endpoint\n",
    "rg_model_vs = ml_client.models.get(registered_model_name, label=\"latest\")._version\n",
    "\n",
    "rg_model_asset_id = (\n",
    "    \"azureml://locations/\"\n",
    "    f\"{ai_project.location}\"\n",
    "    \"/workspaces/\"\n",
    "    f\"{ai_project._workspace_id}\"\n",
    "    \"/models/\"\n",
    "    f\"{registered_model_name}\"\n",
    "    \"/versions/\"\n",
    "    f\"{rg_model_vs}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serverless endpoint - names must be unique, we will use suffix of the model\n",
    "short_id = registered_model_name[-9:]\n",
    "serverless_endpoint_name = \"my-endpoint-\" + short_id\n",
    "\n",
    "serverless_endpoint = ServerlessEndpoint(\n",
    "    name=serverless_endpoint_name,\n",
    "    model_id=rg_model_asset_id,\n",
    ")\n",
    "\n",
    "created_endpoint = ml_client.serverless_endpoints.begin_create_or_update(\n",
    "    serverless_endpoint\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample inference against the deployed endpoint (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = created_endpoint.scoring_uri\n",
    "key = ml_client.serverless_endpoints.get_keys(created_endpoint.name).primary_key\n",
    "model = ChatCompletionsClient(\n",
    "    endpoint=url,\n",
    "    credential=AzureKeyCredential(key),\n",
    ")\n",
    "\n",
    "response = model.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=\"Answer the following multiple-choice question by selecting the correct option.\\n\\nQuestion: Can you name a good reason for attending school?\\nAnswer Choices:\\n(A) get smart\\n(B) boredom\\n(C) colds and flu\\n(D) taking tests\\n(E) spend time\"\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup endpoints created (optional)\n",
    "\n",
    "Endpoint deployments are chargeable and incurr costs on the subscription. Optionally clean up the endpoints after finishing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ml_client.serverless_endpoints.begin_delete(TEACHER_MODEL_ENDPOINT_NAME)\n",
    "_ = ml_client.serverless_endpoints.begin_delete(serverless_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
