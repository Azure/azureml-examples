{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #0078d4 0%, #106ebe 50%, #005a9e 100%); color: white; padding: 30px; border-radius: 12px; margin: 20px 0; box-shadow: 0 4px 15px rgba(0, 120, 212, 0.3);\">\n",
    "    <h1 style=\"margin: 0; text-align: center; font-size: 2.2em; font-weight: 600; letter-spacing: 0.5px; font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, 'Helvetica Neue', sans-serif;\">\n",
    "        Ignite Demo to Train, Customize, Optimize and Host Reasoning Models in AzureML\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\"> Sections Breakdown </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol style=\"color: #2c3e50; line-height: 1.8;\">\n",
    "<li>üîß <b>Setup Workspace:</b> Configure Azure ML workspace and authenticate</li>\n",
    "<li>üß† <b>RFT Training (GRPO):</b> Fine-tune reasoning model using Group Relative Policy Optimization</li>\n",
    "<li>‚ö° <b>RFT Training (Reinforce++):</b> Fine-tune using critic-free reinforcement learning</li>\n",
    "<li>üì¶ <b>Create Data Assets:</b> Convert pipeline outputs to reusable data assets</li>\n",
    "<li>üìä <b>Model Performance Comparison:</b> Evaluate and compare base model vs GRPO vs Reinforce++</li>\n",
    "<li>üéØ <b>Create Draft Model:</b> Train EAGLE3 draft model for speculative decoding</li>\n",
    "<li>üîó <b>Combine Draft and Base Model:</b> Package base and draft models for deployment</li>\n",
    "<li>üöÄ <b>Deploy Speculative Endpoint:</b> Deploy managed online endpoint with speculative decoding</li>\n",
    "<li>üì° <b>Deploy Base Endpoint:</b> Deploy baseline endpoint for performance comparison</li>\n",
    "<li>üß™ <b>Test Base and Speculative Decoding Endpoints:</b> Validate both endpoints with inference requests</li>\n",
    "<li>üìà <b>Endpoints Performance Evaluation:</b> Compare metrics between base and speculative decoding endpoints</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">Prerequisites & Requirements</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Requirements\n",
    "* **Training:** Standard_ND96isr_H100_v5, Standard_ND96amsr_A100_v4\n",
    "* **Deployment:** Kubernetes cluster with GPU instances (octagpu)\n",
    "##### Dataset & Models\n",
    "* **Dataset:** [FinQA](https://finqasite.github.io/) - 2.8k financial reports with 8k Q&A pairs\n",
    "* **Models:** [Llama-3.1-8B-Instruct-FP8](https://huggingface.co/nvidia/Llama-3.1-8B-Instruct-FP8), [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #e7f3ff; border: 1px solid #b3d9ff; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
    "    <p style=\"margin: 0; color: #0066cc;\">\n",
    "        <strong>üí° Note:</strong> Ensure your Azure ML workspace has access to the required compute resources and GPU instances before proceeding with the training and deployment steps.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #0078d4 0%, #106ebe 50%, #005a9e 100%); color: white; padding: 30px; border-radius: 12px; margin: 20px 0; box-shadow: 0 4px 15px rgba(0, 120, 212, 0.3);\">\n",
    "    <h1 style=\"margin: 0; text-align: center; font-size: 2.2em; font-weight: 600; letter-spacing: 0.5px; font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, 'Helvetica Neue', sans-serif;\">\n",
    "        RFT Finetuning - GRPO & Reinforce Plus Plus\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">‚öôÔ∏è Section 1. Setup Workspace and Register Components</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "<p>This section establishes connectivity to your workspace and sets up the required authentication.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scripts.utils import setup_workspace\n",
    "from scripts.dataset import prepare_finqa_dataset\n",
    "from scripts.run import get_run_metrics\n",
    "from scripts.reinforcement_learning import run_rl_training_pipeline\n",
    "from scripts.evaluation import run_evaluation_pipeline\n",
    "from scripts.speculative_decoding import (\n",
    "    run_draft_model_pipeline,\n",
    "    prepare_combined_model_for_deployment,\n",
    "    deploy_speculative_decoding_endpoint,\n",
    ")\n",
    "from scripts.deployment import create_managed_deployment, test_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Azure ML workspace and registry connections\n",
    "ml_client, registry_ml_client = setup_workspace(\n",
    "    config_path=\"./config.json\", registry_name=\"Ignite_2025_Demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Prepare dataset for Finetuning. This would save train, test and valid dataset under data folder</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path, test_data_path, valid_data_path = prepare_finqa_dataset(\n",
    "    ml_client, data_dir=\"data\", register_datasets=False\n",
    ")  # Prepare the FinQA dataset for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### üìñ Components and Pipelines used in this notebook can be installed locally by following the instructions listed here : [Ignite Components and Pipelines](Ignite_Components_And_Pipelines/README.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üß© Section 2. Run RFT Training Pipeline (GRPO)</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>GRPO (Group Relative Policy Optimization) is an advanced reinforcement learning technique for fine-tuning LLMs that uses relative learning instead of absolute rewards by comparing model outputs within groups/batches. \n",
    "<ul><li>This approach processes multiple responses simultaneously to learn relative preferences through direct policy optimization using reinforcement learning signals and preference learning from human feedback or reward models.</li> \n",
    "<li>Common use cases include instruction following improvement, mathematical reasoning enhancement, code generation optimization, and general conversational AI alignment. </li>\n",
    "<li>In this notebook, we use GRPO to fine-tune an LLM on financial reasoning tasks, improving the model's ability to solve complex financial questions with step-by-step reasoning.</li>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The RFT run will output multiple model checkpoints base on value of <b>trainer_save_freq</b> which is defined in config.\n",
    "<p>\n",
    "<i>For example, if this value is 20, the model checkpoint is stored for every 20th optimization step of the trainer. \n",
    "Where model checkpoint is a fully deployable copy of model's weights fine-tuned until that point.</i></p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete RL training pipeline: train model, register model\n",
    "grpo_job, status, grpo_registered_model = run_rl_training_pipeline(\n",
    "    ml_client=ml_client,\n",
    "    registry_ml_client=registry_ml_client,\n",
    "    base_model_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",  # Huggingface ID ot the model which is to be RFT finetuned.\n",
    "    compute_cluster=\"k8s-a100-compute\",  # Name of the Kubernetes Cluster in Workspace\n",
    "    rl_method=\"grpo\",  # RL methodology to be selected for training run.\n",
    "    train_data_path=train_data_path,  # Path to training dataset\n",
    "    valid_data_path=valid_data_path,  # Path to validation dataset\n",
    "    config={\n",
    "        \"num_nodes_finetune\": 1,  # Training specific arguments which can be overridden by user.\n",
    "        \"trainer_total_epochs\": 1,\n",
    "        \"trainer_save_freq\": 20,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üß©Section 3. Run RFT Training Pipeline ( Reinforce++ )</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "<p>Reinforce++ is a critic-free reinforcement learning framework that addresses key limitations of traditional RLHF algorithms like PPO by introducing Global Advantage Normalization instead of prompt-level normalization.\n",
    "<ul><li>This method eliminates the computational and memory overhead of critic networks while providing more stable and theoretically sound advantage estimation by normalizing across entire global batches rather than small prompt-specific groups.</li>\n",
    "<li>Reinforce++ offers significant advantages including removal of critic network overhead, theoretically unbiased estimation (bias vanishes as batch size increases), superior stability compared to local normalization methods like GRPO/RLOO, and better resistance to overfitting in RLHF scenarios.</li>\n",
    "<li>In this notebook, we use Reinforce++ to fine-tune an LLM on financial reasoning tasks, leveraging its global advantage normalization to achieve more stable policy updates and superior performance in complex agentic reasoning scenarios.</li>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The RFT run will output multiple model checkpoints base on value of <b>trainer_save_freq</b> which is defined in config.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete RL training pipeline: verify datasets, register data, train model, register model\n",
    "rlpp_job, status, rlpp_registered_model = run_rl_training_pipeline(\n",
    "    ml_client=ml_client,\n",
    "    registry_ml_client=registry_ml_client,\n",
    "    base_model_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",  # Huggingface ID ot the model which is to be RFT finetuned.\n",
    "    compute_cluster=\"k8s-a100-compute\",  # Name of the Kubernetes Cluster in workspace.\n",
    "    rl_method=\"reinforce_plus_plus\",  # RL methodology to be selected for training run.\n",
    "    train_data_path=train_data_path,  # Path to training dataset\n",
    "    valid_data_path=valid_data_path,  # Path to validation dataset\n",
    "    config={\n",
    "        \"num_nodes_finetune\": 1,\n",
    "        \"trainer_total_epochs\": 1,  # Training specific arguments which can be overridden by user.\n",
    "        \"trainer_save_freq\": 20,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üìäSection 4. Compare Model Performance across Base Model vs GRPO vs Reinforce++ </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "\n",
    "<p>This section evaluates and compares the performance of different finetuned models and base model across key metrics:</p>\n",
    "\n",
    "\n",
    "\n",
    "<p><strong>Evaluation Process:</strong></p>\n",
    "<ul>\n",
    "<li>Tests multiple checkpoints from each training method</li>\n",
    "<li>Evaluates on FinQA validation dataset for financial reasoning accuracy</li>\n",
    "<li>Provides comprehensive metrics to determine the best performing model</li>\n",
    "</ul>\n",
    "\n",
    "<p><em>üí° The evaluation will help identify which RL method produces the most effective model for financial reasoning tasks.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We will now submit evaluation job, with grpo and rlpp model outputs </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_job, status = (\n",
    "    run_evaluation_pipeline(  # Function which invokes the model evaluation pipeline.\n",
    "        ml_client=ml_client,\n",
    "        registry_ml_client=registry_ml_client,\n",
    "        compute_cluster=\"k8s-a100-compute\",\n",
    "        grpo_model_dir=grpo_registered_model.path,  # Output from GPRO RL provided as data asset created from earlier step.\n",
    "        rlpp_model_dir=rlpp_registered_model.path,  # Output from Reinforce_plus_plus RL provided as data asset created from earlier step.\n",
    "        validation_dataset_path=test_data_path,  # Path to test dataset\n",
    "        run_config={\n",
    "            \"num_nodes\": 1,  # Number of nodes to be used for evaluation run.\n",
    "            \"number_of_gpu_to_use\": 8,  # Number of GPUs in a node to be used for evaluation run.\n",
    "            \"base_path_1_label\": \"GRPO\",  # Label to identify GRPO model outputs.\n",
    "            \"base_path_2_label\": \"RLPP\",  # Label to identify RLPP model outputs.\n",
    "            \"explore_pattern_1\": \"global_step_{checkpoint}/actor/lora_adapter/\",\n",
    "            \"explore_pattern_2\": \"global_step_{checkpoint}/actor/lora_adapter/\",\n",
    "            \"checkpoint_values_1\": \"12\",\n",
    "            \"checkpoint_values_2\": \"12\",\n",
    "            \"use_lora_adapters_1\": True,\n",
    "            \"use_lora_adapters_2\": True,\n",
    "            \"evaluate_base_model\": True,  # Set to True to evaluate base model along with RL finetuned models.\n",
    "            \"hf_model_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",  # Huggingface ID of the base model\n",
    "            \"max_prompt_length\": 8196,\n",
    "            \"max_response_length\": 1024,\n",
    "            \"dtype\": \"bfloat16\",\n",
    "            \"tensor_parallel_size\": 4,\n",
    "        },  # Configuration parameters for evaluation run.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now, lets fetch metrics from evalution run inorder to show comparison</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = get_run_metrics(eval_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_metrics = {k: v for k, v in eval_metrics.items() if \"base_model\" in k}\n",
    "GRPO_metrics = {k: v for k, v in eval_metrics.items() if \"GRPO\" in k}\n",
    "RLPP_metrics = {k: v for k, v in eval_metrics.items() if \"RLPP\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_base_accuracy = (\n",
    "    min([v for k, v in BASE_metrics.items() if \"min\" in k]) if BASE_metrics else 0\n",
    ")\n",
    "max_grpo_accuracy = (\n",
    "    max([v for k, v in GRPO_metrics.items() if \"max\" in k]) if GRPO_metrics else 0\n",
    ")\n",
    "max_rlpp_accuracy = (\n",
    "    max([v for k, v in RLPP_metrics.items() if \"max\" in k]) if RLPP_metrics else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>GRPO vs Reinforce++ vs Base Model Performance Comparison</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"Baseline Model\", \"GRPO Model\", \"RL++ Model\"]\n",
    "values = [min_base_accuracy, max_grpo_accuracy, max_rlpp_accuracy]\n",
    "\n",
    "plt.bar(categories, values, color=[\"blue\", \"orange\", \"green\"])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Model Type\", fontsize=12, labelpad=10, color=\"#BC1B1B\")\n",
    "plt.ylabel(\"Accuracy\", fontsize=12, labelpad=10, color=\"#BC1B1B\")\n",
    "plt.title(\n",
    "    \"Graph Comparing Baseline, GRPO, and RL++ Model Accuracies\", pad=10, color=\"#BC1B1B\"\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The evaluation results demonstrate that both GRPO and Reinforce++ fine-tuning methods significantly improve financial reasoning performance compared to the base model. \n",
    "These accuracy metrics help identify the optimal checkpoint for deployment in the speculative decoding pipeline.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #0078d4 0%, #106ebe 50%, #005a9e 100%); color: white; padding: 30px; border-radius: 12px; margin: 20px 0; box-shadow: 0 4px 15px rgba(0, 120, 212, 0.3);\">\n",
    "    <h1 style=\"margin: 0; text-align: center; font-size: 2.2em; font-weight: 600; letter-spacing: 0.5px; font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, 'Helvetica Neue', sans-serif;\">\n",
    "        Speculative Decoding\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following sections would cover creation of draft model, combining base and draft model, deploying speculative decoding model, as well as endpoint benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üß©Section 5. Create Draft Model for Speculative Decoding</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "<p>EAGLE3 (Enhanced Adaptive Generation with Lookahead for Efficient Execution) is the latest advancement in speculative decoding that provides significant performance improvements:</p>\n",
    "\n",
    "<ul>\n",
    "<li><strong>Direct Token Prediction with Multi-layer Fusion:</strong> Abandons feature prediction for direct token prediction using advanced multi-layer feature fusion, enabling more accurate speculation and full benefit from scaled training data</li>\n",
    "<li><strong>Superior Performance:</strong> Achieves speedup ratios up to 6.5x (1.4x improvement over EAGLE-2) while maintaining identical output quality through advanced speculative decoding techniques</li>\n",
    "</ul>\n",
    "\n",
    "<p>This pipeline creates a specialized draft model that works alongside the base model to enable dramatically improved inference performance for reasoning tasks. The EAGLE3 approach is particularly effective for complex financial reasoning scenarios where maintaining accuracy while achieving significant speed improvements is crucial.</p>\n",
    "\n",
    "<p><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2503.01840\">https://arxiv.org/abs/2503.01840</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EAGLE3 draft model for speculative decoding\n",
    "draft_job, draft_status = run_draft_model_pipeline(\n",
    "    ml_client=ml_client,\n",
    "    registry_ml_client=registry_ml_client,\n",
    "    compute_cluster=\"k8s-a100-compute\",  # Name of the Kubernetes Cluster in Workspace.\n",
    "    num_epochs=1,  # Number of train epochs to be run by draft trainer.\n",
    "    monitor=False,  # Set to True to wait for completion.\n",
    "    base_model_mlflow_path=\"azureml://registries/azureml-meta/models/Meta-Llama-3-8B-Instruct/versions/9\",\n",
    "    draft_train_data_path=\"./data_for_draft_model/train/sharegpt_train_small.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üîÑSection 6. Prepare Combined Model for Deployment</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<p>For creation of a <strong>speculative decoding endpoint</strong>, we need <strong>two models</strong> working in tandem:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Base Model:</strong> The primary model (e.g., Llama-3.1-8B-Instruct-FP8) that generates high-quality outputs</li>\n",
    "    <li><strong>Draft Model:</strong> The EAGLE3 model that quickly generates candidate tokens for speculation</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Why Combine Into Single AML Model?</strong></p>\n",
    "\n",
    "<p>We'll package both models into a <strong>single Azure ML model</strong> to:</p>\n",
    "<ul>\n",
    "    <li>Simplify deployment to Azure ML online endpoints</li>\n",
    "    <li>Ensure both models are versioned and managed together</li>\n",
    "    <li>Streamline the endpoint creation process</li>\n",
    "    <li>Enable seamless speculative decoding inference</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download draft model, download base model, combine and register for deployment\n",
    "combined_model = prepare_combined_model_for_deployment(\n",
    "    ml_client=ml_client,\n",
    "    registry_ml_client=registry_ml_client,\n",
    "    draft_job_name=draft_job.name,  # Previous Draft Trainer job name for downloading draft model.\n",
    "    base_model_hf_id=\"nvidia/Llama-3.1-8B-Instruct-FP8\",  # Huggingface ID of the base model paired along with draft model.\n",
    "    model_name=\"speculative-decode-model\",  # User provided model name for combined model.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üöÄSection 7. Deploy Speculative Decoding Endpoint</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This section creates and deploys a <strong>managed online endpoint</strong> that leverages the combined model for speculative decoding inference.</p>\n",
    "<strong>What happens during deployment:</strong>\n",
    "<ul>\n",
    "    <li><strong>Endpoint Creation:</strong> Sets up a managed online endpoint in Azure ML.</li>\n",
    "    <li><strong>Model Loading:</strong> Loads both the base model and EAGLE3 draft model onto GPU instances, setting it up for inference.</li>\n",
    "</ul>\n",
    "<p>The deployment process typically takes 15-20 minutes depending on instance availability.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy managed online endpoint with speculative decoding\n",
    "endpoint_name = deploy_speculative_decoding_endpoint(\n",
    "    ml_client=ml_client,  # ML Client which specifies the workspace where endpoint gets deployed.\n",
    "    combined_model=combined_model,  # Reference from previous steps where combined model is created.\n",
    "    instance_type=\"octagepu\",  # Instance type Kubernetes Cluster\n",
    "    compute_name=\"k8s-a100-compute\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üöÄSection 8. Deploy Base Model Endpoint for Comparison</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This section creates and deploys a <strong>managed online endpoint</strong> with just the base model for performance comparison against the speculative decoding endpoint.</p>\n",
    "\n",
    "<strong>What happens during deployment:</strong>\n",
    "<ul>\n",
    "    <li><strong>Endpoint Creation:</strong> Sets up a standard managed online endpoint in Azure ML.</li>\n",
    "    <li><strong>Base Model Loading:</strong> Loads only the base model onto GPU instances for standard inference.</li>\n",
    "    <li><strong>Performance Baseline:</strong> Provides a baseline to measure the speedup achieved by speculative decoding.</li>\n",
    "</ul>\n",
    "\n",
    "<p>This baseline endpoint allows you to compare inference speed between standard generation and speculative decoding approaches.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy managed online endpoint with base model\n",
    "base_endpoint_name = create_managed_deployment(  # Function to create endpoint for base model.\n",
    "    ml_client=ml_client,  # ML Client which specifies the workspace where endpoint gets deployed.\n",
    "    model_asset_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",  # Huggingface ID of the base model.\n",
    "    instance_type=\"Standard_ND96amsr_A100_v4\",  # Compute SKU on which base model will be deployed.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üß™Section 9. Test Deployment</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This section tests both the speculative decoding endpoint and base model endpoint.</p>\n",
    "\n",
    "<strong>What happens during testing:</strong>\n",
    "<ul>\n",
    "    <li><strong>Endpoint Validation:</strong> Confirms both endpoints are responding correctly to inference requests.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The testing process validates that the deployed models can handle requests and respond successfully.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speculative_result = test_deployment(\n",
    "    ml_client, endpoint_name\n",
    ")  # Test the deployed endpoint with a financial reasoning question\n",
    "base_result = test_deployment(\n",
    "    ml_client, base_endpoint_name\n",
    ")  # Test the deployed endpoint with a financial reasoning question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\"> </span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin: 0; text-align: center;\">üìäSection 10. Performance Evaluation Pipeline</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This section launches a comprehensive evaluation pipeline to compare performance metrics between the base model endpoint and speculative decoding endpoint.</p>\n",
    "\n",
    "\n",
    "<p><strong>What happens during evaluation:</strong></p>\n",
    "<ul>\n",
    "    <li><strong>Performance Comparison:</strong> Analyzes speed improvements achieved by speculative decoding</li>\n",
    "    <li><strong>Statistical Analysis:</strong> Provides detailed metrics and visualizations of performance gains</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation job to compare base model and speculative decoding endpoints' performance\n",
    "evaluation_job = run_evaluation_speculative_decoding(\n",
    "    ml_client=ml_client,\n",
    "    base_endpoint_name=base_endpoint_name,  # Base model endpoint from previous step.\n",
    "    speculative_endpoint_name=endpoint_name,  # Speculative endpoint from previous step.\n",
    "    base_model=\"meta-llama/Meta-Llama-3-8B-Instruct\",  # HuggingFace repo ID of the model used in base endpoint, used for tokenization.\n",
    "    speculative_model=\"meta-llama/Meta-Llama-3-8B-Instruct\",  # HuggingFace repo ID of the model used in speculative decoding endpoint, used for tokenization.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following metrics are used to evaluate the performance of the endpoints:\n",
    " \n",
    "- **Input Throughput (Tokens/sec)**: Measures how many input tokens per second the model/server can process.\n",
    "- **Output Throughput (Tokens/sec)**: Measures how many output tokens per second the model/server can generate.\n",
    "- **Request Throughput (Requests/sec)**: Measures how many complete requests the model/server can handle per second.\n",
    " \n",
    "It is expected that the **speculative decoding endpoint will outperform the base model endpoint** across all these metrics, demonstrating the efficiency gains achieved through speculative decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"metrics-base-target-spec-dec.png\" alt=\"Performance Metrics: Base Model vs Speculative Decoding\" style=\"max-width: 100%; height: auto;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
