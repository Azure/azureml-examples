{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Synthetic Data Generation with Large Language Models\n",
    "## Notebook details\n",
    "This notebook generates synthetic data with an LLM on a sample NLI dataset.\n"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Install the dependencies in your environment\n",
    "\n",
    "Install the libraries/dependencies required to run the python code."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "%pip install datasets\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow\n",
    "%pip install fsspec"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TASK : NLI Synthetic Data generation"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Natural Language Inference (NLI)\n",
    "\n",
    "Synthetic data generation is targeted towards cases where user does not have labeled data, so teacher LLM is used to create high quality, synthetic labels for the data.\n",
    "\n",
    "This notebook assumes the data to have the above three fields: 'premise', 'hypothesis'. The 'label' can optionally be used to compute metrics based on original ground truth. However, the purpose of synthetic data generation is to replace the labels with the high quality labels generated by a large, capable LLM.\n",
    "\n",
    "Natural Language Inference or Recognizing Textual Entailment (RTE) is the task of classifying a pair of premise and hypothesis sentences into three classes: **contradiction, neutral, and entailment**. For example:\n",
    "\n",
    "| premise                                           | hypothesis                                             | label         |\n",
    "|---------------------------------------------------|--------------------------------------------------------|---------------|\n",
    "| A man inspects the uniform of a figure in some East Asian country. | The man is sleeping.                                   | contradiction |\n",
    "| An older and younger man smiling.                 | Two men are smiling and laughing at the cats playing on the floor. | neutral       |\n",
    "| A soccer game with multiple males playing.        | Some men are playing a sport.                          | entailment    |\n",
    "\n"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Consume input dataset\n",
    "\n",
    "The classes in this cell handle the responsibility of ingesting the input dataset. Dataset can be anything, HuggingFace, Locally hosted, JSON, string etc. For our NLI example, we have written a `NLIHuggingFaceInputDataset` class to ingests input from HuggingFace datasets.\n",
    "\n",
    "Example NLI Dataset looks like the following:\n",
    "```json\n",
    "{\n",
    "    \"premise\": \"Aside from the Indigenous population, nearly all Argentines or their ancestors immigrated within the past five centuries.\",\n",
    "    \"hypothesis\": \"Aside from the Indigenous population, some Argentines or their ancestors immigrated within the past five centuries.\",\n",
    "    \"label\": 0\n",
    "}\n",
    "\n",
    "Labels 0, 1, 2 correspond to entailment, neutral and contradiction respectively."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import NLIHuggingFaceInputDataset\n",
    "\n",
    "# We can define train and test sample sizes here.\n",
    "train_sample_size = 2\n",
    "val_sample_size = 2\n",
    "test_sample_size = 2\n",
    "\n",
    "# Sample notebook using the dataset: https://huggingface.co/datasets/cestwc/conjnli\n",
    "dataset_name = \"cestwc/conjnli\"\n",
    "input_dataset = NLIHuggingFaceInputDataset()\n",
    "\n",
    "# Note: train_split_name and test_split_name can vary by dataset. They are passed as arguments in load_hf_dataset.\n",
    "# If val_split_name is None, the below function will split the train set to create the specified sized validation set.\n",
    "train, val, test = input_dataset.load_hf_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    train_sample_size=train_sample_size,\n",
    "    val_sample_size=val_sample_size,\n",
    "    test_sample_size=test_sample_size,\n",
    "    train_split_name=\"adversarial\",\n",
    "    val_split_name=None,\n",
    "    test_split_name=\"dev\",\n",
    ")\n",
    "\n",
    "print(\"Len of train data sample is \" + str(len(train)))\n",
    "print(\"Len of validation data sample is \" + str(len(val)))\n",
    "print(\"Len of test data sample is \" + str(len(test)))"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721738954273
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check format of data"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train[0]"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721738970221
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Generate prompt for inference\n",
    "\n",
    "We generate the prompts in the required format to be able to output a desired answer.\n",
    "\n",
    "So the previous cell prompt \n",
    "```json\n",
    "{\n",
    "    \"premise\": \"Aside from the Indigenous population, nearly all Argentines or their ancestors immigrated within the past five centuries.\",\n",
    "    \"hypothesis\": \"Aside from the Indigenous population, some Argentines or their ancestors immigrated within the past five centuries.\",\n",
    "    \"label\": 0\n",
    "}\n",
    "```\n",
    "**transforms to**\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Write out in a step by step manner your reasoning about the answer using no more than 80 words. Based on the reasoning, produce the final answer. Your response should be in JSON format without using any backticks. The JSON is a dictionary whose keys are 'reason' and 'answer_choice'.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Given the following two texts, your task is to determine the logical relationship between them. The first text is the 'premise' and the second text is the 'hypothesis'. The relationship should be labeled as one of the following: 'entailment' if the premise entails the hypothesis, 'contradiction' if the premise contradicts the hypothesis, or 'neutral' if the premise neither entails nor contradicts the hypothesis.\\n\\nPremise: Aside from the Indigenous population, nearly all Argentines or their ancestors immigrated within the past five centuries.\\nHypothesis:Aside from the Indigenous population, some Argentines or their ancestors immigrated within the past five centuries.\\n\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " #### We have abstracted out this functionality in a separate class which you can use as follows."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# An example of how a final NLI prompt looks like\n",
    "from utils import NLIPromptGenerator\n",
    "\n",
    "# You can set the enable chain of thought flag to True to enable CoT prompting\n",
    "\n",
    "nli_prompt_generator = NLIPromptGenerator(enable_chain_of_thought=True)\n",
    "nli_prompt_generator.generate_prompt(train[0])"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721739016231
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Setup inference with Azure ML endpoints\n",
    "\n",
    "### First deploy the teacher model in Azure AI Studio\n",
    "* Go to Azure AI Studio (ai.azure.com)\n",
    "* Select Meta-Llama-3.1-405B-Instruct model from Model catalog.\n",
    "* Deploy with \"Pay-as-you-go\"\n",
    "* Once deployed successfully, you should be assigned for an API endpoint and a security key for inference.\n",
    "\n",
    "The following cell builds the Azure ML endpoints to be able to get outputs from the LLama endpoint set up in Azure. You can directly use the `AzureInference` class that handles this."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import AzureInference\n",
    "\n",
    "url = \"<Chat completion teacher model endpoint URL>\"\n",
    "key = \"<API key>\"\n",
    "\n",
    "az_llama_405b_model_inf = AzureInference(url=url, key=key)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721739068307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Build the final dataset with synthetic labels\n",
    "\n",
    "In the following cell, we utilize the previously built classes to get input dataset, prompt engineer it, call the LLM from Azure ML endpoints, generate the output and write it to a file.\n",
    "Sample final output: \n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Write out in a step by step manner your reasoning about the answer using no more than 80 words. Based on the reasoning, produce the final answer. Your response should be in JSON format without using any backticks. The JSON is a dictionary whose keys are 'reason' and 'answer_choice'.\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Given the following two texts, your task is to determine the logical relationship between them. The first text is the 'premise' and the second text is the 'hypothesis'. The relationship should be labeled as one of the following: 'entailment' if the premise entails the hypothesis, 'contradiction' if the premise contradicts the hypothesis, or 'neutral' if the premise neither entails nor contradicts the hypothesis.\\n\\nPremise: None but Jake managed to win their game.\\nHypothesis: Jake managed to win their game.\",\n",
    "            \"role\": \"user\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"entailment\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "The answer \"entailment\" in the above sample JSON is generated as a response by the LLM. We wrap it as a response generated by the \"assistant\"."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### We have abstracted out the above functionality in `NLISyntheticDatasetBuilder` which builds prompts, calls Llama endpoint, and then writes the final dataset in your local directory."
   ],
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import NLISyntheticDatasetBuilder\n",
    "\n",
    "nli_dataset_builder = NLISyntheticDatasetBuilder(\n",
    "    nli_prompt_generator, inference_pointer=az_llama_405b_model_inf\n",
    ")\n",
    "\n",
    "# Write synthetic training and validation data to local directory.\n",
    "nli_dataset_builder.build_dataset(train, file_name=\"train_nli\")\n",
    "nli_dataset_builder.build_dataset(val, file_name=\"valid_nli\")"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1721739108156
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "name": "python310-sdkv2",
   "language": "python",
   "display_name": "Python 3.10 - SDK v2"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}