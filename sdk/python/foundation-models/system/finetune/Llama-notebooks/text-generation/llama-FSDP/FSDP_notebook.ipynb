{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Distributed training of LLaMA models using FSDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Requirements/Prerequisites\n",
    "- An Azure acoount with active subscription [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- Azure Machine Learning workspace [Configure workspace](../../../configuration.ipynb) \n",
    "- Python Environment\n",
    "- Install Azure ML Python SDK Version 2\n",
    "### Learning Objectives\n",
    "- Connect to workspace using Python SDK v2\n",
    "- use LLaMA model for text-generation task\n",
    "- Distributed finetuning of LLaMA 7b/13b/70b model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "### 1.1 Import Libraries and connect to workspace using Default Credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694211066967
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as e:\n",
    "    # enter details of your AML workspace and get a handle to the workspace\n",
    "    ml_client = MLClient(\n",
    "        credential=credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<AML_WORKSPACE_NAME>\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Compute target setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694211067297
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "compute_name = \"ND40rs-low-priority-3\"\n",
    "compute_cluster = None\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute(\n",
    "        name=compute_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"STANDARD_ND40RS_V2\",\n",
    "        idle_time_before_scale_down=300,\n",
    "        min_instances=0,\n",
    "        max_instances=6,\n",
    "        tier=\"low_priority\",\n",
    "    )\n",
    "    ml_client.begin_create_or_update(compute_config).result()\n",
    "    compute_cluster = ml_client.compute.get(compute_name)\n",
    "compute_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create a new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694211067460
    }
   },
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "env_name = \"LLaMA-FSDP\"\n",
    "try:\n",
    "    env_object = ml_client.environments.list(env_name).next()\n",
    "    print(f\"Found exising environment. {env_object}\")\n",
    "except ResourceNotFoundError as ex:\n",
    "    print(f\"Environment {env_name} not found. Creating a new one.\")\n",
    "    env_docker_context = Environment(\n",
    "        build=BuildContext(path=\"./env/context\"),\n",
    "        name=env_name,\n",
    "        description=\"Environment created for trying FSDP\",\n",
    "    )\n",
    "    env_object = ml_client.environments.create_or_update(env_docker_context)\n",
    "    print(env_object)\n",
    "fsdp_env = f\"{env_name}@latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2. Launch the distributed training job\n",
    "\n",
    "### 2.1 Create the job \n",
    "\n",
    "In this section we will be configuring and running two standalone jobs. \n",
    "\n",
    "- `command` for distributed training job.\n",
    "\n",
    "\n",
    "The `command` allows user to configure the following key aspects.\n",
    "- `code` - This is the path where the code to run the command is located\n",
    "- `command` - This is the command that needs to be run\n",
    "- `inputs` - This is the dictionary of inputs using name value pairs to the command. The key is a name for the input within the context of the job and the value is the input value. Inputs can be referenced in the `command` using the `${{inputs.<input_name>}}` expression. To use files or folders as inputs, we can use the `Input` class. The `Input` class supports three parameters:\n",
    "    - `type` - The type of input. This can be a `uri_file` or `uri_folder`. The default is `uri_folder`.         \n",
    "    - `path` - The path to the file or folder. These can be local or remote files or folders. For remote files - http/https, wasb are supported. \n",
    "        - Azure ML `data`/`dataset` or `datastore` are of type `uri_folder`. To use `data`/`dataset` as input, you can use registered dataset in the workspace using the format '<data_name>:<version>'. For e.g Input(type='uri_folder', path='my_dataset:1')\n",
    "    - `mode` - \tMode of how the data should be delivered to the compute target. Allowed values are `ro_mount`, `rw_mount` and `download`. Default is `ro_mount`\n",
    "- `environment` - This is the environment needed for the command to run. Curated or custom environments from the workspace can be used. Or a custom environment can be created and used as well. Check out the [environment](../../../../assets/environment/environment.ipynb) notebook for more examples.\n",
    "- `compute` - The compute on which the command will run.\n",
    "- `distribution` - Distribution configuration for distributed training scenarios. Azure Machine Learning supports PyTorch, TensorFlow, and MPI-based distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694211067849
    }
   },
   "outputs": [],
   "source": [
    "# Get LLaMA Model path\n",
    "registry_name = (\n",
    "    \"azureml-meta\"  # Change this to your registry name Where model is present\n",
    ")\n",
    "model_name = \"Llama-2-70b\"  # Change this to your model name\n",
    "\n",
    "registry_ml_client = MLClient(credential, registry_name=registry_name)\n",
    "model = registry_ml_client.models.get(model_name, label=\"latest\")\n",
    "model.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694211067983
    }
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "# All the setting defined in config files(./code/configs) can be passed as args through CLI. A subset of arguments are listed below.\n",
    "\n",
    "training_parameters = {\n",
    "    \"task_name\": \"text-generation\",  # text-classification, text-generation\n",
    "    # text-classification(for datasets such as emotion detection),\n",
    "    # text-generation(for samsum, alpaca dataset, grammar dataset)\n",
    "    \"dataset\": \"samsum_dataset\",  # \"emotion_detection_dataset\", \"samsum_dataset\" , \"grammar_dataset\", \"alpaca_dataset\"\n",
    "    # \"task_type\":  \"CAUSAL_LM\", # SEQ_CLS, TOKEN_CLS, CAUSAL_LM\n",
    "    # task type to be used for peft. Default is \"CAUSAL_LM\".\n",
    "    \"enable_fsdp\": True,  # Flag to enable FSDP mode\n",
    "    \"use_peft\": True,\n",
    "    \"use_fp16\": True,\n",
    "    \"cpu_offload\": True,  # Setting this to true will do the cpu_offloading(ZeRO).\n",
    "    \"low_cpu_fsdp\": True,  # Setting this to true will reduce the cpu memory footprint. For finetuing 70b on ND40, turn this on.\n",
    "    # It uses meta tensors to save to memory. It load the full model on rank zero and model with meta tensors on other ranks initially.\n",
    "    # After doing model sharding, it will sync the model weights from rank zero, this would increase the training time, but reduce the cpu memory usage drastically.\n",
    "    \"save_model\": True,\n",
    "    \"batch_size_training\": 4,\n",
    "    \"val_batch_size\": 4,\n",
    "    \"lr\": 3e-4,\n",
    "    \"num_epochs\": 2,\n",
    "    \"generate_predictions\": True,\n",
    "}\n",
    "\n",
    "# sku details\n",
    "num_nodes = 7\n",
    "nproc_per_node = (\n",
    "    8\n",
    "    if compute_cluster.size in [\"STANDARD_ND40RS_V2\", \"STANDARD_ND96AMSR_A100_V4\"]\n",
    "    else 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694211068141
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "training_args = Namespace(**training_parameters)\n",
    "cmd_args = \"\"\n",
    "for arg_name, arg_val in training_args._get_kwargs():\n",
    "    if type(arg_val) == bool:\n",
    "        if arg_val:\n",
    "            cmd_args += f\"--{arg_name} \"\n",
    "    else:\n",
    "        cmd_args += f\"--{arg_name} {arg_val} \"\n",
    "cmd_args = cmd_args.strip()\n",
    "cmd_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694211323468
    }
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"model_path\": Input(type=AssetTypes.MLFLOW_MODEL, path=model.id),\n",
    "    \"model_config_path\": \"data/model\",\n",
    "}\n",
    "\n",
    "outputs = {\n",
    "    \"finetuned_model\": Output(\n",
    "        type=AssetTypes.CUSTOM_MODEL,\n",
    "    ),\n",
    "    \"artifacts_folder\": Output(\n",
    "        type=AssetTypes.URI_FOLDER,\n",
    "    ),\n",
    "}\n",
    "\n",
    "job = command(\n",
    "    code=\"./code\",\n",
    "    command=f\"python llama_finetuning.py \\\n",
    "        --model_name ${{inputs.model_path}}/{inputs['model_config_path']} \\\n",
    "        --output_dir ${{outputs.finetuned_model}} \\\n",
    "        --artifacts_dir ${{outputs.artifacts_folder}} {cmd_args}\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    compute=compute_name,\n",
    "    instance_count=num_nodes,\n",
    "    environment=fsdp_env,\n",
    "    distribution={\n",
    "        \"type\": \"pytorch\",\n",
    "        \"process_count_per_instance\": nproc_per_node,  # number of gpus on a single node\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 2.2 Run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1694214479745
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "returned_job = ml_client.create_or_update(job)\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
