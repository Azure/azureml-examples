{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation for Text Summarization with Large Language Models\n",
    "\n",
    "### Notebook details\n",
    "\n",
    "This sample demonstrates how to train the selected student model using the teacher model, for the text summarization task.\n",
    "\n",
    "We will use the Meta Llama 3.1 405B Instruct as the teacher model and the Meta Llama 3.1 8B Instruct as the student model.\n",
    "\n",
    "**Note :**\n",
    "\n",
    "- Distillation offering is only available in **West US 3** regions.\n",
    "- Distillation inputs should be in the chat completion format:\n",
    "\n",
    "  > {\"messages\": [ \\\n",
    "  > &nbsp;&nbsp;{\"role\": \"system\", \"content\": \"Instructions for summarization\"}, \\\n",
    "  > &nbsp;&nbsp;{\"role\": \"user\", \"content\": \"Text to summarize\"} \\\n",
    "  > ]}\n",
    "\n",
    "- The Meta Llama 3.1 405B Instruct model can only be used as a teacher model.\n",
    "- The Meta Llama 3.1 8B Instruct can only be used as a student (target) model.\n",
    "\n",
    "**Prerequisites :**\n",
    "\n",
    "- Subscribe to the Meta Llama 3.1 405B Instruct and Meta Llama 3.1 8B Instruct, see [how to subscribe your project to the model offering in MS Learn](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless?tabs=azure-ai-studio#subscribe-your-project-to-the-model-offering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install SDK v2 and verify the imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests\n",
    "# %pip install datasets\n",
    "# %pip install azure-ai-ml\n",
    "# %pip install azure-identity\n",
    "# %pip install tqdm\n",
    "\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import datasets\n",
    "import requests\n",
    "from azure.ai.ml import Input, MLClient\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Data, ServerlessEndpoint\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "For this task we will use the [griffin/chain_of_density](https://huggingface.co/datasets/griffin/chain_of_density) dataset. This dataset consists of 1000 news articles which we will use to train and test our endpoints.\n",
    "\n",
    "We will begin by downloading the dataset and preparing the data in chat completion format. AzureML expects both train and validation datasets for distillation. We will reserve some samples to test the distilled model. Hence we will split the data into 3 parts:\n",
    "\n",
    "| Split      | Size |\n",
    "| ---------- | ---- |\n",
    "| Train      | 500  |\n",
    "| Validation | 400  |\n",
    "| Test       | 100  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataset = datasets.load_dataset(\"griffin/chain_of_density\", name=\"unannotated\")[\"train\"]\n",
    "\n",
    "dataset = dataset.shuffle(seed=41)\n",
    "\n",
    "train_n = 500\n",
    "valid_n = 400\n",
    "test_n = 100\n",
    "\n",
    "train_dataset = dataset.select(range(train_n))\n",
    "valid_dataset = dataset.select(range(train_n, train_n + valid_n))\n",
    "test_dataset = dataset.select(range(train_n + valid_n, train_n + valid_n + test_n))\n",
    "\n",
    "# Keep assertions to guard against data changes in upstream\n",
    "assert len(train_dataset) == train_n\n",
    "assert len(valid_dataset) == valid_n\n",
    "assert len(test_dataset) == test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./data/\"\n",
    "if not os.path.exists(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "\n",
    "src_train_data_file_name = root_dir + \"src_train_\" + str(train_n) + \".txt\"\n",
    "src_valid_data_file_name = root_dir + \"src_valid_\" + str(valid_n) + \".txt\"\n",
    "src_test_data_file_name = root_dir + \"src_test_\" + str(test_n) + \".txt\"\n",
    "\n",
    "\n",
    "def write_to_rawfile(data, fname):\n",
    "    with open(fname, \"w\") as file:\n",
    "        for row in data:\n",
    "            file.write(row[\"article\"] + \"\\n\")\n",
    "\n",
    "\n",
    "# Write raw data files\n",
    "write_to_rawfile(train_dataset, src_train_data_file_name)\n",
    "write_to_rawfile(valid_dataset, src_valid_data_file_name)\n",
    "write_to_rawfile(test_dataset, src_test_data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in chat completion format\n",
    "cc_train_data_file_name = root_dir + \"cc_train_\" + str(train_n) + \".jsonl\"\n",
    "cc_valid_data_file_name = root_dir + \"cc_valid_\" + str(valid_n) + \".jsonl\"\n",
    "cc_test_data_file_name = root_dir + \"cc_test_\" + str(test_n) + \".jsonl\"\n",
    "\n",
    "SYSTEM_PROMPT = \"You will generate concise, entity-dense summary of the given article. Only generate the summary text. Do not exceed 80 words.\"\n",
    "\n",
    "\n",
    "def prepare_in_cc_format(filename):\n",
    "    dataset = []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": SYSTEM_PROMPT,\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Article: {line}\",\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_cc_data = prepare_in_cc_format(src_train_data_file_name)\n",
    "valid_cc_data = prepare_in_cc_format(src_valid_data_file_name)\n",
    "test_cc_data = prepare_in_cc_format(src_test_data_file_name)\n",
    "\n",
    "\n",
    "def write_to_ccfile(fname, data):\n",
    "    with open(fname, \"w\") as file:\n",
    "        for row in data:\n",
    "            file.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "\n",
    "write_to_ccfile(cc_train_data_file_name, train_cc_data)\n",
    "write_to_ccfile(cc_valid_data_file_name, valid_cc_data)\n",
    "write_to_ccfile(cc_test_data_file_name, test_cc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the connection to AzureML and obtaining registries\n",
    "\n",
    "From the AzureML portal ([https://portal.azure.com](https://portal.azure.com/#home)) obtain the subscripton, workspace and resource group info. A workspace in **West US 3** is required. Populate the below cell with the info. We will use this to obtain the workspace.\n",
    "\n",
    "We will also obtain the registries which hold the meta models (azureml-meta) and the distillation pipeline (azureml).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"75703df0-38f9-4e2e-8328-45f6fc810286\"\n",
    "RESOURCE_GROUP_NAME = \"rg-sasumai\"\n",
    "WORKSPACE_NAME = \"sasum-westus3-ws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Azure and get workspace and registries\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "# Refers to the current account to be used for experiments\n",
    "workspace_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP_NAME,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    ")\n",
    "\n",
    "# Regsitry which holds Meta models\n",
    "model_registry_client = MLClient(credential, registry_name=\"azureml-meta\")\n",
    "\n",
    "# Regsitry which holds the Distillation component\n",
    "distillation_registry_client = MLClient(credential, registry_name=\"maas-test-registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the prepared data to your workspace\n",
    "\n",
    "We will upload the train and validation files created in previous cells to our workspace. These will be consumed by our distillation pipeline. If data assets with the same name already exist in our workspace, the version will be updated. The latest version is used by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data to Azure workspace\n",
    "train_data_name = \"distillation-summ-exp-train\"\n",
    "valid_data_name = \"distillation-summ-exp-valid\"\n",
    "\n",
    "train_data = workspace_client.data.create_or_update(\n",
    "    Data(\n",
    "        path=cc_train_data_file_name,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Training dataset\",\n",
    "        name=train_data_name,\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_data = workspace_client.data.create_or_update(\n",
    "    Data(\n",
    "        path=cc_valid_data_file_name,\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Validation dataset\",\n",
    "        name=valid_data_name,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of density\n",
    "\n",
    "Chain-of-density is a new way to operate distillation in which the system prompt for the teacher is replaced by Azure with an enhanced prompt. This produces better quality of outputs from the teacher thus improving accuracy of distilled model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to false if you would like to use your own prompt anyway\n",
    "ENABLE_CHAIN_OF_DENSITY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure distillation params\n",
    "\n",
    "Parameters commonly changed are teacher_model_max_new_tokens, teacher_model_temperature and teacher_model_top_p. Meta-Llama-3.1-8B, Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B and Meta-Llama-3.1-70B-Instruct are supported as student models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure distillation teacher and student models, obtain the distillation component\n",
    "TEACHER_MODEL_ENDPOINT_NAME = \"Meta-Llama-3-1-405B-Instruct-efu\"\n",
    "STUDENT_MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "STUDENT_MODEL_VERSION = 1\n",
    "\n",
    "student_model = model_registry_client.models.get(\n",
    "    STUDENT_MODEL_NAME, STUDENT_MODEL_VERSION\n",
    ")\n",
    "\n",
    "DISTILLATION_PIPELINE_NAME = \"oss_distillation_pipeline\"\n",
    "distillation_pipeline_component = distillation_registry_client.components.get(\n",
    "    name=DISTILLATION_PIPELINE_NAME, version=\"0.0.6.testv7\"\n",
    ")\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def distillation_pipeline(\n",
    "    teacher_model_endpoint_name,\n",
    "    system_properties,\n",
    "    input_finetune_model,\n",
    "    registered_model_name,\n",
    "    train_file_data_asset,\n",
    "    valid_file_data_asset,\n",
    "):\n",
    "    oss_distillation = distillation_pipeline_component(\n",
    "        teacher_model_endpoint_name=teacher_model_endpoint_name,\n",
    "        teacher_model_max_new_tokens=(1024 if ENABLE_CHAIN_OF_DENSITY else 256),\n",
    "        teacher_model_temperature=0,\n",
    "        teacher_model_top_p=1,\n",
    "        enable_chain_of_density=ENABLE_CHAIN_OF_DENSITY,\n",
    "        train_file_path=train_file_data_asset,\n",
    "        validation_file_path=valid_file_data_asset,\n",
    "        # Finetune\n",
    "        mlflow_model_path=input_finetune_model,\n",
    "        model_asset_id=student_model.id,\n",
    "        registered_model_name=registered_model_name,\n",
    "        system_properties=system_properties,\n",
    "        ## hyperparams\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=5,\n",
    "        data_generation_task_type=\"SUMMARIZATION\",\n",
    "    )\n",
    "\n",
    "    return {\"output_model\": oss_distillation.outputs.output_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure system properties for the job, these help make the model appear in AI Studio\n",
    "system_properties = {\n",
    "    \"finetune_oss\": \"True\",\n",
    "    \"PipelineType\": \"Finetune\",\n",
    "    \"azureml.PipelineType\": \"Finetune\",\n",
    "    \"model_asset_id\": student_model.id,\n",
    "}\n",
    "\n",
    "json_str = json.dumps(system_properties).replace(\" \", \"\")\n",
    "\n",
    "system_properties_b64_encoded = base64.b64encode(json_str.encode(\"utf-8\")).decode(\n",
    "    \"utf-8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure distillation parameters\n",
    "short_id = str(uuid.uuid4())[:8]\n",
    "train_file_path_input = Input(type=\"uri_file\", path=train_data.path)\n",
    "valid_file_path_input = Input(type=\"uri_file\", path=valid_data.path)\n",
    "input_finetune_model = Input(type=\"mlflow_model\", path=student_model.id)\n",
    "experiment_name = f\"distillation-summarization\".replace(\".\", \"-\")\n",
    "# do not use underscores in the name, that's unsupported\n",
    "registered_model_name = \"my-summ-model-\" + short_id\n",
    "\n",
    "distillation_job = distillation_pipeline(\n",
    "    teacher_model_endpoint_name=TEACHER_MODEL_ENDPOINT_NAME,\n",
    "    system_properties=system_properties_b64_encoded,\n",
    "    input_finetune_model=input_finetune_model,\n",
    "    registered_model_name=registered_model_name,\n",
    "    train_file_data_asset=train_file_path_input,\n",
    "    valid_file_data_asset=valid_file_path_input,\n",
    ")\n",
    "\n",
    "distillation_job.display_name = f\"distillation-summarization\"\n",
    "distillation_job.experiment_name = experiment_name\n",
    "distillation_job.settings.default_compute_type = \"serverless\"\n",
    "distillation_job.continue_on_step_failure = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit pipeline job to workspace\n",
    "dst_job = workspace_client.jobs.create_or_update(distillation_job)\n",
    "workspace_client.jobs.stream(dst_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a serverless endpoint to consume the model\n",
    "\n",
    "Our distillation job is now complete, we will deploy the model from it as a serverless endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model url for registered endpoint\n",
    "# The version is 1 for the first run\n",
    "rg_model_vs = 1\n",
    "\n",
    "workspace = workspace_client.workspaces.get(WORKSPACE_NAME)\n",
    "rg_model_asset_id = (\n",
    "    \"azureml://locations/\"\n",
    "    f\"{workspace.location}\"\n",
    "    \"/workspaces/\"\n",
    "    f\"{workspace._workspace_id}\"\n",
    "    \"/models/\"\n",
    "    f\"{registered_model_name}\"\n",
    "    \"/versions/\"\n",
    "    f\"{rg_model_vs}\"\n",
    ")\n",
    "\n",
    "# Create serverless endpoint - names must be unique\n",
    "serverless_endpoint_name = \"my-endpoint-\" + short_id\n",
    "\n",
    "serverless_endpoint = ServerlessEndpoint(\n",
    "    name=serverless_endpoint_name,\n",
    "    model_id=rg_model_asset_id,\n",
    ")\n",
    "\n",
    "created_endpoint = workspace_client.serverless_endpoints.begin_create_or_update(\n",
    "    serverless_endpoint\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing the model on the test file _(optional)_\n",
    "\n",
    "We will evaluate the test dataset on the distilled endpoint and store it as a file. We will then evaluate the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTLLED_ENDPOINT_URL = created_endpoint.scoring_uri + \"/v1/chat/completions\"\n",
    "DISTLLED_ENDPOINT_KEY = workspace_client.serverless_endpoints.get_keys(\n",
    "    created_endpoint.name\n",
    ").primary_key\n",
    "\n",
    "\n",
    "op_test_data_file_name = root_dir + \"op_test_\" + str(test_n) + \".jsonl\"\n",
    "\n",
    "auth_headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + DISTLLED_ENDPOINT_KEY,\n",
    "}\n",
    "\n",
    "\n",
    "def get_summary_from_llm(messages):\n",
    "    data = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"max_new_tokens\": 256,\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url=DISTLLED_ENDPOINT_URL, headers=auth_headers, data=json.dumps(data)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response_dict = json.loads(response.text)\n",
    "        res = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(response.text, e)\n",
    "        res = \"error\"\n",
    "    return res\n",
    "\n",
    "\n",
    "summaries = []\n",
    "for item in tqdm(test_cc_data):\n",
    "    summary = get_summary_from_llm(item[\"messages\"])\n",
    "    summaries.append({\"article\": item[\"messages\"][1][\"content\"], \"summary\": summary})\n",
    "\n",
    "with open(op_test_data_file_name, \"w\") as file:\n",
    "    for summary in summaries:\n",
    "        file.write(json.dumps(summary) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model output\n",
    "\n",
    "We will score the output on a modified version of the entity-density metric. We capture the entities in the summary and retain only the relevant entities. A relevant entity is one that appears in the original article. This is divided by the size of the summary in tokens to adjust for overly verbose outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy\n",
    "# %pip install nltk\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def calculate_density(article, summary):\n",
    "    tokens = nltk.word_tokenize(summary)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    article_ents = [ent.text for ent in nlp(article).ents]\n",
    "    doc_ents = [ent.text for ent in nlp(summary).ents]\n",
    "    common_ents = [ent for ent in doc_ents if ent in article_ents]\n",
    "    num_entities = len(common_ents)\n",
    "\n",
    "    density = num_entities / num_tokens\n",
    "    return density, num_tokens\n",
    "\n",
    "\n",
    "SKIP_ERROR = True\n",
    "density_sum = 0\n",
    "token_sum = 0\n",
    "sample_cnt = 0\n",
    "for row in summaries:\n",
    "    article = row[\"article\"]\n",
    "    summary = row[\"summary\"]\n",
    "    if summary == \"error\" and SKIP_ERROR:\n",
    "        continue\n",
    "    density, num_tokens = calculate_density(article=article, summary=summary)\n",
    "    density_sum += density\n",
    "    token_sum += num_tokens\n",
    "    sample_cnt += 1\n",
    "\n",
    "average_density = round(density_sum / sample_cnt, 5)\n",
    "average_token_length = round(token_sum / sample_cnt, 5)\n",
    "print(\n",
    "    f\"The mean density is {average_density} with average token length of {average_token_length} accross {sample_cnt} samples\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "runbooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
