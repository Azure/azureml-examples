{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_load_data(data_file_path: str) -> bool:\n",
    "    \"\"\"Validate the user passed.\"\"\"\n",
    "    from datasets.load import load_dataset\n",
    "    from pathlib import Path\n",
    "    from functools import partial\n",
    "\n",
    "    if Path(data_file_path).is_dir():\n",
    "        print(\n",
    "            f\"WARNING! Directory, {data_file_path} is passed as input. Skipping validation!\"\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    # format to load function map\n",
    "    format_load_func_map = {\n",
    "        \"json\": partial(load_dataset, path=\"json\"),\n",
    "        \"parquet\": partial(load_dataset, path=\"parquet\"),\n",
    "        \"csv\": partial(load_dataset, path=\"csv\"),\n",
    "        \"tsv\": partial(load_dataset, path=\"csv\", sep=\"\\t\"),\n",
    "        \"jsonl\": partial(load_dataset, path=\"json\"),\n",
    "    }\n",
    "\n",
    "    # try fetching file format post the dot operator\n",
    "    loader_func = format_load_func_map.get(Path(data_file_path).suffix[1:], None)\n",
    "    if loader_func:\n",
    "        try:\n",
    "            _ = loader_func(data_files={\"train\": data_file_path}, split=\"train\")\n",
    "            return True\n",
    "        except Exception:\n",
    "            raise ValueError(\n",
    "                \"WARNING! Error while loading the dataset.\\n\"\n",
    "                \"Submitting the finetune job with this dataset is going to fail the run. Please fix the dataset and submit again.\"\n",
    "            )\n",
    "\n",
    "    print(\"WARNING! Unable to find the file format. Skipping validation.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def _validate_finetune_data(\n",
    "    train_file_path: str, validation_file_path: str, test_file_path: str\n",
    "):\n",
    "    \"\"\"Validate the data finetune data selected by the user.\"\"\"\n",
    "    print(\"Validating train file path\")\n",
    "    _validate_load_data(train_file_path.replace(\"azureml:\", \"\"))\n",
    "    print(\"Validating train file path\")\n",
    "    _validate_load_data(validation_file_path.replace(\"azureml:\", \"\"))\n",
    "    print(\"Test train file path\")\n",
    "    _validate_load_data(test_file_path.replace(\"azureml:\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.ai.ml.constants._compute import ComputeType\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "\n",
    "def _validate_finetune_compute(\n",
    "    compute_name: str, finetune_nodes: str, ws_ml_client: MLClient\n",
    "):\n",
    "    \"\"\"validate if the compute has nodes as requested by the user.\"\"\"\n",
    "    # check if compute exists\n",
    "    try:\n",
    "        compute = ws_ml_client.compute.get(compute_name)\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Couldn't find compute with name: {compute_name}\")\n",
    "\n",
    "    # check if the compute can scale to the number of nodes selected by the user\n",
    "    if compute.type == ComputeType.COMPUTEINSTANCE and int(finetune_nodes) > 1:\n",
    "        raise ValueError(\n",
    "            f\"Finetune nodes requested: {finetune_nodes}. Max nodes compute, {compute_name} can scale to: 1. \"\n",
    "            f\"Please create a compute cluster with max_intances parameter to at least {finetune_nodes} while creating the compute.\"\n",
    "        )\n",
    "    if compute.type == ComputeType.AMLCOMPUTE and compute.max_instances < int(\n",
    "        finetune_nodes\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Finetune nodes requested: {finetune_nodes}. Max nodes compute, {compute_name} can scale to: {compute.max_instances}. \"\n",
    "            f\"Please set the max_intances parameter to at least {finetune_nodes} while creating the compute.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import PipelineJob\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def _get_param_value(pipeline_job_dict: Dict[str, Any], param_name: str):\n",
    "    \"\"\"Fetch the param value.\"\"\"\n",
    "    pipeline_name = next(iter(pipeline_job_dict[\"jobs\"]))\n",
    "\n",
    "    # check if user customized the value\n",
    "    param_value = pipeline_job_dict[\"jobs\"][pipeline_name][\"inputs\"].get(param_name)\n",
    "    if param_value is not None:\n",
    "        return param_value\n",
    "\n",
    "    # fetch the default otherwise\n",
    "    return pipeline_job_dict[\"jobs\"][pipeline_name][\"component\"][\"inputs\"][param_name][\n",
    "        \"default\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def validate_pipeline(ft_pipeline: PipelineJob, ws_ml_client: MLClient):\n",
    "    \"\"\"Validate the user created pipeline.\"\"\"\n",
    "    # get compute cluster\n",
    "    pipeline_job_dict = ft_pipeline._to_dict()\n",
    "\n",
    "    # validate compute\n",
    "    compute_name = _get_param_value(pipeline_job_dict, \"compute_finetune\")\n",
    "    num_nodes_finetune = _get_param_value(pipeline_job_dict, \"num_nodes_finetune\")\n",
    "    _validate_finetune_compute(\n",
    "        compute_name=compute_name,\n",
    "        finetune_nodes=num_nodes_finetune,\n",
    "        ws_ml_client=ws_ml_client,\n",
    "    )\n",
    "\n",
    "    # validate nodes selected by user\n",
    "    train_file_path = _get_param_value(pipeline_job_dict, \"train_file_path\")[\"path\"]\n",
    "    validation_file_path = _get_param_value(pipeline_job_dict, \"validation_file_path\")[\n",
    "        \"path\"\n",
    "    ]\n",
    "    test_file_path = _get_param_value(pipeline_job_dict, \"test_file_path\")[\"path\"]\n",
    "    _validate_finetune_data(\n",
    "        train_file_path=train_file_path,\n",
    "        validation_file_path=validation_file_path,\n",
    "        test_file_path=test_file_path,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streaming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
