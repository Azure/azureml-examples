{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Multimodal Classification using pipeline component\n",
    "\n",
    "This sample shows how to use `multimodal_classification_pipeline` component from the `azureml` system registry to fine tune a model for multi-class multimodal classification task using AirBnb Dataset. We then deploy the fine tuned model to an online endpoint for real time inference.\n",
    "\n",
    "### Training data\n",
    "We will use the [AirBnb](https://cvbp-secondary.z19.web.core.windows.net/datasets/multimodal_classification/AirBnb.zip) dataset.\n",
    "\n",
    "### Model\n",
    "We will use the `mmeft` model in this notebook.\n",
    "\n",
    "### Outline\n",
    "1. Install dependencies\n",
    "2. Setup pre-requisites such as compute\n",
    "3. Pick a model to fine tune\n",
    "4. Prepare dataset for finetuning the model\n",
    "5. Submit the fine tuning job using transformers specific image-classification component\n",
    "6. Review training and evaluation metrics\n",
    "7. Register the fine tuned model\n",
    "8. Deploy the fine tuned model for real time inference\n",
    "9. Test deployed end point\n",
    "9. Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies\n",
    "Before starting off, if you are running the notebook on Azure Machine Learning Studio or running first time locally, you will need the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-ai-ml==1.8.0\n",
    "! pip install azure-identity==1.13.0\n",
    "! pip install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Connect to Azure Machine Learning workspace\n",
    "\n",
    "Before we dive in the code, you'll need to connect to your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most scenarios. If you want to learn more about other available credentials, go to [set up authentication doc](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk), [azure-identity reference doc](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python).\n",
    "\n",
    "Replace `AML_WORKSPACE_NAME`, `RESOURCE_GROUP` and `SUBSCRIPTION_ID` with their respective values in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1634852261744
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "name": "mlclient-setup",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\n",
    "experiment_name = (\n",
    "    \"AzureML-Train-Finetune-Multimodal-MultiClass-Samples\"  # can rename to any valid name\n",
    ")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "workspace_ml_client = None\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential)\n",
    "    subscription_id = workspace_ml_client.subscription_id\n",
    "    resource_group = workspace_ml_client.resource_group_name\n",
    "    workspace_name = workspace_ml_client.workspace_name\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"SUBSCRIPTION_ID\"\n",
    "    resource_group = \"RESOURCE_GROUP\"\n",
    "    workspace_name = \"AML_WORKSPACE_NAME\"\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace_name\n",
    ")\n",
    "registry_ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id,\n",
    "    resource_group,\n",
    "    registry_name=\"azureml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Create compute\n",
    "\n",
    "In order to finetune a model on Azure Machine Learning studio, you will need to create a compute resource first. **Creating a compute will take 3-4 minutes.** \n",
    "\n",
    "For additional references, see [Azure Machine Learning in a Day](https://github.com/Azure/azureml-examples/blob/main/tutorials/azureml-in-a-day/azureml-in-a-day.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create CPU compute for model selection component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "model_import_cluster_name = \"sample-model-import-cluster\"\n",
    "try:\n",
    "    _ = workspace_ml_client.compute.get(model_import_cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute(\n",
    "        name=model_import_cluster_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_D12_v2\",\n",
    "        idle_time_before_scale_down=120,\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "    )\n",
    "    workspace_ml_client.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create GPU compute for finetune component\n",
    "\n",
    "The list of GPU machines can be found [here](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_cluster_name = \"sample-finetune-cluster-gpu\"\n",
    "\n",
    "try:\n",
    "    _ = workspace_ml_client.compute.get(finetune_cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute(\n",
    "        name=finetune_cluster_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_NC6s_v3\",\n",
    "        idle_time_before_scale_down=120,\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "    )\n",
    "    workspace_ml_client.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pick a foundation model to fine tune\n",
    "\n",
    "We will use the `mmeft` model in this notebook. It is available in `azureml` system registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_registry_model_name = \"mmeft\"\n",
    "use_model_name = aml_registry_model_name\n",
    "foundation_models = registry_ml_client.models.list(aml_registry_model_name)\n",
    "foundation_model = max(foundation_models, key=lambda x: x.version)\n",
    "print(\n",
    "    f\"\\n\\nUsing model name: {foundation_model.name}, version: {foundation_model.version}, id: {foundation_model.id} for fine tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare the dataset for fine-tuning the model\n",
    "\n",
    "We will use the [AirBnb](https://cvbp-secondary.z19.web.core.windows.net/datasets/multimodal_classification/AirBnb.zip) dataset. It has a `.csv` file with features and label. Along with it images are stored separately in `room_images` folder. Column name that stores label is `room_type`. \n",
    "\n",
    "#### 4.1 Download the Data\n",
    "We first download and unzip the data locally. By default, the data would be downloaded in `./data` folder in current directory. \n",
    "If you prefer to download the data at a different location, update it in `dataset_parent_dir = ...` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Change to a different location if you prefer\n",
    "dataset_parent_dir = \"./data\"\n",
    "\n",
    "# Create data folder if it doesnt exist.\n",
    "os.makedirs(dataset_parent_dir, exist_ok=True)\n",
    "\n",
    "# Download data\n",
    "download_url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/multimodal_classification/AirBnb.zip\"\n",
    "\n",
    "# Extract current dataset name from dataset url\n",
    "dataset_name = os.path.split(download_url)[-1].split(\".\")[0]\n",
    "# Get dataset path for later use\n",
    "dataset_dir = os.path.join(dataset_parent_dir, dataset_name)\n",
    "images_dir = os.path.join(dataset_dir, \"room_images\")\n",
    "\n",
    "image_column_name = \"picture_url\"\n",
    "label_column_name = \"room_type\"\n",
    "\n",
    "# columns to be ignored while training\n",
    "columns_to_drop = \"id,listing_url,scrape_id,last_scraped,host_id,host_url,host_name,host_since,neighbourhood_group,last_review,host_thumbnail_url,host_picture_url,calendar_last_scraped,first_review,last_review\"\n",
    "\n",
    "csv_file_path = os.path.join(dataset_dir, \"airbnb_multiclass_dataset.csv\")\n",
    "\n",
    "# Get the data zip file path\n",
    "data_file = os.path.join(dataset_parent_dir, f\"{dataset_name}.zip\")\n",
    "\n",
    "# Download the dataset\n",
    "urllib.request.urlretrieve(download_url, filename=data_file)\n",
    "\n",
    "# Extract files\n",
    "with ZipFile(data_file, \"r\") as zip:\n",
    "    print(\"extracting files...\")\n",
    "    zip.extractall(path=dataset_parent_dir)\n",
    "    print(\"done\")\n",
    "# Delete zip file\n",
    "os.remove(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a sample row from dataset\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(f\"rows = {df.shape[0]}, columns = {df.shape[1]} \\n\")\n",
    "print(\"Sample row\\n\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Upload the images to Datastore through an AML Data asset (URI Folder)\n",
    "\n",
    "In order to use the data for training in Azure ML, we upload it to our default Azure Blob Storage of our  Azure ML Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image files by creating a 'data asset URI FOLDER':\n",
    "\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_data = Data(\n",
    "    path=images_dir,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"AirBnb Room images\",\n",
    "    name=\"airbnb-roomtype-multimodal-multiclass-classif\",\n",
    ")\n",
    "\n",
    "uri_folder_data_asset = workspace_ml_client.data.create_or_update(my_data)\n",
    "\n",
    "print(uri_folder_data_asset)\n",
    "print(\"Path to folder in Blob Storage:\")\n",
    "print(uri_folder_data_asset.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Update image url in dataset\n",
    "\n",
    "[csv_processor.py](../utils/csv_processor.py) script updates the path to images in .csv files, from local path to path in AML datastore, where we uploaded the images in Step 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../utils/csv_processor.py \\\n",
    "    --img_col_name {img_col_name} \\\n",
    "    --image_url_prefix {uri_folder_data_asset.path} \\\n",
    "    --file_name {csv_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Split the downloaded data into Train/Validation dataset\n",
    "\n",
    "For documentation on preparing the datasets beyond this notebook, refer to the [documentation on how to prepare datasets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-prepare-datasets-for-automl-images).\n",
    "\n",
    "In order to use this data to create an AzureML MLTable, we first need either `.csv` or `.jsonl` format. The following script is creating two `.csv` files (one for training and one for validation) in the corresponding MLTable folder. In this example, 20% of the data is kept for validation.\n",
    "\n",
    "Here we also replace local image path with path to same image in datastore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We will copy each JSONL file within its related MLTable folder\n",
    "training_mltable_path = os.path.join(dataset_parent_dir, \"training-mltable-folder\")\n",
    "validation_mltable_path = os.path.join(dataset_parent_dir, \"validation-mltable-folder\")\n",
    "\n",
    "# Create the folders if they don't exist\n",
    "os.makedirs(training_mltable_path, exist_ok=True)\n",
    "os.makedirs(validation_mltable_path, exist_ok=True)\n",
    "\n",
    "train_validation_ratio = 0.2\n",
    "train_df, val_df = train_test_split(df, test_size=train_validation_ratio, random_state=0, stratify=df[[label_column_name]])\n",
    "\n",
    "# Path to the training and validation files\n",
    "train_annotations_file = os.path.join(training_mltable_path, \"train_annotations.csv\")\n",
    "validation_annotations_file = os.path.join(\n",
    "    validation_mltable_path, \"validation_annotations.csv\"\n",
    ")\n",
    "\n",
    "train_df.to_csv(train_annotations_file, index=False)\n",
    "val_df.to_csv(validation_annotations_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Create MLTable data input\n",
    "\n",
    "Create MLTable data input using the jsonl files created above.\n",
    "\n",
    "For documentation on creating your own MLTable assets for jobs beyond this notebook, please refer to below resources\n",
    "- [MLTable YAML Schema](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-mltable) - covers how to write MLTable YAML, which is required for each MLTable asset.\n",
    "- [Create MLTable data asset](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-data-assets?tabs=Python-SDK#create-a-mltable-data-asset) - covers how to create MLTable data asset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_table_file(filename):\n",
    "    \"\"\"Create ML Table definition\"\"\"\n",
    "\n",
    "    return (\n",
    "        \"$schema: https://azuremlschemas.azureedge.net/latest/MLTable.schema.json\\n\\n\"\n",
    "        \"type: mltable\\n\\n\"\n",
    "        \"paths:\\n\"\n",
    "        \"  - file: ./{0}\\n\\n\"\n",
    "        \"transformations:\\n\"\n",
    "        \"- read_delimited:\\n\"\n",
    "        \"    delimiter: ',' \\n\"\n",
    "        \"    empty_as_string: false \\n\"\n",
    "        \"    encoding: utf8 \\n\"\n",
    "        \"    header: all_files_same_headers\\n\"\n",
    "        \"    include_path_column: false \\n\"\n",
    "        \"    infer_column_types: true \\n\"\n",
    "        \"    support_multi_line: false\\n\"\n",
    "    ).format(filename)\n",
    "\n",
    "\n",
    "def save_ml_table_file(output_path, mltable_file_contents):\n",
    "    with open(os.path.join(output_path, \"MLTable\"), \"w\") as f:\n",
    "        f.write(mltable_file_contents)\n",
    "\n",
    "\n",
    "# Create and save train mltable\n",
    "train_mltable_file_contents = create_ml_table_file(\n",
    "    os.path.basename(train_annotations_file)\n",
    ")\n",
    "save_ml_table_file(training_mltable_path, train_mltable_file_contents)\n",
    "\n",
    "# Create and save validation mltable\n",
    "validation_mltable_file_contents = create_ml_table_file(\n",
    "    os.path.basename(validation_annotations_file)\n",
    ")\n",
    "save_ml_table_file(validation_mltable_path, validation_mltable_file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Submit the fine tuning job using `multimodal_classification_pipeline` component\n",
    " \n",
    "Create the job that uses the `multimodal_classification_pipeline` component for multi-class multimodal-classification task. Learn more in 5.2 about all the parameters supported for fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Create component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_PIPELINE_COMPONENT_NAME = \"multimodal_classification_pipeline\"\n",
    "pipeline_component_transformers_func = workspace_ml_client.components.get(\n",
    "    name=FINETUNE_PIPELINE_COMPONENT_NAME, label=\"latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Create arguments to be passed to `multimodal_classification_pipeline` component\n",
    "\n",
    "The `multimodal_classification_pipeline` component consists of model selection and finetuning components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_config_path = \"./deepspeed_configs/zero1.json\"\n",
    "if not os.path.exists(deepspeed_config_path):\n",
    "    print(\"DeepSpeed config file not found\")\n",
    "    deepspeed_config_path = None\n",
    "\n",
    "pipeline_component_args = {\n",
    "    ## Model selector component args\n",
    "    \"data_modalities\": \"text-image-tabular\",\n",
    "    \"model_id\": \"openai/clip-vit-base-patch32\",\n",
    "\n",
    "    ## Data preprocessing args\n",
    "    \"problem_type\": \"single_label_classification\",\n",
    "    \"label_column\": label_column_name,\n",
    "    \"image_column\": image_column_name,\n",
    "    \"drop_columns\": columns_to_drop,\n",
    "    # We try top auto detect the data type of values in column. \n",
    "    # But still if you want to explicitly specify data type (categorical, numerical or textual), then you can do so by providing comma separated column names in below fields.\n",
    "    # \"numerical_columns_overrides\":\n",
    "    # \"categorical_columns_overrides\":\n",
    "    #\"text_columns_overrides\":\n",
    "\n",
    "    ## Finetune_args\n",
    "    \"deepspeed_config\": deepspeed_config_path,\n",
    "    \"apply_lora\": \"false\",\n",
    "    \"merge_lora_weights\": \"false\",\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"number_of_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"training_batch_size\": 8,\n",
    "    \"validation_batch_size\": 8,\n",
    "    \"auto_find_batch_size\": \"false\",\n",
    "    \"optimizer\": \"adamw_hf\",\n",
    "    \"optimizer\": 0.001,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"gradient_accumulation_steps\": 64,\n",
    "    \"learning_rate_scheduler\": \"linear\",\n",
    "    \"precision\": 32,\n",
    "    \"random_seed\": 42,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"evaluation_steps_interval\": 0.0,\n",
    "    \"evaluation_steps\": 500,\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 500,\n",
    "    \"primary_metric\": \"loss\",\n",
    "    \"resume_from_checkpoint\": \"false\",\n",
    "    \"save_total_limit\": -1,\n",
    "    \"apply_early_stopping\": \"false\",\n",
    "    \"early_stopping_patience\": 1,\n",
    "    \"early_stopping_threshold\": 0.0,\n",
    "    \"apply_deepspeed\": \"false\",\n",
    "    \"apply_ort\": \"false\",\n",
    "    \"save_as_mlflow_model\": \"true\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Utility function to create pipeline using `multimodal_classification_pipeline` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import PipelineComponent\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "process_count_per_instance = 1\n",
    "instance_count = 1\n",
    "\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "    \"\"\"Create pipeline.\"\"\"\n",
    "\n",
    "    pipeline_component: PipelineComponent = pipeline_component_transformers_func(\n",
    "        compute_model_selector=cpu_cluster_name,\n",
    "        compute_preprocess=cpu_cluster_name,\n",
    "        compute_finetune=gpu_cluster_name,\n",
    "        training_data=Input(type=AssetTypes.MLTABLE, path=training_mltable_path),\n",
    "        validation_data=Input(type=AssetTypes.MLTABLE, path=validation_mltable_path),\n",
    "        # test data\n",
    "        # Using the same data for validation and test. If you want to use a different dataset for test, specify it below\n",
    "        test_data=Input(type=AssetTypes.MLTABLE, path=validation_mltable_path),\n",
    "        mlflow_model_path=Input(type=AssetTypes.MLFLOW_MODEL, path=foundation_model.id),\n",
    "        instance_count=instance_count,\n",
    "        process_count_per_instance=process_count_per_instance,\n",
    "        **pipeline_component_args,\n",
    "    )\n",
    "    return {\n",
    "        # Map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model. Registering the model is required to deploy the model to an online or batch endpoint.\n",
    "        \"mlflow_model_folder\": pipeline_component.outputs.mlflow_model_folder,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Run the fine tuning job using `multimodal_classification_pipeline` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_object = create_pipeline()\n",
    "\n",
    "pipeline_object.display_name = (\n",
    "    \"mmeft_multimodal_multiclass_pipeline_component_run\"\n",
    ")\n",
    "# Don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "\n",
    "print(\"Submitting pipeline\")\n",
    "\n",
    "pipeline_run = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "\n",
    "print(f\"Pipeline created. URL: {pipeline_run.studio_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.jobs.stream(pipeline_run.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Get metrics from finetune component\n",
    "\n",
    "The model training happens as part of the finetune component. Please follow below steps to extract validation metrics from the run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 Initialize MLFlow Client\n",
    "\n",
    "The models and artifacts that are produced by AutoML can be accessed via the MLFlow interface.\n",
    "Initialize the MLFlow client here, and set the backend as Azure ML, via. the MLFlow Client.\n",
    "\n",
    "IMPORTANT - You need to have installed the latest MLFlow packages with:\n",
    "\n",
    "    pip install azureml-mlflow\n",
    "    pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Obtain the tracking URL from MLClient\n",
    "MLFLOW_TRACKING_URI = workspace_ml_client.workspaces.get(\n",
    "    name=workspace_ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "\n",
    "print(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MLFLOW TRACKING URI\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "print(f\"\\nCurrent tracking uri: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "# Initialize MLFlow client\n",
    "mlflow_client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Get the training and evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat 'tags.mlflow.rootRunId=' and pipeline_job.name in single quotes as filter variable\n",
    "filter = \"tags.mlflow.rootRunId='\" + pipeline_run.name + \"'\"\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name], filter_string=filter, output_format=\"list\"\n",
    ")\n",
    "\n",
    "# Get the training and evaluation runs.\n",
    "for run in runs:\n",
    "    # Check if run.data.metrics.epoch exists\n",
    "    if \"epoch\" in run.data.metrics:\n",
    "        training_run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Get training metrics\n",
    "\n",
    "Access the results (such as Models, Artifacts, Metrics) of a previously completed run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(training_run.data.metrics, index=[0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Register the fine tuned model with the workspace\n",
    "\n",
    "We will register the model from the output of the fine tuning job. This will track lineage between the fine tuned model and the fine tuning job. The fine tuning job, further, tracks lineage to the foundation model, data and training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# Check if the `trained_model` output is available\n",
    "print(\n",
    "    f\"Pipeline job outputs: {workspace_ml_client.jobs.get(pipeline_run.name).outputs}\"\n",
    ")\n",
    "\n",
    "# Fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "model_path_from_job = (\n",
    "    f\"azureml://jobs/{pipeline_run.name}/outputs/trained_model\"\n",
    ")\n",
    "print(f\"Path to register model: {model_path_from_job}\")\n",
    "\n",
    "finetuned_model_name = (\n",
    "    f\"{use_model_name.replace('/', '-')}-fridge-objects-multiclass-classification\"\n",
    ")\n",
    "finetuned_model_description = f\"{use_model_name.replace('/', '-')} fine tuned model for fridge objects multiclass classification\"\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=finetuned_model_name,\n",
    "    version=timestamp,  # use timestamp as version to avoid version conflict\n",
    "    description=finetuned_model_description,\n",
    ")\n",
    "print(f\"Prepare to register model: \\n{prepare_to_register_model}\")\n",
    "\n",
    "# Register the model from pipeline job output\n",
    "registered_model = workspace_ml_client.models.create_or_update(\n",
    "    prepare_to_register_model\n",
    ")\n",
    "print(f\"Registered model: {registered_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
