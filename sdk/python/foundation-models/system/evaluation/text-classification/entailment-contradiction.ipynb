{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Evaluation - Entailment v/s Contradiction\n",
    "\n",
    "This sample shows how use the evaluate a group of models against a given set of metrics for the `text-classification` task. \n",
    "\n",
    "### Evaluation dataset\n",
    "The Multi-Genre Natural Language Inference Corpus, or MNLI is a crowd sourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The [MNLI](https://huggingface.co/datasets/glue) dataset is a subset of the larger [General Language Understanding Evaluation](https://gluebenchmark.com/) dataset. A copy of this dataset is available in the [glue-mnli](./glue-mnli/) folder. \n",
    "\n",
    "### Model\n",
    "The goal of evaluating models is to compare their performance on a variety of metrics. `text-classification` is generic task type that can be used for scenarios such as sentiment analysis, emotion detection, grammar checking, spam filtering, etc. As such, the models you pick to compare must be finetuned for same scenario. Given that we have the entailment v/s contradiction dataset, we would like to look for models finetuned for this specific scenario. We will compare `roberta-large-mnli`, `microsoft-deberta-large-mnli` and `microsoft-deberta-base-mnli` in this sample, which are available in the `azureml` system registry.\n",
    "\n",
    "If you'd like to evaluate models that are not in the system registry, you can import those models to your workspace or organization registry and then evaluate them using the approach outlined in this sample. Review the sample notebook for [importing models](../../import/import-model-from-huggingface.ipynb). \n",
    "\n",
    "### Outline\n",
    "* Setup pre-requisites such as compute.\n",
    "* Pick the models to evaluate.\n",
    "* Pick and explore evaluate data.\n",
    "* Configure the evaluation jobs.\n",
    "* Run the evaluation jobs.\n",
    "* Review the evaluation metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry\n",
    "* Set an optional experiment name\n",
    "* Check or create compute. A single GPU node can have multiple GPU cards. For example, in one node of `Standard_ND40rs_v2` there are 8 NVIDIA V100 GPUs while in `Standard_NC12s_v3`, there are 2 NVIDIA V100 GPUs. Refer to the [docs](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) for this information. The number of GPU cards per node is set in the param `gpus_per_node` below. Setting this value correctly will ensure utilization of all GPUs in the node. The recommended GPU compute SKUs can be found [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series) and [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies by running below cell. This is not an optional step if running in a new environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "%pip install datasets==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1679319346668
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = None\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential)\n",
    "    subscription_id = workspace_ml_client.subscription_id\n",
    "    workspace = workspace_ml_client.workspace_name\n",
    "    resource_group = workspace_ml_client.resource_group_name\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential, subscription_id, resource_group, workspace\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "preview_registry = \"azureml-preview\"\n",
    "registry = \"azureml-staging\"\n",
    "\n",
    "preview_registry_ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, registry_name=preview_registry\n",
    ")\n",
    "print(preview_registry_ml_client)\n",
    "\n",
    "registry_ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, registry_name=registry\n",
    ")\n",
    "registry_ml_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "compute_cluster = \"gpu-cluster-big\"\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "    print(f\"GPU compute '{compute_cluster}' found.\")\n",
    "except Exception as ex:\n",
    "    print(f\"GPU compute '{compute_cluster}' not found. Creating new one.\")\n",
    "    compute = AmlCompute(\n",
    "        name=compute_cluster,\n",
    "        size=\"Standard_ND40rs_v2\",\n",
    "        max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "    )\n",
    "    workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "\n",
    "# genrating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below snippet will allow us to query number of GPU's present on the compute. We can use it to set `gpu_per_node` to ensure utilization of all GPUs in the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute.\n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpus_per_node = 1  # default value\n",
    "gpu_count_found = False\n",
    "ws_computes = workspace_ml_client.compute.list_sizes()\n",
    "for ws_compute in ws_computes:\n",
    "    if ws_compute.name.lower() == compute.size.lower():\n",
    "        gpu_count_found = True\n",
    "        gpus_per_node = ws_compute.gpus\n",
    "        print(f\"Number of GPU's in compute {ws_compute.name} are {ws_compute.gpus}\")\n",
    "# if gpu_count_found not found, then print an error\n",
    "gpu_count_found or print(f\"Error: Number of GPU's in compute {compute.size} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick the models to evaluate\n",
    "\n",
    "Verify that the models selected for evaluation are available in system registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to specify model versions until the bug to support fetching the latest version using latest label is fixed\n",
    "models = [\n",
    "    {\"name\": \"distilbert-base-uncased-finetuned-sst-2-english\", \"version\": \"1\"},\n",
    "    {\"name\": \"finiteautomata-bertweet-base-sentiment-analysis\", \"version\": \"1\"},\n",
    "    {\"name\": \"microsoft-deberta-base-mnli\", \"version\": \"1\"},\n",
    "    {\"name\": \"microsoft-deberta-large-mnli\", \"version\": \"1\"},\n",
    "    {\"name\": \"roberta-base-openai-detector\", \"version\": \"1\"},\n",
    "    {\"name\": \"roberta-large-mnli\", \"version\": \"1\"},\n",
    "    {\"name\": \"roberta-large-openai-detector\", \"version\": \"1\"},\n",
    "]\n",
    "for model in models:\n",
    "    model = registry_ml_client.models.get(model[\"name\"], version=model[\"version\"])\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pick the test dataset for evaluation\n",
    "A copy of the MNLI is available in the [ glue-mnli](./glue-mnli/) folder. The next few cells show basic data preparation:\n",
    "* Visualize some data rows\n",
    "* Replace numerical categories in data with the actual string labels. This mapping is available in the [./glue-mnli/label.json](./glue-mnli/label.json). This step is needed because the selected models will return labels such `CONTRADICTION`, `CONTRADICTION`, etc. when running prediction. If the labels in your ground truth data are left as `0`, `1`, `2`, etc., then they would not match with prediction labels returned by the models.\n",
    "* The dataset contains `premise` and `hypothesis` as two different columns. However, the models expect a single string for prediction in the format `[CLS] <premise text> [SEP] <hypothesis text> [SEP]`. Hence we merge the columns and drop the original columns.\n",
    "* We want this sample to run quickly, so save smaller dataset containing 10% of the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = \"./glue-mnli-dataset\"\n",
    "data_file = \"train.jsonl\"\n",
    "\n",
    "# load the train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(os.path.join(dataset_dir, data_file), lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the id2label json element of the label.json file into pandas table with keys as 'label' column of int64 type and values as 'label_string' column as string type\n",
    "import json\n",
    "\n",
    "label_file = \"label.json\"\n",
    "with open(os.path.join(dataset_dir, label_file)) as f:\n",
    "    id2label = json.load(f)\n",
    "    id2label = id2label[\"id2label\"]\n",
    "    label_df = pd.DataFrame.from_dict(\n",
    "        id2label, orient=\"index\", columns=[\"label_string\"]\n",
    "    )\n",
    "    label_df[\"label\"] = label_df.index.astype(\"int64\")\n",
    "    label_df = label_df[[\"label\", \"label_string\"]]\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the train, validation and test dataframes with the id2label dataframe to get the label_string column\n",
    "df = df.merge(label_df, on=\"label\", how=\"left\")\n",
    "# concat the premise and hypothesis columns to with \"[CLS]\" in the beginning and \"[SEP]\" in the middle and end to get the text column\n",
    "df[\"input_string\"] = \"[CLS] \" + df[\"premise\"] + \" [SEP] \" + df[\"hypothesis\"] + \" [SEP]\"\n",
    "# drop the idx, premise and hypothesis columns as they are not needed\n",
    "df = df.drop(columns=[\"idx\", \"premise\", \"hypothesis\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10% of the rows from the train, validation and test dataframes into files with small_ prefix in the ./dataset_dir folder\n",
    "small_data_file = \"small_train.jsonl\"\n",
    "df.sample(frac=0.1).to_json(\n",
    "    os.path.join(dataset_dir, small_data_file), orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Submit the evaluation jobs using the model and data as inputs\n",
    " \n",
    "Create the job that uses the `model_evaluation_pipeline` component. We will submit one job per model. \n",
    "\n",
    "Note that the metrics that the evaluation jobs need to calculate are specified in the [eval_config.json](./eval_config.json) file. We calculate `accuracy`, `f1_score_micro` and `average_precision_score_macro` in this sample.\n",
    "\n",
    "All supported evaluation configurations for `text-classification` can be found in [README](./README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = preview_registry_ml_client.components.get(\n",
    "    name=\"model_evaluation_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def evaluation_pipeline(mlflow_model):\n",
    "    evaluation_job = pipeline_component_func(\n",
    "        # specify the foundation model available in the azureml system registry or a model from the workspace\n",
    "        # mlflow_model = Input(type=AssetTypes.MLFLOW_MODEL, path=f\"{mlflow_model_path}\"),\n",
    "        mlflow_model=mlflow_model,\n",
    "        # test data\n",
    "        test_data=Input(\n",
    "            type=AssetTypes.URI_FILE, path=os.path.join(dataset_dir, small_data_file)\n",
    "        ),\n",
    "        # The following parameters map to the dataset fields\n",
    "        input_column_names=\"input_string\",\n",
    "        label_column_name=\"label_string\",\n",
    "        # Evaluation settings\n",
    "        task=\"text-classification\",\n",
    "        # config file containing the details of evaluation metrics to calculate\n",
    "        evaluation_config=Input(type=AssetTypes.URI_FILE, path=\"./eval-config.json\"),\n",
    "    )\n",
    "    return {\"evaluation_result\": evaluation_job.outputs.evaluation_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the jobs, passing the model as a parameter to the pipeline created in the above step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# submit the pipeline job for each model that we want to evaluate\n",
    "# you could consider submitting the pipeline jobs in parallel, provided your cluster has multiple nodes\n",
    "pipeline_jobs = []\n",
    "\n",
    "experiment_name = \"text-classification-mnli-evaluation\"\n",
    "\n",
    "for model in models:\n",
    "    model_object = registry_ml_client.models.get(\n",
    "        model[\"name\"], version=model[\"version\"]\n",
    "    )\n",
    "    pipeline_object = evaluation_pipeline(\n",
    "        mlflow_model=Input(type=AssetTypes.MLFLOW_MODEL, path=f\"{model_object.id}\"),\n",
    "    )\n",
    "    # don't reuse cached results from previous jobs\n",
    "    pipeline_object.settings.force_rerun = True\n",
    "    pipeline_object.settings.default_compute = compute_cluster\n",
    "    pipeline_object.display_name = f\"eval-{model['name']}-{timestamp}\"\n",
    "    pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "        pipeline_object, experiment_name=experiment_name\n",
    "    )\n",
    "    # add model['name'] and pipeline_job.name as key value pairs to a dictionary\n",
    "    pipeline_jobs.append({\"model_name\": model[\"name\"], \"job_name\": pipeline_job.name})\n",
    "    # wait for the pipeline job to complete\n",
    "    workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Review evaluation metrics\n",
    "Viewing the job in AzureML studio is the best way to analyze logs, metrics and outputs of jobs. You can create custom charts and compare metics across different jobs. See https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics?tabs=interactive#view-jobsruns-information-in-the-studio to learn more. \n",
    "\n",
    "![Model evaluation dashboard in AzureML studio](./mnli-eval-dashboard.png)\n",
    "\n",
    "However, we may need to access and review metrics programmatically for which we will use MLflow, which is the recommended client for logging and querying metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, json\n",
    "\n",
    "mlflow_tracking_uri = workspace_ml_client.workspaces.get(\n",
    "    workspace_ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "metrics_df = pd.DataFrame()\n",
    "for job in pipeline_jobs:\n",
    "    # concat 'tags.mlflow.rootRunId=' and pipeline_job.name in single quotes as filter variable\n",
    "    filter = \"tags.mlflow.rootRunId='\" + job[\"job_name\"] + \"'\"\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_names=[experiment_name], filter_string=filter, output_format=\"list\"\n",
    "    )\n",
    "    # get the compute_metrics runs.\n",
    "    # using a hacky way till 'Bug 2320997: not able to show eval metrics in FT notebooks - mlflow client now showing display names' is fixed\n",
    "    for run in runs:\n",
    "        # else, check if run.data.metrics.accuracy exists\n",
    "        if \"accuracy\" in run.data.metrics:\n",
    "            # get the metrics from the mlflow run\n",
    "            run_metric = run.data.metrics\n",
    "            # add the model name to the run_metric dictionary\n",
    "            run_metric[\"model_name\"] = job[\"model_name\"]\n",
    "            # convert the run_metric dictionary to a pandas dataframe\n",
    "            temp_df = pd.DataFrame(run_metric, index=[0])\n",
    "            # concat the temp_df to the metrics_df\n",
    "            metrics_df = pd.concat([metrics_df, temp_df], ignore_index=True)\n",
    "\n",
    "# move the model_name columns to the first column\n",
    "cols = metrics_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "metrics_df = metrics_df[cols]\n",
    "metrics_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
