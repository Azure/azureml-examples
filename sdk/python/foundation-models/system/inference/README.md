# Foundation Model Inferencing
For inferencing foundation models on Azure ML, the curated container __foundation-model-inference__ is used. The container leverages both DeepSpeed-FastGen and VLLM to give optimized inferencing results. For more information on this container and to use it with foundation models, see the [text-generation example](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/system/inference/text-generation/llama-safe-online-deployment.ipynb). Currently, text generation and chat completion tasks are supported with more on the way.