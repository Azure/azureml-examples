{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Inference & Content Moderation using Online Endpoints and Azure Content Safety\n",
    "\n",
    "This sample shows how to deploy `text-generation` type models to an online endpoint for inference and how you can moderate the response with Azure Content Safety before give that generate content to users.\n",
    "\n",
    "### Task\n",
    "`text-generation`  is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase. Some common applications of text generation are code generation and story generation.\n",
    "\n",
    "### Model\n",
    "Models that can perform the `text-generation` task are tagged with `task: text-generation`. We will use the `gpt2` model in this notebook. If you opened this notebook from a specific model card, remember to replace the specific model name. If you don't find a model that suits your scenario or domain, you can discover and [import models from HuggingFace hub](../../import/import-model-from-huggingface.ipynb) and then use them for inference. \n",
    "\n",
    "### Inference data\n",
    "We will use the [book corpus](https://huggingface.co/datasets/bookcorpus) dataset. A copy of this dataset is available in the [book-corpus-dataset](./book-corpus-dataset/) folder.\n",
    "\n",
    "### Azure Content Safety\n",
    "We will use [Azure Content Safety](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/content-moderator/#faq) to validate that the content is safe.\n",
    "\n",
    "### Outline\n",
    "* Set up pre-requisites.\n",
    "* Pick a model to deploy.\n",
    "* Prepare data for inference. \n",
    "* Deploy the model for real time inference.\n",
    "* Test the endpoint\n",
    "* Filter content based on response\n",
    "* Clean up resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Set up pre-requisites for Azure Machine Learning\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    "    ClientSecretCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    ")\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml-preview\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Set up pre-requisites for Azure Content Safety\n",
    "1.\tPlease Log in to the [Azure portal](https://ms.portal.azure.com/?microsoft_azure_marketplace_ItemHideKey=microsoft_azure_cognitiveservices_contentsafety&feature.canmodifystamps=true&Microsoft_Azure_ProjectOxford=stage1#create/Microsoft.CognitiveServicesContentSafety) and apply for an Azure AI Content Safety resource. We offer two region options: West Europe and East US.\n",
    "2.\tAfter your Azure AI Content Safety resource is successful created, you will receive an API key, and you can use Azure AI Content Safety APIs by referring to this [product document](https://aka.ms/acs-doc).\n",
    "3.\tYou could also try our service by [interactive Studio](https://aka.ms/acsstudio).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a model to deploy\n",
    "\n",
    "Browse models in the Model Catalog in the AzureML Studio, filtering by the `text-generation` task. In this example, we use the `gpt2` model. If you have opened this notebook for a different model, replace the model name and version accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using model name: gpt2, version: 3, id: azureml://registries/azureml-preview/models/gpt2/versions/3 for inferencing\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "model_version = \"3\"\n",
    "foundation_model = registry_ml_client.models.get(model_name, model_version)\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for inferencing\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare data for inference.\n",
    "\n",
    "A copy of the book corpus dataset is available in the [book-corpus-dataset](./book-corpus-dataset/) folder. The next few cells show basic data preparation:\n",
    "* Visualize some data rows\n",
    "* Save few samples in the format that can be passed as input to the online-inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually , he would be tearing around the living room , playing with his toys .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but just one look at a minion sent him practically catatonic .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             text\n",
       "0  usually , he would be tearing around the living room , playing with his toys .\n",
       "1  but just one look at a minion sent him practically catatonic .                "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the ./book-corpus-dataset/train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "train_df = pd.read_json(\"./book-corpus-dataset/train.jsonl\", lines=True)\n",
    "train_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deploy the model to an online endpoint\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    OnlineRequestSettings,\n",
    ")\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "timestamp = int(time.time())\n",
    "online_endpoint_name = \"text-generation-\" + str(timestamp)\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + foundation_model.name\n",
    "    + \", for text-generation task\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instance type Standard_DS2_v2 may be too small for compute resources. Minimum recommended compute SKU is Standard_DS3_v2 for general purpose endpoints. Learn more about SKUs here: https://learn.microsoft.com/en-us/azure/machine-learning/referencemanaged-online-endpoints-vm-sku-list\n",
      "Check: endpoint text-generation-1683912751 exists\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "# create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=foundation_model.id,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=60000,\n",
    "    ),\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the endpoint with sample data\n",
    "\n",
    "We will fetch some sample data from the test dataset and submit to online endpoint for inference. We will then show the display the scored labels alongside the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57104</th>\n",
       "      <td>`` so you \\'re going to continue to be cavalier about this ? \\'\\'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    text\n",
       "57104  `` so you \\'re going to continue to be cavalier about this ? \\'\\'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# read the ./book-corpus-dataset/train.jsonl file into a pandas dataframe\n",
    "df = pd.read_json(\"./book-corpus-dataset/train.jsonl\", lines=True)\n",
    "# escape single and double quotes in the text column\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"'\", \"\\\\'\").str.replace('\"', '\\\\\"')\n",
    "# pick 1 random row\n",
    "sample_df = df.sample(1)\n",
    "# create a json object with the key as \"inputs\" and value as a list of values from the article column of the sample_df dataframe\n",
    "sample_json = {\"inputs\": sample_df[\"text\"].tolist()}\n",
    "# save the json object to a file named sample_score.json in the ./book-corpus-dataset folder\n",
    "test_json = {\"inputs\": {\"input_string\": sample_df[\"text\"].tolist()}}\n",
    "# save the json object to a file named sample_score.json in the ./book-corpus-dataset folder\n",
    "with open(os.path.join(\".\", \"book-corpus-dataset\", \"sample_score.json\"), \"w\") as f:\n",
    "    json.dump(test_json, f)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw response: \n",
      " [{\"0\": \"`` so you \\\\'re going to continue to be cavalier about this ? \\\\'\\\\' \\\"\\n\\nThis comment could refer to this one. (Note that the \\\"Cavalry'' does not have to be a noun, at least in the senses\"}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "generated_response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"demo\",\n",
    "    request_file=\"./book-corpus-dataset/sample_score.json\",\n",
    ")\n",
    "\n",
    "print(\"raw response: \\n\", generated_response , \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned response: \n",
      " `` so you \\'re going to continue to be cavalier about this ? \\'\\' \"\n",
      "\n",
      "This comment could refer to this one. (Note that the \"Cavalry'' does not have to be a noun, at least in the senses \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean and Parse result\n",
    "cleaned_response_data = json.loads(generated_response)[0][\"0\"]\n",
    "print(\"cleaned response: \\n\", cleaned_response_data , \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Azure Content Safety (ACS) Content Moderation\n",
    "Use Azure Content Safety for content moderation. We will run the text generate result through the Azure Content Safety endpoint and return a generic message telling the user their content was filtered if the generate text was flag for medium or high severity for [Hate, Self Harm, Sexual or Violence.](https://aka.ms/acs-doc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call ACS Text API with a sample request \n",
    "1. Go to the Azure Content Safety resource you created in Step 1b. The key can be found in the Keys and Endpoint section in the left pane. \n",
    "1. Find your Resource Endpoint URL in your Azure Portal in the **Resource Overview** page under the **Endpoint** field. \n",
    "1. Substitute the `<Endpoint>` term with your Resource Endpoint URL.\n",
    "1. Paste your subscription key into the `Ocp-Apim-Subscription-Key` field.\n",
    "1. Change the body of the request to whatever string of text you'd like to analyze.\n",
    "\n",
    "> **NOTE:**\n",
    ">\n",
    "> The samples may contain offensive content, user discretion advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def analyze_text(input_text):\n",
    "    endpoint = \"<ENDPOINT>\"\n",
    "    key = '<SUBSCRIPTION_KEY>'\n",
    "\n",
    "    # Build request\n",
    "    url = endpoint + '/contentsafety/text:analyze?api-version=2023-04-30-preview'\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key ,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": input_text,\n",
    "        \"categories\": [\n",
    "            \"Hate\",\"Sexual\",\"SelfHarm\",\"Violence\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Analyze text\n",
    "    try:\n",
    "        acs_response = requests.post(url, headers=headers, json=data)\n",
    "    except Exception as e:\n",
    "        print(\"Error code: {}\".format(e.error.code))\n",
    "        print(\"Error message: {}\".format(e.error.message))\n",
    "        return\n",
    "    \n",
    "    json_acs_content = json.loads(acs_response.content)\n",
    "    print(\"Text API response: \\n\", json.dumps(json_acs_content, indent=4))\n",
    "    \n",
    "    for key in json_acs_content:\n",
    "        if key != \"blocklistsMatchResults\":\n",
    "            if json_acs_content[key][\"severity\"] < 2:\n",
    "                continue\n",
    "            else:\n",
    "                category = json_acs_content[key][\"category\"]\n",
    "                return \"Text Generated for User: This content was filter for {0}, as the severity of the response medium or high\".format(category)\n",
    "    return \"Text Generated for User: {0}\".format(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text API response: \n",
      " {\n",
      "    \"blocklistsMatchResults\": [],\n",
      "    \"hateResult\": {\n",
      "        \"category\": \"Hate\",\n",
      "        \"severity\": 0\n",
      "    },\n",
      "    \"selfHarmResult\": {\n",
      "        \"category\": \"SelfHarm\",\n",
      "        \"severity\": 0\n",
      "    },\n",
      "    \"sexualResult\": {\n",
      "        \"category\": \"Sexual\",\n",
      "        \"severity\": 0\n",
      "    },\n",
      "    \"violenceResult\": {\n",
      "        \"category\": \"Violence\",\n",
      "        \"severity\": 0\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Text Generated for User: `` so you \\\\\\'re going to continue to be cavalier about this ? \\\\\\'\\\\\\' \"\\n\\nThis comment could refer to this one. (Note that the \"Cavalry\\'\\' does not have to be a noun, at least in the senses'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_text(cleaned_response_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpret Text API response\n",
    "\n",
    "You should see results displayed as JSON data. For example:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"blocklistsMatchResults\": [],\n",
    "    \"hateResult\": {\n",
    "        \"category\": \"Hate\",\n",
    "        \"severity\": 0\n",
    "    },\n",
    "    \"selfHarmResult\": {\n",
    "        \"category\": \"SelfHarm\",\n",
    "        \"severity\": 0\n",
    "    },\n",
    "    \"sexualResult\": {\n",
    "        \"category\": \"Sexual\",\n",
    "        \"severity\": 0\n",
    "    },\n",
    "    \"violenceResult\": {\n",
    "        \"category\": \"Violence\",\n",
    "        \"severity\": 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The JSON fields in the output are defined in the following table:\n",
    "\n",
    "| Name         | Description                                                  | Type    |\n",
    "| :----------- | :----------------------------------------------------------- | ------- |\n",
    "| **category** | Each output class that the API predicts. Classification can be multi-labeled. For example, when a text is run through a text content safmodel, it could be classified as sexual content as well as violence. | String  |\n",
    "| **severity** | The higher the severity of input content, the larger this value is. The values could be: 0,2,4,6. | Integer |\n",
    "\n",
    "\n",
    "> **NOTE: Why severity level is not continuous**\n",
    ">\n",
    "> Currently, we only use levels 0, 2, 4, and 6. In the future, we may be able to extend the severity levels to 0, 1, 2, 3, 4, 5, 6, 7: seven levels with finer granularity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Delete the online endpoint\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
