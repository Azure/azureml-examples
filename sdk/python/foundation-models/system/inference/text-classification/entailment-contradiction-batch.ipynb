{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Text Classification Inference using Batch Endpoints\n",
    "\n",
    "This sample shows how to run inference in batch model for `text-classification` task.\n",
    "\n",
    "### Task\n",
    "`text-classification` is generic task type that can be used for scenarios such as sentiment analysis, emotion detection, grammar checking, spam filtering, etc. In this example, we will test for entailment v/s contradiction, meaning given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). \n",
    "\n",
    "### Inference data\n",
    "The Multi-Genre Natural Language Inference Corpus, or MNLI is a crowd sourced collection of sentence pairs with textual entailment annotations.The [MNLI](https://huggingface.co/datasets/glue) dataset is a subset of the larger [General Language Understanding Evaluation](https://gluebenchmark.com/) dataset. A copy of this dataset is available in the [glue-mnli](./glue-mnli/) folder.\n",
    "\n",
    "### Model\n",
    "Look for models tagged with `text-classification` in the system registry. Just looking for `text-classification` is not sufficient, you need to check if the model is specifically finetuned for  entailment v/s contradiction by studying the model card and looking at the input/output samples or signatures of the model. In this notebook, we use the `microsoft-deberta-base-mnli` model.\n",
    "\n",
    "  \n",
    "### Outline\n",
    "* Setup pre-requisites.\n",
    "* Pick a model to deploy.\n",
    "* Prepare data for inference. \n",
    "* Deploy the model for batch inference.\n",
    "* Run a batch inference job.\n",
    "* Review inference predictions.\n",
    "* Clean up resources.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Check or create compute.\n",
    "* Connect to `azureml` system registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, ClientSecretCredential\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id =  \"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name =  \"<RESOURCE_GROUP>\",\n",
    "        workspace_name =  \"<WORKSPACE_NAME>\"\n",
    "    )\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml-preview-test1\")\n",
    "\n",
    "compute_cluster = \"gpu-cluster-big\"\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "except Exception as ex:\n",
    "    compute = AmlCompute(\n",
    "        name=compute_cluster,\n",
    "        size=\"Standard_ND40rs_v2\",\n",
    "        max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "    )\n",
    "    workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "\n",
    "# genrating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time())) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 2. Pick a model to deploy\n",
    "\n",
    "Browse models in the Model Catalog in the AzureML Studio, filtering by the `text-classification` task. In this example, we use the `microsoft-deberta-base-mnli` model. If you have opened this notebook for a different model, replace the model name and version accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"microsoft-deberta-base-mnli\"\n",
    "model_version = \"6\"\n",
    "foundation_model=registry_ml_client.models.get(model_name, model_version)\n",
    "print (\"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(foundation_model.name, foundation_model.version, foundation_model.id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare data for inference.\n",
    "\n",
    "A copy of the MNLI is available in the [ glue-mnli](./glue-mnli/) folder. The next few cells show basic data preparation:\n",
    "* Visualize some data rows\n",
    "* Replace numerical categories in data with the actual string labels. This mapping is available in the [./glue-mnli/label.json](./glue-mnli/label.json). This step is needed because the selected models will return labels such `CONTRADICTION`, `CONTRADICTION`, etc. when running prediction. If the labels in your ground truth data are left as `0`, `1`, `2`, etc., then they would not match with prediction labels returned by the models.\n",
    "* The dataset contains `premise` and `hypothesis` as two different columns. However, the models expect a single string for prediction in the format `[CLS] <premise text> [SEP] <hypothesis text> [SEP]`. Hence we merge the columns and drop the original columns.\n",
    "* We want this sample to run quickly, so save smaller dataset containing 10% of the original. \n",
    "* Since we are using a `mlflow` model, we don't need to write any inference code. However, we need the inference data to be in a shape can can be used for inference. Specifically, batch inference does not support jsonl lines files, but supports `csv` and `parquet`. We will dump a csv version from the pandas dataframe. Next, the rows of the batch inference csv file must strictly contain only the columns that will be passed to the model as input and the column header must match the model signature. In our case, the model signature which can be found in the `MLmodel` file in the model artifacts expects `input_string` as input. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, we load the input file and look at some sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "dataset_dir = \"./glue-mnli-dataset\"\n",
    "data_file=\"train.jsonl\"\n",
    "bath_sample_data_file = \"small_batch_train.jsonl\"\n",
    "batch_dir = os.path.join(dataset_dir,\"batch\")\n",
    "os.makedirs(batch_dir, exist_ok=True)\n",
    "batch_input_file = \"batch_input.csv\"\n",
    "\n",
    "# load the train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0) # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(os.path.join(dataset_dir,data_file), lines=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace numerical labels with string labels, drop the columns not needed and take a smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the id2label json element of the label.json file into pandas table with keys as 'label' column of int64 type and values as 'label_string' column as string type\n",
    "import json\n",
    "import csv\n",
    "label_file=\"label.json\"\n",
    "with open(os.path.join(dataset_dir,label_file)) as f:\n",
    "    id2label = json.load(f)\n",
    "    id2label = id2label['id2label']\n",
    "    label_df = pd.DataFrame.from_dict(id2label, orient='index', columns=['label_string'])\n",
    "    label_df['label'] = label_df.index.astype('int64')\n",
    "    label_df = label_df[['label', 'label_string']]\n",
    "\n",
    "# join the train, validation and test dataframes with the id2label dataframe to get the label_string column\n",
    "df =df.merge(label_df, on='label', how='left')\n",
    "# concat the premise and hypothesis columns to with \"[CLS]\" in the beginning and \"[SEP]\" in the middle and end to get the text column\n",
    "df['text'] = \"[CLS] \" + df['premise'] + \" [SEP] \" + df['hypothesis'] + \" [SEP]\"\n",
    "# drop the idx, premise and hypothesis columns as they are not needed\n",
    "df = df.drop(columns=['idx', 'premise', 'hypothesis', 'label'])\n",
    "# rename the label_string column to ground_truth_label\n",
    "df = df.rename(columns={'label_string': 'ground_truth_label'})\n",
    "# get 10% of the rows so that the sample runs faster\n",
    "df = df.sample(frac=0.1)\n",
    "# reset index of the dataframe\n",
    "df = df.reset_index(drop=True)\n",
    "# save dataframe to a json lines file - we will us this file to compare output of batch inference with ground truth\n",
    "df.to_json(os.path.join(dataset_dir, bath_sample_data_file), orient='records', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the batch inference input csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the text column in batch_df dataframe as input for batch inference data should not contain any columns that are not passed to the model\n",
    "batch_df = df[['text']]\n",
    "# rename text column to input_string\n",
    "batch_df = batch_df.rename(columns={'text': 'input_string'})\n",
    "# save the rows in batch_df dataframe to a csv file named in the batch_dir folder, containing only the batch_input_file\n",
    "batch_df.to_csv(os.path.join(batch_dir, batch_input_file), index=False, quoting=csv.QUOTE_ALL)\n",
    "batch_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy the model for batch inference.\n",
    "\n",
    "Batch endpoints are endpoints that are used batch score large datasets in job model. Batch endpoints receive pointers to data and run jobs asynchronously to process the data in parallel on compute clusters. Batch endpoints store outputs to a data store for further analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a batch endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import BatchEndpoint, BatchDeployment, Model, AmlCompute, Data\n",
    "batch_endpoint_name = \"entail-contra-\" + timestamp\n",
    "endpoint = BatchEndpoint(\n",
    "    name=batch_endpoint_name,\n",
    "    description=\"Batch endpoint for \" + foundation_model.name + \", to detect entailment v/s contradiction\",\n",
    ")\n",
    "workspace_ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a batch deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants import BatchDeploymentOutputAction\n",
    "from azure.ai.ml.entities import BatchRetrySettings\n",
    "\n",
    "deployment = BatchDeployment(\n",
    "    name=\"demo\",\n",
    "    description=\"Batch deployment for \" + foundation_model.name + \", to detect entailment v/s contradiction\",\n",
    "    endpoint_name=endpoint.name,\n",
    "    model=foundation_model,\n",
    "    compute=compute_cluster,\n",
    "    instance_count=1,\n",
    "    max_concurrency_per_instance=2,\n",
    "    mini_batch_size=10,\n",
    "    output_action=BatchDeploymentOutputAction.APPEND_ROW,\n",
    "    output_file_name=\"predictions.csv\",\n",
    "    retry_settings=BatchRetrySettings(max_retries=3, timeout=300),\n",
    "    logging_level=\"info\",\n",
    ")\n",
    "workspace_ml_client.batch_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set demo deployment as default. Default deployment is used when no deployment name is specified when invoking the batch endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "endpoint = workspace_ml_client.batch_endpoints.get(batch_endpoint_name)\n",
    "endpoint.defaults.deployment_name = \"demo\"\n",
    "workspace_ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
    "endpoint = workspace_ml_client.batch_endpoints.get(batch_endpoint_name)\n",
    "print(f\"The default deployment is {endpoint.defaults.deployment_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run a batch inference job.\n",
    "\n",
    "Invoke the batch endpoint with the input parameter pointing to the folder containing the batch inference input. This creates a pipeline job using the default deployment in the endpoint. Wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "input = Input(type=AssetTypes.URI_FOLDER, path=batch_dir)\n",
    "job = workspace_ml_client.batch_endpoints.invoke(endpoint_name=endpoint.name, input=input)\n",
    "workspace_ml_client.jobs.stream(job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Review inference predictions.\n",
    "\n",
    "Download the predictions from the job output and review the predictions using a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "scoring_job = list(workspace_ml_client.jobs.list(parent_job_name=job.name))[0]\n",
    "workspace_ml_client.jobs.download(name=scoring_job.name, download_path=dataset_dir, output_name=\"score\")\n",
    "predictions_file = os.path.join(dataset_dir, \"named-outputs\", \"score\", \"predictions.csv\")\n",
    "# load the batch predictions file that has no headers into a score_df dataframe\n",
    "score_df = pd.read_csv(predictions_file, header=None)\n",
    "# rename column 0 as row_number, column 1 to prediction and column 2 to batch_input_file_name \n",
    "score_df.columns = ['row_number', 'prediction', 'batch_input_file_name']\n",
    "score_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the predictions with input data to compare ground truth with predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the row_number and batch_input_file_name columns as they are not needed\n",
    "score_df = score_df.drop(columns=['row_number', 'batch_input_file_name'])\n",
    "# join the df dataframe with the score_df dataframe on the index row\n",
    "df = df.join(score_df)\n",
    "# show the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean up resources\n",
    "\n",
    "Batch endpoints use compute resources only when jobs are submitted. You can keep the batch endpoint for your reference without worrying out compute bills, or choose to delete it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.batch_endpoints.begin_delete(batch_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "amlv2"
  },
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "429d412e307b288f3a8cba821a3ba110e77b02cf5672d0d0b14db25cc0bc89f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
