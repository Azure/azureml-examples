{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Inference using Online Endpoints\n",
    "\n",
    "This sample shows how to deploy `text-classification` type models to an online endpoint for inference.\n",
    "\n",
    "### Task\n",
    "`text-classification` is generic task type that can be used for scenarios such as sentiment analysis, emotion detection, grammar checking, spam filtering, etc. In this example, we will test for entailment v/s contradiction, meaning given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). \n",
    "\n",
    "### Inference data\n",
    "The Multi-Genre Natural Language Inference Corpus, or MNLI is a crowd sourced collection of sentence pairs with textual entailment annotations.The [MNLI](https://huggingface.co/datasets/glue) dataset is a subset of the larger [General Language Understanding Evaluation](https://gluebenchmark.com/) dataset. A copy of this dataset is available in the [glue-mnli](./glue-mnli/) folder.\n",
    "\n",
    "### Model\n",
    "Look for models tagged with `text-classification` in the system registry. Just looking for `text-classification` is not sufficient, you need to check if the model is specifically finetuned for  entailment v/s contradiction by studying the model card and looking at the input/output samples or signatures of the model. In this notebook, we use the `microsoft-deberta-base-mnli` model.\n",
    "\n",
    "   \n",
    "\n",
    "### Outline\n",
    "* Set up pre-requisites.\n",
    "* Pick a model to deploy.\n",
    "* Prepare data for inference. \n",
    "* Deploy the model for real time inference.\n",
    "* Test the endpoint\n",
    "* Clean up resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, ClientSecretCredential\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id =  \"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name =  \"<RESOURCE_GROUP>\",\n",
    "        workspace_name =  \"<WORKSPACE_NAME>\"\n",
    "    )\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml-preview\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a model to deploy\n",
    "\n",
    "Browse models in the Model Catalog in the AzureML Studio, filtering by the `fill-mask` task. In this example, we use the `bert-base-uncased` model. If you have opened this notebook for a different model, replace the model name and version accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft-deberta-base-mnli\"\n",
    "model_version = \"2\"\n",
    "foundation_model=registry_ml_client.models.get(model_name, model_version)\n",
    "print (\"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(foundation_model.name, foundation_model.version, foundation_model.id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare data for inference.\n",
    "\n",
    "A copy of the MNLI is available in the [ glue-mnli-dataset](./glue-mnli-dataset/) folder. The next few cells show basic data preparation:\n",
    "* Visualize some data rows\n",
    "* Replace numerical categories in data with the actual string labels. This mapping is available in the [./glue-mnli-dataset/label.json](./glue-mnli-dataset/label.json). This step is needed because the selected models will return labels such `CONTRADICTION`, `CONTRADICTION`, etc. when running prediction. If the labels in your ground truth data are left as `0`, `1`, `2`, etc., then they would not match with prediction labels returned by the models.\n",
    "* The dataset contains `premise` and `hypothesis` as two different columns. However, the models expect a single string for prediction in the format `[CLS] <premise text> [SEP] <hypothesis text> [SEP]`. Hence we merge the columns and drop the original columns.\n",
    "* We want this sample to run quickly, so save smaller dataset containing 10% of the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset_dir = \"./glue-mnli-dataset\"\n",
    "data_file=\"train.jsonl\"\n",
    "\n",
    "# load the train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0) # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(os.path.join(dataset_dir,data_file), lines=True)\n",
    "df.head()\n",
    "\n",
    "# load the id2label json element of the label.json file into pandas table with keys as 'label' column of int64 type and values as 'label_string' column as string type\n",
    "import json\n",
    "label_file=\"label.json\"\n",
    "with open(os.path.join(dataset_dir,label_file)) as f:\n",
    "    id2label = json.load(f)\n",
    "    id2label = id2label['id2label']\n",
    "    label_df = pd.DataFrame.from_dict(id2label, orient='index', columns=['label_string'])\n",
    "    label_df['label'] = label_df.index.astype('int64')\n",
    "    label_df = label_df[['label', 'label_string']]\n",
    "\n",
    "# join the train, validation and test dataframes with the id2label dataframe to get the label_string column\n",
    "df =df.merge(label_df, on='label', how='left')\n",
    "# concat the premise and hypothesis columns to with \"[CLS]\" in the beginning and \"[SEP]\" in the middle and end to get the text column\n",
    "df['text'] = \"[CLS] \" + df['premise'] + \" [SEP] \" + df['hypothesis'] + \" [SEP]\"\n",
    "# drop the idx, premise and hypothesis columns as they are not needed\n",
    "df = df.drop(columns=['idx', 'premise', 'hypothesis', 'label'])\n",
    "# rename the label_string column to ground_truth_label\n",
    "df = df.rename(columns={'label_string': 'ground_truth_label'})\n",
    "\n",
    "# save 10% of the rows from the train, validation and test dataframes into files with small_ prefix in the ./dataset_dir folder\n",
    "small_data_file = \"small_train.jsonl\"\n",
    "df.sample(frac=0.1).to_json(os.path.join(dataset_dir,small_data_file), orient='records', lines=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deploy the model to an online endpoint\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "timestamp = int(time.time())\n",
    "online_endpoint_name = \"entail-contra-\" + str(timestamp)\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \" + foundation_model.name + \", to detect entailment v/s contradiction\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=foundation_model.id,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the endpoint with sample data\n",
    "\n",
    "We will fetch some sample data from the test dataset and submit to online endpoint for inference. We will then show the display the scored labels alongside the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_file_small = \"small_train.jsonl\"\n",
    "score_file = \"sample_score.json\"\n",
    "# read the data file into a pandas dataframe \n",
    "df = pd.read_json(os.path.join(dataset_dir,data_file_small), lines=True)\n",
    "# escape single and double quotes in the masked_text column\n",
    "# pick 5 random rows\n",
    "sample_df=df.sample(5)\n",
    "# reset the index of sample_df\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "\n",
    "# save the json object to a file named sample_score.json in the \n",
    "test_json = {\"inputs\": {\"input_string\": sample_df[\"text\"].tolist()}}\n",
    "# save the json object to a file named sample_score.json in the ./glue-mnli-dataset folder\n",
    "with open(os.path.join(\".\", dataset_dir, score_file), \"w\") as f:\n",
    "    json.dump(test_json, f)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "response=workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"demo\",\n",
    "    request_file=os.path.join(\".\", dataset_dir, score_file),\n",
    ")\n",
    "print(\"raw response: \\n\", response, \"\\n\")\n",
    "# convert the json response to a pandas dataframe \n",
    "response_df = pd.read_json(response)\n",
    "# rename label column to predicted_label\n",
    "response_df = response_df.rename(columns={'label': 'predicted_label'})\n",
    "response_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the sample_df and response_df dataframes\n",
    "merged_df = sample_df.merge(response_df, left_index=True, right_index=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Delete the online endpoint\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
