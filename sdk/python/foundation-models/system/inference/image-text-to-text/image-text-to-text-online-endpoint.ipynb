{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-text to Text Inference using Online Endpoints\n",
    "\n",
    "This sample notebook shows how to deploy `image-text-to-text` type models to an online endpoint for inference.\n",
    "\n",
    "### Task\n",
    "`image-text-to-text` takes as input image-prompt pairs and generates an answer for each pair. The prompt can consist of a single question or a dialog between the user and the model that ends with a question from the user.\n",
    "\n",
    "### Model\n",
    "Models suitable to the `image-text-to-text` task are tagged with `image-text-to-text`. We will use the `llava-7b` model in this notebook. If you opened this notebook from a specific model card, remember to replace the specific model name. If you don't find a model that suits your scenario or domain, you can discover and [import models from HuggingFace hub](../../import/import_model_into_registry.ipynb) and then use them for inference.\n",
    "\n",
    "### Inference data\n",
    "We will use images from the [fridgeObjects](https://cvbp-secondary.z19.web.core.windows.net/datasets/image_classification/fridgeObjects.zip) dataset.\n",
    "\n",
    "\n",
    "### Outline\n",
    "1. Setup pre-requisites\n",
    "2. Pick a model to deploy\n",
    "3. Prepare data for inference\n",
    "4. Deploy the model to an online endpoint for real time inference\n",
    "5. Test the endpoint\n",
    "6. Clean up resources - delete the online endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llava-torch==1.0.2 --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential)\n",
    "    subscription_id = workspace_ml_client.subscription_id\n",
    "    resource_group = workspace_ml_client.resource_group_name\n",
    "    workspace_name = workspace_ml_client.workspace_name\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    # Enter details of your AML workspace\n",
    "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "    resource_group = \"<RESOURCE_GROUP>\"\n",
    "    workspace_name = \"<AML_WORKSPACE_NAME>\"\n",
    "workspace_ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace_name\n",
    ")\n",
    "\n",
    "# The models are available in the AzureML system registry, \"azureml\"\n",
    "registry_ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id,\n",
    "    resource_group,\n",
    "    registry_name=\"azureml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a model to deploy\n",
    "\n",
    "Browse models in the Model Catalog in the AzureML Studio, filtering by the `image-text-to-text` task. In this example, we use the `llava-7b` model. If you have opened this notebook for a different model, replace the model name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llava-7b\"\n",
    "\n",
    "foundation_model = registry_ml_client.models.get(name=model_name, label=\"latest\")\n",
    "print(\n",
    "    f\"\\n\\nUsing model name: {foundation_model.name}, version: {foundation_model.version}, id: {foundation_model.id} for inferencing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare data for inference\n",
    "\n",
    "We will use images in the [fridgeObjects](https://cvbp-secondary.z19.web.core.windows.net/datasets/image_classification/fridgeObjects.zip) dataset to construct example image-prompt pairs for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "# Change to a different location if you prefer\n",
    "dataset_parent_dir = \"./data\"\n",
    "\n",
    "# Create data folder if it doesnt exist.\n",
    "os.makedirs(dataset_parent_dir, exist_ok=True)\n",
    "\n",
    "# Download data\n",
    "download_url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/image_classification/fridgeObjects.zip\"\n",
    "\n",
    "# Extract current dataset name from dataset url\n",
    "dataset_name = os.path.split(download_url)[-1].split(\".\")[0]\n",
    "# Get dataset path for later use\n",
    "dataset_dir = os.path.join(dataset_parent_dir, dataset_name)\n",
    "\n",
    "# Get the data zip file path\n",
    "data_file = os.path.join(dataset_parent_dir, f\"{dataset_name}.zip\")\n",
    "\n",
    "# Download the dataset\n",
    "urllib.request.urlretrieve(download_url, filename=data_file)\n",
    "\n",
    "# Extract files\n",
    "with ZipFile(data_file, \"r\") as zip:\n",
    "    print(\"extracting files...\")\n",
    "    zip.extractall(path=dataset_parent_dir)\n",
    "    print(\"done\")\n",
    "# Delete zip file\n",
    "os.remove(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "sample_image = os.path.join(dataset_dir, \"milk_bottle\", \"99.jpg\")\n",
    "Image(filename=sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deploy the model to an online endpoint for real time inference\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    ")\n",
    "\n",
    "\n",
    "# Endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "timestamp = int(time.time())\n",
    "online_endpoint_name = \"image-text-to-text-\" + str(timestamp)\n",
    "# Create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + foundation_model.name\n",
    "    + \", for image-text-to-text task\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import OnlineRequestSettings, ProbeSettings\n",
    "\n",
    "\n",
    "deployment_name = \"image-text-to-text-mlflow-deploy\"\n",
    "\n",
    "# Create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=foundation_model.id,\n",
    "    instance_type=\"Standard_NC6s_v3\",\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=1,\n",
    "        request_timeout_ms=90000,\n",
    "        max_queue_wait_ms=500,\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=49,\n",
    "        success_threshold=1,\n",
    "        timeout=299,\n",
    "        period=180,\n",
    "        initial_delay=180,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=10,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=10,\n",
    "        initial_delay=10,\n",
    "    ),\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the endpoint\n",
    "\n",
    "We will use an image from the dataset and your questions about it to form example inputs for the model. When needed, the questions will be converted to the right format for the model version and submitted to the online endpoint for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Direct question\n",
    "\n",
    "We provide functionality to directly submit a question to the model about an image, internally converting the question to the prompt format required by the model version. This is done by setting the \"prompt\" column of the input to the empty string and the \"direct_question\" column to the question, see the `make_request_direct_question()` function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "\n",
    "REQUEST_FILE_NAME = \"request.json\"\n",
    "\n",
    "\n",
    "def make_request_direct_question(image_bytes, direct_question):\n",
    "    request_json = {\n",
    "        \"input_data\": {\n",
    "            \"columns\": [\"image\", \"prompt\", \"direct_question\"],\n",
    "            \"data\": [\n",
    "                [base64.encodebytes(image_bytes).decode(\"utf-8\"), \"\", direct_question],\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    with open(REQUEST_FILE_NAME, \"wt\") as f:\n",
    "        json.dump(request_json, f)\n",
    "\n",
    "\n",
    "def get_response():\n",
    "    response = workspace_ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        deployment_name=demo_deployment.name,\n",
    "        request_file=REQUEST_FILE_NAME,\n",
    "    )\n",
    "\n",
    "    return json.loads(response)[0][\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bytes = open(os.path.join(dataset_dir, \"milk_bottle\", \"99.jpg\"), \"rb\").read()\n",
    "question = \"What is in this image?\"\n",
    "\n",
    "make_request_direct_question(image_bytes, question)\n",
    "print(get_response())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Dialog\n",
    "\n",
    "We provide functionality to submit a prompt to the model, which can encode either a question or a dialog ending with a question for the model. This is done by setting the \"prompt\" column of the input, see the `make_request_prompt()` function below; note that the \"direct_question\" column is ignored in this case. We rely on functionality in the `llava-torch` package to manage the dialog and convert it into a prompt for the model, see the `run_dialog_loop()` function below.\n",
    "\n",
    "Due to memory limitations on V100 machines, a dialog with the `llava-7b` model can only have a small number of questions, e.g. 7. To ask additional questions, a new dialog can be made; if there is a need for dialogs longer than 10 questions, we suggest using an A100 machine (or better) and the `llava-13b` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.constants import DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates\n",
    "\n",
    "\n",
    "NUM_QUESTIONS = 7\n",
    "\n",
    "\n",
    "def make_request_prompt(image_bytes, prompt):\n",
    "    request_json = {\n",
    "        \"input_data\": {\n",
    "            \"columns\": [\"image\", \"prompt\", \"direct_question\"],\n",
    "            \"data\": [\n",
    "                [base64.encodebytes(image_bytes).decode(\"utf-8\"), prompt, \"\"],\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    with open(REQUEST_FILE_NAME, \"wt\") as f:\n",
    "        json.dump(request_json, f)\n",
    "\n",
    "\n",
    "def run_dialog_loop(image_bytes):\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    roles = conv.roles\n",
    "\n",
    "    first_time = True\n",
    "    for _ in range(NUM_QUESTIONS):\n",
    "        try:\n",
    "            inp = input(f\"{roles[0]}: \")\n",
    "        except EOFError:\n",
    "            inp = \"\"\n",
    "        if not inp:\n",
    "            print(\"exit...\")\n",
    "            break\n",
    "\n",
    "        print(f\"{roles[1]}: \", end=\"\")\n",
    "\n",
    "        if first_time:\n",
    "            first_time = False\n",
    "            inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + inp\n",
    "            conv.append_message(conv.roles[0], inp)\n",
    "        else:\n",
    "            # later messages\n",
    "            conv.append_message(conv.roles[0], inp)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "\n",
    "        make_request_prompt(image_bytes, prompt)\n",
    "        response = get_response()\n",
    "\n",
    "        conv.messages[-1][-1] = response\n",
    "\n",
    "        printed_response = response\n",
    "        if printed_response.endswith(\"<|im_end|>\"):\n",
    "            printed_response = printed_response[: -len(\"<|im_end|>\")]\n",
    "        if printed_response.endswith(\"</s>\"):\n",
    "            printed_response = printed_response[: -len(\"</s>\")]\n",
    "        print(printed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bytes = open(os.path.join(dataset_dir, \"milk_bottle\", \"99.jpg\"), \"rb\").read()\n",
    "\n",
    "run_dialog_loop(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Clean up resources (delete the online endpoint)\n",
    "Do not forget to delete the online endpoint. You will be billed for the compute used by the endpoint if it is not deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
