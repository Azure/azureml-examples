{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring your model into AzureML\n",
    "\n",
    "This sample show how to import a Huggingface model and register it as custom model. Next how to deploy registered model using a custom environment and soring script.\n",
    "\n",
    "### What models are supported for import?\n",
    "Model files present in a public github repository or in the azure blobstorage with public read and list access can be imported.\n",
    "In this sample we will import a Huggingface [databricks/dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b) model and then use a custom script to deploy model in the workspace.\n",
    "\n",
    "### Outline\n",
    "1. Set up pre-requisites\n",
    "2. Create and execute pipeline job\n",
    "4. Deploy model\n",
    "5. Test endpoint\n",
    "6. Cleanup resouces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry\n",
    "* Setup compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from azure.ai.ml import MLClient, UserIdentityConfiguration\n",
    "from azure.ai.ml import load_environment\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    OnlineRequestSettings,\n",
    "    ProbeSettings,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to the workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](https://aka.ms/azureml-workspace-configuration), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get a handle to the workspace and the registry\n",
    "\n",
    "We use the config file to connect to a workspace. The Azure ML workspace should be configured with a computer cluster. [Check this notebook for configure a workspace](https://aka.ms/azureml-workspace-configuration)\n",
    "\n",
    "If config file is not available user can update following parameters in place holders\n",
    "- SUBSCRIPTION_ID\n",
    "- RESOURCE_GROUP\n",
    "- WORKSPACE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle to workspace\n",
    "try:\n",
    "    mlclient_ws = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    mlclient_ws = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    )\n",
    "\n",
    "mlclient_registry = MLClient(credential, registry_name=\"azureml\")\n",
    "\n",
    "mlclient_ws"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compute target setup\n",
    "\n",
    "#### Create or Attach existing AmlCompute\n",
    "A compute target is required to execute the Automated ML run. In this tutorial, you create AmlCompute as your training compute resource.\n",
    "\n",
    "#### Creation of AmlCompute takes approximately 5 minutes. \n",
    "If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "compute_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    _ = mlclient_ws.compute.get(compute_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute(\n",
    "        name=compute_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"STANDARD_DS12_V2\",\n",
    "        idle_time_before_scale_down=120,\n",
    "        min_instances=0,\n",
    "        max_instances=6,\n",
    "    )\n",
    "    mlclient_ws.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and execute pipeline job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fetch coponents from azureml production registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model = mlclient_registry.components.get(name=\"download_model\", label=\"latest\")\n",
    "register_model = mlclient_registry.components.get(name=\"register_model\", label=\"latest\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a pipleine job and submit run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline()\n",
    "def import_model(\n",
    "    model_id,\n",
    "    custom_model_name=None,\n",
    "    model_source=\"Huggingface\",\n",
    "    model_type=\"custom_model\",\n",
    "    model_metadata=None,\n",
    "    model_version=None,\n",
    "    registry_name=None,\n",
    "):\n",
    "    download = download_model(\n",
    "        model_source=model_source,\n",
    "        model_id=model_id,\n",
    "    )\n",
    "\n",
    "    register = register_model(\n",
    "        model_type=model_type,\n",
    "        model_name=custom_model_name,\n",
    "        registry_name=registry_name,\n",
    "        model_metadata=model_metadata,\n",
    "        model_download_metadata=download.outputs.model_download_metadata,\n",
    "        model_path=download.outputs.model_output,\n",
    "        model_version=model_version,\n",
    "    )\n",
    "\n",
    "    return {\"model_registration_details\": register.outputs.registration_details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_model_id = \"databricks/dolly-v2-12b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = import_model(model_id=import_model_id)\n",
    "job.identity = UserIdentityConfiguration()\n",
    "job.display_name = f\"import {import_model_id}\"\n",
    "job.settings.default_compute = compute_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_job = mlclient_ws.jobs.create_or_update(job)\n",
    "\n",
    "# wait for job to complete\n",
    "mlclient_ws.jobs.stream(submitted_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Download pipeline run output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlclient_ws.jobs.download(\n",
    "    name=submitted_job.name,\n",
    "    output_name=\"model_registration_details\",\n",
    "    download_path=\"./\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Initialise model registration details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registeration_details = {}\n",
    "path = \"./named-outputs/model_registration_details/registration_details\"\n",
    "with open(path) as f:\n",
    "    model_registeration_details = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get registered model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_registeration_details[\"id\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy model\n",
    "We will perform custom model deployment next which would need us to:\n",
    "1. Create an online endpoint\n",
    "2. Create custom environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create an online endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"dolly-v2-12b\"\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    auth_mode=\"key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlclient_ws.online_endpoints.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create environment for deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Create env\n",
    "Create env or move to section 3.1.2 to use an existing env in workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_spec_file = \"./env/spec.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_env = mlclient_ws.environments.create_or_update(\n",
    "    load_environment(source=environment_spec_file)\n",
    ")\n",
    "\n",
    "deploy_env.id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Fetch existing env details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_env = mlclient_ws.environments.get(\n",
    "    name=\"custom-transformers-deployment\", label=\"latest\"\n",
    ")\n",
    "deploy_env.id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initalise deployment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"default\"\n",
    "instance_type = \"Standard_ND40rs_v2\"  ## to be able use CUDA for loading model\n",
    "instance_count = 1\n",
    "\n",
    "# request settings\n",
    "max_concurrent_requests_per_instance = 1\n",
    "request_timeout_ms = 60000\n",
    "max_queue_wait_ms = 60000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ManagedOnlineDeployment(\n",
    "    code_configuration=CodeConfiguration(code=\"./code\", scoring_script=\"predict.py\"),\n",
    "    environment=deploy_env.id,\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model_id,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=max_concurrent_requests_per_instance,\n",
    "        request_timeout_ms=request_timeout_ms,\n",
    "        max_queue_wait_ms=max_queue_wait_ms,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlclient_ws.online_deployments.begin_create_or_update(deployment).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Update endpoint to take traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {deployment_name: 100}\n",
    "mlclient_ws.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the endpoint\n",
    "Now when we have an active deployment, we can invoke endpoint using a test payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_payload_file = \"./data/payload.json\"\n",
    "response = mlclient_ws.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    deployment_name=deployment_name,\n",
    "    request_file=inference_payload_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean up resources\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlclient_ws.online_endpoints.begin_delete(name=endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}