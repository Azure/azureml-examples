{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import models from Hugging Face hub\n",
    "This sample shows how to import and register models from [HuggingFace hub](https://huggingface.co/models). \n",
    "\n",
    "### How does import work?\n",
    "The import process runs as a job in your AzureML workspace using components from the `azureml` system registry. The models are downloaded and converted to MLflow packaged format. You can then register the models to your AzureML Workspace or Registry that makes them available for inference or fine tuning. \n",
    "\n",
    "### What models are supported for import?\n",
    "Any model from Hugging Face hub can be downloaded using the `download_model` component. Only the following set of tasks are supported for MLflow conversion:\n",
    "* fill-mask\n",
    "* token-classification\n",
    "* question-answering\n",
    "* summarization\n",
    "* text-generation\n",
    "* text-classification\n",
    "* translation\n",
    "* image-classification\n",
    "* text-to-image\n",
    "Conversion to MLflow will fail if you attempt to download a model that has a task type other than the above.\n",
    "\n",
    "### Why convert to MLflow?\n",
    "MLflow is AzureML's recommended model packaging format. \n",
    "* **Inference benefits**: AzureML supports no-code-deployment for models packaged as MLflow that enables a seamless inference experience for the models. Learn more about [MLflow model packaging](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow-models) and [no-code-deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models-online-endpoints?tabs=sdk). \n",
    "* **Fine tuning benefits**: Foundation models imported and converted to MLflow format can be fine tuned using AzureML's fine tuning pipelines. You can use the no-code UI wizards, or the code based job submission with the SDK or CLI/YAML. AzureML's fine tuning pipelines are built using components. This gives you the flexibility to compose your own fine tuning pipelines containing your own jobs for data transformation, post processing and the AzureML fine tuning components. Learn more about pipelines using [sdk](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python) or [CLI](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipelines-cli).\n",
    "\n",
    "### What happens if I just download model and register models without converting to MLflow? That's because the task of the model I'm interested in is not among the supported list of tasks.\n",
    "You can still download and register the model using the outputs of the `download_model` job. You need to [write your own inference code](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?tabs=python) in this case. It also means that fine tuning is not yet supported if the task type of the model you are interested in is not in the supported list.\n",
    "\n",
    "### Outline\n",
    "* Setup pre-requisites such as compute.\n",
    "* Pick a model to import.\n",
    "* Configure the import job.\n",
    "* Run the fine tuning job.\n",
    "* Register the fine tuned model. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry\n",
    "* Set an optional experiment name\n",
    "* Check or create compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, ClientSecretCredential\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id =  \"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name =  \"<RESOURCE_GROUP>\",\n",
    "        workspace_name =  \"<WORKSPACE_NAME>\"\n",
    ")\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml-preview\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml-preview-test1\")\n",
    "\n",
    "experiment_name = \"question-answering-extractive-qna\"\n",
    "\n",
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "compute_cluster = \"gpu-cluster-big\"\n",
    "try:\n",
    "    workspace_ml_client.compute.get(compute_cluster)\n",
    "except Exception as ex:\n",
    "    compute = AmlCompute(\n",
    "        name = compute_cluster, \n",
    "        size= \"Standard_ND40rs_v2\",\n",
    "        max_instances= 2 # For multi node training set this to an integer value more than 1\n",
    "    )\n",
    "    workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "\n",
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute. \n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpus_per_node = 2 \n",
    "\n",
    "# genrating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time())) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a model to import\n",
    "Browse models on [HuggingFace hub](https://huggingface.co/models) and identify a model to import. Make sure the task type of the model is among the supported tasks as explained in the introduction section. Copy the model id which is available in the URI of the page or can be copied using the copy icon next to the model name and assign it to the variable `huggingface_model_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model_id = \"microsoft/deberta-base\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure the model import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# fetch the download_model component from the system registry\n",
    "download_model_func = registry_ml_client.components.get(name=\"download_model\", version=\"0.0.2\")\n",
    "# fetch the mlflow converter component from the system registry\n",
    "convert_model_to_mlflow_func = registry_ml_client.components.get(name=\"convert_model_to_mlflow\", version=\"0.0.1\")\n",
    "# fetch the model registration component - cannot use system registry version since it needs managed identity.\n",
    "# we are investigating how to use obo\n",
    "register_model_func = load_component(source=\"./tools/register-model/register.yml\")\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "\n",
    "    download_model_job = download_model_func(\n",
    "        model_id = huggingface_model_id,\n",
    "        model_source = \"Huggingface\"\n",
    "    )\n",
    "    convert_model_to_mlflow_job = convert_model_to_mlflow_func(\n",
    "        model_download_details = download_model_job.outputs.model_info,\n",
    "        model_path = download_model_job.outputs.model_output,\n",
    "        # we will do away with the need to specify task_type soon\n",
    "        task_type = \"fill-mask\"\n",
    "    )\n",
    "\n",
    "# blocked on Bug 2317665: how to use obo in pipeline dsl?\n",
    "#    register_model_job = register_model_func(\n",
    "#        model_path = convert_model_to_mlflow_job.outputs.mlflow_model_folder,\n",
    "#        download_details = download_model_job.outputs.model_info,\n",
    "#    )\n",
    "#    register_model_job.identity.type = \"user_identity\"\n",
    "\n",
    "    return {\n",
    "        \"downloaded_model\": download_model_job.outputs.model_output,\n",
    "        \"downloaded_model_mlflow\": convert_model_to_mlflow_job.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "pipeline_object.settings.default_compute  = compute_cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Submit the import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(pipeline_object, experiment_name=experiment_name)\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Register the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "# check if the `trained_model` output is available\n",
    "print (\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job.name).outputs)\n",
    "\n",
    "#fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "model_path_from_job = (\"azureml://jobs/{0}/outputs/{1}\".format(pipeline_job.name, \"downloaded_model_mlflow\"))\n",
    "\n",
    "# replace any slashes in the model name with dashes because slash is not a valid character in for model names in AzureML\n",
    "huggingface_model_id_registered = huggingface_model_id.replace(\"/\", \"-\")\n",
    "\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=huggingface_model_id_registered,\n",
    "    version=timestamp, # use timestamp as version to avoid version conflict\n",
    "    description=\"Imported from Huggingface. View the model card at https://huggingface.co/\" + huggingface_model_id\"\n",
    ")\n",
    "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
    "#register the model from pipeline job output \n",
    "registered_model = workspace_ml_client.models.create_or_update(prepare_to_register_model)\n",
    "print (\"registered model: \\n\", registered_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
