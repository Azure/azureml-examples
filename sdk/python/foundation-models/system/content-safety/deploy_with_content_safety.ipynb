{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create an Safety-Enabled LLaMa Online Endpoint\n",
    "## This notebook will walk you through the steps to create a Safety-Enabled LLaMa Online Endpoint.\n",
    "### The steps are:\n",
    "1. Create a Content Safety for moderating the request from user and response from the LLaMa Online Endpoint\n",
    "2. Create a new LLaMa Online Endpoint\n",
    "3. Create a new Safety-Enabled LLaMa Online Endpoint with a custom score.py file which will be used to moderate the request and response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prerequisites\n",
    "#### 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-identity==1.14.0b1\n",
    "%pip install azure-mgmt-cognitiveservices==13.4.0\n",
    "%pip install --pre azure-ai-ml \n",
    "%pip install --pre azure-mgmt-msi\n",
    "%pip install --pre azure-mgmt-authorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Assign variables for the workspace and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace_name = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Decide on a name for your Safety LLaMa Online Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# rand = random.randint(0, 10000)\n",
    "# endpoint_name = f\"safetyllama{rand}\" \n",
    "endpoint_name = \"<ONLINE-ENDPOINT-NAME>\" # the final endpoint name of the safety enabled llama endpoint\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Connect to your AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T09:28:38.486754700Z",
     "start_time": "2023-07-04T09:28:35.703401800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "from azure.ai.ml import MLClient, Input, Output \n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information to contain\n",
    "    #       your subscription ID, resource group name, and workspace name\n",
    "    client_config = {\n",
    "        \"subscription_id\": subscription_id,\n",
    "        \"resource_group\": resource_group,\n",
    "        \"workspace_name\": workspace_name,\n",
    "    }\n",
    "    # write and reload from config file\n",
    "    config_path = \"../.azureml/config.json\"\n",
    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "    with open(config_path, \"w\") as fo:\n",
    "        fo.write(json.dumps(client_config))\n",
    "    ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "    \n",
    "print(ml_client)\n",
    "workspace_location = ml_client.workspaces.get(ml_client.workspace_name).location\n",
    "subscription_id = ml_client.subscription_id\n",
    "resource_group_name = ml_client.resource_group_name\n",
    "workspace_name = ml_client.workspace_name\n",
    "print(workspace_location)\n",
    "print(subscription_id)\n",
    "print(resource_group_name)\n",
    "print(workspace_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Azure AI Content Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Choose a region for your Content Safety\n",
    "Currently, Azure AI Content Safety only available in the following regions:\n",
    "- East US\n",
    "- West Europe\n",
    "- Central US EUAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will create aacs in east us\n"
     ]
    }
   ],
   "source": [
    "aacs_location = \"east us\" # please choose the nearest location to your workspace\n",
    "print(f\"will create aacs in {aacs_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.mgmt.cognitiveservices.models import Account, Sku, AccountProperties\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "client = CognitiveServicesManagementClient(credential, subscription_id)\n",
    "# create a new Cognitive Services Account\n",
    "# name of the AACS to create\n",
    "name_of_aacs = f\"{endpoint_name}-aacs\"\n",
    "kind = \"ContentSafety\"\n",
    "sku_name = \"S0\"\n",
    "parameters = Account(sku=Sku(name=sku_name), kind=kind, location=aacs_location, properties= AccountProperties(custom_sub_domain_name=name_of_aacs, public_network_access=\"Enabled\"))\n",
    "# How many seconds to wait between checking the status of an async operation.\n",
    "wait_time = 10\n",
    "\n",
    "poller = client.accounts.begin_create(resource_group_name, name_of_aacs, parameters)\n",
    "while (False == poller.done()) :\n",
    "    print (\"Waiting {wait_time} seconds for operation to finish.\".format (wait_time = wait_time))\n",
    "    time.sleep (wait_time)\n",
    "    # This will raise an exception if the server responded with an error.\n",
    "    result = poller.result()\n",
    "\n",
    "\n",
    "print(\"Resource created.\")\n",
    "\n",
    "aacs=client.accounts.get(resource_group_name, name_of_aacs)\n",
    "aacs_endpoint = aacs.properties.endpoint\n",
    "aacs_resource_id = aacs.id\n",
    "print(aacs_endpoint)\n",
    "print(aacs_resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create LLaMa Oneline endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Decide on SKU and instance count for the LLama Oneline endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_sku_for_llama=\"Standard_DS5_v2\" # the sku of the compute instance for LLaMa endpoint\n",
    "compute_instance_count_for_llama=1 # the number of compute instance\n",
    "llama_endpoint_name=f\"{endpoint_name}-llama\"\n",
    "print(f\"Will create LLaMa endpoint {llama_endpoint_name} using {compute_instance_count_for_llama} {compute_sku_for_llama} compute instance(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Check if LLaMa model is available in the aml registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\" # TODO(mingtwan) change to LLaMa\n",
    "auth_mode_for_llama = \"aml_token\" # we will use AML token for authentication\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "version_list = list(registry_ml_client.models.list(model_name))\n",
    "\n",
    "foundation_model = None\n",
    "if len(version_list) == 0:\n",
    "    print(\"Model not found in registry\")\n",
    "else:\n",
    "    model_version = version_list[0].version\n",
    "    foundation_model = registry_ml_client.models.get(model_name, model_version)\n",
    "    print(\n",
    "        \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for inferencing\".format(\n",
    "            foundation_model.name, foundation_model.version, foundation_model.id\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Create LLaMa Online endpoint\n",
    "This step may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, OnlineRequestSettings\n",
    "\n",
    "# create an online endpoint\n",
    "llama_endpoint = ManagedOnlineEndpoint(\n",
    "        name=llama_endpoint_name,\n",
    "        description=\"Online endpoint for LLaMa\",\n",
    "        auth_mode=auth_mode_for_llama,\n",
    "    )\n",
    "ml_client.begin_create_or_update(llama_endpoint).result()\n",
    "\n",
    "deployment_name=\"demo\"\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=llama_endpoint_name,\n",
    "    model=foundation_model.id,\n",
    "    instance_type=compute_sku_for_llama,\n",
    "    instance_count=compute_instance_count_for_llama,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=60000,\n",
    "    )\n",
    ")\n",
    "ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "# deployment takes 100 traffic\n",
    "llama_endpoint.traffic = {deployment_name: 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(llama_endpoint)\n",
    "\n",
    "llama_endpoint = ml_client.online_endpoints.get(name=llama_endpoint_name)\n",
    "print(llama_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create `score.py` for the Safety enabled LLaMa endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Create a folder to save the score.py and conda dependencies file.\n",
    "First create a source folder for the score.py file and conda dependencies file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "scoring_src_dir = \"./safety-llama\"\n",
    "os.makedirs(scoring_src_dir, exist_ok=True)\n",
    "print(f\"Scoring script directory: {scoring_src_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Create the score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Create the conda.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {scoring_src_dir}/conda.yaml\n",
    "name: aacs-conda\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip:\n",
    "    - azure-identity==1.14.0b1\n",
    "    - azure-ai-ml==1.8.0\n",
    "    - azureml-inference-server-http==0.8.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create a Managed Identity for the safety enabled LLaMa endpoint\n",
    "As you can see in the above steps, we specified auth_mode to aml_token for the LLaMa online endpoint and Content Safey is support token based authentication by default, which means we need to create a managed identity for the safety enabled LLaMa endpoint, so that it can access the Azure AI Content Safety and the LLaMa endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Decide on the name of your user identity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_name = f\"{endpoint_name}-uai\"\n",
    "print(f\"Will create UAI {uai_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Get a handle to the ManagedServiceIdentityClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.msi import ManagedServiceIdentityClient\n",
    "from azure.mgmt.msi.models import Identity\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "    \n",
    "msi_client = ManagedServiceIdentityClient(\n",
    "    subscription_id=subscription_id,\n",
    "    credential=credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Create the user identity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi_client.user_assigned_identities.create_or_update(\n",
    "    resource_group_name=resource_group_name,\n",
    "    resource_name=uai_name,\n",
    "    parameters=Identity(location=workspace_location),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Retrieve the identity object\n",
    "we need to retrieve the identity object so that we can use it to configure the Safety-Enabled LLaMa Online Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_identity = msi_client.user_assigned_identities.get(\n",
    "    resource_group_name=resource_group_name,\n",
    "    resource_name=uai_name,\n",
    ")\n",
    "uai_principal_id = uai_identity.principal_id\n",
    "uai_client_id = uai_identity.client_id\n",
    "uai_id = uai_identity.id\n",
    "print(f\"UAI principal id: {uai_principal_id}\")\n",
    "print(f\"UAI client id: {uai_client_id}\")\n",
    "print(f\"UAI id: {uai_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Give Access permission to the Managed Identity we created above.\n",
    "Note: In order to successfully run scripts in current step, your must have owner permission on the AACS resource and the LLaMa endpoint, which we created in the previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Grant the user identity access to the LLaMa endpoint by updating LLaMa online endpoint's tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_endpoint = ml_client.online_endpoints.get(name=llama_endpoint_name)\n",
    "llama_endpoint.tags = {\"AllowlistedObjectIds\": uai_principal_id}\n",
    "ml_client.online_endpoints.begin_create_or_update(llama_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Get an AuthorizationManagementClient to list Role Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.authorization import AuthorizationManagementClient\n",
    "from azure.mgmt.authorization.v2018_01_01_preview.models import RoleDefinition\n",
    "import uuid\n",
    "\n",
    "role_definition_client = AuthorizationManagementClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    "    api_version=\"2018-01-01-preview\",\n",
    ")\n",
    "\n",
    "from azure.mgmt.authorization.v2020_10_01_preview.models import RoleAssignmentCreateParameters\n",
    "\n",
    "role_assignment_client = AuthorizationManagementClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    "    api_version=\"2020-10-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Grant the user identity access to the Content Safety\n",
    "Cognitive Services User role is required to access the Content Safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"Cognitive Services User\"\n",
    "scope = aacs_resource_id\n",
    "\n",
    "role_defs = role_definition_client.role_definitions.list(scope=scope)\n",
    "role_def = next((r for r in role_defs if r.role_name == role_name))\n",
    "\n",
    "role_assignment_client.role_assignments.create(\n",
    "    scope=scope,\n",
    "    role_assignment_name=str(uuid.uuid4()),\n",
    "    parameters=RoleAssignmentCreateParameters(\n",
    "        role_definition_id=role_def.id,\n",
    "        principal_id=uai_principal_id,\n",
    "        principal_type=\"ServicePrincipal\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Assign AcrPull at the workspace container registry scope\n",
    "Since we will create the safety enabled llama endpoint with User Assigned Identity, the user's managed identity must have Storage blob data reader permission on the storage account for the workspace, and AcrPull permission on the Azure Container Registry (ACR) for the workspace. Make sure your User Assigned Identity has the right permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = ml_client.workspaces.get(workspace_name)\n",
    "container_registry = workspace.container_registry\n",
    "\n",
    "role_name = \"AcrPull\"\n",
    "acr_scope = container_registry\n",
    "\n",
    "role_defs = role_definition_client.role_definitions.list(scope=acr_scope)\n",
    "role_def = next((r for r in role_defs if r.role_name == role_name))\n",
    "\n",
    "role_assignment_client.role_assignments.create(\n",
    "    scope=acr_scope,\n",
    "    role_assignment_name=str(uuid.uuid4()),\n",
    "    parameters=RoleAssignmentCreateParameters(\n",
    "        role_definition_id=role_def.id,\n",
    "        principal_id=uai_principal_id,\n",
    "        principal_type=\"ServicePrincipal\",\n",
    "    ),\n",
    ")\n",
    "print(\"Role assignment for AcrPull at the workspace container registry completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"Storage Blob Data Reader\"\n",
    "blob_scope = workspace.storage_account\n",
    "\n",
    "role_defs = role_definition_client.role_definitions.list(scope=blob_scope)\n",
    "role_def = next((r for r in role_defs if r.role_name == role_name))\n",
    "\n",
    "role_assignment_client.role_assignments.create(\n",
    "    scope=blob_scope,\n",
    "    role_assignment_name=str(uuid.uuid4()),\n",
    "    parameters=RoleAssignmentCreateParameters(\n",
    "        role_definition_id=role_def.id,\n",
    "        principal_id=uai_principal_id,\n",
    "        principal_type=\"ServicePrincipal\",\n",
    "    ),\n",
    ")\n",
    "print(\"Role assignment for `Storage Blob Data Reader` at the workspace storage account completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Create Safety-Enabled LLaMa Online Endpoint using above score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 Decide on SKU and instance count for the Safety-Enabled LLaMa Online Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_sku_for_safety_proxy = \"Standard_DS5_v2\"\n",
    "compute_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 Create the Safety-Enabled LLaMa Online Endpoint\n",
    "This step may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment variables that will be used in the scoring script\n",
    "env_key_of_aacs_endpoint = \"AACS_ENDPOINT\"\n",
    "env_key_of_llama_score_uri = \"LLAMA_SCORE_URI\"\n",
    "env_key_of_uai_id = \"UAI_CLIENT_ID\"\n",
    "env_key_of_subscription_id = \"SUBSCRIPTION_ID\"\n",
    "env_key_of_resource_group_name = \"RESOURCE_GROUP_NAME\"\n",
    "env_key_of_workspace_name = \"WORKSPACE_NAME\"\n",
    "\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineDeployment,\n",
    "    ManagedOnlineEndpoint,\n",
    "    CodeConfiguration,\n",
    "    Environment,\n",
    "    ManagedIdentityConfiguration,\n",
    "    IdentityConfiguration\n",
    ")\n",
    "\n",
    "llama_score_uri = llama_endpoint.scoring_uri\n",
    "if not llama_score_uri:\n",
    "    raise Exception(\"LLaMa Endpoint has no scoring uri.\")\n",
    "else:\n",
    "    print(f\"LLaMa Endpoint scoring uri: {llama_score_uri}\")\n",
    "\n",
    "if not aacs_endpoint:\n",
    "    raise Exception(\"AACS Endpoint is not valid.\")\n",
    "else:\n",
    "    print(f\"AACS Endpoint: {aacs_endpoint}\")\n",
    " \n",
    "deployment = ManagedOnlineDeployment(\n",
    "        name=\"blue\",\n",
    "        endpoint_name=endpoint_name,\n",
    "        code_configuration=CodeConfiguration(\n",
    "            code=f\"{scoring_src_dir}\", scoring_script=\"score.py\"\n",
    "        ),\n",
    "        environment=Environment(\n",
    "            conda_file=f\"{scoring_src_dir}/conda.yaml\",\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "        ),\n",
    "        instance_type=compute_sku_for_safety_proxy,\n",
    "        instance_count=compute_count,\n",
    "        environment_variables={\n",
    "            env_key_of_uai_id: uai_client_id,\n",
    "            env_key_of_aacs_endpoint: aacs_endpoint,\n",
    "            env_key_of_llama_score_uri: llama_score_uri,\n",
    "            env_key_of_subscription_id: subscription_id,\n",
    "            env_key_of_resource_group_name: resource_group_name,\n",
    "            env_key_of_workspace_name: workspace_name\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "        name=endpoint_name,\n",
    "        auth_mode=\"key\",\n",
    "        identity=IdentityConfiguration(\n",
    "            type=\"user_assigned\",\n",
    "            user_assigned_identities=[\n",
    "                ManagedIdentityConfiguration(resource_id=uai_id)\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "# create online endpoint\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "\n",
    "endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
    "print(endpoint.identity.type)\n",
    "print(endpoint.identity.user_assigned_identities)\n",
    "\n",
    "# create deployment\n",
    "ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
    "# check status\n",
    "deployment = ml_client.online_deployments.get(\n",
    "        endpoint_name=endpoint_name, name=deployment.name\n",
    "    )\n",
    "print(deployment)\n",
    "# Set traffic to 100% for deployment\n",
    "endpoint.traffic = {str(deployment.name): 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Test the Safety Enabled LLaMa online endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1 Prepare sample request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test script directory: ./safety-llama-test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "test_src_dir = \"./safety-llama-test\"\n",
    "os.makedirs(test_src_dir, exist_ok=True)\n",
    "print(f\"test script directory: {test_src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./safety-llama-test/sample-request.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile {test_src_dir}/sample-request.json\n",
    "{\"data\": \"Hello World\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = f\"{test_src_dir}/sample-request.json\"\n",
    "ml_client.online_endpoints.invoke(endpoint_name=endpoint_name, request_file=sample_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
