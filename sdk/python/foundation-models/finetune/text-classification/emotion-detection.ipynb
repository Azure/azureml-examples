{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification - Emotion Detection \n",
    "\n",
    "This sample shows how to fine tune a model to detect emotions using emotion dataset and deploy it to an endpoint for real time inference.\n",
    "\n",
    "### Training data\n",
    "We will use the [emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset a copy of which is available in the `azureml` system registry for easy access.\n",
    "\n",
    "### Model\n",
    "Models that can perform the `fill-mask` task are generally good candidates to fine tune for `text-classification`. We will list all models of the `fill-mask` type, from which you can pick one. If you opened this notebook from a specific model card, copy past the model `Asset ID`. Optionally, if you need to fine tune a model that is available on HuggingFace, but not available in `azureml` system registry, you can either [import](https://github.com/Azure/azureml-examples) the model or use the `huggingface_id` parameter to use a model directly from HuggingFace. [Learn more]().\n",
    "\n",
    "### Outline\n",
    "* Setup pre-requisites such as compute.\n",
    "* Pick a model to fine tune.\n",
    "* Pick and explore training data.\n",
    "* Configure the fine tuning job.\n",
    "* Run the fine tuning job.\n",
    "* Register the fine tuned model. \n",
    "* Deploy the fine tuned model for real time inference.\n",
    "* Clean up resources. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry\n",
    "* Set an optional experiment name\n",
    "* Check or create compute. A single GPU node can have multiple GPU cards. For example, in one node of `Standard_ND40rs_v2` there are 8 NVIDIA V100 GPUs while in `Standard_NC12s_v3`, there are 2 NVIDIA V100 GPUs. Refer to the [docs](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) for this information. The number of GPU cards per node is set in the param `gpus_per_node` below. Setting this value correctly will ensure utilization of all GPUs in the node. The recommended GPU compute SKUs can be found [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series) and [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, ClientSecretCredential\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id =  \"21d8f407-c4c4-452e-87a4-e609bfb86248\", #\"<SUBSCRIPTION_ID>\"\n",
    "        resource_group_name =  \"rg-contoso-819prod\", #\"<RESOURCE_GROUP>\",\n",
    "        workspace_name =  \"mlw-contoso-819prod\", #\"WORKSPACE_NAME>\",\n",
    ")\n",
    " \n",
    "registry_ml_client = MLClient(\n",
    "    credential,\n",
    "    registry_name=\"azureml-preview\",\n",
    ")\n",
    "\n",
    "experiment_name = \"text-classification-emotion-detection\"\n",
    "\n",
    "compute_cluster = \"gpu-cluster-big\"\n",
    "try:\n",
    "    workspace_ml_client.compute.get(compute_cluster)\n",
    "except Exception as ex:\n",
    "    compute = AmlCompute(\n",
    "        name = compute_cluster, # If you already have a gpu cluster, mention it here.\n",
    "        size= \"Standard_ND40rs_v2\",\n",
    "        max_instances= 2 # For multi node training set this to an integer value more than 1\n",
    "    )\n",
    "    workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "\n",
    "gpus_per_node = 2 # This is the number of GPUs in a single node of the selected 'vm_size' compute\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a model to fine tune\n",
    "\n",
    "We will query the `azureml` system registry and list all models of the type `fill-mask`. Any of these models will work for `text-classification`, but in this example, we use `bert-base-uncased`. If you have opened this notebook for a specific mode, replace the model name and version accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = registry_ml_client.models.list()\n",
    "for model in models:\n",
    "    versions=registry_ml_client.models.list(model.name) # replace this with get the latest version?\n",
    "    for version in versions:\n",
    "        if (version.tags['task'] == 'fill-mask'):\n",
    "            print (\"Model name: {0}, version: {1}\".format(version.name, version.version))\n",
    "        break       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model_version = \"3\"\n",
    "print (\"\\n\\nUsing model name: {0}, version: {1} for fine tuning\".format(model_name, model_version))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the dataset for to fine tune\n",
    "> This notebook pulls from HuggingFace datasets but we will change this to point to system registry after we onboard sample data to system registry and fine tune component supports data splitting\n",
    "\n",
    "Start by fetching dataset label names. The actual data contains label numeric categories so we will use this metadata to add a column that contains actual label names when we download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import pandas as pd\n",
    "# toto - this data asset should be loaded from the system registry\n",
    "data_asset = workspace_ml_client.data.get(name=\"emotion\", version=1)\n",
    "print(data_asset)\n",
    "\n",
    "# todo - show some sample data from the data asset\n",
    "# df = pd.read_json(data_asset.path, lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Submit the fine tuning job using the the model and data as inputs\n",
    " \n",
    "Create the job that uses the `text-classification` pipeline component. [Learn more]() about all the parameters supported for fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(name=\"textclassificationsinglelabel_pipelinecomponent\", version=\"0.0.13\")\n",
    "\n",
    "# temporary registry until split_dataset is available in fine tune component\n",
    "registry_data_ml_client = MLClient(\n",
    "    credential,\n",
    "    registry_name=\"sample-data\",\n",
    ")\n",
    "split_dataset_func = registry_data_ml_client.components.get(name=\"split_dataset\", version=\"0.0.11\")\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "    split_data_job = split_dataset_func(\n",
    "        data_file = Input(type=\"uri_file\", path=data_asset.path),\n",
    "        train_split = 0.05, # dataset has 50k+ rows, so picking a small number for sample pipeline\n",
    "        validation_split = 0.005, # 10% of train split\n",
    "        test_split = 0.005, # 10% of train split\n",
    "    )\n",
    "    split_data_job.compute = compute_cluster\n",
    "\n",
    "    finetuning_job = pipeline_component_func( \n",
    "        huggingface_id = model_name, # this needs to change to use model from system registry\n",
    "        compute_model_selector = compute_cluster,\n",
    "        compute_preprocess = compute_cluster,\n",
    "        compute_finetune = compute_cluster,\n",
    "        compute_model_evaluation = compute_cluster,\n",
    "        train_file_path = split_data_job.outputs.train_file, \n",
    "        valid_file_path = split_data_job.outputs.validation_file,\n",
    "        test_file_path = split_data_job.outputs.test_file,\n",
    "        sentence1_key = \"text\", # picked up by visualizing the sample data in step 3\n",
    "        label_key = \"label_string\", # picked up by visualizing the sample data in step 3\n",
    "        test_data_input_column_names = \"text\", # picked up by visualizing the sample data in step 3\n",
    "        process_count_per_instance_finetune = gpus_per_node, # set to the number of GPUs available in the compute\n",
    "        epochs = 2,\n",
    "        learning_rate = 2e-5, \n",
    "    )\n",
    "    return {\n",
    "        \"trained_model\": finetuning_job.outputs.mlflow_model_folder_finetune\n",
    "    }\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "pipeline_object.display_name =  \"text-classification-using-\" + model_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = workspace_ml_client.jobs.create_or_update(pipeline_object, experiment_name=experiment_name)\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Register the fine tuned model with the workspace\n",
    "\n",
    "We will register the model from the output of the fine tuning job. This will track lineage between the fine tuned model and the fine tuning job. The fine tuning job, further, tracks lineage to the foundation model, data and training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "# check if the `trained_model` output is available\n",
    "print (\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job.name).outputs)\n",
    "\n",
    "# fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "# model_path_from_job = (\"azureml://jobs/{0}/outputs/{1}\".format(pipeline_job.name, \"trained_model\"))\n",
    "\n",
    "for level1_job in workspace_ml_client.jobs.list(parent_job_name=pipeline_job.name): # pipeline component job\n",
    "    for level2_job in workspace_ml_client.jobs.list(parent_job_name=level1_job.name): # pipeline component subgraph job (not shown in UI)\n",
    "        for level3_job in workspace_ml_client.jobs.list(parent_job_name=level2_job.name): # child jobs\n",
    "            if (level3_job.display_name == \"finetune\"):\n",
    "                model_path_from_job = (\"azureml://jobs/{0}/outputs/{1}\".format(level3_job.name, \"mlflow_model_folder\"))\n",
    "\n",
    "finetuned_model_name = model_name + \"-emotion-detection\"\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "#prepare_to_register_model = Model(\n",
    "#    path=model_path_from_job,\n",
    "#    type=AssetTypes.MLFLOW_MODEL,\n",
    "#    name=finetuned_model_name\n",
    "#    version=1,\n",
    "#    description=model_name + \" fine tuned model for emotion detection\"\n",
    "#)\n",
    "#print(prepare_to_register_model)\n",
    "# register the model from pipeline job output \n",
    "# registered_model = workspace_ml_client.models.create_or_update(prepare_to_register_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cli to register model, with path from the above output until \n",
    "! az ml model create --path azureml://jobs/4765c75a-871a-476f-ac5b-5fba7cd01263/outputs/mlflow_model_folder --name bert-base-uncased-emotion-detection --version 1 --type mlflow_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Deploy the fine tuned model to an online endpoint\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "finetuned_model_name = \"bert-base-uncased\" + \"-emotion-detection\"\n",
    "\n",
    "registered_model = workspace_ml_client.models.get(name=finetuned_model_name, version=1)\n",
    "\n",
    "timestamp = str(int(time.time())) # endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "# Create online endpoint\n",
    "online_endpoint_name = \"emotion-\" + timestamp\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \" + registered_model.name + \", fine tuned model for emotion detection\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test the endpoint with sample data\n",
    "\n",
    "We will fetch some sample data from the test data and submit to online endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "429d412e307b288f3a8cba821a3ba110e77b02cf5672d0d0b14db25cc0bc89f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
