{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example code for creating Faiss index\n",
    "\n",
    "Efficient text retrieval and matching from a large volume of text are crucial for building a Q&A system. One common method is to convert the text into vector representations and create index based on these vectors, which enables fast retrieval by utilizing the similarity between vectors. This example demonstrates the process of spliting the document into small chunks, leveraging an embedding store to convert the text into vectors and generating Faiss index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install promptflow-vectordb SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install promptflow-vectordb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from promptflow_vectordb.core.contracts import (\n",
    "    EmbeddingModelType,\n",
    "    StorageType,\n",
    "    StoreCoreConfig,\n",
    ")\n",
    "from promptflow_vectordb.core.embeddingstore_core import EmbeddingStoreCore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your data\n",
    "For convenience, a few Azure Machine Learning documentation webpages are selected here as sample data. You can replace them with your own dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PREFIX = \"https://learn.microsoft.com/en-us/azure/machine-learning/\"\n",
    "URL_NAME_LIST = [\n",
    "    \"tutorial-azure-ml-in-a-day\",\n",
    "    \"overview-what-is-azure-machine-learning\",\n",
    "    \"concept-v2\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data to local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(local_file_path, exist_ok=True)\n",
    "for url_name in URL_NAME_LIST:\n",
    "    url = os.path.join(URL_PREFIX, url_name)\n",
    "    destination_path = os.path.join(local_file_path, url_name)\n",
    "    urllib.request.urlretrieve(url, destination_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and create an embedding store\n",
    "promptflow-vectordb sdk supports multiple types of embedding models (Azure OpenAI, OpenAI) and multiple types of store path (local path, HTTP URL, Azure blob). In this example, configure an embedding store with Azure OpenAI embedding model and local store path.\n",
    "\n",
    "Please refer to [create a resource and deploy a model using Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal) to set up an AOAI embedding model deployment. The output vector returned by different embedding models has different dimensions. It is recommended to deploy `text-embedding-ada-002` model, and the dimension of the output vector returned by this model is 1536. \n",
    "\n",
    "To use AOAI model, please store `Azure_OpenAI_MODEL_ENDPOINT` and `Azure_OpenAI_MODEL_API_KEY` as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_API_VERSION = \"2023-05-15\"\n",
    "MODEL_DEPLOYMENT_NAME = \"text-embedding-ada-002\"\n",
    "DIMENSION = 1536\n",
    "\n",
    "# Configure an embedding store to store index file.\n",
    "store_path = os.path.join(os.getcwd(), \"faiss_index_store\")\n",
    "config = StoreCoreConfig.create_config(\n",
    "    storage_type=StorageType.LOCAL,\n",
    "    store_identifier=store_path,\n",
    "    model_type=EmbeddingModelType.AOAI,\n",
    "    model_api_base=os.environ[\"Azure_OpenAI_MODEL_ENDPOINT\"],\n",
    "    model_api_key=os.environ[\"Azure_OpenAI_MODEL_API_KEY\"],\n",
    "    model_api_version=MODEL_API_VERSION,\n",
    "    model_name=MODEL_DEPLOYMENT_NAME,\n",
    "    dimension=DIMENSION,\n",
    "    create_if_not_exists=True,\n",
    ")\n",
    "store = EmbeddingStoreCore(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split document to chunks, embed chunks and create Faiss index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_chunks(file_name: str) -> List[str]:\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        page_content = f.read()\n",
    "        # use BeautifulSoup to parse HTML content\n",
    "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "        text = soup.get_text(\" \", strip=True)\n",
    "        chunks = []\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=10)\n",
    "        for chunk in splitter.split_text(text):\n",
    "            chunks.append(chunk)\n",
    "        return chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inserting chunks into embedding store, the chunks are transformed into embeddings and Faiss index is generated under the store path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(local_file_path):\n",
    "    for file in files:\n",
    "        each_file_path = os.path.join(root, file)\n",
    "\n",
    "        # Split the file into chunks.\n",
    "        chunks = get_file_chunks(each_file_path)\n",
    "        count = len(chunks)\n",
    "        if URL_PREFIX is not None:\n",
    "            metadatas = [\n",
    "                {\"title\": file, \"source\": os.path.join(URL_PREFIX, file)}\n",
    "            ] * count\n",
    "        else:\n",
    "            metadatas = [{\"title\": file}] * count\n",
    "\n",
    "        # Embed chunks into embeddings, generate index in embedding store.\n",
    "        # If your data is large, inserting too many chunks at once may cause\n",
    "        # rate limit errorï¼Œyou can refer to the following link to find solution\n",
    "        # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/quotas-limits\n",
    "        store.batch_insert_texts(chunks, metadatas)\n",
    "        print(f\"Create index for {file} file successfully.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear index cache in memory \n",
    " If you run the cell to insert chunks into embedding store again, you should clear the index memory cache to prevent the index increase unexpected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.clear():"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "Now you have successfully created Faiss index. To build a complete Q&A system, you can use [Faiss Index Lookup tool](https://aka.ms/faiss_index_lookup_tool) to search relavant texts from the created index by [Azure Machine Learning Prompt Flow](https://aka.ms/AMLPromptflow)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlsubmitjobs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
