{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Index with custom cracking and chunking using Azure Document Intelligence\n",
    "\n",
    "Create an index with custom cracking and chunking using the Azure Document Intelligence aka Azure Form Recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U azure-ai-ml>=1.10\n",
    "%pip install -U 'azureml-rag[azure,cognitive_search]>=0.2.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"subscription_id\": \"<subscription_id>\",\n",
    "    \"resource_group\": \"<resource_group_name>\",\n",
    "    \"workspace_name\": \"<workspace_name>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `crack_and_chunk_with_doc_intel` Component which can be used in place of the `crack_and_chunk` Component in Vector Index creation Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from azure.ai.ml import Input, Output, command\n",
    "from azure.ai.ml.entities import BuildContext, Environment\n",
    "\n",
    "llm_rag_embeddings_doc_intel_environment = Environment(\n",
    "    name=\"llm_rag_embeddings_doc_intel\",\n",
    "    description=\"AzureML RAGs base crack_and_chunk environment with azure-ai-formrecognizer installed.\",\n",
    "    build=BuildContext(path=Path.cwd() / \"doc_intel_env\"),\n",
    ")\n",
    "\n",
    "crack_and_chunk_with_doc_intel_component = command(\n",
    "    version=\"0.0.1\",\n",
    "    name=\"crack_and_chunk_with_doc_intel\",\n",
    "    display_name=\"Crack and Chunk Data leveraging Azure AI Document Intelligence for PDFs\",\n",
    "    description=\"\"\"Creates chunks from source data leveraging Azure AI Document Intelligence for PDFs.\n",
    "\n",
    "    Supported formats: md, txt, html/htm, pdf, ppt(x), doc(x), xls(x), py\"\"\",\n",
    "    inputs={\n",
    "        # Input AzureML Data\n",
    "        \"input_data\": Input(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        # Files to handle from source\n",
    "        \"input_glob\": Input(\n",
    "            type=\"string\",\n",
    "            default=\"**/*\",\n",
    "            description=\"Limit files opened from `input_data`, defaults to '**/*'\",\n",
    "        ),\n",
    "        \"allowed_extensions\": Input(\n",
    "            type=\"string\",\n",
    "            optional=True,\n",
    "            description=\"Comma separated list of extensions to include, if not provided the default list of supported extensions will be used. e.g. '.md,.txt,.html,.py,.pdf'\",\n",
    "        ),\n",
    "        # Chunking options\n",
    "        \"chunk_size\": Input(\n",
    "            type=\"integer\",\n",
    "            default=768,\n",
    "            description=\"Maximum number of tokens per chunk.\",\n",
    "        ),\n",
    "        \"chunk_overlap\": Input(\n",
    "            type=\"integer\",\n",
    "            default=0,\n",
    "            description=\"Number of tokens to overlap between chunks.\",\n",
    "        ),\n",
    "        \"use_rcts\": Input(\n",
    "            type=\"boolean\",\n",
    "            default=True,\n",
    "            description=\"Use langchain RecursiveTextSplitter to split chunks.\",\n",
    "        ),\n",
    "        # Augmentation options\n",
    "        \"data_source_url\": Input(\n",
    "            type=\"string\",\n",
    "            optional=True,\n",
    "            description=\"Base URL to join with file paths to create full source file URL for chunk metadata.\",\n",
    "        ),\n",
    "        \"document_path_replacement_regex\": Input(\n",
    "            type=\"string\",\n",
    "            optional=True,\n",
    "            description=\"A JSON string with two fields, 'match_pattern' and 'replacement_pattern' to be used with re.sub on the source url. e.g. '{\\\"match_pattern\\\": \\\"(.*)/articles/(.*)\\\", \\\"replacement_pattern\\\": \\\"\\\\1/\\\\2\\\"}' would remove '/articles' from the middle of the url.\",\n",
    "        ),\n",
    "        \"doc_intel_connection_id\": Input(\n",
    "            type=\"string\",\n",
    "            description=\"AzureML Connection ID for Custom Workspace Connection containing the `endpoint` key and `api_key` secret for an Azure AI Document Intelligence Service.\",\n",
    "        ),\n",
    "        \"use_layout\": Input(\n",
    "            type=\"boolean\",\n",
    "            default=False,\n",
    "            description=\"Use 'prebuilt-layout' model from Azure AI Document Intelligence, more expensive and slower but maintains more structure from original doc.\",\n",
    "        ),\n",
    "    },\n",
    "    outputs={\n",
    "        \"output_chunks\": Output(type=\"uri_folder\"),\n",
    "    },\n",
    "    code=Path.cwd() / \"crack_and_chunk_with_doc_intel\",\n",
    "    command=\"\"\"python crack_and_chunk.py\\\n",
    "    --input_data ${{inputs.input_data}}\\\n",
    "    --input_glob '${{inputs.input_glob}}'\\\n",
    "    $[[--allowed_extensions ${{inputs.allowed_extensions}}]]\\\n",
    "    --output_chunks ${{outputs.output_chunks}}\\\n",
    "    --chunk_size ${{inputs.chunk_size}}\\\n",
    "    --chunk_overlap ${{inputs.chunk_overlap}}\\\n",
    "    --use_rcts ${{inputs.use_rcts}}\\\n",
    "    $[[--data_source_url ${{inputs.data_source_url}}]]\\\n",
    "    $[[--document_path_replacement_regex '${{inputs.document_path_replacement_regex}}']]\\\n",
    "    --doc_intel_connection_id '${{inputs.doc_intel_connection_id}}'\\\n",
    "    --use_layout ${{inputs.use_layout}}\\\n",
    "    \"\"\",\n",
    "    environment=llm_rag_embeddings_doc_intel_environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline using the custom `crack_and_chunk_with_doc_intel` Component along with the AzureML provided Components to embed and index your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_registry = MLClient(credential=ml_client._credential, registry_name=\"azureml\")\n",
    "\n",
    "# Reads input folder of files containing chunks and their metadata as batches, in parallel, and generates embeddings for each chunk. Output format is produced and loaded by `azureml.rag.embeddings.EmbeddingContainer`.\n",
    "generate_embeddings_component = ml_registry.components.get(\n",
    "    \"llm_rag_generate_embeddings\", label=\"latest\"\n",
    ")\n",
    "# Reads an input folder produced by `azureml.rag.embeddings.EmbeddingsContainer.save()` and pushes all documents (chunk, metadata, embedding_vector) into an Azure Cognitive Search index. Writes an MLIndex yaml detailing the index and embeddings model information.\n",
    "update_acs_index_component = ml_registry.components.get(\n",
    "    \"llm_rag_update_acs_index\", label=\"latest\"\n",
    ")\n",
    "# Takes a uri to a storage location where an MLIndex yaml is stored and registers it as an MLIndex Data asset in the AzureML Workspace.\n",
    "register_mlindex_asset_component = ml_registry.components.get(\n",
    "    \"llm_rag_register_mlindex_asset\", label=\"latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities._job.pipeline._io import PipelineInput\n",
    "\n",
    "\n",
    "def use_automatic_compute(\n",
    "    component, instance_count=1, instance_type=\"Standard_D4as_v4\"\n",
    "):\n",
    "    component.set_resources(\n",
    "        instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        properties={\"compute_specification\": {\"automatic\": True}},\n",
    "    )\n",
    "    return component\n",
    "\n",
    "\n",
    "def optional_pipeline_input_provided(input: PipelineInput):\n",
    "    return input._data is not None\n",
    "\n",
    "\n",
    "@pipeline(default_compute=\"serverless\")\n",
    "def uri_to_acs(\n",
    "    input_data: Input,\n",
    "    doc_intel_connection_id: str,\n",
    "    embeddings_model: str,\n",
    "    acs_config: str,\n",
    "    acs_connection_id: str,\n",
    "    asset_name: str,\n",
    "    chunk_size: int = 512,\n",
    "    data_source_glob: str = None,\n",
    "    data_source_url: str = None,\n",
    "    document_path_replacement_regex: str = None,\n",
    "    use_layout: bool = False,\n",
    "    aoai_connection_id: str = None,\n",
    "    embeddings_container: Input = None,\n",
    "):\n",
    "    crack_and_chunk = crack_and_chunk_with_doc_intel_component(\n",
    "        input_data=input_data,\n",
    "        input_glob=data_source_glob,\n",
    "        chunk_size=chunk_size,\n",
    "        use_rcts=True,\n",
    "        data_source_url=data_source_url,\n",
    "        document_path_replacement_regex=document_path_replacement_regex,\n",
    "        doc_intel_connection_id=doc_intel_connection_id,\n",
    "        use_layout=use_layout,\n",
    "    )\n",
    "    use_automatic_compute(crack_and_chunk)\n",
    "\n",
    "    generate_embeddings = generate_embeddings_component(\n",
    "        chunks_source=crack_and_chunk.outputs.output_chunks,\n",
    "        embeddings_container=embeddings_container,\n",
    "        embeddings_model=embeddings_model,\n",
    "    )\n",
    "\n",
    "    use_automatic_compute(generate_embeddings)\n",
    "    if optional_pipeline_input_provided(aoai_connection_id):\n",
    "        generate_embeddings.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_AOAI\"\n",
    "        ] = aoai_connection_id\n",
    "    if optional_pipeline_input_provided(embeddings_container):\n",
    "        # If provided, previous_embeddings is expected to be a URI to an 'embeddings container' folder.\n",
    "        # Each folder under this folder is generated by a `generate_embeddings_component` run and can be reused for subsequent embeddings runs.\n",
    "        generate_embeddings.outputs.embeddings = Output(\n",
    "            type=\"uri_folder\", path=f\"{embeddings_container.path}/{{name}}\"\n",
    "        )\n",
    "\n",
    "    update_acs_index = update_acs_index_component(\n",
    "        embeddings=generate_embeddings.outputs.embeddings,\n",
    "        acs_config=acs_config,\n",
    "    )\n",
    "    use_automatic_compute(update_acs_index)\n",
    "    if optional_pipeline_input_provided(acs_connection_id):\n",
    "        update_acs_index.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_ACS\"\n",
    "        ] = acs_connection_id\n",
    "\n",
    "    register_mlindex = register_mlindex_asset_component(\n",
    "        storage_uri=update_acs_index.outputs.index,\n",
    "        asset_name=asset_name,\n",
    "    )\n",
    "    use_automatic_compute(register_mlindex)\n",
    "    return {\n",
    "        \"mlindex_asset_uri\": update_acs_index.outputs.index,\n",
    "        \"mlindex_asset_id\": register_mlindex.outputs.asset_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the connections to Azure OpenAI (for embeddings with `text-embedding-ada-002`) and Azure Cognitive Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_connection = ml_client.connections.get(\"azureml-rag-oai\")\n",
    "acs_connection = ml_client.connections.get(\"azureml-rag-acs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Custom Connection with details for an Azure AI Document Intelligence Service.\n",
    "[Setup instructions for Azure AI Document Intelligence](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-3.1.0)\n",
    "\n",
    "Use the Connections UI in an AzureML Workspace, under the Promptflow tab, to create a connection with these fields: ![custom_doc_intel_connection.png](./assets/custom_doc_intel_connection.png)\n",
    "\n",
    "It's not yet supported to create/retrieve Custom Connections using SDK, so you will need to create it using the UI and we'll use string replacement below to get the ID for this custom connection to pass to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_intelligence_connection_id = aoai_connection.id.replace(\n",
    "    \"azureml-rag-oai\", \"azureml-rag-documentintelligence\"\n",
    ")\n",
    "document_intelligence_connection_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "embeddings_model = (\n",
    "    \"azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "data_source = Path.cwd() / \"pdfs\"\n",
    "asset_name = f\"doc_intel_{embeddings_model.split(':')[0]}_acs\"\n",
    "\n",
    "pipeline_job = uri_to_acs(\n",
    "    input_data=Input(type=\"uri_folder\", path=str(data_source)),\n",
    "    data_source_glob=\"**/*\",\n",
    "    data_source_url=None,\n",
    "    document_path_replacement_regex=None,\n",
    "    doc_intel_connection_id=document_intelligence_connection_id,\n",
    "    use_layout=False,\n",
    "    embeddings_model=embeddings_model,\n",
    "    aoai_connection_id=aoai_connection.id,\n",
    "    embeddings_container=Input(\n",
    "        type=\"uri_folder\",\n",
    "        path=f\"azureml://datastores/workspaceblobstore/paths/embeddings/{asset_name}\",\n",
    "    ),\n",
    "    acs_config=json.dumps(\n",
    "        {\n",
    "            \"index_name\": asset_name,\n",
    "        }\n",
    "    ),\n",
    "    acs_connection_id=acs_connection.id,\n",
    "    asset_name=asset_name,\n",
    ")\n",
    "pipeline_job.display_name = asset_name\n",
    "\n",
    "# Properties for Vector Index UI\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetName\"] = asset_name\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetKind\"] = \"acs\"\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetSource\"] = \"AzureML Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Submitting pipeline job to experiment: {asset_name}\")\n",
    "running_pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=asset_name\n",
    ")\n",
    "\n",
    "print(f\"Submitted run, url: {running_pipeline_job.studio_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(running_pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.mlindex import MLIndex\n",
    "\n",
    "question = \"What is RAG?\"\n",
    "\n",
    "retriever = MLIndex(\n",
    "    ml_client.data.get(asset_name, label=\"latest\")\n",
    ").as_langchain_retriever()\n",
    "retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-baker310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
