{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1688129460670
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install -U 'azureml-rag[faiss]>=0.1.13'\n",
    "# If using hugging_face embeddings add `hugging_face` extra, e.g. `azureml-rag[faiss,hugging_face]`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Create a FAISS based Vector Index for Incremental Document Retrieval with AzureML for unstructured data\n",
    "\n",
    "In this notebook, we'll walk through setting up an AzuremML Pipeline which pulls some unstructured data from a couple docx files, chunks it, incrementally embeds the chunks and creates a LangChain compatible FAISS Vector Index. By unstructured data, we mean data in files with the extensions `.pdf`, `.ppt`, `.pptx`, `.doc`, `.docx`, `.xls` and `.xlsx`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "### Get client for AzureML Workspace\n",
    "The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "If you don't have a Workspace and want to create and Index locally see [here to create one](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2).\n",
    "\n",
    "Enter your Workspace details below, running this still will write a `workspace.json` file to the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile workspace.json\n",
    "{\n",
    "    \"subscription_id\": \"<subscription_id>\",\n",
    "    \"resource_group\": \"<resource_group_name>\",\n",
    "    \"workspace_name\": \"<workspace_name>\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "`MLClient` is how you interact with AzureML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1688130064653
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential, path=\"workspace.json\")\n",
    "except Exception as ex:\n",
    "    raise Exception(\n",
    "        \"Failed to create MLClient from config file. Please modify and then run the above cell with your AzureML Workspace details.\"\n",
    "    ) from ex\n",
    "    # ml_client = MLClient(\n",
    "    #     credential=credential,\n",
    "    #     subscription_id=\"\",\n",
    "    #     resource_group_name=\"\",\n",
    "    #     workspace_name=\"\"\n",
    "    # )\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=ml_client.subscription_id,\n",
    "    resource_group=ml_client.resource_group_name,\n",
    "    workspace_name=ml_client.workspace_name,\n",
    ")\n",
    "ml_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Which Embeddings Model to use?\n",
    "There are currently two supported Embedding options: OpenAI's `text-embedding-ada-002` embedding model or HuggingFace embedding models. Here are some factors that might influence your decision:\n",
    "\n",
    "#### OpenAI\n",
    "OpenAI has [great documentation](https://platform.openai.com/docs/guides/embeddings) on their Embeddings model `text-embedding-ada-002`, it can handle up to 8191 tokens and can be accessed using [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models#embeddings-models) or OpenAI directly. If you have an existing Azure OpenAI Instance you can connect it to AzureML, if you don't AzureML provisions a default one for you called `Default_AzureOpenAI`. The main limitation when using `text-embedding-ada-002` is cost/quota available for the model. Otherwise it provides high quality embeddings across a wide array of text domains while being simple to use.\n",
    "\n",
    "#### HuggingFace\n",
    "HuggingFace hosts many different models capable of embedding text into single-dimensional vectors. The [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) ranks the performance of embeddings models on a few axis, not all models ranked can be run locally (e.g. `text-embedding-ada-002` is on the list), though many can and there is a range of larger and smaller models. When embedding with HuggingFace the model is loaded locally for inference, this will potentially impact your choice of compute resources.\n",
    "\n",
    "**NOTE**: The default PromptFlow Runtime does not come with HuggingFace model dependencies installed, Indexes created using HuggingFace embeddings will not work in PromptFlow by default. **Pick OpenAI if you want to use PromptFlow**.\n",
    "\n",
    "#### For this example, we will be using an OpenAI embedding model\n",
    "\n",
    "We can use the automatically created `Default_AzureOpenAI` connection.\n",
    "\n",
    "If you would rather use an existing Azure OpenAI connection then change `aoai_connection_name` below. If you would rather use an existing Azure OpenAI resource, but don't have a connection created, modify `aoai_connection_name` and the details under the `# Create New Connection` code comment, or navigate to the `PromptFlow` section in your AzureML Workspace and use the Connections create UI flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "gather": {
     "logged": 1688130551200
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "aoai_connection_name = \"Default_AzureOpenAI\"\n",
    "aoai_connection_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.rag.utils.connections import (\n",
    "    get_connection_by_name_v2,\n",
    "    create_connection_v2,\n",
    ")\n",
    "\n",
    "try:\n",
    "    aoai_connection = get_connection_by_name_v2(ws, aoai_connection_name)\n",
    "except Exception as ex:\n",
    "    # Create New Connection\n",
    "    # Modify the details below to match the `Endpoint` and API key of your AOAI resource, these details can be found in Azure Portal\n",
    "    raise RuntimeError(\n",
    "        \"Have you entered your AOAI resource details below? If so, delete me!\"\n",
    "    )\n",
    "    target = \"<target>\"  # example: 'https://<endpoint>.openai.azure.com/'\n",
    "    key = \"<key>\"\n",
    "    apiVersion = \"2023-03-15-preview\"\n",
    "    if key == \"<key>\":\n",
    "        raise RuntimeError(f\"Please provide a valid key for the Azure OpenAI service\")\n",
    "    if target == \"<target>\":\n",
    "        raise RuntimeError(\n",
    "            f\"Please provide a valid target for the Azure OpenAI service\"\n",
    "        )\n",
    "    if apiVersion == \"<api_version>\":\n",
    "        raise RuntimeError(\n",
    "            f\"Please provide a valid api-version for the Azure OpenAI service\"\n",
    "        )\n",
    "    aoai_connection_id = create_connection_v2(\n",
    "        workspace=ws,\n",
    "        name=aoai_connection,\n",
    "        category=\"AzureOpenAI\",\n",
    "        target=target,\n",
    "        auth_type=\"ApiKey\",\n",
    "        credentials={\"key\": key},\n",
    "        metadata={\"ApiType\": \"azure\", \"ApiVersion\": apiVersion},\n",
    "    )[\"id\"]\n",
    "\n",
    "aoai_connection_id = aoai_connection[\"id\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now that your Workspace has a connection to Azure OpenAI we will make sure the `text-embedding-ada-002` model has been deployed ready for inference. This cell will fail if there is not deployment for the embeddings model, [follow these instructions](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#deploy-a-model) to deploy a model with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1688130554324
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.rag.utils.deployment import infer_deployment\n",
    "\n",
    "aoai_embedding_model_name = \"text-embedding-ada-002\"\n",
    "try:\n",
    "    aoai_embedding_deployment_name = infer_deployment(\n",
    "        aoai_connection, aoai_embedding_model_name\n",
    "    )\n",
    "    print(\n",
    "        f\"Deployment name in AOAI workspace for model '{aoai_embedding_model_name}' is '{aoai_embedding_deployment_name}'\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Deployment name in AOAI workspace for model '{aoai_embedding_model_name}' is not found.\"\n",
    "    )\n",
    "    if \"ResourceId\" in aoai_connection[\"properties\"][\"metadata\"]:\n",
    "        aoai_resource_url = f\"https://portal.azure.com/resource/{aoai_connection['properties']['metadata']['ResourceId']}/overview\"\n",
    "        print(\n",
    "            f\"Please create a deployment for this model by following the deploy instructions on the resource page: {aoai_resource_url}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Please create a deployment for this model by following the deploy instructions on the resource page for '{aoai_connection['properties']['target']}' in Azure Portal.\"\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Finally we will combine the deployment and model information into a uri form which the AzureML embeddings components expect as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gather": {
     "logged": 1688130562511
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "embeddings_model_uri = f\"azure_open_ai://deployment/{aoai_embedding_deployment_name}/model/{aoai_embedding_model_name}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Setup Pipeline to process data into Index\n",
    "AzureML [Pipelines](https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2) connect together multiple [Components](https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2). Each Component defines inputs, code that consumes the inputs and outputs produced from the code. Pipelines themselves can have inputs, and outputs produced by connecting together individual sub Components. To process your data for embedding and indexing we will chain together multiple components each performing their own step of the workflow.\n",
    "\n",
    "The Components are published to a [Registry](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-registries?view=azureml-api-2&tabs=cli), azureml, which should have access to after signing up to the Generative AI Private Preview, it can be accessed from any Workspace as long as your Tenant has been granted access. In the below cell we get the Component Definitions from the azureml registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "gather": {
     "logged": 1688130558073
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ml_registry = MLClient(credential=credential, registry_name=\"azureml\")\n",
    "\n",
    "# Clones git repository to output folder of pipeline, by default this will be on the default Workspace Datastore `workspaceblobstore`\n",
    "git_clone_component = ml_registry.components.get(\"llm_rag_git_clone\", label=\"latest\")\n",
    "# Walks input folder according to provided glob pattern (all files by default: '**/*') and attempts to open them, extract text chunks and further chunk if necessary to fir within provided `chunk_size`.\n",
    "crack_and_chunk_component = ml_registry.components.get(\n",
    "    \"llm_rag_crack_and_chunk\", label=\"latest\"\n",
    ")\n",
    "# Reads input folder of files containing chunks and their metadata as batches, in parallel, and generates embeddings for each chunk. Output format is produced and loaded by `azureml.rag.embeddings.EmbeddingContainer`.\n",
    "generate_embeddings_component = ml_registry.components.get(\n",
    "    \"llm_rag_generate_embeddings\", label=\"latest\"\n",
    ")\n",
    "# # Reads an input folder produced by `azureml.rag.embeddings.EmbeddingsContainer.save()` and pushes all documents (chunk, metadata, embedding_vector) into an FAISS index. Writes an MLIndex yaml detailing the index and embeddings model information.\n",
    "create_faiss_index_component = ml_registry.components.get(\n",
    "    \"llm_rag_create_faiss_index\", label=\"latest\"\n",
    ")\n",
    "# Takes a uri to a storage location where an MLIndex yaml is stored and registers it as an MLIndex Data asset in the AzureML Workspace.\n",
    "register_mlindex_component = ml_registry.components.get(\n",
    "    \"llm_rag_register_mlindex_asset\", label=\"latest\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Each Component has documentation which provides an overall description of the Components purpose and each of the inputs/outputs. For example we can see understand what `crack_and_chunk` does by inspecting the Component definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1688165103745
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "crack_and_chunk_component"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Below a Pipeline is built by defining a python function which chains together the above components inputs and outputs. Arguments to the function are inputs to the Pipeline itself and the return value is a dictionary defining the outputs of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gather": {
     "logged": 1688130563850
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities._job.pipeline._io import PipelineInput\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def use_automatic_compute(component, instance_count=1, instance_type=\"Standard_E8s_v3\"):\n",
    "    \"\"\"Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\n",
    "\n",
    "    This avoids the need to provision a compute cluster to run the component.\n",
    "    \"\"\"\n",
    "    component.set_resources(\n",
    "        instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        properties={\"compute_specification\": {\"automatic\": True}},\n",
    "    )\n",
    "    return component\n",
    "\n",
    "\n",
    "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n",
    "    \"\"\"Checks if optional pipeline inputs are provided.\"\"\"\n",
    "    return input is not None and input._data is not None\n",
    "\n",
    "\n",
    "# If you have an existing compute cluster you want to use instead of automatic compute, uncomment the following line, replace `dedicated_cpu_compute` with the name of your cluster.\n",
    "# Also comment out the `component.set_resources` line in `use_automatic_compute` above and the `default_compute='serverless'` line below.\n",
    "# @pipeline(compute=dedicated_cpu_compute)\n",
    "@pipeline(default_compute=\"serverless\")\n",
    "def urifolder_to_faiss(\n",
    "    input_data: Input,\n",
    "    embeddings_model: str,\n",
    "    asset_name: str,\n",
    "    data_source_glob: str = None,\n",
    "    data_source_url: str = None,\n",
    "    document_path_replacement_regex: str = None,\n",
    "    chunk_size: int = 1024,\n",
    "    aoai_connection_id=None,\n",
    "    embeddings_container=None,\n",
    "):\n",
    "    crack_and_chunk = crack_and_chunk_component(\n",
    "        input_data=input_data,\n",
    "        input_glob=data_source_glob,\n",
    "        chunk_size=chunk_size,\n",
    "        data_source_url=data_source_url,\n",
    "        document_path_replacement_regex=document_path_replacement_regex,\n",
    "    )\n",
    "    use_automatic_compute(crack_and_chunk)\n",
    "\n",
    "    generate_embeddings = generate_embeddings_component(\n",
    "        chunks_source=crack_and_chunk.outputs.output_chunks,\n",
    "        embeddings_container=embeddings_container,\n",
    "        embeddings_model=embeddings_model,\n",
    "    )\n",
    "    use_automatic_compute(generate_embeddings)\n",
    "    if optional_pipeline_input_provided(aoai_connection_id):\n",
    "        generate_embeddings.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_AOAI\"\n",
    "        ] = aoai_connection_id\n",
    "\n",
    "    if optional_pipeline_input_provided(embeddings_container):\n",
    "        # If provided, previous_embeddings is expected to be a URI to an 'embeddings container' folder.\n",
    "        # Each folder under this folder is generated by a `create_embeddings_component` run and can be reused for subsequent embeddings runs.\n",
    "        generate_embeddings.outputs.embeddings = Output(\n",
    "            type=\"uri_folder\", path=f\"{embeddings_container.path}/{{name}}\"\n",
    "        )\n",
    "\n",
    "    create_faiss_index = create_faiss_index_component(\n",
    "        embeddings=generate_embeddings.outputs.embeddings,\n",
    "    )\n",
    "    use_automatic_compute(create_faiss_index)\n",
    "    register_mlindex = register_mlindex_component(\n",
    "        storage_uri=create_faiss_index.outputs.index, asset_name=asset_name\n",
    "    )\n",
    "    use_automatic_compute(register_mlindex)\n",
    "    return {\n",
    "        \"mlindex_asset_uri\": create_faiss_index.outputs.index,\n",
    "        \"mlindex_asset_id\": register_mlindex.outputs.asset_id,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now we can create the Pipeline Job by calling the `@pipeline` annotated function and providing input arguments. `asset_name` will be used when registering the MLIndex Data Asset produced by the `register_mlindex` component in the pipeline. This is how you can refer to the MLIndex within AzureML. For this job, the input data is an asset of type `URI_FOLDER` that lives in the default Datastore of the AzureML Workspace being used. The folder contains two files:\n",
    "\n",
    "1. MSFT_FY23Q1_10Q.docx\n",
    "2. MSFT_FY23Q2_10Q.docx\n",
    "\n",
    "These files contain Microsoft's quarterly financial reports for the first and second quarter of the 2023 fiscal year. They are publicly available and are also available in the `data` folder relative to the location of this notebook file. In the below pipeline job, we only use the first one by filtering out the second one by using the glob pattern `**/*[Q1]_10Q.docx`. We do this to demonstrate the incremental embedding capability. In a later run, we will include both files and see how it affects the scenario.\n",
    "\n",
    "Here are screenshots of the two tables from the financial report that our model will be referencing to answer questions.\n",
    "\n",
    "Q1\n",
    "![image-alt-text](data/General_and_administrative_q1.png)\n",
    "\n",
    "Q2\n",
    "![image-alt-text](data/General_and_administrative_q2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1688133601533
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "asset_name = \"microsoft-earnings-fy23\"\n",
    "data_source_glob = \"**/*[Q1]_10Q.docx\"\n",
    "\n",
    "pipeline_job = urifolder_to_faiss(\n",
    "    input_data=Input(\n",
    "        type=AssetTypes.URI_FOLDER, path=\"data/\"\n",
    "    ),  # This will upload the data folder to the default Workspace Datastore `workspaceblobstore`\n",
    "    data_source_glob=data_source_glob,\n",
    "    embeddings_model=embeddings_model_uri,\n",
    "    asset_name=asset_name,\n",
    "    aoai_connection_id=aoai_connection_id,\n",
    "    embeddings_container=Input(\n",
    "        type=\"uri_folder\",\n",
    "        path=f\"azureml://datastores/workspaceblobstore/paths/embeddings/{asset_name}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Note**: By default AzureML Pipelines will reuse the output of previous component Runs when inputs have not changed.\n",
    "If you want to rerun the Pipeline every time each time so that any changes to upstream data sources are processed uncomment the below line.\n",
    "`pipeline_job.settings.force_rerun = True`. Rerun each time so that the `crack_and_chunk` component isn't cached, if intent is to ingest latest data.\n",
    "\n",
    "Finally we add some properties to `pipeline_job` which ensure the Index generation progress and final Artifact appear in the PromptFlow Vector Index UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# These are added so that in progress index generations can be listed in UI, this tagging is done automatically by UI.\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetName\"] = asset_name\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetKind\"] = \"faiss\"\n",
    "pipeline_job.properties[\"azureml.mlIndexAssetSource\"] = \"Uri Folder\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Submit Pipeline\n",
    "**In case of any errors see** TROUBLESHOOT.md.\n",
    "\n",
    "The output of each step in the pipeline can be inspected via the Workspace UI, click the link under 'Details Page' after running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "running_pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"incremental_embedding_with_table\"\n",
    ")\n",
    "running_pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(running_pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Use Index with langchain\n",
    "The Data Asset produced by the AzureML Pipeline above contains a yaml file named `MLIndex` which contains all the information needed to use the FAISS index. For instance if an AOAI deployment was used to embed the documents the details of that deployment and a reference to the secret are there. This allows easy loading of the MLIndex into a langchain retriever. If you have not deployed `gpt-35-turbo` on your Azure OpenAI resource the below cell will fail indicated the `API deployment for this resource does not exist`. Follow the previous instructions for deploying `text-embedding-ada-002` to deploy `gpt-35-turbo`, note the chosen deployment name below and use the same or update it if you choose different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "gather": {
     "logged": 1688133704014
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.rag.mlindex import MLIndex\n",
    "from langchain.chains import RetrievalQA\n",
    "from azureml.rag.models import init_llm, parse_model_uri\n",
    "\n",
    "\n",
    "model_config = parse_model_uri(\n",
    "    \"azure_open_ai://deployment/gpt-35-turbo/model/gpt-35-turbo\"\n",
    ")\n",
    "model_config[\"api_base\"] = aoai_connection[\"properties\"][\"target\"]\n",
    "model_config[\"key\"] = aoai_connection[\"properties\"][\"credentials\"][\"key\"]\n",
    "model_config[\"temperature\"] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "gather": {
     "logged": 1688133714206
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the three months ended September 30, 2022, Microsoft's expenses in the 'General and administrative' segment were $1,398 million. As a percentage of the revenue, it was 3%.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = MLIndex(\n",
    "    ml_client.data.get(asset_name, label=\"latest\")\n",
    ").as_langchain_retriever()\n",
    "question = \"In the three months ended September 30, 2022, what were the expenses of Microsoft in the 'General and administrative' segment and as a percentage of the revenue?\"\n",
    "retriever.get_relevant_documents(question)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=init_llm(model_config), chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "qa.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Amazing! The model was able to successfully retrieve information that was only available in a table in the provided Word document and answer the question accordingly. Following is the snippet of the table for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "gather": {
     "logged": 1688133744982
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The financial information for the three months ended December 31, 2022 is not provided in the given context. The latest financial information available in the context is for the three months ended September 30, 2022. In that period, the expenses of Microsoft in the 'General and administrative' segment were $1,398 million, and as a percentage of revenue, it was 3%.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"In the three months ended December 31, 2022, what were the expenses of Microsoft in the 'General and administrative' segment and as a percentage of the revenue?\"\n",
    "retriever.get_relevant_documents(question)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=init_llm(model_config), chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "qa.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Note from the response that the model is able to retrieve the closest document to answer the question and is smart enough to determine that it doesn't have the requested information. Now let's give it more information and see it reacts to the same question. To do so, we update the `data_source_glob` to `**/*[Q12]_10Q.docx` so that it matches both Word documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1688134097039
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data_source_glob = \"**/*[Q12]_10Q.docx\"\n",
    "pipeline_job = urifolder_to_faiss(\n",
    "    input_data=Input(\n",
    "        type=AssetTypes.URI_FOLDER,\n",
    "        path=\"azureml://datastores/workspaceblobstore/paths/msft_earnings_fy23\",\n",
    "    ),\n",
    "    data_source_glob=data_source_glob,\n",
    "    embeddings_model=embeddings_model_uri,\n",
    "    asset_name=asset_name,\n",
    "    aoai_connection_id=aoai_connection_id,\n",
    "    embeddings_container=Input(\n",
    "        type=\"uri_folder\",\n",
    "        path=f\"azureml://datastores/workspaceblobstore/paths/embeddings/{asset_name}\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "running_pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"incremental_embedding_with_table\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(running_pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "gather": {
     "logged": 1688134924467
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the three months ended December 31, 2022, Microsoft's General and administrative expenses were $2,337 million and represented 4% of the revenue.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = MLIndex(\n",
    "    ml_client.data.get(asset_name, label=\"latest\")\n",
    ").as_langchain_retriever()\n",
    "retriever.get_relevant_documents(question)\n",
    "question = \"In the three months ended December 31, 2022, what were the expenses of Microsoft in the 'General and administrative' segment and as a percentage of the revenue?\"\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=init_llm(model_config), chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "qa.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "This time, when asked the same question, the model was able to answer the question correctly because our new index contained the information about the second quarter financial data. \n",
    "\n",
    "Something that's not visible to the eye but happens in the backend is incremental embedding. What this means is that, during the second MLIndex creation, we don't re-embed the files that were already embedded during the first MLIndex creation. This can be verified from the user logs of the `LLM - Generate Embeddings Parallel` component of the corresponding job. Here is a snippet of it that shows the embedding of the first quarter being skipped.\n",
    "\n",
    "`[2023-06-30 14:04:46] INFO     azureml.rag.azureml.rag.embeddings - Processing document: MSFT_FY23Q1_10Q.docx0 (embeddings.py:646)\n",
    "INFO:azureml.rag.azureml.rag.embeddings:Processing document: MSFT_FY23Q1_10Q.docx0`\n",
    "\n",
    "`[2023-06-30 14:04:46] INFO     azureml.rag.azureml.rag.embeddings - Skip embedding document MSFT_FY23Q1_10Q.docx0 as it has not been modified since last embedded (embeddings.py:670)`\n",
    "\n",
    "The benefit of this is that it makes embedding generation really fast and it is very noticeable and most helpful when you have a large number of files.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
