{"content": "# AzureML MLIndex Asset creation\n\n# AzureML MLIndex Asset creation\nMLIndex assets in AzureML represent a model used to generate embeddings from text and an index which can be searched using embedding vectors.\nRead more about their structure [here](./docs/mlindex.md).", "metadata": {"doc_id": "README.md0", "chunk_hash": "845cee45d70ccece62224e881c52af49077ff177e181a30a36f6043eed9f3ebc", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "0"}, "stats": {"tiktokens": 56, "chars": 267, "lines": 5}, "markdown_heading": {"heading": "AzureML MLIndex Asset creation", "level": 1}}, "document_id": "README.md0"}
{"content": "# AzureML MLIndex Asset creation\n\n# AzureML MLIndex Asset creation\n## Pre-requisites\n0. Install `azure-ai-ml` and `azureml-rag:\n    - `pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/`\n    - `pip install -U 'azureml-rag[document_parsing,faiss,cognitive_search]>=0.2.0'`\n1. You have unstructured data.\n    - In one of [AzureMLs supported data sources](https://learn.microsoft.com/azure/machine-learning/concept-data?view=azureml-api-2): Blob, ADLSgen2, OneLake, S3, Git\n    - In any of these supported file formats: md, txt, py, pdf, ppt(x), doc(x)\n2. You have an embedding model.\n    - [Create", "metadata": {"doc_id": "README.md1", "chunk_hash": "0d9520be70920a41775fe07c4efba32786b2cc0f6ac1067e9522ed2f5f0e9b7a", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "1"}, "stats": {"tiktokens": 213, "chars": 697, "lines": 12}, "markdown_heading": {"heading": "Pre-requisites", "level": 2}}, "document_id": "README.md1"}
{"content": "# AzureML MLIndex Asset creation\n\n# AzureML MLIndex Asset creation\n## Pre-requisites\n an Azure OpenAI service + connection](https://learn.microsoft.com/azure/machine-learning/prompt-flow/concept-connections?view=azureml-api-2)\n    - Use a HuggingFace `sentence-transformer` model (you can just use it now, to leverage the MLIndex in PromptFlow a [Custom Runtime](https://promptflow.azurewebsites.net/how-to-guides/how-to-customize-environment-runtime.html) will be required)\n3. You have an Index to ingest data to.\n    - [Create an Azure Cognitive Search service + connection](https://learn.microsoft.com/azure/machine-learning/prompt-flow/concept-connections?view=azureml-api-2)\n    - Use a Faiss index (you can just use it now)", "metadata": {"doc_id": "README.md2", "chunk_hash": "e1f8cc29677df8d30cba41af8f9bad8dda7633f50663e962ed2f6637d2468711", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "2"}, "stats": {"tiktokens": 178, "chars": 729, "lines": 9}, "markdown_heading": {"heading": "Pre-requisites", "level": 2}}, "document_id": "README.md2"}
{"content": "# AzureML MLIndex Asset creation\n\n# AzureML MLIndex Asset creation\n## Let's Ingest and Index\nA DataIndex job is configured using the `azure-ai-ml` python sdk/cli, either directly in code or with a yaml file.", "metadata": {"doc_id": "README.md3", "chunk_hash": "124abe065ab8e85fcace0695135a65b0e6a770b87b652619f6d7b39e68a08982", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "3"}, "stats": {"tiktokens": 53, "chars": 207, "lines": 5}, "markdown_heading": {"heading": "Let's Ingest and Index", "level": 2}}, "document_id": "README.md3"}
{"content": "# AzureML MLIndex Asset creation\n\n# AzureML MLIndex Asset creation\n## Let's Ingest and Index\n### SDK\nThe examples are runnable as Python scripts, assuming the pre-requisites have been acquired and configured in the script.  \nOpening them in vscode enables executing each block below a `# %%` comment like a jupyter notebook cell.", "metadata": {"doc_id": "README.md4", "chunk_hash": "6d53561c36b82439d6df6cea68eeff71f494de8a4c062bee6b47045a9f2bddd5", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "4"}, "stats": {"tiktokens": 72, "chars": 329, "lines": 7}, "markdown_heading": {"heading": "SDK", "level": 3}}, "document_id": "README.md4"}
{"content": "# AzureML MLIndex Asset creation\n\n# AzureML MLIndex Asset creation\n## Let's Ingest and Index\n### SDK\n#### Cloud Creation\n##### Process this documentation using Azure OpenAI and Azure Cognitive Search", "metadata": {"doc_id": "README.md5", "chunk_hash": "8bec6a1213d6c7627206e3adfa661172bfcb012ca70b4566a8bf871a2634c812", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "5"}, "stats": {"tiktokens": 43, "chars": 199, "lines": 7}, "markdown_heading": {"heading": "Cloud Creation", "level": 4}}, "document_id": "README.md5"}
{"content": "# AzureML MLIndex Asset creation\n\n- [local_docs_to_acs_mlindex.py](./data_index_job/local_docs_to_acs_mlindex.py)\n- [local_docs_to_acs_mlindex.ipynb](./data_index_job/local_docs_to_acs_mlindex.ipynb)\n##### Index data from S3 using OneLake", "metadata": {"doc_id": "README.md6", "chunk_hash": "5ad54746dfa7ab2d3793298c82c7b4f536fc463532c2480f1a1a3c5b3d04fe49", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "6"}, "stats": {"tiktokens": 69, "chars": 238, "lines": 5}, "markdown_heading": {"heading": "- [local_docs_to_acs_mlindex.py](./data_index_job/local_docs_to_acs_mlindex.py)\n- [local_docs_to_acs_mlindex.ipynb](./data_index_job/local_docs_to_acs_mlindex.ipynb)", "level": 0}}, "document_id": "README.md6"}
{"content": "# AzureML MLIndex Asset creation\n\n- [s3_to_acs_mlindex.py](./data_index_job/s3_to_acs_mlindex.py)\n- [scheduled_s3_to_asc_mlindex.py](./data_index_job/scheduled_s3_to_asc_mlindex.py)\n##### Ingest Azure Search docs from GitHub into a Faiss Index", "metadata": {"doc_id": "README.md7", "chunk_hash": "67f338ae613c4c12940f1c0027edc3f6b3f73a406de6aa8de64ba859b1dc4076", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "7"}, "stats": {"tiktokens": 70, "chars": 243, "lines": 5}, "markdown_heading": {"heading": "- [s3_to_acs_mlindex.py](./data_index_job/s3_to_acs_mlindex.py)\n- [scheduled_s3_to_asc_mlindex.py](./data_index_job/scheduled_s3_to_asc_mlindex.py)", "level": 0}}, "document_id": "README.md7"}
{"content": "# AzureML MLIndex Asset creation\n\n- [cog_search_docs_faiss_mlindex.py](./data_index_job/cog_search_docs_faiss_mlindex.py)\n#### Local Creation", "metadata": {"doc_id": "README.md8", "chunk_hash": "689844a268c0329e17c8276458bf58f9a8de8bdec437b8799fbbcf6bddfb9b21", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "8"}, "stats": {"tiktokens": 37, "chars": 141, "lines": 4}, "markdown_heading": {"heading": "- [cog_search_docs_faiss_mlindex.py](./data_index_job/cog_search_docs_faiss_mlindex.py)", "level": 0}}, "document_id": "README.md8"}
{"content": "# AzureML MLIndex Asset creation\n\n- [cog_search_docs_faiss_mlindex.py](./data_index_job/cog_search_docs_faiss_mlindex.py)\n##### Process this documentation using Azure OpenAI and Azure Cognitive Search\n- [local_docs_to_acs_aoai_mlindex.py](./mlindex_local/local_docs_to_acs_aoai_mlindex.py)", "metadata": {"doc_id": "README.md9", "chunk_hash": "90d84b86317b6e3b1b64880db741cb07f7ba59cfcfcfc0a02bda632b952e2f1d", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "9"}, "stats": {"tiktokens": 77, "chars": 289, "lines": 5}, "markdown_heading": {"heading": "Process this documentation using Azure OpenAI and Azure Cognitive Search", "level": 5}}, "document_id": "README.md9"}
{"content": "# AzureML MLIndex Asset creation\n\n- [cog_search_docs_faiss_mlindex.py](./data_index_job/cog_search_docs_faiss_mlindex.py)\n##### Process this documentation using SentenceTransformers and Faiss\n- [local_docs_to_faiss_mlindex.py](./mlindex_local/local_docs_to_faiss_mlindex.py)\n- [local_docs_to_faiss_mlindex_with_promptflow.py](./mlindex_local/local_docs_to_faiss_mlindex_with_promptflow.py)\n    - Learn more about [Promptflow here](https://microsoft.github.io/promptflow/)", "metadata": {"doc_id": "README.md10", "chunk_hash": "544ba9a5361194aa738bc8eac5b841d649eca626722b4fcb4c914bcf08f2d2c9", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "10"}, "stats": {"tiktokens": 120, "chars": 471, "lines": 7}, "markdown_heading": {"heading": "Process this documentation using SentenceTransformers and Faiss", "level": 5}}, "document_id": "README.md10"}
{"content": "# AzureML MLIndex Asset creation\n\n- [cog_search_docs_faiss_mlindex.py](./data_index_job/cog_search_docs_faiss_mlindex.py)\n##### Use a Langchain Documents to create an Index\n- [langchain_docs_to_mlindex.py](./mlindex_local/langchain_docs_to_mlindex.py)", "metadata": {"doc_id": "README.md11", "chunk_hash": "af3b22b8cecab1796b86686290abd7d078b575d23f2e51f78badaad7e1ec7769", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "11"}, "stats": {"tiktokens": 67, "chars": 251, "lines": 5}, "markdown_heading": {"heading": "Use a Langchain Documents to create an Index", "level": 5}}, "document_id": "README.md11"}
{"content": "# AzureML MLIndex Asset creation\n\n- [cog_search_docs_faiss_mlindex.py](./data_index_job/cog_search_docs_faiss_mlindex.py)\n## Using the MLIndex asset\nMore information about how to use MLIndex in various places [here]().", "metadata": {"doc_id": "README.md12", "chunk_hash": "25ecaa3815108d40c07f85e17536faedffdf541bfbf5928f3b45a00026f54ce3", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "12"}, "stats": {"tiktokens": 56, "chars": 218, "lines": 5}, "markdown_heading": {"heading": "Using the MLIndex asset", "level": 2}}, "document_id": "README.md12"}
{"content": "# AzureML MLIndex Asset creation\n\n- [cog_search_docs_faiss_mlindex.py](./data_index_job/cog_search_docs_faiss_mlindex.py)\n## Appendix\n### Which Embeddings Model to use?", "metadata": {"doc_id": "README.md13", "chunk_hash": "af765c8a5fa1353f0125530b8a31e39d6b5473b4f64960ea9aa76e387c08b04f", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "13"}, "stats": {"tiktokens": 45, "chars": 168, "lines": 5}, "markdown_heading": {"heading": "Appendix", "level": 2}}, "document_id": "README.md13"}
{"content": "# AzureML MLIndex Asset creation\n\nThere are currently two supported Embedding options: OpenAI's `text-embedding-ada-002` embedding model or HuggingFace embedding models. Here are some factors that might influence your decision:\n#### OpenAI", "metadata": {"doc_id": "README.md14", "chunk_hash": "61f1dd17fd89aef446f38645d0645359b836ce095d6bae831bcb99d74e8f217e", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "14"}, "stats": {"tiktokens": 51, "chars": 239, "lines": 4}, "markdown_heading": {"heading": "There are currently two supported Embedding options: OpenAI's `text-embedding-ada-002` embedding model or HuggingFace embedding models. Here are some factors that might influence your decision:", "level": 0}}, "document_id": "README.md14"}
{"content": "# AzureML MLIndex Asset creation\n\nOpenAI has [great documentation](https://platform.openai.com/docs/guides/embeddings) on their Embeddings model `text-embedding-ada-002`, it can handle up to 8191 tokens and can be accessed using [Azure OpenAI](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/models#embeddings-models) or OpenAI directly.\nIf you have an existing Azure OpenAI Instance you can connect it to AzureML, if you don't AzureML provisions a default one for you called `Default_AzureOpenAI`.\nThe main limitation when using `text-embedding-ada-002` is cost/quota available for the model. Otherwise it provides high quality embeddings across a wide array of text domains while being simple to use.\n#### HuggingFace", "metadata": {"doc_id": "README.md15", "chunk_hash": "ffd70db7760f4d4559c6425b56d00ee55c1ef34e449db206dd406d09e3896d4e", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "15"}, "stats": {"tiktokens": 170, "chars": 740, "lines": 6}, "markdown_heading": {"heading": "OpenAI has [great documentation](https://platform.openai.com/docs/guides/embeddings) on their Embeddings model `text-embedding-ada-002`, it can handle up to 8191 tokens and can be accessed using [Azure OpenAI](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/modelsembeddings-models) or OpenAI directly.\nIf you have an existing Azure OpenAI Instance you can connect it to AzureML, if you don't AzureML provisions a default one for you called `Default_AzureOpenAI`.\nThe main limitation when using `text-embedding-ada-002` is cost/quota available for the model. Otherwise it provides high quality embeddings across a wide array of text domains while being simple to use.", "level": 0}}, "document_id": "README.md15"}
{"content": "# AzureML MLIndex Asset creation\n\nHuggingFace hosts many different models capable of embedding text into single-dimensional vectors. The [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) ranks the performance of embeddings models on a few axis, not all models ranked can be run locally (e.g. `text-embedding-ada-002` is on the list), though many can and there is a range of larger and smaller models. When embedding with HuggingFace the model is loaded locally for inference, this will potentially impact your choice of compute resources.\n\n**NOTE:** The default PromptFlow Runtime does not come with HuggingFace model dependencies installed, Indexes created using HuggingFace embeddings will not work in PromptFlow by default. **Pick OpenAI if you want to use PromptFlow**\n### Setting up OneLake and S3", "metadata": {"doc_id": "README.md16", "chunk_hash": "611483090b2500e90299102c46582212c19f1a13e48aa32b14410f2a23fe04c7", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "16"}, "stats": {"tiktokens": 178, "chars": 819, "lines": 6}, "markdown_heading": {"heading": "HuggingFace hosts many different models capable of embedding text into single-dimensional vectors. The [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) ranks the performance of embeddings models on a few axis, not all models ranked can be run locally (e.g. `text-embedding-ada-002` is on the list), though many can and there is a range of larger and smaller models. When embedding with HuggingFace the model is loaded locally for inference, this will potentially impact your choice of compute resources.\n\n**NOTE:** The default PromptFlow Runtime does not come with HuggingFace model dependencies installed, Indexes created using HuggingFace embeddings will not work in PromptFlow by default. **Pick OpenAI if you want to use PromptFlow**", "level": 0}}, "document_id": "README.md16"}
{"content": "# AzureML MLIndex Asset creation\n\n[Create a lakehouse with OneLake](https://learn.microsoft.com/fabric/onelake/create-lakehouse-onelake)\n\n[Setup a shortcut to S3](https://learn.microsoft.com/fabric/onelake/create-s3-shortcut)\n", "metadata": {"doc_id": "README.md17", "chunk_hash": "316411eb24f559a295fc9e5c8d205bc3e68000036563bbbb5a43623dcb0385f8", "source": {"title": "AzureML MLIndex Asset creation", "filename": "README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/README.md", "mtime": 1694042161.2260115, "chunk_id": "17"}, "stats": {"tiktokens": 59, "chars": 226, "lines": 5}, "markdown_heading": {"heading": "[Create a lakehouse with OneLake](https://learn.microsoft.com/fabric/onelake/create-lakehouse-onelake)\n\n[Setup a shortcut to S3](https://learn.microsoft.com/fabric/onelake/create-s3-shortcut)", "level": 0}}, "document_id": "README.md17"}
{"content": "# MLIndex\n\n# MLIndex\nAn example MLIndex file:\n\n```yaml\nembeddings:\n  api_base: https://azureml-rag-oai.openai.azure.com\n  api_type: azure\n  api_version: 2023-03-15-preview\n  batch_size: \"1\"\n  connection:\n    id: Default_AzureOpenAI\n  connection_type: environment\n  deployment: text-embedding-ada-002\n  dimension: 1536\n  kind: open_ai\n  model: text-embedding-ada-002\n  schema_version: \"2\"\nindex:\n  api_version: 2023-07-01-preview\n  connection:\n    id: /subs/<sub>/rgs/<rg>/wss/<ws>/conns/<conn>\n  connection_type: workspace_connection\n  endpoint: https://azureml-rag-acs.search.windows.net\n  engine: azure-sdk\n  field_mapping:\n    content: content\n    embedding:", "metadata": {"doc_id": "docs/mlindex.md0", "chunk_hash": "14a7af951d46e676ca5253d482dbbf0d7f0a5ecb44d1925068cdb15ccd1bef63", "source": {"title": "MLIndex", "filename": "docs/mlindex.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/docs/mlindex.md", "mtime": 1694042161.242611, "chunk_id": "0"}, "stats": {"tiktokens": 204, "chars": 661, "lines": 29}, "markdown_heading": {"heading": "MLIndex", "level": 1}}, "document_id": "docs/mlindex.md0"}
{"content": "# MLIndex\n\n# MLIndex\n content_vector_open_ai\n    filename: sourcefile\n    metadata: meta_json_string\n    title: title\n    url: sourcepage\n  index: azure-docs-aoai-embeddings-rcts\n  kind: acs\n```", "metadata": {"doc_id": "docs/mlindex.md1", "chunk_hash": "7147b5397f016823ec3b825e688261585c40012101276617c8cfe81252ad1c6d", "source": {"title": "MLIndex", "filename": "docs/mlindex.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/docs/mlindex.md", "mtime": 1694042161.242611, "chunk_id": "1"}, "stats": {"tiktokens": 59, "chars": 194, "lines": 11}, "markdown_heading": {"heading": "MLIndex", "level": 1}}, "document_id": "docs/mlindex.md1"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py\n\n# %% Pre-requisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[document_parsing,faiss]>=0.2.0'\n# %pip install -U 'promptflow[azure]' promptflow-tools promptflow-vectordb\n\n# %% Get Azure Cognitive Search Connection\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py0", "chunk_hash": "0d8398ceb0494bc0fe33c99cb52e5c07636d23a773b1cec88d10683ffaa0c39c", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "mtime": 1694110514.6854193, "chunk_id": "0"}, "stats": {"tiktokens": 141, "chars": 505, "lines": 10}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py0"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py\n\nml_client = MLClient.from_config(credential=DefaultAzureCredential())\n\naoai_connection = ml_client.connections.get(\"azureml-rag-oai\")\n\n# %% Build MLIndex\nfrom azureml.rag.mlindex import MLIndex\n\n# Process data into FAISS Index using Azure OpenAI embeddings\nmlindex_name = \"mlindex_docs_none_faiss\"\nmlindex_local_path = f\"./{mlindex_name}\"", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py1", "chunk_hash": "73e6e184c64aff43a95320abf1f1cc153475dcf023658630ea68bf3e97278cbd", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "mtime": 1694110514.6854193, "chunk_id": "1"}, "stats": {"tiktokens": 102, "chars": 406, "lines": 12}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py1"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py\n\nmlindex = MLIndex.from_files(\n    source_uri=\"../\",\n    source_glob=\"**/*\",\n    chunk_size=200,\n    embeddings_model=\"none://\",\n    #embeddings_connection=aoai_connection,\n    embeddings_container=f\"./.embeddings_cache/{mlindex_name}\",\n    index_type=\"faiss\",\n    output_path=mlindex_local_path,\n)\n\n# %% Get Promptflow client\nimport promptflow\n\npf = promptflow.PFClient()", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py2", "chunk_hash": "3ba1a276b992d82c9a30c93bbe9819bd4e35bdcc8ede88d90994c2ff603841f3", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "mtime": 1694110514.6854193, "chunk_id": "2"}, "stats": {"tiktokens": 111, "chars": 439, "lines": 17}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py2"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py\n\n# %% List all the available connections\nfor c in pf.connections.list():\n    print(c.name + \" (\" + c.type + \")\")\n\n# %% Load index qna flow\nfrom pathlib import Path\n\nflow_path = Path.cwd().parent / \"flows\" / \"chat-with-index\"\n\n\n# %% Run qna flow\noutput = pf.flows.test(\n    flow_path,\n    inputs={\n        \"chat_history\": [],\n        \"mlindex_uri\": str(Path.cwd() / mlindex_local_path),\n        \"question\": \"what is an MLIndex?\",\n    },\n)", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py3", "chunk_hash": "0b6c68d47e140fbeb968157b870b0fa85733e6cf9ed081b0899def6261c1e572", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "mtime": 1694110514.6854193, "chunk_id": "3"}, "stats": {"tiktokens": 133, "chars": 504, "lines": 21}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py3"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py\n\nanswer = output[\"answer\"]\nfor part in answer:\n    print(part, end=\"\")\n\nprint(output[\"context\"])\n\n# %% Run qna flow with multiple inputs\ndata_path = Path.cwd().parent / \"flows\" / \"data\" / \"rag_docs_questions.jsonl\"\n\nconfig = {\n    \"CHAT_MODEL_DEPLOYMENT_NAME\": \"gpt-35-turbo\",\n    \"PROMPT_TOKEN_LIMIT\": 2000,\n    \"MAX_COMPLETION_TOKENS\": 256,\n    \"VERBOSE\": True,\n}", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py4", "chunk_hash": "6cdf46707eac77ffedb98f4897dd9feb2080e3165402125a90c236b9ba67e31c", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "mtime": 1694110514.6854193, "chunk_id": "4"}, "stats": {"tiktokens": 126, "chars": 432, "lines": 17}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py4"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py\n\ncolumn_mapping = {\n    \"chat_history\": \"${data.chat_history}\",\n    \"mlindex_uri\": str(\n        Path.cwd() / mlindex_local_path,\n    ),\n    \"question\": \"${data.chat_input}\",\n    \"answer\": \"${data.answer}\",\n    \"config\": config,\n}\nrun = pf.run(flow=flow_path, data=data_path, column_mapping=column_mapping)\npf.stream(run)\n\nprint(f\"{run}\")\n\n# %%", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py5", "chunk_hash": "5c9c3557b29f53e6c82accd3bac1658c536d3af49066487426a72c82b9a5eedf", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py", "mtime": 1694110514.6854193, "chunk_id": "5"}, "stats": {"tiktokens": 106, "chars": 410, "lines": 17}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py5"}
{"content": "Title: mlindex_local/langchain_docs_to_mlindex.py\n\n# %%[markdown]\n# # Build an ACS Index using langchain data loaders and MLIndex SDK\n\n# %% Pre-requisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[cognitive_search]>=0.2.0'\n# %pip install wikipedia\n\n# %% Get Azure Cognitive Search Connection\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential", "metadata": {"doc_id": "mlindex_local/langchain_docs_to_mlindex.py0", "chunk_hash": "958f4e3e6a0a3583993f38240791d2508eef4f6286b615031e34b5b30d4f2236", "source": {"title": "mlindex_local/langchain_docs_to_mlindex.py", "filename": "mlindex_local/langchain_docs_to_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/langchain_docs_to_mlindex.py", "mtime": 1694056741.4916399, "chunk_id": "0"}, "stats": {"tiktokens": 137, "chars": 516, "lines": 13}}, "document_id": "mlindex_local/langchain_docs_to_mlindex.py0"}
{"content": "Title: mlindex_local/langchain_docs_to_mlindex.py\n\nml_client = MLClient.from_config(\n    credential=DefaultAzureCredential(), path=\"config.json\"\n)\n\nacs_connection = ml_client.connections.get(\"azureml-rag-acs\")\naoai_connection = ml_client.connections.get(\"azureml-rag-oai\")\n\n# %% https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/wikipedia.html\nfrom langchain.document_loaders import WikipediaLoader\n\ndocs = WikipediaLoader(query=\"HUNTER X HUNTER\", load_max_docs=10).load()\nlen(docs)\n\n# %%\nfrom langchain.text_splitter import MarkdownTextSplitter", "metadata": {"doc_id": "mlindex_local/langchain_docs_to_mlindex.py1", "chunk_hash": "0c2ab3b5dcff37f865bfe1e366864c2c18f7ada0007e9bd8ce6dfc56e9f31bf6", "source": {"title": "mlindex_local/langchain_docs_to_mlindex.py", "filename": "mlindex_local/langchain_docs_to_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/langchain_docs_to_mlindex.py", "mtime": 1694056741.4916399, "chunk_id": "1"}, "stats": {"tiktokens": 134, "chars": 576, "lines": 17}}, "document_id": "mlindex_local/langchain_docs_to_mlindex.py1"}
{"content": "Title: mlindex_local/langchain_docs_to_mlindex.py\n\nsplit_docs = MarkdownTextSplitter.from_tiktoken_encoder(\n    chunk_size=1024\n).split_documents(docs)\n\n# %%\nfrom azureml.rag.mlindex import MLIndex", "metadata": {"doc_id": "mlindex_local/langchain_docs_to_mlindex.py2", "chunk_hash": "9c96360bf0488c682bd41f3332b43fdc6a11f3fc1fc4c76c890febfdf96d8f63", "source": {"title": "mlindex_local/langchain_docs_to_mlindex.py", "filename": "mlindex_local/langchain_docs_to_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/langchain_docs_to_mlindex.py", "mtime": 1694056741.4916399, "chunk_id": "2"}, "stats": {"tiktokens": 50, "chars": 197, "lines": 8}}, "document_id": "mlindex_local/langchain_docs_to_mlindex.py2"}
{"content": "Title: mlindex_local/langchain_docs_to_mlindex.py\n\n# Process data into FAISS Index using HuggingFace embeddings\nmlindex = MLIndex.from_documents(\n    documents=split_docs,\n    embeddings_model=\"azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002\",\n    embeddings_connection=aoai_connection,\n    embeddings_container=\"./.embeddings_cache/hunter_x_hunter_aoai_acs\",\n    index_type=\"acs\",\n    index_connection=acs_connection,\n    index_config={\"index_name\": \"hunter_x_hunter_aoai_acs\"},\n)", "metadata": {"doc_id": "mlindex_local/langchain_docs_to_mlindex.py3", "chunk_hash": "20c47f28996d378a2169b82721f9502984dddb59afe3a671e1958bb4277f936f", "source": {"title": "mlindex_local/langchain_docs_to_mlindex.py", "filename": "mlindex_local/langchain_docs_to_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/langchain_docs_to_mlindex.py", "mtime": 1694056741.4916399, "chunk_id": "3"}, "stats": {"tiktokens": 124, "chars": 512, "lines": 12}}, "document_id": "mlindex_local/langchain_docs_to_mlindex.py3"}
{"content": "Title: mlindex_local/langchain_docs_to_mlindex.py\n\n# %% Query documents, use with inferencing framework\nindex = mlindex.as_langchain_vectorstore()\ndocs = index.similarity_search(\"What is bungie gum?\", k=5)\nprint(docs)", "metadata": {"doc_id": "mlindex_local/langchain_docs_to_mlindex.py4", "chunk_hash": "a49f5e5c1c4408908882e7894282691760e28807fe75bd33e615b84ca8ac724f", "source": {"title": "mlindex_local/langchain_docs_to_mlindex.py", "filename": "mlindex_local/langchain_docs_to_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/langchain_docs_to_mlindex.py", "mtime": 1694056741.4916399, "chunk_id": "4"}, "stats": {"tiktokens": 55, "chars": 217, "lines": 6}}, "document_id": "mlindex_local/langchain_docs_to_mlindex.py4"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex.py\n\n# %%[markdown]\n# # Build a Faiss Index using MLIndex SDK\n\n# %% Pre-requisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[document_parsing,faiss,hugging_face]>=0.2.0'\n\n# %%\nfrom azureml.rag.mlindex import MLIndex", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex.py0", "chunk_hash": "59a49620fe7fec28b90127de4542fe93a7e5412f48a8bbbf47479a1f5637379d", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex.py", "mtime": 1694056741.4985437, "chunk_id": "0"}, "stats": {"tiktokens": 124, "chars": 405, "lines": 11}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex.py0"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex.py\n\n# Process data into FAISS Index using HuggingFace embeddings\nmlindex = MLIndex.from_files(\n    source_uri=\"../\",\n    source_glob=\"**/*\",\n    chunk_size=200,\n    # embeddings_model=sentence_transformers.SentenceTransformer('sentence-transformers/all-mpnet-base-v2'),\n    embeddings_model=\"hugging_face://model/sentence-transformers/all-mpnet-base-v2\",\n    embeddings_container=\"./.embeddings_cache/mlindex_docs_mpnet_faiss\",\n    index_type=\"faiss\",\n)", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex.py1", "chunk_hash": "d3a85e444ee1c645802bf9f49972576b23a5f0157071394f5b368222b56faff7", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex.py", "mtime": 1694056741.4985437, "chunk_id": "1"}, "stats": {"tiktokens": 121, "chars": 502, "lines": 12}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex.py1"}
{"content": "Title: mlindex_local/local_docs_to_faiss_mlindex.py\n\n# %% Query documents, use with inferencing framework\nindex = mlindex.as_langchain_vectorstore()\ndocs = index.similarity_search(\"Topic in my data.\", k=5)\nprint(docs)\n\n# %% Save for later\nmlindex.save(\"./different_index_path\")\nmlindex = MLIndex(\"./different_index_path\")", "metadata": {"doc_id": "mlindex_local/local_docs_to_faiss_mlindex.py2", "chunk_hash": "32e1c888c99b6ac6bcada4ad82df10ab035b55d776d1b92df859340b63ff896e", "source": {"title": "mlindex_local/local_docs_to_faiss_mlindex.py", "filename": "mlindex_local/local_docs_to_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_faiss_mlindex.py", "mtime": 1694056741.4985437, "chunk_id": "2"}, "stats": {"tiktokens": 78, "chars": 321, "lines": 10}}, "document_id": "mlindex_local/local_docs_to_faiss_mlindex.py2"}
{"content": "Title: mlindex_local/local_docs_to_acs_aoai_mlindex.py\n\n# %%[markdown]\n# # Build an ACS Index using MLIndex SDK\n\n# %% Pre-requisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[document_parsing,cognitive_search]>=0.2.0'\n\n# %% Get Azure Cognitive Search Connection\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential", "metadata": {"doc_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py0", "chunk_hash": "0c11b5e98b629009fa732bfddd27386a10523b537ca9f6e883968020e08cdc97", "source": {"title": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "filename": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_acs_aoai_mlindex.py", "mtime": 1694056741.5003362, "chunk_id": "0"}, "stats": {"tiktokens": 134, "chars": 486, "lines": 12}}, "document_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py0"}
{"content": "Title: mlindex_local/local_docs_to_acs_aoai_mlindex.py\n\nml_client = MLClient.from_config(credential=DefaultAzureCredential())\n\nacs_connection = ml_client.connections.get(\"azureml-rag-acs\")\naoai_connection = ml_client.connections.get(\"azureml-rag-oai\")\n\n# %%\nfrom azureml.rag.mlindex import MLIndex", "metadata": {"doc_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py1", "chunk_hash": "96d276e615b01589572c26f84ab1c5ec6eeb78f3ab81d1f4511bed171b50e2b6", "source": {"title": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "filename": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_acs_aoai_mlindex.py", "mtime": 1694056741.5003362, "chunk_id": "1"}, "stats": {"tiktokens": 74, "chars": 297, "lines": 9}}, "document_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py1"}
{"content": "Title: mlindex_local/local_docs_to_acs_aoai_mlindex.py\n\n# Process data into FAISS Index using HuggingFace embeddings\nmlindex = MLIndex.from_files(\n    source_uri=\"../\",\n    source_glob=\"**/*\",\n    chunk_size=200,\n    embeddings_model=\"azure_open_ai://deployment/text-embedding-ada-002/model/text-embedding-ada-002\",\n    embeddings_connection=aoai_connection,\n    embeddings_container=\"./.embeddings_cache/mlindex_docs_aoai_acs\",\n    index_type=\"acs\",\n    index_connection=acs_connection,", "metadata": {"doc_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py2", "chunk_hash": "553792379c44dc5cccc06fae81ba836b0063a7b6a1a5c66ccb0f4f0f90b74d10", "source": {"title": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "filename": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_acs_aoai_mlindex.py", "mtime": 1694056741.5003362, "chunk_id": "2"}, "stats": {"tiktokens": 119, "chars": 487, "lines": 12}}, "document_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py2"}
{"content": "Title: mlindex_local/local_docs_to_acs_aoai_mlindex.py\n\nindex_config={\"index_name\": \"mlindex_docs_aoai_acs\"},\n    output_path=\"./acs_open_ai_index\",\n)", "metadata": {"doc_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py3", "chunk_hash": "f05a9cbbf854b1225a22d0471af22c5d2f0b0806ab3a7fdbd7764d24bc77e9b1", "source": {"title": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "filename": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_acs_aoai_mlindex.py", "mtime": 1694056741.5003362, "chunk_id": "3"}, "stats": {"tiktokens": 43, "chars": 150, "lines": 5}}, "document_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py3"}
{"content": "Title: mlindex_local/local_docs_to_acs_aoai_mlindex.py\n\n# %% Load MLIndex from local\nfrom azureml.rag.mlindex import MLIndex\n\nmlindex = MLIndex(\"./acs_open_ai_index\")\n\n# %% Query documents, use with inferencing framework\nindex = mlindex.as_langchain_vectorstore()\ndocs = index.similarity_search(\"Topic in my data.\", k=5)\nprint(docs)", "metadata": {"doc_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py4", "chunk_hash": "c56df6ea889a6671ff3698398a29dade7ea61c5d93b5f31471c85eee71fa56ec", "source": {"title": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "filename": "mlindex_local/local_docs_to_acs_aoai_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/mlindex_local/local_docs_to_acs_aoai_mlindex.py", "mtime": 1694056741.5003362, "chunk_id": "4"}, "stats": {"tiktokens": 87, "chars": 332, "lines": 11}}, "document_id": "mlindex_local/local_docs_to_acs_aoai_mlindex.py4"}
{"content": "Title: flows/chat-with-index/chat_with_index_tool.py\n\nfrom promptflow import tool\nfrom src.main import chat_with_index\n\n\n@tool\ndef chat_with_index_tool(question: str, mlindex_uri: str, history: list, ready: str):\n    history = convert_chat_history_to_chatml_messages(history)\n\n    stream, context = chat_with_index(question, mlindex_uri, history)\n\n    answer = \"\"\n    for str in stream:\n        answer = answer + str + \"\"\n\n    return {\"answer\": answer, \"context\": context}", "metadata": {"doc_id": "flows/chat-with-index/chat_with_index_tool.py0", "chunk_hash": "e691c0ee133536584efa20a3a1cbb7e7790a6509beaaf55c626d8b3f339b84d1", "source": {"title": "flows/chat-with-index/chat_with_index_tool.py", "filename": "flows/chat-with-index/chat_with_index_tool.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/chat_with_index_tool.py", "mtime": 1694110408.492135, "chunk_id": "0"}, "stats": {"tiktokens": 110, "chars": 472, "lines": 17}}, "document_id": "flows/chat-with-index/chat_with_index_tool.py0"}
{"content": "Title: flows/chat-with-index/chat_with_index_tool.py\n\ndef convert_chat_history_to_chatml_messages(history):\n    messages = []\n    for item in history:\n        messages.append({\"role\": \"user\", \"content\": item[\"inputs\"][\"question\"]})\n        messages.append({\"role\": \"assistant\", \"content\": item[\"outputs\"][\"answer\"]})\n\n    return messages", "metadata": {"doc_id": "flows/chat-with-index/chat_with_index_tool.py1", "chunk_hash": "4b5d5ed71c130e7697bdf5a24001ce2fa1ba255211d1f240907bb0db0a21f67b", "source": {"title": "flows/chat-with-index/chat_with_index_tool.py", "filename": "flows/chat-with-index/chat_with_index_tool.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/chat_with_index_tool.py", "mtime": 1694110408.492135, "chunk_id": "1"}, "stats": {"tiktokens": 73, "chars": 337, "lines": 9}}, "document_id": "flows/chat-with-index/chat_with_index_tool.py1"}
{"content": "Title: flows/chat-with-index/chat_with_index_tool.py\n\ndef convert_chatml_messages_to_chat_history(messages):\n    history = []\n    for i in range(0, len(messages), 2):\n        history.append(\n            {\n                \"inputs\": {\"question\": messages[i][\"content\"]},\n                \"outputs\": {\"answer\": messages[i + 1][\"content\"]},\n            }\n        )\n\n    return history", "metadata": {"doc_id": "flows/chat-with-index/chat_with_index_tool.py2", "chunk_hash": "9af17a3afd9ee52792c4e45cf236410179c43043be4b7a64546b4ffd067fc1f8", "source": {"title": "flows/chat-with-index/chat_with_index_tool.py", "filename": "flows/chat-with-index/chat_with_index_tool.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/chat_with_index_tool.py", "mtime": 1694110408.492135, "chunk_id": "2"}, "stats": {"tiktokens": 82, "chars": 379, "lines": 13}}, "document_id": "flows/chat-with-index/chat_with_index_tool.py2"}
{"content": "Title: flows/chat-with-index/setup_env.py\n\nimport os\nfrom typing import Union\n\nfrom promptflow import tool\nfrom promptflow.connections import AzureOpenAIConnection, OpenAIConnection\n\n\n@tool", "metadata": {"doc_id": "flows/chat-with-index/setup_env.py0", "chunk_hash": "f958f1090af6508ae3b956535a73d932dcceb3f786e8477e59164a95908ea33e", "source": {"title": "flows/chat-with-index/setup_env.py", "filename": "flows/chat-with-index/setup_env.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/setup_env.py", "mtime": 1694042161.4105766, "chunk_id": "0"}, "stats": {"tiktokens": 40, "chars": 189, "lines": 10}}, "document_id": "flows/chat-with-index/setup_env.py0"}
{"content": "Title: flows/chat-with-index/setup_env.py\n\ndef setup_env(connection: Union[AzureOpenAIConnection, OpenAIConnection], config: dict):\n    if not connection or not config:\n        return\n\n    if isinstance(connection, AzureOpenAIConnection):\n        os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n        os.environ[\"OPENAI_API_BASE\"] = connection.api_base\n        os.environ[\"OPENAI_API_KEY\"] = connection.api_key\n        os.environ[\"OPENAI_API_VERSION\"] = connection.api_version", "metadata": {"doc_id": "flows/chat-with-index/setup_env.py1", "chunk_hash": "317591f57fd967760295ba4e227a25362e2ce79aae8fb79734b2d101a559356c", "source": {"title": "flows/chat-with-index/setup_env.py", "filename": "flows/chat-with-index/setup_env.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/setup_env.py", "mtime": 1694042161.4105766, "chunk_id": "1"}, "stats": {"tiktokens": 105, "chars": 470, "lines": 11}}, "document_id": "flows/chat-with-index/setup_env.py1"}
{"content": "Title: flows/chat-with-index/setup_env.py\n\nif isinstance(connection, OpenAIConnection):\n        os.environ[\"OPENAI_API_KEY\"] = connection.api_key\n        if connection.organization is not None:\n            os.environ[\"OPENAI_ORG_ID\"] = connection.organization\n\n    for key in config:\n        os.environ[key] = str(config[key])\n\n    return \"Ready\"", "metadata": {"doc_id": "flows/chat-with-index/setup_env.py2", "chunk_hash": "10e8fe46986136a443763ebab847d66516816b420d1bc99de052aac0282ef346", "source": {"title": "flows/chat-with-index/setup_env.py", "filename": "flows/chat-with-index/setup_env.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/setup_env.py", "mtime": 1694042161.4105766, "chunk_id": "2"}, "stats": {"tiktokens": 75, "chars": 346, "lines": 11}}, "document_id": "flows/chat-with-index/setup_env.py2"}
{"content": "Title: flows/chat-with-index/find_context_tool.py\n\nfrom promptflow import tool\nfrom src.find_context import find_context\n\n\n@tool\ndef find_context_tool(question: str, mlindex_uri: str):\n    prompt, documents = find_context(question, mlindex_uri)\n\n    return {\"prompt\": prompt, \"context\": [d.page_content for d in documents]}", "metadata": {"doc_id": "flows/chat-with-index/find_context_tool.py0", "chunk_hash": "5e82942eae9a45aa0ba752d950f57ba81ff083f7e9644d898fbba1cc0ade3c6a", "source": {"title": "flows/chat-with-index/find_context_tool.py", "filename": "flows/chat-with-index/find_context_tool.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/find_context_tool.py", "mtime": 1694110411.663442, "chunk_id": "0"}, "stats": {"tiktokens": 74, "chars": 323, "lines": 11}}, "document_id": "flows/chat-with-index/find_context_tool.py0"}
{"content": "Title: PyPDF2\n\nPyPDF2\nazureml-rag[faiss]\nopenai\njinja2\npython-dotenv\ntiktoken\npromptflow[azure]\npromptflow-tools", "metadata": {"doc_id": "flows/chat-with-index/requirements.txt0", "chunk_hash": "0b19faf2807c86f7c799e018c2c611897b836868bcc4ee6af3c6a1a75e1455b7", "source": {"title": "PyPDF2", "filename": "flows/chat-with-index/requirements.txt", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/requirements.txt", "mtime": 1694042161.408507, "chunk_id": "0"}, "stats": {"tiktokens": 41, "chars": 112, "lines": 10}}, "document_id": "flows/chat-with-index/requirements.txt0"}
{"content": "Title: flows/chat-with-index/__init__.py\n\nimport sys\nimport os\n\nsys.path.append(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), \"chat_with_index\")\n)", "metadata": {"doc_id": "flows/chat-with-index/__init__.py0", "chunk_hash": "e18e4f4bf0f67c2c431f069bb48c84aef7992e7a715ad8c96456fa008f445104", "source": {"title": "flows/chat-with-index/__init__.py", "filename": "flows/chat-with-index/__init__.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/__init__.py", "mtime": 1694042161.323171, "chunk_id": "0"}, "stats": {"tiktokens": 41, "chars": 162, "lines": 8}}, "document_id": "flows/chat-with-index/__init__.py0"}
{"content": "# Chat with MLIndex\n\n# Chat with MLIndex\nThis is a simple flow that allow you to ask questions about the content of an MLIndex and get answers.\nYou can run the flow with a URL to an MLIndex and question as argument.\nWhen you ask a question, it will look up the index to retrieve relevant content and post the question with the relevant content to OpenAI chat model (gpt-3.5-turbo or gpt4) to get an answer.\n\nTools used in this flow\uff1a\n- custom `python` Tool", "metadata": {"doc_id": "flows/chat-with-index/README.md0", "chunk_hash": "9acd4e1087f29b3296c539d0c0aa7bddf60f999d9de2e3c24aa0e5e16d9651e6", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "0"}, "stats": {"tiktokens": 112, "chars": 455, "lines": 9}, "markdown_heading": {"heading": "Chat with MLIndex", "level": 1}}, "document_id": "flows/chat-with-index/README.md0"}
{"content": "# Chat with MLIndex\n\n# Chat with MLIndex\n## Prerequisites\nInstall dependencies:\n```bash\npip install -r requirements.txt\n```", "metadata": {"doc_id": "flows/chat-with-index/README.md1", "chunk_hash": "bdf701b403e9d085b5be18a7bb227003aeedc85edb414aeef93a389725447c00", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "1"}, "stats": {"tiktokens": 30, "chars": 123, "lines": 8}, "markdown_heading": {"heading": "Prerequisites", "level": 2}}, "document_id": "flows/chat-with-index/README.md1"}
{"content": "# Chat with MLIndex\n\n# Chat with MLIndex\n## Get started\n### Create connection in this folder", "metadata": {"doc_id": "flows/chat-with-index/README.md2", "chunk_hash": "757463a5c0da2521037053747633161beb71dbcd82c8a60d127df115eb9c0771", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "2"}, "stats": {"tiktokens": 22, "chars": 92, "lines": 5}, "markdown_heading": {"heading": "Get started", "level": 2}}, "document_id": "flows/chat-with-index/README.md2"}
{"content": "# Chat with MLIndex\n\n```bash\n# create connection needed by flow", "metadata": {"doc_id": "flows/chat-with-index/README.md3", "chunk_hash": "ca6581aa7fa7272b6535ae912bab163958118e99e9827f170f0ef76aea33deb0", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "3"}, "stats": {"tiktokens": 15, "chars": 63, "lines": 4}, "markdown_heading": {"heading": "```bash", "level": 0}}, "document_id": "flows/chat-with-index/README.md3"}
{"content": "# Chat with MLIndex\n\nif pf connection list | grep open_ai_connection; then\n    echo \"open_ai_connection already exists\"\nelse\n    pf connection create --file ./azure_openai.yml --name open_ai_connection --set api_key=<your_api_key> api_base=<your_api_base>\nfi\n```\n### SDK Example", "metadata": {"doc_id": "flows/chat-with-index/README.md4", "chunk_hash": "afae168742ab7d5dd1215fb8532defd077c1d695886becf48c0b122e48973ddc", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "4"}, "stats": {"tiktokens": 68, "chars": 278, "lines": 9}, "markdown_heading": {"heading": "if pf connection list | grep open_ai_connection; then\n    echo \"open_ai_connection already exists\"\nelse\n    pf connection create --file ./azure_openai.yml --name open_ai_connection --set api_key=<your_api_key> api_base=<your_api_base>\nfi\n```", "level": 0}}, "document_id": "flows/chat-with-index/README.md4"}
{"content": "# Chat with MLIndex\n\nRefer to [local_docs_to_faiss_mlindex_with_promptfow.py](../../mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py)\n### CLI Example", "metadata": {"doc_id": "flows/chat-with-index/README.md5", "chunk_hash": "91b48ac62299cb286c00ddecf86649dd0698d2ad0f6c1440cca2e75e65436d5e", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "5"}, "stats": {"tiktokens": 42, "chars": 160, "lines": 4}, "markdown_heading": {"heading": "Refer to [local_docs_to_faiss_mlindex_with_promptfow.py](../../mlindex_local/local_docs_to_faiss_mlindex_with_promptfow.py)", "level": 0}}, "document_id": "flows/chat-with-index/README.md5"}
{"content": "# Chat with MLIndex\n\n```bash\n# test with flow inputs, you need local or remote MLIndex (refer to SDK examples to create them)", "metadata": {"doc_id": "flows/chat-with-index/README.md6", "chunk_hash": "9902a124d81d73316acb83be877cd2e829cd32046fef472950955e0f64a2a467", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "6"}, "stats": {"tiktokens": 31, "chars": 125, "lines": 4}, "markdown_heading": {"heading": "```bash", "level": 0}}, "document_id": "flows/chat-with-index/README.md6"}
{"content": "# Chat with MLIndex\n\npf flow test --flow . --inputs question=\"\" mlindex_uri=\"../../mlindex_local/mlindex_docs_aoai_faiss\"\n# (Optional) create a random run name", "metadata": {"doc_id": "flows/chat-with-index/README.md7", "chunk_hash": "c17843eaf9946d39e7d8608a23385551c698b71437d4146dc7728f18dceee6f0", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "7"}, "stats": {"tiktokens": 41, "chars": 159, "lines": 4}, "markdown_heading": {"heading": "pf flow test --flow . --inputs question=\"\" mlindex_uri=\"../../mlindex_local/mlindex_docs_aoai_faiss\"", "level": 0}}, "document_id": "flows/chat-with-index/README.md7"}
{"content": "# Chat with MLIndex\n\nrun_name=\"doc_questions_\"$(openssl rand -hex 12)\n# run with multiline data, --name is optional", "metadata": {"doc_id": "flows/chat-with-index/README.md8", "chunk_hash": "9d6b0b6b502649fce907b086be56c198f5e828751615f982ac27c8de93424f10", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "8"}, "stats": {"tiktokens": 30, "chars": 115, "lines": 4}, "markdown_heading": {"heading": "run_name=\"doc_questions_\"$(openssl rand -hex 12)", "level": 0}}, "document_id": "flows/chat-with-index/README.md8"}
{"content": "# Chat with MLIndex\n\npf run create --flow . --data ../data/rag_docs_questions.jsonl --stream --column-mapping question='${data.chat_input}' mlindex_uri='../../mlindex_local/mlindex_docs_aoai_faiss' chat_history='${data.chat_history}' config='{\"CHAT_MODEL_DEPLOYMENT_NAME\": \"gpt-35-turbo\", \"PROMPT_TOKEN_LIMIT\": \"2000\", \"MAX_COMPLETION_TOKENS\": \"256\", \"VERBOSE\": \"True\"}' --name $run_name\n# visualize run output details", "metadata": {"doc_id": "flows/chat-with-index/README.md9", "chunk_hash": "50f75c1fb777ac09a92b8c01287c8b41d4f60dc428f0c2995afa24b162a92ad4", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "9"}, "stats": {"tiktokens": 117, "chars": 418, "lines": 4}, "markdown_heading": {"heading": "pf run create --flow . --data ../data/rag_docs_questions.jsonl --stream --column-mapping question='${data.chat_input}' mlindex_uri='../../mlindex_local/mlindex_docs_aoai_faiss' chat_history='${data.chat_history}' config='{\"CHAT_MODEL_DEPLOYMENT_NAME\": \"gpt-35-turbo\", \"PROMPT_TOKEN_LIMIT\": \"2000\", \"MAX_COMPLETION_TOKENS\": \"256\", \"VERBOSE\": \"True\"}' --name $run_name", "level": 0}}, "document_id": "flows/chat-with-index/README.md9"}
{"content": "# Chat with MLIndex\n\npf run visualize --name $run_name\n```\n", "metadata": {"doc_id": "flows/chat-with-index/README.md10", "chunk_hash": "94f3603505647560deb473a53c1ab65dd284ca4177aa6a8d2e531dacb5d913f2", "source": {"title": "Chat with MLIndex", "filename": "flows/chat-with-index/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/README.md", "mtime": 1694042161.3168821, "chunk_id": "10"}, "stats": {"tiktokens": 17, "chars": 59, "lines": 4}, "markdown_heading": {"heading": "pf run visualize --name $run_name\n```", "level": 0}}, "document_id": "flows/chat-with-index/README.md10"}
{"content": "Title: flows/chat-with-index/rewrite_question_tool.py\n\nfrom promptflow import tool\nfrom src.rewrite_question import rewrite_question\n\n\n@tool\ndef rewrite_question_tool(question: str, history: list, env_ready_signal: str):\n    return rewrite_question(question, history)", "metadata": {"doc_id": "flows/chat-with-index/rewrite_question_tool.py0", "chunk_hash": "fcea61b2853fe2344a61433b8c1444946ec76e0248401cc79f4009508c62d560", "source": {"title": "flows/chat-with-index/rewrite_question_tool.py", "filename": "flows/chat-with-index/rewrite_question_tool.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/rewrite_question_tool.py", "mtime": 1694110418.420703, "chunk_id": "0"}, "stats": {"tiktokens": 56, "chars": 267, "lines": 9}}, "document_id": "flows/chat-with-index/rewrite_question_tool.py0"}
{"content": "Title: flows/chat-with-index/qna_tool.py\n\nfrom promptflow import tool\nfrom src.qna import qna\n\n\n@tool\ndef qna_tool(prompt: str, history: list):\n    stream = qna(prompt, convert_chat_history_to_chatml_messages(history))\n\n    answer = \"\"\n    for str in stream:\n        answer = answer + str + \"\"\n\n    return {\"answer\": answer}", "metadata": {"doc_id": "flows/chat-with-index/qna_tool.py0", "chunk_hash": "fe989d741e0eb3343db7395db583278d03154edb56991c7c361086579a277150", "source": {"title": "flows/chat-with-index/qna_tool.py", "filename": "flows/chat-with-index/qna_tool.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/qna_tool.py", "mtime": 1694110415.3727424, "chunk_id": "0"}, "stats": {"tiktokens": 81, "chars": 324, "lines": 15}}, "document_id": "flows/chat-with-index/qna_tool.py0"}
{"content": "Title: flows/chat-with-index/qna_tool.py\n\ndef convert_chat_history_to_chatml_messages(history):\n    messages = []\n    for item in history:\n        messages.append({\"role\": \"user\", \"content\": item[\"inputs\"][\"question\"]})\n        messages.append({\"role\": \"assistant\", \"content\": item[\"outputs\"][\"answer\"]})\n\n    return messages", "metadata": {"doc_id": "flows/chat-with-index/qna_tool.py1", "chunk_hash": "0ce914677a64f12affa5c0633cd783bc1c6f649212b8c2af99f7c394ba398a86", "source": {"title": "flows/chat-with-index/qna_tool.py", "filename": "flows/chat-with-index/qna_tool.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/qna_tool.py", "mtime": 1694110415.3727424, "chunk_id": "1"}, "stats": {"tiktokens": 72, "chars": 325, "lines": 9}}, "document_id": "flows/chat-with-index/qna_tool.py1"}
{"content": "# Context\n\n\nYou're a smart assistant can answer questions based on provided context and previous conversation history between you and human.\n\nUse the context to answer the question at the end, note that the context has order and importance - e.g. context #1 is more important than #2.\n\nTry as much as you can to answer based on the provided the context, if you cannot derive the answer from the context, you should say you don't know.\nAnswer in the same language as the question.\n\n", "metadata": {"doc_id": "flows/chat-with-index/src/qna_prompt.md0", "chunk_hash": "98769d6f87afa12af988d4e7c8350a5bae47464bffa0af61735cb8eed0829a29", "source": {"title": "Context", "filename": "flows/chat-with-index/src/qna_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/qna_prompt.md", "mtime": 1694042161.3644316, "chunk_id": "0"}, "stats": {"tiktokens": 99, "chars": 481, "lines": 10}, "markdown_heading": {"heading": "flows/chat-with-index/src/qna_prompt.md", "level": 0}}, "document_id": "flows/chat-with-index/src/qna_prompt.md0"}
{"content": "# Context\n\n# Context\n{% for i, c in context %}", "metadata": {"doc_id": "flows/chat-with-index/src/qna_prompt.md1", "chunk_hash": "59d1ed64de0a7a5b0d9836273cbbd6dbcfc12eed10ede078e1c0a31ade9e4797", "source": {"title": "Context", "filename": "flows/chat-with-index/src/qna_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/qna_prompt.md", "mtime": 1694042161.3644316, "chunk_id": "1"}, "stats": {"tiktokens": 15, "chars": 46, "lines": 4}, "markdown_heading": {"heading": "Context", "level": 1}}, "document_id": "flows/chat-with-index/src/qna_prompt.md1"}
{"content": "# Context\n\n# Context\n## Context #{{i+1}}\n{{c.page_content}}\n{% endfor %}", "metadata": {"doc_id": "flows/chat-with-index/src/qna_prompt.md2", "chunk_hash": "2792cb6f059a2d1fb7c814117bbe561d84e0cf5d6e2c629162040b786ea9936a", "source": {"title": "Context", "filename": "flows/chat-with-index/src/qna_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/qna_prompt.md", "mtime": 1694042161.3644316, "chunk_id": "2"}, "stats": {"tiktokens": 24, "chars": 72, "lines": 6}, "markdown_heading": {"heading": "Context {{i+1}}", "level": 2}}, "document_id": "flows/chat-with-index/src/qna_prompt.md2"}
{"content": "# Context\n\n# Question\n{{question}}", "metadata": {"doc_id": "flows/chat-with-index/src/qna_prompt.md3", "chunk_hash": "febc3c3427876644e2bbe395560dbf4e92a0eb4e610e6b7e0d5420e4ce895bf8", "source": {"title": "Context", "filename": "flows/chat-with-index/src/qna_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/qna_prompt.md", "mtime": 1694042161.3644316, "chunk_id": "3"}, "stats": {"tiktokens": 9, "chars": 34, "lines": 4}, "markdown_heading": {"heading": "Question", "level": 1}}, "document_id": "flows/chat-with-index/src/qna_prompt.md3"}
{"content": "Title: flows/chat-with-index/src/rewrite_question.py\n\nfrom jinja2 import Environment, FileSystemLoader\nimport os\nfrom utils.logging import log\nfrom utils.oai import OAIChat, render_with_token_limit", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question.py0", "chunk_hash": "960851a75329f5fd09b5664b63942e58c6e71bd5b2ab7428b04ae423fd8f5c22", "source": {"title": "flows/chat-with-index/src/rewrite_question.py", "filename": "flows/chat-with-index/src/rewrite_question.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question.py", "mtime": 1694042161.3656738, "chunk_id": "0"}, "stats": {"tiktokens": 44, "chars": 197, "lines": 6}}, "document_id": "flows/chat-with-index/src/rewrite_question.py0"}
{"content": "Title: flows/chat-with-index/src/rewrite_question.py\n\ndef rewrite_question(question: str, history: list):\n    template = Environment(\n        loader=FileSystemLoader(os.path.dirname(os.path.abspath(__file__)))\n    ).get_template(\"rewrite_question_prompt.md\")\n    token_limit = int(os.environ[\"PROMPT_TOKEN_LIMIT\"])\n    max_completion_tokens = int(os.environ[\"MAX_COMPLETION_TOKENS\"])", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question.py1", "chunk_hash": "dae6d263bb31c7f9a8db3a6ba63f0e425b58543d12f5f516d4187ce1e9b8dcca", "source": {"title": "flows/chat-with-index/src/rewrite_question.py", "filename": "flows/chat-with-index/src/rewrite_question.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question.py", "mtime": 1694042161.3656738, "chunk_id": "1"}, "stats": {"tiktokens": 83, "chars": 383, "lines": 8}}, "document_id": "flows/chat-with-index/src/rewrite_question.py1"}
{"content": "Title: flows/chat-with-index/src/rewrite_question.py\n\n# Try to render the prompt with token limit and reduce the history count if it fails\n    while True:\n        try:\n            prompt = render_with_token_limit(\n                template, token_limit, question=question, history=history\n            )\n            break\n        except ValueError:\n            history = history[:-1]\n            log(f\"Reducing chat history count to {len(history)} to fit token limit\")", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question.py2", "chunk_hash": "e9bb759537bcc2666563951e8fa5c197560b2d8b2049215665301bc49e292c60", "source": {"title": "flows/chat-with-index/src/rewrite_question.py", "filename": "flows/chat-with-index/src/rewrite_question.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question.py", "mtime": 1694042161.3656738, "chunk_id": "2"}, "stats": {"tiktokens": 94, "chars": 466, "lines": 12}}, "document_id": "flows/chat-with-index/src/rewrite_question.py2"}
{"content": "Title: flows/chat-with-index/src/rewrite_question.py\n\nchat = OAIChat()\n    rewritten_question = chat.generate(\n        messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=max_completion_tokens\n    )\n    log(f\"Rewritten question: {rewritten_question}\")\n\n    return rewritten_question", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question.py3", "chunk_hash": "5763169aeeddba73db8113becea3d1b7a24564c13bb32e70833c2df6a29d327c", "source": {"title": "flows/chat-with-index/src/rewrite_question.py", "filename": "flows/chat-with-index/src/rewrite_question.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question.py", "mtime": 1694042161.3656738, "chunk_id": "3"}, "stats": {"tiktokens": 64, "chars": 289, "lines": 9}}, "document_id": "flows/chat-with-index/src/rewrite_question.py3"}
{"content": "Title: flows/chat-with-index/src/__init__.py\n\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))", "metadata": {"doc_id": "flows/chat-with-index/src/__init__.py0", "chunk_hash": "c8477be2871b42d7daf2a19a6f8c0f34bc076b753344654b795a9889c0f351a1", "source": {"title": "flows/chat-with-index/src/__init__.py", "filename": "flows/chat-with-index/src/__init__.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/__init__.py", "mtime": 1694042161.3416657, "chunk_id": "0"}, "stats": {"tiktokens": 31, "chars": 127, "lines": 6}}, "document_id": "flows/chat-with-index/src/__init__.py0"}
{"content": "Title: flows/chat-with-index/src/qna.py\n\nimport os\n\nfrom utils.oai import OAIChat\n\n\ndef qna(prompt: str, history: list):\n    max_completion_tokens = int(os.environ.get(\"MAX_COMPLETION_TOKENS\"))\n\n    chat = OAIChat()\n    stream = chat.stream(\n        messages=history + [{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=max_completion_tokens,\n    )\n\n    return stream", "metadata": {"doc_id": "flows/chat-with-index/src/qna.py0", "chunk_hash": "2c7155923f4ee4496ce93a32ff60d3d22694473232e5f9d19433434d00893cb5", "source": {"title": "flows/chat-with-index/src/qna.py", "filename": "flows/chat-with-index/src/qna.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/qna.py", "mtime": 1694042161.356671, "chunk_id": "0"}, "stats": {"tiktokens": 92, "chars": 374, "lines": 17}}, "document_id": "flows/chat-with-index/src/qna.py0"}
{"content": "# Chat with Index\n\n# Chat with Index\nThis is a simple Python application that allow you to ask questions about the content of an MLIndex and get answers.\nIt's a console application that you start with a URI to an MLINdex as argument. When you ask a question, it will look up the index to retrieve relevant content and post the question with the relevant content to OpenAI chat model (gpt-3.5-turbo or gpt4) to get an answer.", "metadata": {"doc_id": "flows/chat-with-index/src/README.md0", "chunk_hash": "d3fc7186a98e8e41fdd06c2c670f24a2b606037324b0b6e31fbf233c195e4b32", "source": {"title": "Chat with Index", "filename": "flows/chat-with-index/src/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/README.md", "mtime": 1694042161.3361104, "chunk_id": "0"}, "stats": {"tiktokens": 101, "chars": 424, "lines": 5}, "markdown_heading": {"heading": "Chat with Index", "level": 1}}, "document_id": "flows/chat-with-index/src/README.md0"}
{"content": "# Chat with Index\n\n# Chat with Index\n## How it works?\n## Get started", "metadata": {"doc_id": "flows/chat-with-index/src/README.md1", "chunk_hash": "8a5f488fef42b7724159a49a71630974b3b87c683184130bfc54999c9db684bc", "source": {"title": "Chat with Index", "filename": "flows/chat-with-index/src/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/README.md", "mtime": 1694042161.3361104, "chunk_id": "1"}, "stats": {"tiktokens": 18, "chars": 68, "lines": 5}, "markdown_heading": {"heading": "How it works?", "level": 2}}, "document_id": "flows/chat-with-index/src/README.md1"}
{"content": "# Chat with Index\n\n# Chat with Index\n## How it works?\n### Create .env file in this folder with below content\n```\nOPENAI_API_BASE=<AOAI_endpoint>\nOPENAI_API_KEY=<AOAI_key>\nCHAT_MODEL_DEPLOYMENT_NAME=gpt-35-turbo\nPROMPT_TOKEN_LIMIT=3000\nMAX_COMPLETION_TOKENS=256\nVERBOSE=false\n```\nNote: CHAT_MODEL_DEPLOYMENT_NAME should point to a chat model like gpt-3.5-turbo or gpt-4", "metadata": {"doc_id": "flows/chat-with-index/src/README.md2", "chunk_hash": "1727bc8881cb679777f607952607e90cc42b0c39127738787fdb89b37d084055", "source": {"title": "Chat with Index", "filename": "flows/chat-with-index/src/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/README.md", "mtime": 1694042161.3361104, "chunk_id": "2"}, "stats": {"tiktokens": 114, "chars": 368, "lines": 14}, "markdown_heading": {"heading": "Create .env file in this folder with below content", "level": 3}}, "document_id": "flows/chat-with-index/src/README.md2"}
{"content": "# Chat with Index\n\n# Chat with Index\n## How it works?\n### Run the command line\n```shell\npython main.py <uri-to-mlindex>\n```", "metadata": {"doc_id": "flows/chat-with-index/src/README.md3", "chunk_hash": "b2c1ed1f3c928d64e96e5b5be60f70796519b8aca81ed10f40ccce7d7dd710d6", "source": {"title": "Chat with Index", "filename": "flows/chat-with-index/src/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/README.md", "mtime": 1694042161.3361104, "chunk_id": "3"}, "stats": {"tiktokens": 35, "chars": 123, "lines": 8}, "markdown_heading": {"heading": "Run the command line", "level": 3}}, "document_id": "flows/chat-with-index/src/README.md3"}
{"content": "# Example 1\n\n\nYou are able to reason from previous conversation and the recent question, to come up with a rewrite of the question which is concise but with enough information that people without knowledge of previous conversation can understand the question.\n\nA few examples:\n\n", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md0", "chunk_hash": "72a343bcd4bcadc0381245d7ed32663f9840decb24d959ffc1a02ecf675b5f38", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "0"}, "stats": {"tiktokens": 50, "chars": 278, "lines": 7}, "markdown_heading": {"heading": "flows/chat-with-index/src/rewrite_question_prompt.md", "level": 0}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md0"}
{"content": "# Example 1\n\n# Example 1\n## Previous conversation", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md1", "chunk_hash": "2afa23771accaee622e49df8706705816dff0485ad26d4004d0fd7fd3095eb5e", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "1"}, "stats": {"tiktokens": 13, "chars": 49, "lines": 4}, "markdown_heading": {"heading": "Example 1", "level": 1}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md1"}
{"content": "# Example 1\n\nuser: Who is Bill Clinton?\nassistant: Bill Clinton is an American politician who served as the 42nd President of the United States from 1993 to 2001.\n## Question", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md2", "chunk_hash": "5809865b2221d039448d716b9c352c2affe3520c6d92da1a399f6adda3781ae7", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "2"}, "stats": {"tiktokens": 43, "chars": 174, "lines": 5}, "markdown_heading": {"heading": "user: Who is Bill Clinton?\nassistant: Bill Clinton is an American politician who served as the 42nd President of the United States from 1993 to 2001.", "level": 0}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md2"}
{"content": "# Example 1\n\nuser: When was he born?\n## Rewritten question", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md3", "chunk_hash": "93adcade449127dcae67371c282bbc3d36c1ca5dcd5f7d78460edd46cdc7c797", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "3"}, "stats": {"tiktokens": 16, "chars": 58, "lines": 4}, "markdown_heading": {"heading": "user: When was he born?", "level": 0}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md3"}
{"content": "# Example 1\n\nWhen was Bill Clinton born?\n# Example 2", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md4", "chunk_hash": "823435f0f44a2607bf244da031f486af88732f7ad399fffb5e5c4723abc19369", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "4"}, "stats": {"tiktokens": 15, "chars": 52, "lines": 4}, "markdown_heading": {"heading": "When was Bill Clinton born?", "level": 0}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md4"}
{"content": "# Example 1\n\nWhen was Bill Clinton born?\n## Previous conversation\nuser: What is BERT?\nassistant: BERT stands for \"Bidirectional Encoder Representations from Transformers.\" It is a natural language processing (NLP) model developed by Google. \nuser: What data was used for its training?\nassistant: The BERT (Bidirectional Encoder Representations from Transformers) model was trained on a large corpus of publicly available text from the internet. It was trained on a combination of books, articles, websites, and other sources to learn the language patterns and relationships between words.", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md5", "chunk_hash": "04fa09fa8903077a3115380e27af5e4dba0d6322b74b1504d169828031d40938", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "5"}, "stats": {"tiktokens": 118, "chars": 588, "lines": 8}, "markdown_heading": {"heading": "Previous conversation", "level": 2}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md5"}
{"content": "# Example 1\n\nWhen was Bill Clinton born?\n## Question\nuser: What NLP tasks can it perform well?", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md6", "chunk_hash": "8b47e8eae1713a15b822b6c412f4ce57a4ba1f1d55fbff54240f2e8187a890aa", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "6"}, "stats": {"tiktokens": 25, "chars": 94, "lines": 5}, "markdown_heading": {"heading": "Question", "level": 2}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md6"}
{"content": "# Example 1\n\nWhen was Bill Clinton born?\n## Rewritten question\nWhat NLP tasks can BERT perform well?\n\nNow comes the actual work - please respond with the rewritten question in the same language as the question, nothing else.", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md7", "chunk_hash": "7527d2f1eec64ac56661ecd4027062f9efe9ec367bf65972af8d15c485c66466", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "7"}, "stats": {"tiktokens": 49, "chars": 224, "lines": 7}, "markdown_heading": {"heading": "Rewritten question", "level": 2}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md7"}
{"content": "# Example 1\n\nWhen was Bill Clinton born?\n## Previous conversation\n{% for item in history %}\n{{item[\"role\"]}}: {{item[\"content\"]}}\n{% endfor %}", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md8", "chunk_hash": "eb8d055f36392caf563360b7f2844689fd05a5a58855628cfa9d36a85f0b21a3", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "8"}, "stats": {"tiktokens": 39, "chars": 142, "lines": 7}, "markdown_heading": {"heading": "Previous conversation", "level": 2}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md8"}
{"content": "# Example 1\n\nWhen was Bill Clinton born?\n## Question\n{{question}}", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md9", "chunk_hash": "d8efe43628e5fa8dfe6f4369e27fd8537747e913b78bd72315d42abe2af7906f", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "9"}, "stats": {"tiktokens": 17, "chars": 65, "lines": 5}, "markdown_heading": {"heading": "Question", "level": 2}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md9"}
{"content": "# Example 1\n\nWhen was Bill Clinton born?\n## Rewritten question\n", "metadata": {"doc_id": "flows/chat-with-index/src/rewrite_question_prompt.md10", "chunk_hash": "6d832f0499031b1351c5a6352463e93742f33fb24446c0200aeeec0d3b443d0f", "source": {"title": "Example 1", "filename": "flows/chat-with-index/src/rewrite_question_prompt.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/rewrite_question_prompt.md", "mtime": 1694042161.367573, "chunk_id": "10"}, "stats": {"tiktokens": 16, "chars": 63, "lines": 4}, "markdown_heading": {"heading": "Rewritten question", "level": 2}}, "document_id": "flows/chat-with-index/src/rewrite_question_prompt.md10"}
{"content": "Title: flows/chat-with-index/src/main.py\n\nimport argparse\nfrom dotenv import load_dotenv\nimport os\n\nfrom qna import qna\nfrom find_context import find_context\nfrom rewrite_question import rewrite_question\n\n# from build_index import create_faiss_index\n# from utils.lock import acquire_lock", "metadata": {"doc_id": "flows/chat-with-index/src/main.py0", "chunk_hash": "24eb2006f95768ea38ec7ef29de5809bb4a521b0886e59cd1b93d72bcee52f9e", "source": {"title": "flows/chat-with-index/src/main.py", "filename": "flows/chat-with-index/src/main.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/main.py", "mtime": 1694056741.4908533, "chunk_id": "0"}, "stats": {"tiktokens": 61, "chars": 287, "lines": 12}}, "document_id": "flows/chat-with-index/src/main.py0"}
{"content": "Title: flows/chat-with-index/src/main.py\n\ndef chat_with_index(question: str, mlindex_uri: str, history: list):\n    # with acquire_lock(\"create_folder.lock\"):\n    #     if not os.path.exists(\".mlindex\"):\n    #         os.makedirs(\".mlindex\")\n\n    # index_path = create_faiss_index(pdf_path)\n    q = rewrite_question(question, history)\n    prompt, context = find_context(q, mlindex_uri)\n    stream = qna(prompt, history)\n\n    return stream, context", "metadata": {"doc_id": "flows/chat-with-index/src/main.py1", "chunk_hash": "4b78d9d4a51e9fe23da43fd22c796627aa1d81a81efd38412d7a2f136183969d", "source": {"title": "flows/chat-with-index/src/main.py", "filename": "flows/chat-with-index/src/main.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/main.py", "mtime": 1694056741.4908533, "chunk_id": "1"}, "stats": {"tiktokens": 107, "chars": 446, "lines": 13}}, "document_id": "flows/chat-with-index/src/main.py1"}
{"content": "Title: flows/chat-with-index/src/main.py\n\ndef print_stream_and_return_full_answer(stream):\n    answer = \"\"\n    for str in stream:\n        print(str, end=\"\", flush=True)\n        answer = answer + str + \"\"\n    print(flush=True)\n\n    return answer", "metadata": {"doc_id": "flows/chat-with-index/src/main.py2", "chunk_hash": "036cd7b3717b48d59d2de6db902945d611f90daf7dd9b942986550f646966427", "source": {"title": "flows/chat-with-index/src/main.py", "filename": "flows/chat-with-index/src/main.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/main.py", "mtime": 1694056741.4908533, "chunk_id": "2"}, "stats": {"tiktokens": 55, "chars": 244, "lines": 10}}, "document_id": "flows/chat-with-index/src/main.py2"}
{"content": "Title: flows/chat-with-index/src/main.py\n\ndef main_loop(mlindex_uri: str):\n    load_dotenv(os.path.join(os.path.dirname(__file__), \"..\", \".env\"))\n\n    history = []\n    while True:\n        question = input(\"\\033[92m\" + \"$User (type q! to quit): \" + \"\\033[0m\")\n        if question == \"q!\":\n            break\n\n        stream, context = chat_with_index(question, mlindex_uri, history)", "metadata": {"doc_id": "flows/chat-with-index/src/main.py3", "chunk_hash": "8aeb8ca4e4f343914b356a7290425e97c2cf5cda7ac58ac0d64d4fd9b3e7526e", "source": {"title": "flows/chat-with-index/src/main.py", "filename": "flows/chat-with-index/src/main.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/main.py", "mtime": 1694056741.4908533, "chunk_id": "3"}, "stats": {"tiktokens": 100, "chars": 380, "lines": 12}}, "document_id": "flows/chat-with-index/src/main.py3"}
{"content": "Title: flows/chat-with-index/src/main.py\n\nprint(\"\\033[92m\" + \"$Bot: \" + \"\\033[0m\", end=\" \", flush=True)\n        answer = print_stream_and_return_full_answer(stream)\n        history = history + [\n            {\"role\": \"user\", \"content\": question},\n            {\"role\": \"assistant\", \"content\": answer},\n        ]", "metadata": {"doc_id": "flows/chat-with-index/src/main.py4", "chunk_hash": "c3fe0a4163411e7e471520ffeaff48794fdae1620334c651d6a1bdb62297aa7a", "source": {"title": "flows/chat-with-index/src/main.py", "filename": "flows/chat-with-index/src/main.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/main.py", "mtime": 1694056741.4908533, "chunk_id": "4"}, "stats": {"tiktokens": 78, "chars": 309, "lines": 8}}, "document_id": "flows/chat-with-index/src/main.py4"}
{"content": "Title: flows/chat-with-index/src/main.py\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Ask questions about the contents of an MLIndex.\"\n    )\n    parser.add_argument(\"mlindex_uri\", help=\"URI to MLIndex\")\n    args = parser.parse_args()\n\n    main_loop(args.mlindex_uri)\n\n\nif __name__ == \"__main__\":\n    main()", "metadata": {"doc_id": "flows/chat-with-index/src/main.py5", "chunk_hash": "51da9fa9e8b5f84c0a47eefc8c1262b06223fb9ac8cb14840d673d9149ee7887", "source": {"title": "flows/chat-with-index/src/main.py", "filename": "flows/chat-with-index/src/main.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/main.py", "mtime": 1694056741.4908533, "chunk_id": "5"}, "stats": {"tiktokens": 76, "chars": 333, "lines": 14}}, "document_id": "flows/chat-with-index/src/main.py5"}
{"content": "Title: flows/chat-with-index/src/find_context.py\n\nfrom jinja2 import Environment, FileSystemLoader\nimport os\n\nfrom utils.oai import render_with_token_limit\nfrom utils.logging import log\n\nfrom azureml.rag.mlindex import MLIndex", "metadata": {"doc_id": "flows/chat-with-index/src/find_context.py0", "chunk_hash": "9afc70aaa06e0d089ec51768e9b5ea6abc06ad0d4b51790040ed1616be478a4f", "source": {"title": "flows/chat-with-index/src/find_context.py", "filename": "flows/chat-with-index/src/find_context.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/find_context.py", "mtime": 1694042161.3444278, "chunk_id": "0"}, "stats": {"tiktokens": 50, "chars": 226, "lines": 9}}, "document_id": "flows/chat-with-index/src/find_context.py0"}
{"content": "Title: flows/chat-with-index/src/find_context.py\n\ndef find_context(question: str, index_path: str):\n    mlindex = MLIndex(index_path)\n    index = mlindex.as_native_index_client()\n    snippets = index.similarity_search(question, k=5)\n\n    template = Environment(\n        loader=FileSystemLoader(os.path.dirname(os.path.abspath(__file__)))\n    ).get_template(\"qna_prompt.md\")\n    token_limit = int(os.environ.get(\"PROMPT_TOKEN_LIMIT\"))", "metadata": {"doc_id": "flows/chat-with-index/src/find_context.py1", "chunk_hash": "4638af241821a4b19f608dc427805e0fdaa352484a8eef0c8d9866ef65a8ec08", "source": {"title": "flows/chat-with-index/src/find_context.py", "filename": "flows/chat-with-index/src/find_context.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/find_context.py", "mtime": 1694042161.3444278, "chunk_id": "1"}, "stats": {"tiktokens": 100, "chars": 433, "lines": 11}}, "document_id": "flows/chat-with-index/src/find_context.py1"}
{"content": "Title: flows/chat-with-index/src/find_context.py\n\n# Try to render the template with token limit and reduce snippet count if it fails\n    while True:\n        try:\n            prompt = render_with_token_limit(\n                template, token_limit, question=question, context=enumerate(snippets)\n            )\n            break\n        except ValueError:\n            snippets = snippets[:-1]\n            log(f\"Reducing snippet count to {len(snippets)} to fit token limit\")", "metadata": {"doc_id": "flows/chat-with-index/src/find_context.py2", "chunk_hash": "17837680a4de04b712cb638fe1390dbfa2c1037b11464695ce2aac5ae0e2f307", "source": {"title": "flows/chat-with-index/src/find_context.py", "filename": "flows/chat-with-index/src/find_context.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/find_context.py", "mtime": 1694042161.3444278, "chunk_id": "2"}, "stats": {"tiktokens": 94, "chars": 470, "lines": 12}}, "document_id": "flows/chat-with-index/src/find_context.py2"}
{"content": "Title: flows/chat-with-index/src/find_context.py\n\nreturn prompt, snippets", "metadata": {"doc_id": "flows/chat-with-index/src/find_context.py3", "chunk_hash": "244232cf94ae1e2e60e6040d15ad7e8a975fc7a7cf31706b270c16b5a7cdc1f0", "source": {"title": "flows/chat-with-index/src/find_context.py", "filename": "flows/chat-with-index/src/find_context.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/find_context.py", "mtime": 1694042161.3444278, "chunk_id": "3"}, "stats": {"tiktokens": 15, "chars": 73, "lines": 3}}, "document_id": "flows/chat-with-index/src/find_context.py3"}
{"content": "# pytest cache directory #\n\n# pytest cache directory #\nThis directory contains data from the pytest's cache plugin,\nwhich provides the `--lf` and `--ff` options, as well as the `cache` fixture.\n\n**Do not** commit this to version control.\n\nSee [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.", "metadata": {"doc_id": "flows/chat-with-index/src/.pytest_cache/README.md0", "chunk_hash": "71388670910ae2de8225ba0b3a3d8014172fed30170e060d2e9114527b9a13fc", "source": {"title": "pytest cache directory #", "filename": "flows/chat-with-index/src/.pytest_cache/README.md", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/.pytest_cache/README.md", "mtime": 1694110396.3870788, "chunk_id": "0"}, "stats": {"tiktokens": 77, "chars": 328, "lines": 9}, "markdown_heading": {"heading": "pytest cache directory", "level": 1}}, "document_id": "flows/chat-with-index/src/.pytest_cache/README.md0"}
{"content": "Title: flows/chat-with-index/src/utils/logging.py\n\nimport os\n\n\ndef log(message: str):\n    verbose = os.environ.get(\"VERBOSE\")\n    if verbose.lower() == \"true\":\n        print(message, flush=True)", "metadata": {"doc_id": "flows/chat-with-index/src/utils/logging.py0", "chunk_hash": "9e6a5b3248a97ccad123700653f3e9173d67271599311067d73a858ef5e74697", "source": {"title": "flows/chat-with-index/src/utils/logging.py", "filename": "flows/chat-with-index/src/utils/logging.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/logging.py", "mtime": 1694042161.3776937, "chunk_id": "0"}, "stats": {"tiktokens": 45, "chars": 194, "lines": 9}}, "document_id": "flows/chat-with-index/src/utils/logging.py0"}
{"content": "Title: flows/chat-with-index/src/utils/__init__.py\n\n__path__ = __import__(\"pkgutil\").extend_path(__path__, __name__)  # type: ignore", "metadata": {"doc_id": "flows/chat-with-index/src/utils/__init__.py0", "chunk_hash": "de5b398b93140fd3e83d2fe452ce7d55533fd023f6f18429fce12d1e1460f12c", "source": {"title": "flows/chat-with-index/src/utils/__init__.py", "filename": "flows/chat-with-index/src/utils/__init__.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/__init__.py", "mtime": 1694042161.374285, "chunk_id": "0"}, "stats": {"tiktokens": 36, "chars": 132, "lines": 3}}, "document_id": "flows/chat-with-index/src/utils/__init__.py0"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nfrom typing import List\nimport openai\nimport os\nimport tiktoken\nfrom jinja2 import Template\n\nfrom .retry import (\n    retry_and_handle_exceptions,\n    retry_and_handle_exceptions_for_generator,\n)\nfrom .logging import log", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py0", "chunk_hash": "4f1a405b52188a4f99e10ad9851035a2261c08814ea595cf4faeb7e52bfea1af", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "0"}, "stats": {"tiktokens": 60, "chars": 267, "lines": 13}}, "document_id": "flows/chat-with-index/src/utils/oai.py0"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\ndef extract_delay_from_rate_limit_error_msg(text):\n    import re\n\n    pattern = r\"retry after (\\d+)\"\n    match = re.search(pattern, text)\n    if match:\n        retry_time_from_message = match.group(1)\n        return float(retry_time_from_message)\n    else:\n        return 5  # default retry time", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py1", "chunk_hash": "5e738a2f2b137c8ee3145521d199536af9bd2c617fd0577ed332cf1962775154", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "1"}, "stats": {"tiktokens": 82, "chars": 342, "lines": 12}}, "document_id": "flows/chat-with-index/src/utils/oai.py1"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nclass OAI:\n    def __init__(self):\n        if os.getenv(\"OPENAI_API_TYPE\") is not None:\n            openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n        if os.getenv(\"OPENAI_API_BASE\") is not None:\n            openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n        if os.getenv(\"OPENAI_API_VERSION\") is not None:", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py2", "chunk_hash": "6817c43d7d047ee8cb7fe816872133cc1d29b4d2f642b6ad7dbdc5d575ee7cdd", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "2"}, "stats": {"tiktokens": 94, "chars": 366, "lines": 9}}, "document_id": "flows/chat-with-index/src/utils/oai.py2"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nopenai.api_version = os.environ.get(\"OPENAI_API_VERSION\")\n        if os.getenv(\"OPENAI_ORG_ID\") is not None:\n            openai.organization = os.environ.get(\"OPENAI_ORG_ID\")\n        if os.getenv(\"OPENAI_API_KEY\") is None:\n            raise ValueError(\"OPENAI_API_KEY is not set in environment variables\")", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py3", "chunk_hash": "92fd08fd4538c74a763e472b497a80bdb3440eee61692eded63fd8cef241e043", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "3"}, "stats": {"tiktokens": 84, "chars": 352, "lines": 7}}, "document_id": "flows/chat-with-index/src/utils/oai.py3"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py4", "chunk_hash": "eb576dd02264181f92fb781eb8ea68f3c23ad17103af790949dc82b877cb7fa3", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "4"}, "stats": {"tiktokens": 26, "chars": 96, "lines": 3}}, "document_id": "flows/chat-with-index/src/utils/oai.py4"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\n# A few sanity checks\n        if openai.api_type == \"azure\" and openai.api_base is None:\n            raise ValueError(\n                \"OPENAI_API_BASE is not set in environment variables, this is required when api_type==azure\"\n            )\n        if openai.api_type == \"azure\" and openai.api_version is None:\n            raise ValueError(", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py5", "chunk_hash": "11c54a2044cbab0fa8fbe9a6675089c4f40a86735c8e8137df2906b7268c574b", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "5"}, "stats": {"tiktokens": 86, "chars": 388, "lines": 9}}, "document_id": "flows/chat-with-index/src/utils/oai.py5"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\n\"OPENAI_API_VERSION is not set in environment variables, this is required when api_type==azure\"\n            )\n        if openai.api_type == \"azure\" and openai.api_key.startswith(\"sk-\"):\n            raise ValueError(\n                \"OPENAI_API_KEY should not start with sk- when api_type==azure, are you using openai key by mistake?\"\n            )", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py6", "chunk_hash": "e9c7ff6fd5c2a74bd65d25ecc2abc68c5f2d887f3ac645b76a6e08d367fb0462", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "6"}, "stats": {"tiktokens": 88, "chars": 394, "lines": 8}}, "document_id": "flows/chat-with-index/src/utils/oai.py6"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nclass OAIChat(OAI):\n    @retry_and_handle_exceptions(\n        exception_to_check=(\n            openai.error.RateLimitError,\n            openai.error.APIError,\n            KeyError,\n        ),\n        max_retries=5,\n        extract_delay_from_error_message=extract_delay_from_rate_limit_error_msg,\n    )\n    def generate(self, messages: list, **kwargs) -> List[float]:", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py7", "chunk_hash": "d16486bd7ce04cd99342f9751d5513bfd3f2e5ac286f9ae3676dda2098bbb6f6", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "7"}, "stats": {"tiktokens": 92, "chars": 414, "lines": 13}}, "document_id": "flows/chat-with-index/src/utils/oai.py7"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nif openai.api_type == \"azure\":\n            return openai.ChatCompletion.create(\n                engine=os.environ.get(\"CHAT_MODEL_DEPLOYMENT_NAME\"),\n                messages=messages,\n                **kwargs,\n            )[\"choices\"][0][\"message\"][\"content\"]\n        else:\n            return openai.ChatCompletion.create(", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py8", "chunk_hash": "62aa503ca30b20c7c64804a4333f27d205c44285d5df54b7208edb18720eb7cc", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "8"}, "stats": {"tiktokens": 73, "chars": 369, "lines": 10}}, "document_id": "flows/chat-with-index/src/utils/oai.py8"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nmodel=os.environ.get(\"CHAT_MODEL_DEPLOYMENT_NAME\"),\n                messages=messages,\n                **kwargs,\n            )[\"choices\"][0][\"message\"][\"content\"]", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py9", "chunk_hash": "69d621a365e9007d01a024e6d49e6f80996a77821f68e6b66f49fa7b08ee3db7", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "9"}, "stats": {"tiktokens": 44, "chars": 209, "lines": 6}}, "document_id": "flows/chat-with-index/src/utils/oai.py9"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\n@retry_and_handle_exceptions_for_generator(\n        exception_to_check=(\n            openai.error.RateLimitError,\n            openai.error.APIError,\n            KeyError,\n        ),\n        max_retries=5,\n        extract_delay_from_error_message=extract_delay_from_rate_limit_error_msg,\n    )\n    def stream(self, messages: list, **kwargs):", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py10", "chunk_hash": "be3caf2d858469f4615ca28ce758925e6e11d5d81887cbbb8af857021a554fca", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "10"}, "stats": {"tiktokens": 82, "chars": 387, "lines": 12}}, "document_id": "flows/chat-with-index/src/utils/oai.py10"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nif openai.api_type == \"azure\":\n            response = openai.ChatCompletion.create(\n                engine=os.environ.get(\"CHAT_MODEL_DEPLOYMENT_NAME\"),\n                messages=messages,\n                stream=True,\n                **kwargs,\n            )\n        else:\n            response = openai.ChatCompletion.create(", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py11", "chunk_hash": "87945d09451ecfc7ccaff110bc0465567ae33588b10f0e39dd4a0ec5837d41fa", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "11"}, "stats": {"tiktokens": 70, "chars": 370, "lines": 11}}, "document_id": "flows/chat-with-index/src/utils/oai.py11"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nmodel=os.environ.get(\"CHAT_MODEL_DEPLOYMENT_NAME\"),\n                messages=messages,\n                stream=True,\n                **kwargs,\n            )", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py12", "chunk_hash": "7f7a11314372f9084f141a765031e8bc6af8475e771ab13908f13e0f58f3b47e", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "12"}, "stats": {"tiktokens": 39, "chars": 202, "lines": 7}}, "document_id": "flows/chat-with-index/src/utils/oai.py12"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nfor chunk in response:\n            if \"choices\" not in chunk or len(chunk[\"choices\"]) == 0:\n                continue\n            delta = chunk[\"choices\"][0][\"delta\"]\n            if \"content\" in delta:\n                yield delta[\"content\"]", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py13", "chunk_hash": "3c2df6bee8a8ed6c292de6c4d827dbc2ef3fb2702c1e92ccd322f55574ccc9a5", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "13"}, "stats": {"tiktokens": 63, "chars": 286, "lines": 8}}, "document_id": "flows/chat-with-index/src/utils/oai.py13"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\nclass OAIEmbedding(OAI):\n    @retry_and_handle_exceptions(\n        exception_to_check=openai.error.RateLimitError,\n        max_retries=5,\n        extract_delay_from_error_message=extract_delay_from_rate_limit_error_msg,\n    )\n    def generate(self, text: str) -> List[float]:\n        if openai.api_type == \"azure\":\n            return openai.Embedding.create(", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py14", "chunk_hash": "b3ec3abcafcd62ebeae65e642f615ac221e3e427fd3827dfc282819e0143772b", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "14"}, "stats": {"tiktokens": 94, "chars": 405, "lines": 11}}, "document_id": "flows/chat-with-index/src/utils/oai.py14"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\ninput=text, engine=os.environ.get(\"EMBEDDING_MODEL_DEPLOYMENT_NAME\")\n            )[\"data\"][0][\"embedding\"]\n        else:\n            return openai.Embedding.create(\n                input=text, model=os.environ.get(\"EMBEDDING_MODEL_DEPLOYMENT_NAME\")\n            )[\"data\"][0][\"embedding\"]", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py15", "chunk_hash": "b4ad6d3eac341ed4e5f9e15b0bca8d83f65fd570c2a3223f31732d34ca799db6", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "15"}, "stats": {"tiktokens": 78, "chars": 333, "lines": 8}}, "document_id": "flows/chat-with-index/src/utils/oai.py15"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\ndef count_token(text: str) -> int:\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoding.encode(text))", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py16", "chunk_hash": "cc4be3141a048877348707b19d93dadc64c3b23da465117f476ef6a5d1ea9c9e", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "16"}, "stats": {"tiktokens": 42, "chars": 171, "lines": 5}}, "document_id": "flows/chat-with-index/src/utils/oai.py16"}
{"content": "Title: flows/chat-with-index/src/utils/oai.py\n\ndef render_with_token_limit(template: Template, token_limit: int, **kwargs) -> str:\n    text = template.render(**kwargs)\n    token_count = count_token(text)\n    if token_count > token_limit:\n        message = f\"token count {token_count} exceeds limit {token_limit}\"\n        log(message)\n        raise ValueError(message)\n    return text\n\n\nif __name__ == \"__main__\":\n    print(count_token(\"hello world\"))", "metadata": {"doc_id": "flows/chat-with-index/src/utils/oai.py17", "chunk_hash": "34578aae26830198b1f52edffaf844715bad537d73f0374c43883eb72af4331f", "source": {"title": "flows/chat-with-index/src/utils/oai.py", "filename": "flows/chat-with-index/src/utils/oai.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/oai.py", "mtime": 1694042161.38502, "chunk_id": "17"}, "stats": {"tiktokens": 102, "chars": 450, "lines": 14}}, "document_id": "flows/chat-with-index/src/utils/oai.py17"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\nfrom typing import Tuple, Union, Optional\nimport functools\nimport time\nimport random", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py0", "chunk_hash": "c1cd97e85f51763b9a32cc1508ba97e171e5948d7625e6b8815d49e3d2562923", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "0"}, "stats": {"tiktokens": 29, "chars": 133, "lines": 6}}, "document_id": "flows/chat-with-index/src/utils/retry.py0"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\ndef retry_and_handle_exceptions(\n    exception_to_check: Union[Exception, Tuple[Exception]],\n    max_retries: int = 3,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = False,\n    extract_delay_from_error_message: Optional[any] = None,\n):\n    def deco_retry(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            delay = initial_delay", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py1", "chunk_hash": "a3357b40d623a890e2cb71aa276b08166bd056ad633a25e51db6886ff198e62d", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "1"}, "stats": {"tiktokens": 110, "chars": 448, "lines": 14}}, "document_id": "flows/chat-with-index/src/utils/retry.py1"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\nfor i in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except exception_to_check as e:\n                    if i == max_retries - 1:\n                        raise Exception(", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py2", "chunk_hash": "f1bc187301cebc1aefcb8ef3850cb4314f213e52e5532ef833734836fada5620", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "2"}, "stats": {"tiktokens": 55, "chars": 281, "lines": 8}}, "document_id": "flows/chat-with-index/src/utils/retry.py2"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\n\"Func execution failed after {0} retries: {1}\".format(\n                                max_retries, e\n                            )\n                        )\n                    delay *= exponential_base * (1 + jitter * random.random())", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py3", "chunk_hash": "a80250982e794c839a341e1e78fa3957d47126d13eaf8aa7593e3adb267824b0", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "3"}, "stats": {"tiktokens": 52, "chars": 285, "lines": 7}}, "document_id": "flows/chat-with-index/src/utils/retry.py3"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\ndelay_from_error_message = None\n                    if extract_delay_from_error_message is not None:\n                        delay_from_error_message = extract_delay_from_error_message(\n                            str(e)\n                        )\n                    final_delay = (", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py4", "chunk_hash": "e99d0aae1e7774185eeb21b70a7bcee540638ce5748b00895a8b4357e386dc93", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "4"}, "stats": {"tiktokens": 53, "chars": 331, "lines": 8}}, "document_id": "flows/chat-with-index/src/utils/retry.py4"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\ndelay_from_error_message if delay_from_error_message else delay\n                    )\n                    print(\n                        \"Func execution failed. Retrying in {0} seconds: {1}\".format(\n                            final_delay, e\n                        )", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py5", "chunk_hash": "9e26034b45a80b90049beb9e78d178162d11e47823e20d1dc15f71a2cc8eb2e3", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "5"}, "stats": {"tiktokens": 56, "chars": 316, "lines": 8}}, "document_id": "flows/chat-with-index/src/utils/retry.py5"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\n)\n                    time.sleep(final_delay)", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py6", "chunk_hash": "9bb727f565ece186f00a2399d0f4fccd552e09d38c8bc11f1a81e5af70db1de5", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "6"}, "stats": {"tiktokens": 19, "chars": 94, "lines": 4}}, "document_id": "flows/chat-with-index/src/utils/retry.py6"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\nreturn wrapper\n\n    return deco_retry", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py7", "chunk_hash": "ea6adffbca29bf1c54ff764ea6e747dbdcb001b523f09b278b2f17494f8447c5", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "7"}, "stats": {"tiktokens": 19, "chars": 86, "lines": 5}}, "document_id": "flows/chat-with-index/src/utils/retry.py7"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\ndef retry_and_handle_exceptions_for_generator(\n    exception_to_check: Union[Exception, Tuple[Exception]],\n    max_retries: int = 3,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = False,\n    extract_delay_from_error_message: Optional[any] = None,\n):\n    def deco_retry(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            delay = initial_delay", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py8", "chunk_hash": "3d31fc9b736a8d6cf0bdb9458878802e526b1772e0ecdd7e65899bd2448c2907", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "8"}, "stats": {"tiktokens": 112, "chars": 462, "lines": 14}}, "document_id": "flows/chat-with-index/src/utils/retry.py8"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\nfor i in range(max_retries):\n                try:\n                    for value in func(*args, **kwargs):\n                        yield value\n                    break\n                except exception_to_check as e:\n                    if i == max_retries - 1:", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py9", "chunk_hash": "1bb14ee79cceb857f0f2f74d85647f4999e8e1349e439cb7d3b230604e65d29e", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "9"}, "stats": {"tiktokens": 60, "chars": 309, "lines": 9}}, "document_id": "flows/chat-with-index/src/utils/retry.py9"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\nraise Exception(\n                            \"Func execution failed after {0} retries: {1}\".format(\n                                max_retries, e\n                            )\n                        )", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py10", "chunk_hash": "b48c5b261142e94d8308a5abdb5bb72a6cbcc70af3f3e233762650fe2153c3c2", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "10"}, "stats": {"tiktokens": 42, "chars": 251, "lines": 7}}, "document_id": "flows/chat-with-index/src/utils/retry.py10"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\ndelay *= exponential_base * (1 + jitter * random.random())\n                    delay_from_error_message = None\n                    if extract_delay_from_error_message is not None:\n                        delay_from_error_message = extract_delay_from_error_message(\n                            str(e)", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py11", "chunk_hash": "24f47c22bd8512066a67db04d67e5d56c28d75df3e43a813c6628d6d45a6e2ff", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "11"}, "stats": {"tiktokens": 60, "chars": 348, "lines": 7}}, "document_id": "flows/chat-with-index/src/utils/retry.py11"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\n)\n                    final_delay = (\n                        delay_from_error_message if delay_from_error_message else delay\n                    )\n                    print(\n                        \"Func execution failed. Retrying in {0} seconds: {1}\".format(", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py12", "chunk_hash": "10c346f3b519e9469220c6f689c65efd7ef2607fb374e471bdf5d25d29330478", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "12"}, "stats": {"tiktokens": 55, "chars": 309, "lines": 8}}, "document_id": "flows/chat-with-index/src/utils/retry.py12"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\nfinal_delay, e\n                        )\n                    )\n                    time.sleep(final_delay)", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py13", "chunk_hash": "383bf70a6827cd8a8cda88233e8301ad0586cdf73ca237aacdc70f0d048024fe", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "13"}, "stats": {"tiktokens": 27, "chars": 155, "lines": 6}}, "document_id": "flows/chat-with-index/src/utils/retry.py13"}
{"content": "Title: flows/chat-with-index/src/utils/retry.py\n\nreturn wrapper\n\n    return deco_retry", "metadata": {"doc_id": "flows/chat-with-index/src/utils/retry.py14", "chunk_hash": "ea6adffbca29bf1c54ff764ea6e747dbdcb001b523f09b278b2f17494f8447c5", "source": {"title": "flows/chat-with-index/src/utils/retry.py", "filename": "flows/chat-with-index/src/utils/retry.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/chat-with-index/src/utils/retry.py", "mtime": 1694042161.3869936, "chunk_id": "14"}, "stats": {"tiktokens": 19, "chars": 86, "lines": 5}}, "document_id": "flows/chat-with-index/src/utils/retry.py14"}
{"content": "Title: flows/bring_your_own_data_chat_qna/generate_prompt_context.py\n\nfrom typing import List\nfrom promptflow import tool\nfrom promptflow_vectordb.core.contracts import SearchResultEntity\n\n\n@tool", "metadata": {"doc_id": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py0", "chunk_hash": "eaac72cb73668ac670e9b0a82b9dda44f7805988e550149db07d14bf136b4837", "source": {"title": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "filename": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "mtime": 1694056741.4723716, "chunk_id": "0"}, "stats": {"tiktokens": 44, "chars": 195, "lines": 8}}, "document_id": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py0"}
{"content": "Title: flows/bring_your_own_data_chat_qna/generate_prompt_context.py\n\ndef generate_prompt_context(search_result: List[dict]) -> str:\n    def format_doc(doc: dict):\n        return f\"Content: {doc['Content']}\\nSource: {doc['Source']}\"\n\n    SOURCE_KEY = \"source\"\n    URL_KEY = \"url\"\n\n    retrieved_docs = []\n    for item in search_result:\n        entity = SearchResultEntity.from_dict(item)\n        content = entity.text or \"\"", "metadata": {"doc_id": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py1", "chunk_hash": "26aa6e2b3ad873046d916a1e4effabbe962e67a112e2b0a1ed56f177e953eeb6", "source": {"title": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "filename": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "mtime": 1694056741.4723716, "chunk_id": "1"}, "stats": {"tiktokens": 103, "chars": 423, "lines": 13}}, "document_id": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py1"}
{"content": "Title: flows/bring_your_own_data_chat_qna/generate_prompt_context.py\n\nsource = \"\"\n        if entity.metadata is not None:\n            if SOURCE_KEY in entity.metadata:\n                if URL_KEY in entity.metadata[SOURCE_KEY]:\n                    source = entity.metadata[SOURCE_KEY][URL_KEY] or \"\"\n\n        retrieved_docs.append({\"Content\": content, \"Source\": source})\n    doc_string = \"\\n\\n\".join([format_doc(doc) for doc in retrieved_docs])\n    return doc_string", "metadata": {"doc_id": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py2", "chunk_hash": "dae2d4a2916e7539aa50630d085a015fdd74ce95b1b7c21915c2b85605f7d33b", "source": {"title": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "filename": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/bring_your_own_data_chat_qna/generate_prompt_context.py", "mtime": 1694056741.4723716, "chunk_id": "2"}, "stats": {"tiktokens": 100, "chars": 465, "lines": 11}}, "document_id": "flows/bring_your_own_data_chat_qna/generate_prompt_context.py2"}
{"content": "Title: promptflow[azure]\n\npromptflow[azure]\npromptflow-tools\nazureml-rag[faiss]\nazure-ai-ml", "metadata": {"doc_id": "flows/bring_your_own_data_chat_qna/requirements.txt0", "chunk_hash": "97a3ed3688f51610f7c0653f03bc9136a0b954a63c3b7a1fc4ed0cfd2256dea0", "source": {"title": "promptflow[azure]", "filename": "flows/bring_your_own_data_chat_qna/requirements.txt", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/flows/bring_your_own_data_chat_qna/requirements.txt", "mtime": 1694042161.31225, "chunk_id": "0"}, "stats": {"tiktokens": 29, "chars": 91, "lines": 6}}, "document_id": "flows/bring_your_own_data_chat_qna/requirements.txt0"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\n# %%[markdown]\n# # S3 via OneLake to Azure Cognitive Search Index\n\n# %% Prerequisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[cognitive_search]>=0.2.0'\n\n# %% Authenticate to an AzureML Workspace, you can download a `config.json` from the top-right-hand corner menu of a Workspace.\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py0", "chunk_hash": "9a184294d9022ad51e1c72102f28a9c8379454f06bce7f5c422d3801c1594c3f", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "0"}, "stats": {"tiktokens": 149, "chars": 561, "lines": 12}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py0"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\nml_client = MLClient.from_config(\n    credential=DefaultAzureCredential(), path=\"config.json\"\n)\n\n# %% Create DataIndex configuration\nfrom azureml.rag.dataindex.entities import (\n    Data,\n    DataIndex,\n    IndexSource,\n    Embedding,\n    IndexStore,\n)\n\nasset_name = \"s3_aoai_acs\"", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py1", "chunk_hash": "d61269ced672808e86dd302853d734df79225f3eb60a913a1a9cdd15944efd4c", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "1"}, "stats": {"tiktokens": 85, "chars": 334, "lines": 16}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py1"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\ndata_index = DataIndex(\n    name=asset_name,\n    description=\"S3 data embedded with text-embedding-ada-002 and indexed in Azure Cognitive Search.\",\n    source=IndexSource(\n        input_data=Data(\n            type=\"uri_folder\",\n            path=\"<your path to onelake>\",\n        ),\n        citation_url=\"s3://lupickup-test\",\n    ),\n    embedding=Embedding(\n        model=\"text-embedding-ada-002\",", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py2", "chunk_hash": "8d3b6f089021e861a64196dad2a95a5da0d752d6417ba5150922a5d5b85d6ebc", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "2"}, "stats": {"tiktokens": 111, "chars": 450, "lines": 14}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py2"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\nconnection=\"azureml-rag-oai\",\n        cache_path=f\"azureml://datastores/workspaceblobstore/paths/embeddings_cache/{asset_name}\",\n    ),\n    index=IndexStore(\n        type=\"acs\",\n        connection=\"azureml-rag-acs\",\n    ),\n    # name is replaced with a unique value each time the job is run\n    path=f\"azureml://datastores/workspaceblobstore/paths/indexes/{asset_name}/{{name}}\",\n)", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py3", "chunk_hash": "6f4d385edefc3654367f1170b78fa9865b8dc8cc4d9b77f34b5fbb3c403c44a5", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "3"}, "stats": {"tiktokens": 112, "chars": 435, "lines": 12}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py3"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\n# %% Create the DataIndex Job to be scheduled\nfrom azure.ai.ml import UserIdentityConfiguration\n\nindex_job = ml_client.data.index_data(\n    data_index=data_index,\n    # The DataIndex Job will use the identity of the MLClient within the DataIndex Job to access source data.\n    identity=UserIdentityConfiguration(),\n    # Instead of submitting the Job and returning the Run a PipelineJob configuration is returned which can be used in with a Schedule.\n    submit_job=False,\n)", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py4", "chunk_hash": "762699ddb42965aeed922d651bb4cddcc5ac04df0396ac93e62da43289a15966", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "4"}, "stats": {"tiktokens": 113, "chars": 528, "lines": 12}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py4"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\n# %% Create Schedule for DataIndex Job\nfrom azure.ai.ml.constants import TimeZone\nfrom azure.ai.ml.entities import JobSchedule, RecurrenceTrigger, RecurrencePattern\nfrom datetime import datetime, timedelta\n\nschedule_name = \"onelake_s3_aoai_acs_mlindex_daily\"\n\nschedule_start_time = datetime.utcnow() + timedelta(minutes=1)\nrecurrence_trigger = RecurrenceTrigger(\n    frequency=\"day\",\n    interval=1,\n    # schedule=RecurrencePattern(hours=16, minutes=[15]),\n    start_time=schedule_start_time,\n    time_zone=TimeZone.UTC,\n)", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py5", "chunk_hash": "26e7fb82c8a40b8f69c782fe28a665a0b95baa3db60415d94d3eb105b046824c", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "5"}, "stats": {"tiktokens": 136, "chars": 577, "lines": 17}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py5"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\njob_schedule = JobSchedule(\n    name=schedule_name,\n    trigger=recurrence_trigger,\n    create_job=index_job,\n    properties=index_job.properties,\n)\n\n# %% Enable Schedule\njob_schedule_res = ml_client.schedules.begin_create_or_update(\n    schedule=job_schedule\n).result()\njob_schedule_res", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py6", "chunk_hash": "76bc8f69f54f997d1b5c8695532b434435972d2667da9d8210f3bb0ea6ab9cac", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "6"}, "stats": {"tiktokens": 77, "chars": 341, "lines": 14}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py6"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\n# %% Take a look at the schedule in Workpace Portal\nf\"https://ml.azure.com/schedule/{schedule_name}/details/overview?wsid=/subscriptions/{ml_client.subscription_id}/resourceGroups/{ml_client.resource_group_name}/providers/Microsoft.MachineLearningServices/workspaces/{ml_client.workspace_name}\"\n\n# %% Get the MLIndex Asset\nonelake_s3_index_asset = ml_client.data.get(asset_name, label=\"latest\")\nonelake_s3_index_asset\n\n## %% Try it out with langchain by loading the MLIndex asset using the azureml-rag SDK\nfrom azureml.rag.mlindex import MLIndex", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py7", "chunk_hash": "624875b4f11e44b32fece8b362333a1bdb9cac0d6ecc25430c4965f3116f4a68", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "7"}, "stats": {"tiktokens": 145, "chars": 599, "lines": 11}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py7"}
{"content": "Title: data_index_job/scheduled_s3_to_asc_mlindex.py\n\nmlindex = MLIndex(onelake_s3_index_asset)\n\nindex = mlindex.as_langchain_vectorstore()\ndocs = index.similarity_search(\"What is RAG?\", k=5)\ndocs\n\n# %% Take a look at those chunked docs\nimport json\n\nfor doc in docs:\n    print(json.dumps({\"content\": doc.page_content, **doc.metadata}, indent=2))\n\n# %%", "metadata": {"doc_id": "data_index_job/scheduled_s3_to_asc_mlindex.py8", "chunk_hash": "c4a83e57064a6a13ada6e04b7499c74bb1adcfa7400aa376b4f20b052a23e412", "source": {"title": "data_index_job/scheduled_s3_to_asc_mlindex.py", "filename": "data_index_job/scheduled_s3_to_asc_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/scheduled_s3_to_asc_mlindex.py", "mtime": 1694056741.490887, "chunk_id": "8"}, "stats": {"tiktokens": 96, "chars": 351, "lines": 15}}, "document_id": "data_index_job/scheduled_s3_to_asc_mlindex.py8"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\n# %%[markdown]\n# # Local Documents to Azure Cognitive Search Index\n\n# %% Prerequisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[faiss]>=0.2.0'\n# %pip install 'promptflow[azure]' promptflow-tools promptflow-vectordb", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py0", "chunk_hash": "77494c89495ce710ea7b3d04d82329e4b67b9dc0f7abfd795146de3cc7c03a16", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "0"}, "stats": {"tiktokens": 122, "chars": 413, "lines": 9}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py0"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\n# %% Authenticate to you AzureML Workspace, download a `config.json` from the top right hand corner menu of the Workspace.\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\nml_client = MLClient.from_config(\n    credential=DefaultAzureCredential(), path=\"config.json\"\n)\n\n# %% Create DataIndex configuration\nfrom azureml.rag.dataindex.entities import (\n    Data,\n    DataIndex,\n    IndexSource,\n    CitationRegex,\n    Embedding,\n    IndexStore,\n)\n\nasset_name = \"azure_search_docs_aoai_faiss\"", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py1", "chunk_hash": "80a5e48d3a7632c0efe54c0ee5502dcda08e6ebef7c91d794fb86fb577eca28c", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "1"}, "stats": {"tiktokens": 132, "chars": 579, "lines": 21}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py1"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\ndata_index = DataIndex(\n    name=asset_name,\n    description=\"Azure Cognitive Search docs embedded with text-embedding-ada-002 and indexed in a Faiss Index.\",\n    source=IndexSource(\n        input_data=Data(\n            type=\"uri_folder\",\n            path=\"<This will be replaced later>\",\n        ),\n        input_glob=\"articles/search/**/*\",\n        citation_url=\"https://learn.microsoft.com/en-us/azure\",", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py2", "chunk_hash": "abd73bbc8e749d00edfbaba92faee976f668b1eb8c5ca8ade6481a8a665cfc84", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "2"}, "stats": {"tiktokens": 103, "chars": 462, "lines": 12}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py2"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\n# Remove articles from the final citation url and remove the file extension so url points to hosted docs, not GitHub.\n        citation_url_replacement_regex=CitationRegex(\n            match_pattern=\"(.*)/articles/(.*)(\\\\.[^.]+)$\", replacement_pattern=\"\\\\1/\\\\2\"\n        ),\n    ),\n    embedding=Embedding(\n        model=\"text-embedding-ada-002\",\n        connection=\"azureml-rag-oai\",", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py3", "chunk_hash": "b3a0574326c31317205dcb1249949781f756da3649751bb3082fa8cbf4fc68d5", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "3"}, "stats": {"tiktokens": 105, "chars": 437, "lines": 10}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py3"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\ncache_path=f\"azureml://datastores/workspaceblobstore/paths/embeddings_cache/{asset_name}\",\n    ),\n    index=IndexStore(type=\"faiss\"),\n    # name is replaced with a unique value each time the job is run\n    path=f\"azureml://datastores/workspaceblobstore/paths/indexes/{asset_name}/{{name}}\",\n)", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py4", "chunk_hash": "574dab4ae07d4114f6ee7c3a7cc007ec1bf5a09874fd15d601bdd69ff2381f65", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "4"}, "stats": {"tiktokens": 89, "chars": 348, "lines": 8}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py4"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\n# %% Use git_clone Component to clone Azure Search docs from github\nml_registry = MLClient(credential=ml_client._credential, registry_name=\"azureml\")\n\ngit_clone_component = ml_registry.components.get(\"llm_rag_git_clone\", label=\"latest\")\n\n# %% Clone Git Repo and use as input to index_job\nfrom azure.ai.ml.dsl import pipeline\nfrom azureml.rag.dataindex.data_index import index_data\n\n\n@pipeline(default_compute=\"serverless\")", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py5", "chunk_hash": "535edcc70222d76ad6c86496d09d30792aecb6a2d80b5f8950e9b6311b19448f", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "5"}, "stats": {"tiktokens": 110, "chars": 478, "lines": 13}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py5"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\ndef git_to_faiss(\n    git_url,\n    branch_name=\"\",\n    git_connection_id=\"\",\n):\n    git_clone = git_clone_component(git_repository=git_url, branch_name=branch_name)\n    git_clone.environment_variables[\n        \"AZUREML_WORKSPACE_CONNECTION_ID_GIT\"\n    ] = git_connection_id", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py6", "chunk_hash": "140217de1f0a25fe1dc375987bdb42126868934290c97f11d2abcbb805b8d30e", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "6"}, "stats": {"tiktokens": 81, "chars": 329, "lines": 11}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py6"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\nindex_job = index_data(\n        description=data_index.description,\n        data_index=data_index,\n        input_data_override=git_clone.outputs.output_data,\n        ml_client=ml_client,\n    )\n\n    return index_job.outputs\n\n\n# %%\ngit_index_job = git_to_faiss(\"https://github.com/MicrosoftDocs/azure-docs.git\")\n# Ensure repo cloned each run to get latest, comment out to have first clone reused.\ngit_index_job.settings.force_rerun = True", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py7", "chunk_hash": "14e79cc5bd6c8c85673b7a7bd607cb9ff0b624488d638d9912c3b50462551d1c", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "7"}, "stats": {"tiktokens": 111, "chars": 492, "lines": 16}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py7"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\n# %% Submit the DataIndex Job\ngit_index_run = ml_client.jobs.create_or_update(\n    git_index_job,\n    experiment_name=asset_name,\n)\ngit_index_run\n\n# %% Wait for it to finish\nml_client.jobs.stream(git_index_run.name)\n\n# %% Check the created asset, it is a folder on storage containing an MLIndex yaml file\nmlindex_docs_index_asset = ml_client.data.get(asset_name, label=\"latest\")\nmlindex_docs_index_asset\n\n# %% Try it out with langchain by loading the MLIndex asset using the azureml-rag SDK\nfrom azureml.rag.mlindex import MLIndex", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py8", "chunk_hash": "7c8259f7a73c2b94a08a2ad4f964a4d098bef2043f2a9dd2a6ce8d8438b85b24", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "8"}, "stats": {"tiktokens": 144, "chars": 586, "lines": 18}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py8"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\nmlindex = MLIndex(mlindex_docs_index_asset)\n\nindex = mlindex.as_langchain_vectorstore()\ndocs = index.similarity_search(\"How can I enable Semantic Search on my Index?\", k=5)\ndocs\n\n# %% Take a look at those chunked docs\nimport json\n\nfor doc in docs:\n    print(json.dumps({\"content\": doc.page_content, **doc.metadata}, indent=2))\n\n# %% Try it out with Promptflow\n\nimport promptflow\n\npf = promptflow.PFClient()\n\n# %% List all the available connections\nfor c in pf.connections.list():\n    print(c.name + \" (\" + c.type + \")\")", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py9", "chunk_hash": "d26dd82b38cdf9cd0ae8a16e1725f4d3ca0c46bc1c0c6a669b15068537172d9f", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "9"}, "stats": {"tiktokens": 146, "chars": 575, "lines": 23}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py9"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\n# %% Load index qna flow\nfrom pathlib import Path\n\nflow_path = Path.cwd().parent / \"flows\" / \"bring_your_own_data_chat_qna\"\nmlindex_path = mlindex_docs_index_asset.path\n\n# %% Put MLIndex uri into Vector DB Lookup tool inputs in [bring_your_own_data_chat_qna/flow.dag.yaml](../flows/bring_your_own_data_chat_qna/flow.dag.yaml)\nimport re", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py10", "chunk_hash": "96aef1a857a1781f47b9358a9b030bfbca178d594c0cb72451940fa1582e9d23", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "10"}, "stats": {"tiktokens": 108, "chars": 391, "lines": 10}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py10"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\nwith open(flow_path / \"flow.dag.yaml\", \"r\") as f:\n    flow_yaml = f.read()\n    flow_yaml = re.sub(\n        r\"path: (.*)# Index uri\", f\"path: {mlindex_path} # Index uri\", flow_yaml, re.M\n    )\nwith open(flow_path / \"flow.dag.yaml\", \"w\") as f:\n    f.write(flow_yaml)", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py11", "chunk_hash": "cf0be9f6d5e61aea460ea4aed029b633ffbe0465e03da35244ece05ee83034bf", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "11"}, "stats": {"tiktokens": 99, "chars": 320, "lines": 9}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py11"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\n# %% Run qna flow\noutput = pf.flows.test(\n    flow_path,\n    inputs={\n        \"chat_history\": [],\n        \"chat_input\": \"How recently was Vector Search support added to Azure Cognitive Search?\",\n    },\n)\n\nchat_output = output[\"chat_output\"]\nfor part in chat_output:\n    print(part, end=\"\")\n\n# %% Run qna flow with multiple inputs\ndata_path = Path.cwd().parent / \"flows\" / \"data\" / \"azure_search_docs_questions.jsonl\"", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py12", "chunk_hash": "edd6b397d0a6a9c4732ff4afdfa4338d43db7b988004d96b5af0c12bbd70d983", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "12"}, "stats": {"tiktokens": 118, "chars": 472, "lines": 17}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py12"}
{"content": "Title: data_index_job/cog_search_docs_faiss_mlindex.py\n\ncolumn_mapping = {\n    \"chat_history\": \"${data.chat_history}\",\n    \"chat_input\": \"${data.chat_input}\",\n    \"chat_output\": \"${data.chat_output}\",\n}\nrun = pf.run(flow=flow_path, data=data_path, column_mapping=column_mapping)\npf.stream(run)\n\nprint(f\"{run}\")\n\n\n# %%", "metadata": {"doc_id": "data_index_job/cog_search_docs_faiss_mlindex.py13", "chunk_hash": "8ec2c5c35efe131212752176bf1e79b923d2531dd2af7c1e58c3ca303f68315c", "source": {"title": "data_index_job/cog_search_docs_faiss_mlindex.py", "filename": "data_index_job/cog_search_docs_faiss_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/cog_search_docs_faiss_mlindex.py", "mtime": 1694056741.4741247, "chunk_id": "13"}, "stats": {"tiktokens": 81, "chars": 317, "lines": 14}}, "document_id": "data_index_job/cog_search_docs_faiss_mlindex.py13"}
{"content": "Title: data_index_job/s3_to_acs_mlindex.py\n\n# %%[markdown]\n# # S3 via OneLake to Azure Cognitive Search Index\n\n# %% Prerequisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[cognitive_search]>=0.2.0'\n\n# %% Authenticate to an AzureML Workspace, you can download a `config.json` from the top-right-hand corner menu of a Workspace.\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential", "metadata": {"doc_id": "data_index_job/s3_to_acs_mlindex.py0", "chunk_hash": "d8f9eb4e709825f85e046fe3733b6c18dea52c6e1dc09bf983474dca86968c1c", "source": {"title": "data_index_job/s3_to_acs_mlindex.py", "filename": "data_index_job/s3_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/s3_to_acs_mlindex.py", "mtime": 1694107964.5441494, "chunk_id": "0"}, "stats": {"tiktokens": 148, "chars": 551, "lines": 12}}, "document_id": "data_index_job/s3_to_acs_mlindex.py0"}
{"content": "Title: data_index_job/s3_to_acs_mlindex.py\n\nml_client = MLClient.from_config(\n    credential=DefaultAzureCredential(), path=\"config.json\"\n)\n\n# %% Create DataIndex configuration\nfrom azureml.rag.dataindex.entities import (\n    Data,\n    DataIndex,\n    IndexSource,\n    Embedding,\n    IndexStore,\n)\n\nasset_name = \"s3_aoai_acs\"", "metadata": {"doc_id": "data_index_job/s3_to_acs_mlindex.py1", "chunk_hash": "82d8e2dfc61c04fa70c3a3af43cd10be5b83a1d8b3f37ff628ae9f66a86d7101", "source": {"title": "data_index_job/s3_to_acs_mlindex.py", "filename": "data_index_job/s3_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/s3_to_acs_mlindex.py", "mtime": 1694107964.5441494, "chunk_id": "1"}, "stats": {"tiktokens": 84, "chars": 324, "lines": 16}}, "document_id": "data_index_job/s3_to_acs_mlindex.py1"}
{"content": "Title: data_index_job/s3_to_acs_mlindex.py\n\ndata_index = DataIndex(\n    name=asset_name,\n    description=\"S3 data embedded with text-embedding-ada-002 and indexed in Azure Cognitive Search.\",\n    source=IndexSource(\n        input_data=Data(\n            type=\"uri_folder\",\n            path=\"abfss://9aa7b19e-c117-4a74-8654-cf1559ba9f4f@msit-onelake.dfs.fabric.microsoft.com/1606ee55-ec68-4658-8d6b-58bf8dd26636/Files/lupickup-test-s3\",", "metadata": {"doc_id": "data_index_job/s3_to_acs_mlindex.py2", "chunk_hash": "14e2f3429f82bd30b95a866b57672a3795b67f585d9ade26c44b6b30b922dafa", "source": {"title": "data_index_job/s3_to_acs_mlindex.py", "filename": "data_index_job/s3_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/s3_to_acs_mlindex.py", "mtime": 1694107964.5441494, "chunk_id": "2"}, "stats": {"tiktokens": 142, "chars": 434, "lines": 9}}, "document_id": "data_index_job/s3_to_acs_mlindex.py2"}
{"content": "Title: data_index_job/s3_to_acs_mlindex.py\n\n),\n        citation_url=\"s3://lupickup-test\",\n    ),\n    embedding=Embedding(\n        model=\"text-embedding-ada-002\",\n        connection=\"azureml-rag-oai\",\n        cache_path=f\"azureml://datastores/workspaceblobstore/paths/embeddings_cache/{asset_name}\",\n    ),\n    index=IndexStore(\n        type=\"acs\",\n        connection=\"azureml-rag-acs\",\n    ),", "metadata": {"doc_id": "data_index_job/s3_to_acs_mlindex.py3", "chunk_hash": "9d6c05306d9ca47428d6ec71db09e3fb06d9203500970dc06e460bb7a55be1e3", "source": {"title": "data_index_job/s3_to_acs_mlindex.py", "filename": "data_index_job/s3_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/s3_to_acs_mlindex.py", "mtime": 1694107964.5441494, "chunk_id": "3"}, "stats": {"tiktokens": 104, "chars": 392, "lines": 14}}, "document_id": "data_index_job/s3_to_acs_mlindex.py3"}
{"content": "Title: data_index_job/s3_to_acs_mlindex.py\n\n# name is replaced with a unique value each time the job is run\n    path=f\"azureml://datastores/workspaceblobstore/paths/indexes/{asset_name}/{{name}}\",\n)", "metadata": {"doc_id": "data_index_job/s3_to_acs_mlindex.py4", "chunk_hash": "fad4ad54c1894384367122c94e0fadfa6cf6d21cd3875b2efa28706332e407a4", "source": {"title": "data_index_job/s3_to_acs_mlindex.py", "filename": "data_index_job/s3_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/s3_to_acs_mlindex.py", "mtime": 1694107964.5441494, "chunk_id": "4"}, "stats": {"tiktokens": 54, "chars": 198, "lines": 5}}, "document_id": "data_index_job/s3_to_acs_mlindex.py4"}
{"content": "Title: data_index_job/s3_to_acs_mlindex.py\n\n# %% Create the DataIndex Job to be scheduled\nfrom azure.ai.ml import UserIdentityConfiguration\n\nindex_job = ml_client.data.index_data(\n    data_index=data_index,\n    # The DataIndex Job will use the identity of the MLClient within the DataIndex Job to access source data.\n    identity=UserIdentityConfiguration(),\n)\n\n# %% Wait for it to finish\nml_client.jobs.stream(index_job.name)\n\n# %% Check the created asset, it is a folder on storage containing an MLIndex yaml file\nmlindex_docs_index_asset = ml_client.data.get(data_index.name, label=\"latest\")\nmlindex_docs_index_asset", "metadata": {"doc_id": "data_index_job/s3_to_acs_mlindex.py5", "chunk_hash": "99461e5936dcdb9f1357d8ec354fcbd8d1390b897a27dd4f61e03fed8c98f58f", "source": {"title": "data_index_job/s3_to_acs_mlindex.py", "filename": "data_index_job/s3_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/s3_to_acs_mlindex.py", "mtime": 1694107964.5441494, "chunk_id": "5"}, "stats": {"tiktokens": 140, "chars": 619, "lines": 17}}, "document_id": "data_index_job/s3_to_acs_mlindex.py5"}
{"content": "Title: data_index_job/s3_to_acs_mlindex.py\n\n# %% Try it out with langchain by loading the MLIndex asset using the azureml-rag SDK\nfrom azureml.rag.mlindex import MLIndex\n\nmlindex = MLIndex(mlindex_docs_index_asset)\n\nindex = mlindex.as_langchain_vectorstore()\ndocs = index.similarity_search(\"What is RAG?\", k=5)\ndocs\n\n# %% Take a look at those chunked docs\nimport json\n\nfor doc in docs:\n    print(json.dumps({\"content\": doc.page_content, **doc.metadata}, indent=2))\n\n# %%", "metadata": {"doc_id": "data_index_job/s3_to_acs_mlindex.py6", "chunk_hash": "0d106223147d417a903403dc286ec5e87da3c56aab03ca9cdb51a4e1c1065c5d", "source": {"title": "data_index_job/s3_to_acs_mlindex.py", "filename": "data_index_job/s3_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/s3_to_acs_mlindex.py", "mtime": 1694107964.5441494, "chunk_id": "6"}, "stats": {"tiktokens": 126, "chars": 470, "lines": 18}}, "document_id": "data_index_job/s3_to_acs_mlindex.py6"}
{"content": "Title: data_index_job/local_docs_to_acs_mlindex.py\n\n# %%[markdown]\n# # Local Documents to Azure Cognitive Search Index\n\n# %% Prerequisites\n# %pip install 'azure-ai-ml==1.10.0a20230825006' --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n# %pip install 'azureml-rag[cognitive_search]>=0.2.0'\n\n# %% Authenticate to you AzureML Workspace, download a `config.json` from the top right hand corner menu of the Workspace.\nfrom azure.ai.ml import MLClient, load_data\nfrom azure.identity import DefaultAzureCredential", "metadata": {"doc_id": "data_index_job/local_docs_to_acs_mlindex.py0", "chunk_hash": "29d4768b3ad3e63e4b220f3ca48bb634f7e56cd30e5fffbe97f1831de2ada264", "source": {"title": "data_index_job/local_docs_to_acs_mlindex.py", "filename": "data_index_job/local_docs_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/local_docs_to_acs_mlindex.py", "mtime": 1694056741.4453142, "chunk_id": "0"}, "stats": {"tiktokens": 146, "chars": 566, "lines": 12}}, "document_id": "data_index_job/local_docs_to_acs_mlindex.py0"}
{"content": "Title: data_index_job/local_docs_to_acs_mlindex.py\n\nml_client = MLClient.from_config(\n    credential=DefaultAzureCredential(), path=\"config.json\"\n)\n\n# %% Load DataIndex configuration from file\ndata_index = load_data(\"local_docs_to_acs_mlindex.yaml\")\nprint(data_index)\n\n# %% Submit the DataIndex Job\nindex_job = ml_client.data.index_data(data_index=data_index)\n\n# %% Wait for it to finish\nml_client.jobs.stream(index_job.name)", "metadata": {"doc_id": "data_index_job/local_docs_to_acs_mlindex.py1", "chunk_hash": "dae11c3f47899e508d227c3edbaf8ff57b439048cef94ad4ccdf0fe4b713f02f", "source": {"title": "data_index_job/local_docs_to_acs_mlindex.py", "filename": "data_index_job/local_docs_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/local_docs_to_acs_mlindex.py", "mtime": 1694056741.4453142, "chunk_id": "1"}, "stats": {"tiktokens": 100, "chars": 425, "lines": 15}}, "document_id": "data_index_job/local_docs_to_acs_mlindex.py1"}
{"content": "Title: data_index_job/local_docs_to_acs_mlindex.py\n\n# %% Check the created asset, it is a folder on storage containing an MLIndex yaml file\nmlindex_docs_index_asset = ml_client.data.get(data_index.name, label=\"latest\")\nmlindex_docs_index_asset\n\n# %% Try it out with langchain by loading the MLIndex asset using the azureml-rag SDK\nfrom azureml.rag.mlindex import MLIndex\n\nmlindex = MLIndex(mlindex_docs_index_asset)\n\nindex = mlindex.as_langchain_vectorstore()\ndocs = index.similarity_search(\"What is an MLIndex?\", k=5)\ndocs\n\n# %% Take a look at those chunked docs\nimport json", "metadata": {"doc_id": "data_index_job/local_docs_to_acs_mlindex.py2", "chunk_hash": "1cbbe8a35bf045948b16f2e215eedcaa25f15afb3a1c44e267e0f6851706a482", "source": {"title": "data_index_job/local_docs_to_acs_mlindex.py", "filename": "data_index_job/local_docs_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/local_docs_to_acs_mlindex.py", "mtime": 1694056741.4453142, "chunk_id": "2"}, "stats": {"tiktokens": 144, "chars": 575, "lines": 17}}, "document_id": "data_index_job/local_docs_to_acs_mlindex.py2"}
{"content": "Title: data_index_job/local_docs_to_acs_mlindex.py\n\nfor doc in docs:\n    print(json.dumps({\"content\": doc.page_content, **doc.metadata}, indent=2))\n\n# %% Try it out with Promptflow", "metadata": {"doc_id": "data_index_job/local_docs_to_acs_mlindex.py3", "chunk_hash": "3dcb8c7d5ddf33dd9947a655f5264c369e8572b16d594ed496df09b0941ef51d", "source": {"title": "data_index_job/local_docs_to_acs_mlindex.py", "filename": "data_index_job/local_docs_to_acs_mlindex.py", "url": "https://github.com/Azure/azureml-examples/blob/lupickup/rag/mlindex_pup_refresh/data_index_job/local_docs_to_acs_mlindex.py", "mtime": 1694056741.4453142, "chunk_id": "3"}, "stats": {"tiktokens": 46, "chars": 180, "lines": 6}}, "document_id": "data_index_job/local_docs_to_acs_mlindex.py3"}
