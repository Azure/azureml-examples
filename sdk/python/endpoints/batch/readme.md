# Batch Endpoints examples

Batch endpoints provide a convenient way to run inference over large volumes of data. They simplify the process of hosting your models for batch scoring, so you can focus on machine learning, not infrastructure. Use batch endpoints when:

* You have expensive models that requires a longer time to run inference.
* You need to perform inference over large amounts of data, distributed in multiple files.
* You don't have low latency requirements.
* You can take advantage of parallelization.

## Examples

Example | Description | Input data type | Notebook
-|-|-|-
[Batch score an MLflow model for the Heart Disease Classification problem](heart-classifier-mlflow) | This example shows how you can deploy an MLflow model to a batch endpoint to perform batch predictions. This example uses an MLflow model based on the UCI Heart Disease Data Set. The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence). The model has been trained using an XGBBoost classifier and all the required preprocessing has been packaged as a scikit-learn pipeline, making this model an end-to-end pipeline that goes from raw data to predictions. | Tabular | [See notebook](heart-classifier-mlflow/mlflow-for-batch-tabular.ipynb)
[Batch score an XGBoost model for the Heart Disease Classification problem and write predictions on parquet files](heart-classifier-parquet) | This example shows how you can deploy an MLflow model to a batch endpoint to perform batch predictions. This example uses an MLflow model based on the UCI Heart Disease Data Set. The database contains 76 attributes, but we are using a subset of 14 of them. The model tries to predict the presence of heart disease in a patient. It is integer valued from 0 (no presence) to 1 (presence). The model has been trained using an XGBBoost classifier and all the required preprocessing has been packaged as a scikit-learn pipeline, making this model an end-to-end pipeline that goes from raw data to predictions. This example also customizes the way the endpoint write predictions. | Tabular | [See notebook](heart-classifier-parquet/custom-output-batch.ipynb)
[Batch score a model for MNIST classification with multiple deployments](mnist-classifier) | In this example, we're going to deploy a model to solve the classic MNIST ("Modified National Institute of Standards and Technology") digit recognition problem to perform batch inferencing over large amounts of data (image files). In the first section of this tutorial, we're going to create a batch deployment with a model created using Torch. Such deployment will become our default one in the endpoint. In the second half, we're going to see how we can create a second deployment using a model created with TensorFlow (Keras), test it out, and then switch the endpoint to start using the new deployment as default. | Images | [See notebook](mnist-classifier/mnist-batch.ipynb)
[Batch score and classify images using a ResNet50 model for the ImageNet dataset](imagenet-classifier) | The model we are going to work with was built using TensorFlow along with the RestNet architecture (Identity Mappings in Deep Residual Networks). This example shows also how to perform high performance inference over batches of images on GPU.  | Images | [See notebook](imagenet-classifier/imagenet-classifier-batch.ipynb)
[Batch score and classify images using a ResNet50 model for the ImageNet dataset (MLflow)](imagenet-classifier) | The model we are going to work with was built using TensorFlow along with the RestNet architecture (Identity Mappings in Deep Residual Networks). This example shows you can package the model as MLflow and deploy later.  | Images | [See notebook](imagenet-classifier/imagenet-classifier-mlflow.ipynb)
[Batch score a HuggingFace NLP model for text summarization](bart-text-summarization) | The model we are going to work with was built using the popular library transformers from HuggingFace along with a pre-trained model from Facebook with the BART architecture. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation. | Text | [See notebook](bart-text-summarization/text-summarization-batch.ipynb)