# Batch Endpoints examples

Batch endpoints provide a convenient way to run inference over large volumes of data. They simplify the process of hosting your models for batch scoring, so you can focus on machine learning, not infrastructure. 

Use batch endpoints when:

- &check; You have expensive models that requires a longer time to run inference.
- &check; You need to perform inference over large amounts of data, distributed in multiple files.
- &check; You don't have low latency requirements.
- &check; You can take advantage of parallelization.

In this folder, you will find several examples with concrete scenarios to start working with Batch Endpoints.


Notebook | Description                    | Status  
---------|--------------------------------|---------
[Create and manage a batch endpoint for inferencing](mnist-batch.ipynb) | Demonstrates the basics for working with batch endpoints by deploying two different models that solve the MNIST classification problem. One model is built with TensorFlow and the other with Torch. The example demonstrates how you can deploy multiple model versions under the same endpoint and switch between them as needed. |
[Image processing with batch deployments](imagenet-classifier-batch.ipynb) | Demonstrates how to create a deployment that scores images using a deep learning model for image classification for the classical ImageNet problem built with TensorFlow and Keras. It also demonstrates how to achieve high performance deployments by reading and scoring batches of images. |
[Text processing with batch deployments](text-summarization-batch.ipynb) | Demonstrates how to create a deployment that scores text using a deep learning model based on the popular HuggingFace library. The model generates summaries by reading the input data from CSV files. |
[Use MLflow models in batch deployments for tabular data](mlflow-for-batch-tabular.ipynb) | Demonstrates how to deploy an MLflow model that operates over tabular data to a batch endpoint without indicating an scoring script or environment. It also demonstrates how you can customize how inference is executed with an scoring script if needed. The model solves the classical Heart Disease classification problem and scoring is performed over CSV files. |
[Use MLflow models in batch deployments for image processing](mlflow-for-batch-images.ipynb) | Demonstrates how to deploy an MLflow model that operates over image files to a batch endpoint without indicating an scoring script or environment. It demonstrates how MLflow models can be registered and designed so they can work better on batch deployments. |
[Customize outputs in batch deployments](custom-output-batch.ipynb) | By default, batch endpoints generate the predictions in a single file in CSV format. This example demonstrates how to customize the output and predictions generated by batch deployments by deploying an MLflow model that reads data in CSV format and generate predictions stored in __parquet__ format as separated files. |

To get help about how to use this examples and batch endpoints in general, please check [Use batch endpoints for batch scoring - Azure Machine Learning Docs](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-batch-endpoint).
