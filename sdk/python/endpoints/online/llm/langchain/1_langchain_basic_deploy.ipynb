{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Integration into Azure Machine Learning (AzureML)\n",
    "\n",
    "**Requirements** - In order to benefit from this tutorial, you will need:\n",
    "* A basic understanding of Machine Learning and Large Language Models\n",
    "* An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "* An Azure Machine Learning Workspace, Azure Key Vault, and Azure Container Registry\n",
    "* An OpenAI API Key which can be found in User Settings in OpenAI\n",
    "\n",
    "**Motivations** - The Langchain framework allows for rapid development of applications powered by large language models. This sample creates a chat bot application backed by a large language model and deploys the application to AzureML.\n",
    "\n",
    "**Outline** - \n",
    "1. Test langchain app locally using [**azureml-inference-server-http**](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-inference-server-http?view=azureml-api-2) pacakge.\n",
    "2. Deploy the app to an **AzureML Managed Online Endpoint**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test Locally\n",
    "Before deploy, you could test the langchain app locally. We are using [Langchain ChatGPT plugin](https://python.langchain.com/en/latest/modules/agents/tools/examples/chatgpt_plugins.html) as an example app here. Execute the code below to try out. You can inspect the [simple_agent_app_test.py](../src/langchain/simple_agent_app_test.py) to see the implementation itself. It's a langchain ZERO_SHOT_REACT_DESCRIPTION agent with Klarna plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_TYPE = \"openai\"  # 'azure' or 'openai'\n",
    "OPENAI_API_KEY = \"<OPENAI_API_KEY>\"\n",
    "\n",
    "# required for OpenAI API\n",
    "OPENAI_ORG_ID = \"\"\n",
    "OPENAI_MODEL_ID = \"gpt-3.5-turbo\"\n",
    "\n",
    "# required for Azure OpenAI API\n",
    "AZURE_OPENAI_API_ENDPOINT = \"<AOAI endpoint>\"\n",
    "AZURE_OPENAI_API_DEPLOYMENT_NAME = \"<deployment-name>\"\n",
    "\n",
    "# set to env var for the langchain code to consume\n",
    "%env OPENAI_API_KEY=$OPENAI_API_KEY\n",
    "%env OPENAI_API_TYPE=$OPENAI_API_TYPE\n",
    "%env OPENAI_MODEL_ID=$OPENAI_MODEL_ID\n",
    "%env OPENAI_ORG_ID=$OPENAI_ORG_ID\n",
    "%env AZURE_OPENAI_API_ENDPOINT=$AZURE_OPENAI_API_ENDPOINT\n",
    "%env AZURE_OPENAI_API_DEPLOYMENT_NAME=$AZURE_OPENAI_API_DEPLOYMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to test the app locally\n",
    "# %run -i ../src/langchain/simple_agent_app_test.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test file above is directly running the langchain agent. To deploy to Online Endpoint, let's turn that into a scoring script. You can view the code here: [simple_agent_app_test.py](../src/langchain/simple_agent_app_test.py). We basically create an **init()** and a **run()** function and put the langchain code inside.\n",
    "\n",
    "You can read more documentation about scoring file here: [How to deploy online endpoint - Understand the scoring script](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-cli#understand-the-scoring-script)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 (Optional) Test locally using AzureML local server\n",
    "Before we deploy to Online Endpoint, you could test locally first. Azure Machine Learning offers [**azureml-inference-server-http**](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-inference-server-http?view=azureml-api-2) to help. We will\n",
    "* Start a subprocess to start the server locally\n",
    "* Test the server by sending JSON requests\n",
    "* Kill the server\n",
    "\n",
    "Start the server in a sub process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import subprocess\n",
    "\n",
    "p = subprocess.Popen(\n",
    "    \"cd ../src/langchain/; azmlinfsrv --entry_script simple_agent_score.py\",\n",
    "    shell=True,\n",
    "    stdout=subprocess.PIPE,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stream the logs to see what's going on by executing this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import time\n",
    "\n",
    "while p.poll() is None:\n",
    "    # Read one line from the output pipe\n",
    "    output = p.stdout.readline()\n",
    "    error = None\n",
    "    if p.stderr:\n",
    "        error = p.stderr.readline()\n",
    "    if output:\n",
    "        print(output.decode(\"utf-8\").strip())\n",
    "    if error:\n",
    "        print(error.decode(\"utf-8\").strip())\n",
    "    time.sleep(0.1)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the log cell, now let's call the local server to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import requests, json, time\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "url = \"http://localhost:5001/score\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = json.dumps(\n",
    "    {\"question\": \"what are the top 5 results for womens t shirts on klarna?\"}\n",
    ")\n",
    "\n",
    "response = requests.post(url, headers=headers, data=payload)\n",
    "print(f\"Result:\\n\", response.text)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see response similar to this\n",
    "```\n",
    "    The top 5 results for women's t-shirts on Klarna are: \n",
    "    1. Armani Exchange Slim Fit Logo Cotton T-shirt - Black\n",
    "    2. Balmain White Printed T-Shirt\n",
    "    3. Versace Jeans Logo T-shirt - Black\n",
    "    4. Icebreaker Women's Tech Lite II Merino Short Sleeve T-shirt - Grey\n",
    "    5. Off-White Logo T-Shirt White\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done testing, let's kill the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkill = subprocess.Popen(\"kill $(lsof -t -i :5001)\", shell=True, stdout=subprocess.PIPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our scoring script is working, let's deploy to online endpoint."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deploy Online Endpoint\n",
    "On a high level, we will perform the following tasks:\n",
    "\n",
    "* **Preparation**\n",
    "    * Store OpenAI key in Azure Keyvault to keep it safe.\n",
    "* **Create Managed Online Endpoint**, and give the endpoint permission to access Azure Keyvault above.\n",
    "* **Deploy to Managed Online Endpoint**\n",
    "* **Test**\n",
    "\n",
    "## 2.1 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your AML workspace\n",
    "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
    "RESOURCE_GROUP = \"<RESOURCE_GROUP>\"\n",
    "AML_WORKSPACE_NAME = \"<AML_WORKSPACE_NAME>\"\n",
    "\n",
    "# Keyvault information\n",
    "KEYVAULT_NAME = \"<KEY_VAULT_NAME>\"\n",
    "# Key name in keyvault for the OPENAI-AI-KEY\n",
    "KV_OPENAI_KEY = \"OPENAI-API-KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate clients\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential(additionally_allowed_tenants=[\"*\"])\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential(additionally_allowed_tenants=[\"*\"])\n",
    "\n",
    "# If login doesn't work above, uncomment the code below and login using device code\n",
    "# !az login --use-device-code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 (Optional) Store Secrets in Azure Keyvault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MY_OBJECT_ID = !az ad signed-in-user show --query id -o tsv\n",
    "KEYVAULT_RESOURCE_URI = f\"/subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/providers/Microsoft.KeyVault/vaults/{KEYVAULT_NAME}\"\n",
    "\n",
    "need_interactive_auth = False\n",
    "if \"AADSTS530003\".lower() in MY_OBJECT_ID[0].lower():\n",
    "    need_interactive_auth = True\n",
    "    print(\"\\n\".join(MY_OBJECT_ID))\n",
    "    print(\n",
    "        \"\\nYou are geting this error probably because you are using a device login. And this operation needs interactive login. If you can't login interactively, you could simply copy and run the following command in Azure Cloud Shell in Bash mode.\\n\"\n",
    "    )\n",
    "    print(\"MY_OBJECT_ID=`az ad signed-in-user show --query id -o tsv`\")\n",
    "    print(\n",
    "        f\"az role assignment create --role 'Key Vault Administrator' --scope {KEYVAULT_RESOURCE_URI}  --assignee-object-id $MY_OBJECT_ID --assignee-principal-type User\"\n",
    "    )\n",
    "    print(\n",
    "        f\"az keyvault secret set --name {KV_OPENAI_KEY} --vault-name {KEYVAULT_NAME} --value {OPENAI_API_KEY}\"\n",
    "    )\n",
    "else:\n",
    "    # Let's set OpenAI key as a secret in keyvault\n",
    "    need_interactive_auth = False\n",
    "    !az role assignment create --role \"Key Vault Administrator\" --scope {KEYVAULT_RESOURCE_URI}  --assignee-object-id {MY_OBJECT_ID[0]} --assignee-principal-type User\n",
    "    !az keyvault secret set --name {KV_OPENAI_KEY} --vault-name {KEYVAULT_NAME} --value {OPENAI_API_KEY}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Manage Online Endpoint\n",
    "## 3.1 Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a endpoint\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    ")\n",
    "\n",
    "from azure.ai.ml import (\n",
    "    MLClient,\n",
    ")\n",
    "\n",
    "online_endpoint_name = \"aml-llm-demo-langchain-endpoint\"\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(credential, SUBSCRIPTION_ID, RESOURCE_GROUP, AML_WORKSPACE_NAME)\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"online endpoint for Langchain server\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 (Optional) Grant Endpoint Permission to Dependencies\n",
    "If using keyvault to store your OpenAI API key, uncomment the below code. The endpoint will use AAD to access dependent resources, so you don't have to hardcode secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Allow the endpoint to access secrets in keyvault\n",
    "KEYVAULT_RESOURCE_URI = f\"/subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/providers/Microsoft.KeyVault/vaults/{KEYVAULT_NAME}\"\n",
    "need_interactive_auth = False\n",
    "if need_interactive_auth:\n",
    "    print(\n",
    "        \"If you can't login interactively, you could run the following command in Azure Cloud Bash Shell.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"az role assignment create --role 'Key Vault Secrets User' --scope {KEYVAULT_RESOURCE_URI}  --assignee {endpoint.identity.principal_id}\"\n",
    "    )\n",
    "else:\n",
    "    !az role assignment create --role \"Key Vault Secrets User\" --scope {KEYVAULT_RESOURCE_URI}  --assignee {endpoint.identity.principal_id}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Deploy to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineDeployment,\n",
    "    OnlineRequestSettings,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "\n",
    "KEYVAULT_URL = f\"https://{KEYVAULT_NAME}.vault.azure.net\"\n",
    "\n",
    "env = Environment(\n",
    "    conda_file=\"deployments/env.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/minimal-ubuntu20.04-py38-cpu-inference:latest\",\n",
    ")\n",
    "\n",
    "deployment_name = f\"deploy-{str(datetime.datetime.now().strftime('%m%d%H%M%f'))}\"\n",
    "sk_deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../src\", scoring_script=\"langchain/simple_agent_score.py\"\n",
    "    ),\n",
    "    request_settings=OnlineRequestSettings(request_timeout_ms=60000),\n",
    "    environment_variables={\n",
    "        \"OPENAI_API_KEY\": OPENAI_API_KEY,\n",
    "        \"OPENAI_API_TYPE\": OPENAI_API_TYPE,\n",
    "        \"OPENAI_MODEL_ID\": OPENAI_MODEL_ID,\n",
    "        \"OPENAI_ORG_ID\": OPENAI_ORG_ID,\n",
    "        \"AZURE_OPENAI_API_ENDPOINT\": AZURE_OPENAI_API_ENDPOINT,\n",
    "        \"AZURE_OPENAI_API_DEPLOYMENT_NAME\": AZURE_OPENAI_API_DEPLOYMENT_NAME,\n",
    "    },\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    instance_type=\"Standard_F2s_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "ml_client.online_deployments.begin_create_or_update(sk_deployment).result()\n",
    "\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test\n",
    "Now endpoint has been deployed, let's test it. We are going to re-use the same request when we test it locally earlier on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "url_parts = urlsplit(endpoint.scoring_uri)\n",
    "url = url_parts.scheme + \"://\" + url_parts.netloc\n",
    "\n",
    "token = ml_client.online_endpoints.get_keys(name=online_endpoint_name).primary_key\n",
    "headers = {\"Authorization\": \"Bearer \" + token, \"Content-Type\": \"application/json\"}\n",
    "payload = json.dumps(\n",
    "    {\"question\": \"what are the top 5 results for womens t shirts on klarna?\"}\n",
    ")\n",
    "\n",
    "response = requests.post(f\"{url}/score\", headers=headers, data=payload)\n",
    "print(f\"Response:\\n\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Delete the deployment and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
