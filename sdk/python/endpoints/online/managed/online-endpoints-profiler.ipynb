{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML Online Endpoints Model Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Inferencing machine learning models is a time and compute intensive process. It is vital to quantify the performance of model inferencing to ensure that you make the best use of compute resources and reduce cost to reach the desired performance SLA (e.g. latency, throughput).\n",
    "\n",
    "Online Endpoints Model Profiler (Preview) provides fully managed experience that makes it easy to benchmark your model performance served through [Online Endpoints](https://docs.microsoft.com/en-us/azure/machine-learning/concept-endpoints).\n",
    "\n",
    "* Use the benchmarking tool of your choice.\n",
    "\n",
    "* Easy to use CLI experience.\n",
    "  \n",
    "* Support for CI/CD MLOps pipelines to automate profiling.\n",
    "  \n",
    "* Thorough performance report containing latency percentiles and resource utilization metrics.\n",
    "\n",
    "## A brief introduction on benchmarking tools\n",
    "\n",
    "The online endpoints model profiler currently supports 3 types of benchmarking tools: wrk, wrk2, and labench.\n",
    "\n",
    "* `wrk`: wrk is a modern HTTP benchmarking tool capable of generating significant load when run on a single multi-core CPU. It combines a multithreaded design with scalable event notification systems such as epoll and kqueue. For detailed info please refer to this link: https://github.com/wg/wrk.\n",
    "\n",
    "* `wrk2`: wrk2 is wrk modifed to produce a constant throughput load, and accurate latency details to the high 9s (i.e. can produce accuracy 99.9999% if run long enough). In addition to wrk's arguments, wrk2 takes a throughput argument (in total requests per second) via either the --rate or -R parameters (default is 1000). For detailed info please refer to this link: https://github.com/giltene/wrk2.\n",
    "\n",
    "* `labench`: LaBench (for LAtency BENCHmark) is a tool that measures latency percentiles of HTTP GET or POST requests under very even and steady load. For detailed info please refer to this link: https://github.com/microsoft/LaBench."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "The following prerequisites are required to run the notebook:\n",
    "- An Azure subscription\n",
    "- A resource group with ownership permissions or an existing compute instance with the Contributor role\n",
    "- The following additional Python packages are required: \n",
    "    - [azure-mgmt-authorization](https://pypi.org/project/azure-mgmt-authorization/): Used to assign roles\n",
    "\n",
    "If you donâ€™t have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://aka.ms/AMLFree) today.\n",
    "\n",
    "Install the additional Python requirements with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-mgmt-authorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient, command\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    CodeConfiguration,\n",
    "    Environment,\n",
    "    ComputeInstance,\n",
    "    IdentityConfiguration,\n",
    "    Data,\n",
    "    CommandJob,\n",
    "    Job,\n",
    "    OnlineRequestSettings,\n",
    ")\n",
    "from azure.ai.ml import Input\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Set workspace details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace_name = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.randint(0, 100000)\n",
    "endpoint_name = f\"endpt-moe-{rand}\"\n",
    "profiler_compute_name = f\"profiler{rand}\"\n",
    "profiler_compute_size = \"Standard_DS4_v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Get a handle to the workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential,\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group_name=resource_group,\n",
    "    workspace_name=workspace_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an online endpoint\n",
    "\n",
    "You will need a simple online endpoint as a target for the profiler. For more information see [online-endpoints-simple-deployment.ipynb](online-endpoints-simple-deployment.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=Model(path=\"../model-1/model/sklearn_regression_model.pkl\"),\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    environment=Environment(\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "        conda_file=\"../model-1/environment/conda.yml\",\n",
    "    ),\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=3000, max_concurrent_requests_per_instance=1024\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ml_client.online_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a compute to host the profiler\n",
    "You will need a compute to host the profiler, to send requests to the online endpoint, and generate performance report.\n",
    "\n",
    "This compute is NOT the same one that you used above to deploy your model. Please choose a compute SKU with proper network bandwidth (considering the inference request payload size and profiling traffic, we'd recommend Standard_F4s_v2) in the same region as the online endpoint.\n",
    "\n",
    "The compute needs to have contributor role to the machine learning workspace. For more information, see [Assign Azure roles using Azure CLI](https://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-cli)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create the compute instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute = ComputeInstance(\n",
    "    name=profiler_compute_name,\n",
    "    size=\"Standard_DS4_v2\",\n",
    "    identity=IdentityConfiguration(type=\"system_assigned\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute = ml_client.compute.begin_create_or_update(compute).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Get Authorization Management Clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.authorization import AuthorizationManagementClient\n",
    "from azure.mgmt.authorization.v2018_01_01_preview.models import RoleDefinition\n",
    "import uuid\n",
    "\n",
    "role_definition_client = AuthorizationManagementClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    "    api_version=\"2018-01-01-preview\",\n",
    ")\n",
    "\n",
    "from azure.mgmt.authorization.v2020_10_01_preview.models import (\n",
    "    RoleAssignment,\n",
    "    RoleAssignmentCreateParameters,\n",
    ")\n",
    "\n",
    "role_assignment_client = AuthorizationManagementClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    "    api_version=\"2020-10-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Assign the Contributor role to the compute instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"Contributor\"\n",
    "scope = ml_client.workspaces.get(workspace_name).id\n",
    "\n",
    "role_defs = role_definition_client.role_definitions.list(scope=scope)\n",
    "role_def = next((r for r in role_defs if r.role_name == role_name))\n",
    "\n",
    "role_assignment_client.role_assignments.create(\n",
    "    scope=scope,\n",
    "    role_assignment_name=str(uuid.uuid4()),\n",
    "    parameters=RoleAssignmentCreateParameters(\n",
    "        role_definition_id=role_def.id,\n",
    "        principal_id=compute.identity.principal_id,\n",
    "        principal_type=\"ServicePrincipal\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a profiling job\n",
    "\n",
    "A profiling job simulates how an online endpoint serves live requests. It produces a throughput load to the online endpoint and generates performance report.\n",
    "\n",
    "Profiling job parameters can be passed using environment variables or a JSON configuration file. In this example, we'll use environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Upload the payload\n",
    "The payload contains a separate JSON payload for the endpoint on each line. Wrapping the payload in a Data object exposes a `path` on the `workspaceblobstore` that will be used for the profiling job.\n",
    "\n",
    "```json\n",
    "{\"data\": [[1,2,3,4,5,6,7,8,9,10], [10,9,8,7,6,5,4,3,2,1]]}\n",
    "{\"data\": [[1,2,3,4,5,6,7,8,9,10], [10,9,8,7,6,5,4,3,2,1]]}\n",
    "{\"data\": [[1,2,3,4,5,6,7,8,9,10], [10,9,8,7,6,5,4,3,2,1]]}\n",
    "{\"data\": [[1,2,3,4,5,6,7,8,9,10], [10,9,8,7,6,5,4,3,2,1]]}\n",
    "{\"data\": [[1,2,3,4,5,6,7,8,9,10], [10,9,8,7,6,5,4,3,2,1]]}\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = Data(\n",
    "    name=\"payload\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    path=\"profiler/profiling/payload.txt\",\n",
    "    datastore=\"workspaceblobstore\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = ml_client.data.create_or_update(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create a profiling job\n",
    "To create a profiling job a `CommandJob` object is used with the `online-endpoints-model-profiler` image. For general command job parameters, see the [CLI v2 Command Job YAML Schema](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-command). \n",
    "\n",
    "The key parameters for a profiling job are: \n",
    "\n",
    "| Key | Type  | Description | Allowed values | Default value |\n",
    "| --- | ----- | ----------- | -------------- | ------------- |\n",
    "| `command` | string | The command for running the profiling job. | `python -m online_endpoints_model_profiler ${{inputs.payload}}` | - |\n",
    "| `experiment_name` | string | The experiment name of the profiling job. An experiment is a group of jobs. | - | - |\n",
    "| `display_name` | string | The profiling job name. | - | A random string guid, such as `willing_needle_wrzk3lt7j5` |\n",
    "| `environment.image` | string | An Azure Machine Learning curated image containing benchmarking tools and profiling scripts. | mcr.microsoft.com/azureml/online-endpoints-model-profiler:latest | - |\n",
    "| `environment_variables` | string | Environment vairables for the profiling job. | [Profiling related environment variables](#YAML-profiling-related-environment_variables)<br><br>[Benchmarking tool related environment variables](#YAML-benchmarking-tool-related-environment_variables) | - |\n",
    "| `compute` | string | The aml compute for running the profiling job. | - | - |\n",
    "| `inputs.payload` | string | Payload file that is stored in an AML registered datastore. | [Example payload file content](https://github.com/Azure/azureml-examples/blob/xiyon/mir-profiling/cli/endpoints/online/profiling/payload.txt) | - |\n",
    "\n",
    "Key environment variables that configure the profiling job include:\n",
    "| Key | Description | Default Value | wrk | wrk2 | labench |\n",
    "| --- | ----------- | ------------- | --- | ---- | ------- |\n",
    "| `DURATION` | Period of time for running the benchmarking tool. | `300s` | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n",
    "| `CONNECTIONS` | No. of connections for the benchmarking tool. The default value will be set to the value of `max_concurrent_requests_per_instance` | `1` | :heavy_check_mark: | :heavy_check_mark: | :x: |\n",
    "| `THREAD` | No. of threads allocated for the benchmarking tool. | `1` | :heavy_check_mark: | :heavy_check_mark: | :x: |\n",
    "| `TARGET_RPS` | Target requests per second for the benchmarking tool. | `50` | :x: | :heavy_check_mark: | :heavy_check_mark: |\n",
    "| `CLIENTS` | No. of clients for the benchmarking tool. The default value will be set to the value of `max_concurrent_requests_per_instance` | `1` | :x: | :x: | :heavy_check_mark: |\n",
    "| `TIMEOUT` | Timeout in seconds for each request. | `10s` | :x: | :x: | :heavy_check_mark: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = command(\n",
    "    command=\"python -m online_endpoints_model_profiler --payload_path ${{inputs.payload}}\",\n",
    "    code=\".\",\n",
    "    experiment_name=\"profiling-job\",\n",
    "    display_name=f\"{profiler_compute_size}:1\",\n",
    "    environment=Environment(\n",
    "        image=\"mcr.microsoft.com/azureml/online-endpoints-model-profiler:latest\"\n",
    "    ),\n",
    "    environment_variables={\n",
    "        \"ONLINE_ENDPOINT\": endpoint.name,\n",
    "        \"DEPLOYMENT\": deployment.name,\n",
    "        \"PROFILING_TOOL\": \"wrk\",\n",
    "        \"DURATION\": \"10\",\n",
    "        \"CONNECTIONS\": \"1\",\n",
    "        \"TARGET_RPS\": \"50\",\n",
    "        \"CLIENTS\": \"1\",\n",
    "        \"TIMEOUT\": \"10\",\n",
    "        \"THREAD\": \"1\",\n",
    "    },\n",
    "    compute=profiler_compute_name,\n",
    "    inputs={\n",
    "        \"payload\": Input(\n",
    "            type=\"uri_file\",\n",
    "            path=payload.path,\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 View the profiling job in AzureML Studio\n",
    "The `Metrics` tab contains metrics gathered from the profiling job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.stream(name=job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Delete assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.compute.begin_delete(name=profiler_compute_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "551a8cf72b4d3415950b8ca8640992b04259286a8f1f283af90604d953b514a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
