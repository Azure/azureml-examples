{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy MLflow model to online endpoints\n",
    "Learn how to deploy your [MLflow](https://www.mlflow.org/) model to an [online endpoint](https://docs.microsoft.com/azure/machine-learning/concept-endpoints). When you deploy your MLflow model to an online endpoint, it's a no-code-deployment. It doesn't require scoring script and environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements - In order to benefit from this tutorial, you will need:\n",
    "- This sample notebook assumes you're using online endpoints; for more information, see [What are Azure Machine Learning endpoints?](https://docs.microsoft.com/azure/machine-learning/concept-endpoints).\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with computer cluster - [Configure workspace](../../jobs/configuration.ipynb)\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../README.md) - check the getting started section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "The [workspace](https://docs.microsoft.com/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# Import additional libraries for improved logging\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from IPython.display import display, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('azure-ml-notebook')\n",
    "logger.info('Notebook execution started')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your AML workspace\n",
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "try:\n",
    "    logger.info(f\"Connecting to workspace: {workspace} in resource group: {resource_group}\")\n",
    "    ml_client = MLClient(\n",
    "        DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    "    )\n",
    "    logger.info(f\"Successfully connected to workspace: {workspace}\")\n",
    "    # Display workspace information\n",
    "    print(f\"Workspace information:\")\n",
    "    print(f\"  Name: {ml_client.workspace_name}\")\n",
    "    print(f\"  Resource Group: {ml_client.resource_group_name}\")\n",
    "    print(f\"  Location: {ml_client.location}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error connecting to workspace: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you are working in a compute instance in Azure Machine Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working in a compute instance in Azure Machine Learning\n",
    "try:\n",
    "    logger.info(\"Connecting to workspace from compute instance configuration\")\n",
    "    ml_client = MLClient.from_config(DefaultAzureCredential())\n",
    "    logger.info(f\"Successfully connected to workspace: {ml_client.workspace_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error connecting from compute instance: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Online Endpoint\n",
    "\n",
    "Online endpoints are endpoints that are used for online (real-time) inferencing. Online endpoints contain deployments that are ready to receive data from clients and can send responses back in real time.\n",
    "\n",
    "To create an online endpoint we will use `ManagedOnlineEndpoint`. This class allows user to configure the following key aspects:\n",
    "\n",
    "- `name` - Name of the endpoint. Needs to be unique at the Azure region level\n",
    "- `auth_mode` - The authentication method for the endpoint. Key-based authentication and Azure ML token-based authentication are supported. Key-based authentication doesn't expire but Azure ML token-based authentication does. Possible values are `key` or `aml_token`.\n",
    "- `identity`- The managed identity configuration for accessing Azure resources for endpoint provisioning and inference.\n",
    "    - `type`- The type of managed identity. Azure Machine Learning supports `system_assigned` or `user_assigned identity`.\n",
    "    - `user_assigned_identities` - List (array) of fully qualified resource IDs of the user-assigned identities. This property is required is `identity.type` is user_assigned.\n",
    "- `description`- Description of the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Configure the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "logger.info(f\"Creating online endpoint with name: {online_endpoint_name}\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint for mlflow model\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"foo\": \"bar\"},\n",
    ")\n",
    "\n",
    "# Print endpoint configuration for better visibility\n",
    "print(f\"Endpoint configuration:\")\n",
    "print(f\"  Name: {endpoint.name}\")\n",
    "print(f\"  Auth Mode: {endpoint.auth_mode}\")\n",
    "print(f\"  Tags: {endpoint.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create the endpoint\n",
    "Using the `MLClient` created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the endpoint with progress updates\n",
    "try:\n",
    "    logger.info(\"Starting endpoint creation...\")\n",
    "    start_time = time.time()\n",
    "    result = ml_client.begin_create_or_update(endpoint).result()\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Endpoint creation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Display the endpoint information in a structured format\n",
    "    print(f\"\\nEndpoint created successfully:\")\n",
    "    print(f\"  Name: {result.name}\")\n",
    "    print(f\"  Provisioning State: {result.provisioning_state}\")\n",
    "    print(f\"  Created: {result.creation_context.created_on}\")\n",
    "    print(f\"  Scoring URI: {result.scoring_uri}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating endpoint: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a blue deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. We will create a deployment for our endpoint using the `ManagedOnlineDeployment` class. This class allows user to configure the following key aspects.\n",
    "\n",
    "- `name` - Name of the deployment.\n",
    "- `endpoint_name` - Name of the endpoint to create the deployment under.\n",
    "- `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
    "- `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](https://docs.microsoft.com/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
    "- `instance_count` - The number of instances to use for the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No code deployment\n",
    "For MLflow no-code-deployment (NCD) to work, setting `type` to `MLFLOW` is mandatory. \n",
    "When you deploy a MLflow model to managed online endpoint, scoring script and environment is generated for you.\n",
    "\n",
    "Azure ML models consist of the binary file(s) that represent a machine learning model and any corresponding metadata. Models can be created from a local file or directory. The created model will be tracked in the workspace under the specified name and version.\n",
    "\n",
    "The Model class can be used to create a model. It accepts the following parameters:\n",
    "\n",
    "- `name` - Name of the model.\n",
    "- `version` - Version of the model. If omitted, Azure ML will autogenerate a version.\n",
    "- `path` - Local path to the model file(s). This can point to either a file or a directory.\n",
    "- `type` - Storage format of the model. Applicable for no-code deployment scenarios. Allowed values are `CUSTOM`, `MLFLOW` and `TRITON`\n",
    "- `description` - Description of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configure the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "blue_deployment"
   },
   "outputs": [],
   "source": [
    "# create a blue deployment\n",
    "logger.info(\"Configuring model deployment\")\n",
    "try:\n",
    "    model = Model(\n",
    "        path=\"sklearn-diabetes/model\",\n",
    "        type=AssetTypes.MLFLOW_MODEL,\n",
    "        description=\"my sample mlflow model\",\n",
    "    )\n",
    "    logger.info(f\"Model configured: {model.path}\")\n",
    "    \n",
    "    blue_deployment = ManagedOnlineDeployment(\n",
    "        name=\"blue\",\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        model=model,\n",
    "        instance_type=\"Standard_F4s_v2\",\n",
    "        instance_count=1,\n",
    "    )\n",
    "    \n",
    "    # Print deployment configuration details\n",
    "    print(f\"Deployment configuration:\")\n",
    "    print(f\"  Name: {blue_deployment.name}\")\n",
    "    print(f\"  Endpoint: {blue_deployment.endpoint_name}\")\n",
    "    print(f\"  Model: {model.path} (Type: {model.type})\")\n",
    "    print(f\"  VM Size: {blue_deployment.instance_type}\")\n",
    "    print(f\"  Instance Count: {blue_deployment.instance_count}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error configuring deployment: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create the deployment\n",
    "\n",
    "Using the `MLClient` created earlier, we will now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "ml_client"
   },
   "outputs": [],
   "source": [
    "# Create the deployment with progress tracking\n",
    "try:\n",
    "    logger.info(\"Starting blue deployment creation...\")\n",
    "    print(\"Creating deployment (this may take several minutes)...\")\n",
    "    start_time = time.time()\n",
    "    deployment_result = ml_client.online_deployments.begin_create_or_update(blue_deployment).result()\n",
    "    end_time = time.time()\n",
    "    deployment_time = end_time - start_time\n",
    "    logger.info(f\"Deployment completed in {deployment_time:.2f} seconds\")\n",
    "    \n",
    "    # Display detailed deployment information\n",
    "    print(f\"\\nDeployment created successfully:\")\n",
    "    print(f\"  Name: {deployment_result.name}\")\n",
    "    print(f\"  Provisioning State: {deployment_result.provisioning_state}\")\n",
    "    print(f\"  Deployment Time: {deployment_time:.2f} seconds\")\n",
    "    print(f\"  Model: {deployment_result.model.name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating deployment: {str(e)}\")\n",
    "    print(f\"Deployment failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "endpoint.traffic"
   },
   "outputs": [],
   "source": [
    "# Update traffic allocation with better error handling\n",
    "try:\n",
    "    logger.info(\"Updating traffic allocation to 100% for blue deployment\")\n",
    "    endpoint.traffic = {\"blue\": 100}\n",
    "    traffic_update = ml_client.begin_create_or_update(endpoint).result()\n",
    "    logger.info(\"Traffic allocation updated successfully\")\n",
    "    \n",
    "    # Print traffic allocation details\n",
    "    print(f\"Traffic allocation updated:\")\n",
    "    print(f\"  Deployment 'blue': {endpoint.traffic['blue']}%\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error updating traffic allocation: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test the deployment\n",
    "\n",
    "Using the `MLClient` created earlier, we will get a handle to the endpoint. The endpoint can be invoked using the invoke command with the following parameters:\n",
    "\n",
    "- `endpoint_name` - Name of the endpoint\n",
    "- `request_file` - File with request data\n",
    "- `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We will send a sample request using a [sample-request-lightgbm.json](sample-request-lightgbm.json) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the deployment with improved visualization\n",
    "try:\n",
    "    logger.info(f\"Invoking endpoint {online_endpoint_name} with deployment 'blue'\")\n",
    "    print(\"Sending test request to endpoint...\")\n",
    "    \n",
    "    # Display the input data for reference\n",
    "    with open(\"sample-request-sklearn.json\", \"r\") as f:\n",
    "        request_data = json.load(f)\n",
    "    print(\"\\nRequest data:\")\n",
    "    display(JSON(request_data))\n",
    "    \n",
    "    # Invoke the endpoint\n",
    "    response = ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        deployment_name=\"blue\",\n",
    "        request_file=\"sample-request-sklearn.json\",\n",
    "    )\n",
    "    \n",
    "    # Display the response\n",
    "    print(\"\\nResponse:\")\n",
    "    try:\n",
    "        response_json = json.loads(response)\n",
    "        display(JSON(response_json))\n",
    "    except json.JSONDecodeError:\n",
    "        print(response)\n",
    "        \n",
    "    logger.info(\"Endpoint invocation completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error invoking endpoint: {str(e)}\")\n",
    "    print(f\"Endpoint invocation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Get endpoint details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the endpoint details with better visualization\n",
    "try:\n",
    "    logger.info(f\"Retrieving details for endpoint: {online_endpoint_name}\")\n",
    "    endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "    \n",
    "    # Print comprehensive endpoint details\n",
    "    print(f\"\\nEndpoint Details:\")\n",
    "    print(f\"  Name: {endpoint.name}\")\n",
    "    print(f\"  Scoring URI: {endpoint.scoring_uri}\")\n",
    "    print(f\"  Traffic Configuration: {endpoint.traffic}\")\n",
    "    print(f\"  Auth Mode: {endpoint.auth_mode}\")\n",
    "    print(f\"  Provisioning State: {endpoint.provisioning_state}\")\n",
    "    print(f\"  Created On: {endpoint.creation_context.created_on}\")\n",
    "    print(f\"  Created By: {endpoint.creation_context.created_by}\")\n",
    "    \n",
    "    # Get and display deployment details\n",
    "    print(\"\\nDeployment Details:\")\n",
    "    deployment = ml_client.online_deployments.get(name=\"blue\", endpoint_name=endpoint.name)\n",
    "    print(f\"  Name: {deployment.name}\")\n",
    "    print(f\"  Model: {deployment.model.name}\")\n",
    "    print(f\"  Instance Type: {deployment.instance_type}\")\n",
    "    print(f\"  Instance Count: {deployment.instance_count}\")\n",
    "    print(f\"  Provisioning State: {deployment.provisioning_state}\")\n",
    "    \n",
    "    logger.info(\"Successfully retrieved endpoint and deployment details\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving endpoint details: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Delete the deployment and endopoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint with confirmation and status updates\n",
    "try:\n",
    "    confirm = input(f\"Type 'yes' to confirm deletion of endpoint '{online_endpoint_name}': \")\n",
    "    if confirm.lower() == 'yes':\n",
    "        logger.info(f\"Starting deletion of endpoint {online_endpoint_name}\")\n",
    "        print(\"Deleting endpoint (this may take a few minutes)...\")\n",
    "        start_time = time.time()\n",
    "        operation = ml_client.online_endpoints.begin_delete(name=online_endpoint_name)\n",
    "        result = operation.result()\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Endpoint deletion completed in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Endpoint '{online_endpoint_name}' deleted successfully\")\n",
    "    else:\n",
    "        print(\"Deletion cancelled\")\n",
    "        logger.info(\"Endpoint deletion cancelled by user\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error deleting endpoint: {str(e)}\")\n",
    "    print(f\"Error deleting endpoint: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display notebook execution summary\n",
    "logger.info(\"Notebook execution completed\")\n",
    "print(\"\\nNotebook Execution Summary:\")\n",
    "print(f\"  Endpoint Name: {online_endpoint_name}\")\n",
    "print(f\"  Model Type: MLflow model (sklearn-diabetes)\")\n",
    "print(f\"  Deployment Name: blue\")\n",
    "print(\"\\nResources have been cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Deploy an mlflow model to an online endpoint. This will be a no-code-deployment. It doesn't require scoring script and environment."
  },
  "interpreter": {
   "hash": "f6c16bbccc10ca03d07b4bd30ccd59ce8e25aee703e3c8d834bba9b524ce7685"
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
