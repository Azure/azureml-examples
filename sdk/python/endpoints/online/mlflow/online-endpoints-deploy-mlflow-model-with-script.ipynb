{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy MLflow model to online endpoints with a custom environment and scoring script\n",
    "\n",
    "Learn how to deploy your [MLflow](https://www.mlflow.org/) model to an [online endpoint](https://docs.microsoft.com/azure/machine-learning/concept-endpoints) using a custom environment and scoring script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements - In order to benefit from this tutorial, you will need:\n",
    "- This sample notebook assumes you're using online endpoints; for more information, see [What are Azure Machine Learning endpoints?](https://docs.microsoft.com/azure/machine-learning/concept-endpoints).\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with computer cluster - [Configure workspace](../../jobs/configuration.ipynb)\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../README.md) - check the getting started section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "The [workspace](https://docs.microsoft.com/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# Additional imports for logging\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "from IPython.display import display, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up basic logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger('mlflow-deploy')\n",
    "\n",
    "# Check if running in CI environment\n",
    "is_ci = os.environ.get('CI', 'false').lower() == 'true'\n",
    "\n",
    "# Simple wrapper to log section headers\n",
    "def log_section(title):\n",
    "    logger.info(f\"\\n{'=' * 30}\\n{title}\\n{'=' * 30}\")\n",
    "\n",
    "logger.info(\"Starting MLflow model deployment notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your AML workspace\n",
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace = \"<AML_WORKSPACE_NAME>\"\n",
    "\n",
    "# Use environment variables in CI environments\n",
    "if os.environ.get('CI', 'false').lower() == 'true':\n",
    "    subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\", subscription_id)\n",
    "    resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\", resource_group)\n",
    "    workspace = os.environ.get(\"AZURE_ML_WORKSPACE\", workspace)\n",
    "    logger.info(\"Using environment variables for workspace configuration\")\n",
    "\n",
    "logger.info(f\"Workspace configuration: {workspace} in {resource_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "try:\n",
    "    log_section(\"Connecting to Azure ML Workspace\")\n",
    "    ml_client = MLClient(\n",
    "        DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    "    )\n",
    "    logger.info(f\"Successfully connected to {ml_client.workspace_name} in {ml_client.location}\")\n",
    "    \n",
    "    # Print workspace information for user\n",
    "    print(f\"Workspace: {ml_client.workspace_name}\")\n",
    "    print(f\"Resource Group: {ml_client.resource_group_name}\")\n",
    "    print(f\"Location: {ml_client.location}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to connect to workspace: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Workspace connection failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you are working in a compute instance in Azure Machine Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working in a compute instance in Azure Machine Learning\n",
    "try:\n",
    "    if 'AZUREML_SERVICE_ENDPOINT' in os.environ:\n",
    "        logger.info(\"Using compute instance connection method\")\n",
    "        ml_client = MLClient.from_config(DefaultAzureCredential())\n",
    "        logger.info(f\"Connected to workspace: {ml_client.workspace_name}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Note: Compute instance connection attempt failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Online Endpoint\n",
    "\n",
    "Online endpoints are endpoints that are used for online (real-time) inferencing. Online endpoints contain deployments that are ready to receive data from clients and can send responses back in real time.\n",
    "\n",
    "To create an online endpoint we will use `ManagedOnlineEndpoint`. This class allows user to configure the following key aspects:\n",
    "\n",
    "- `name` - Name of the endpoint. Needs to be unique at the Azure region level\n",
    "- `auth_mode` - The authentication method for the endpoint. Key-based authentication and Azure ML token-based authentication are supported. Key-based authentication doesn't expire but Azure ML token-based authentication does. Possible values are `key` or `aml_token`.\n",
    "- `identity`- The managed identity configuration for accessing Azure resources for endpoint provisioning and inference.\n",
    "    - `type`- The type of managed identity. Azure Machine Learning supports `system_assigned` or `user_assigned identity`.\n",
    "    - `user_assigned_identities` - List (array) of fully qualified resource IDs of the user-assigned identities. This property is required is `identity.type` is user_assigned.\n",
    "- `description`- Description of the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Configure the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import datetime\n",
    "\n",
    "log_section(\"Creating Online Endpoint\")\n",
    "\n",
    "# Create unique name with timestamp and random suffix\n",
    "timestamp = datetime.datetime.now().strftime(\"%m%d%H%M\")\n",
    "suffix = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(5))\n",
    "endpoint_name = f\"diabetes-{timestamp}-{suffix}\"\n",
    "\n",
    "# Add CI prefix if running in CI environment\n",
    "if is_ci:\n",
    "    endpoint_name = f\"ci-{endpoint_name}\"\n",
    "\n",
    "logger.info(f\"Endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an online endpoint\n",
    "try:\n",
    "    logger.info(\"Configuring endpoint\")\n",
    "    \n",
    "    # Add CI-specific tags if in CI environment\n",
    "    tags = {\"foo\": \"bar\"} \n",
    "    if is_ci:\n",
    "        tags.update({\n",
    "            \"ci_run\": \"true\",\n",
    "            \"run_id\": os.environ.get(\"GITHUB_RUN_ID\", \"unknown\")\n",
    "        })\n",
    "    \n",
    "    endpoint = ManagedOnlineEndpoint(\n",
    "        name=endpoint_name,\n",
    "        description=\"Online endpoint for diabetes prediction using MLflow model\",\n",
    "        auth_mode=\"key\",\n",
    "        tags=tags,\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Endpoint configuration completed\")\n",
    "    print(f\"Endpoint: {endpoint.name}\")\n",
    "    print(f\"Auth Mode: {endpoint.auth_mode}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error configuring endpoint: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Endpoint configuration failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create the endpoint\n",
    "Using the `MLClient` created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the endpoint in the workspace\n",
    "try:\n",
    "    logger.info(\"Creating endpoint - this may take several minutes...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create the endpoint\n",
    "    result = ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "    \n",
    "    # Log completion info\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"Endpoint created successfully in {duration:.1f} seconds\")\n",
    "    logger.info(f\"Scoring URI: {result.scoring_uri}\")\n",
    "    \n",
    "    # Display details for user\n",
    "    print(f\"\\nEndpoint created:\")\n",
    "    print(f\"  Name: {result.name}\")\n",
    "    print(f\"  Scoring URI: {result.scoring_uri}\")\n",
    "    print(f\"  State: {result.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create endpoint: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Endpoint creation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a blue deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. We will create a deployment for our endpoint using the `ManagedOnlineDeployment` class. This class allows user to configure the following key aspects.\n",
    "\n",
    "- `name` - Name of the deployment.\n",
    "- `endpoint_name` - Name of the endpoint to create the deployment under.\n",
    "- `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
    "- `environment` - The environment where the model will run.\n",
    "- `code_configuration` - The scoring script used to serve the model.\n",
    "- `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](https://docs.microsoft.com/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
    "- `instance_count` - The number of instances to use for the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configure the deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Register the model\n",
    "try:\n",
    "    log_section(\"Registering Model\")\n",
    "    logger.info(\"Configuring model\")\n",
    "    \n",
    "    # Use consistent naming for the model\n",
    "    model_name = \"sklearn-diabetes\"\n",
    "    model_path = \"sklearn-diabetes/model\"\n",
    "    \n",
    "    # Log model information \n",
    "    logger.info(f\"Model path: {model_path}\")\n",
    "    \n",
    "    # Verify model path exists\n",
    "    if not os.path.exists(model_path):\n",
    "        logger.error(f\"Model path not found: {model_path}\")\n",
    "        raise FileNotFoundError(f\"Model path not found: {model_path}\")\n",
    "    \n",
    "    model = Model(\n",
    "        path=model_path,\n",
    "        type=AssetTypes.MLFLOW_MODEL,\n",
    "        description=\"MLflow model for diabetes prediction\",\n",
    "    )\n",
    "    logger.info(f\"Model configured: {model.path} (Type: {model.type})\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error configuring model: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Model configuration failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an environment to perform inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create custom environment for model deployment\n",
    "try:\n",
    "    log_section(\"Creating Environment\")\n",
    "    logger.info(\"Configuring environment\")\n",
    "    \n",
    "    conda_file = \"sklearn-diabetes/environment/conda.yaml\"\n",
    "    \n",
    "    # Verify conda file exists\n",
    "    if not os.path.exists(conda_file):\n",
    "        logger.error(f\"Conda file not found: {conda_file}\")\n",
    "        raise FileNotFoundError(f\"Conda file not found: {conda_file}\")\n",
    "    \n",
    "    environment = Environment(\n",
    "        conda_file=conda_file,\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Environment configured with conda file: {conda_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error configuring environment: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Environment configuration failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "blue_deployment"
   },
   "outputs": [],
   "source": [
    "# Configure the deployment\n",
    "try:\n",
    "    log_section(\"Configuring Deployment\")\n",
    "    logger.info(\"Setting up deployment configuration\")\n",
    "    \n",
    "    # Validate code path and scoring script\n",
    "    code_path = \"sklearn-diabetes/src\"\n",
    "    scoring_script = \"score.py\"\n",
    "    full_script_path = os.path.join(code_path, scoring_script)\n",
    "    \n",
    "    if not os.path.exists(code_path):\n",
    "        logger.error(f\"Code directory not found: {code_path}\")\n",
    "        raise FileNotFoundError(f\"Code directory not found: {code_path}\")\n",
    "        \n",
    "    if not os.path.exists(full_script_path):\n",
    "        logger.error(f\"Scoring script not found: {full_script_path}\")\n",
    "        raise FileNotFoundError(f\"Scoring script not found: {full_script_path}\")\n",
    "    \n",
    "    logger.info(f\"Using scoring script: {full_script_path}\")\n",
    "    \n",
    "    # Create deployment config\n",
    "    blue_deployment = ManagedOnlineDeployment(\n",
    "        name=\"blue\",\n",
    "        endpoint_name=endpoint_name,\n",
    "        model=model,\n",
    "        environment=environment,\n",
    "        code_configuration=CodeConfiguration(\n",
    "            code=code_path, scoring_script=scoring_script\n",
    "        ),\n",
    "        instance_type=\"Standard_F4s_v2\",\n",
    "        instance_count=1,\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Configured deployment '{blue_deployment.name}' with {blue_deployment.instance_count}x {blue_deployment.instance_type}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error configuring deployment: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Deployment configuration failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create the deployment\n",
    "\n",
    "Using the `MLClient` created earlier, we will now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "ml_client"
   },
   "outputs": [],
   "source": [
    "# create the deployment \n",
    "try:\n",
    "    log_section(\"Creating Deployment\")\n",
    "    logger.info(\"Starting deployment - this may take 5-10 minutes...\")\n",
    "    print(\"Creating deployment (this will take several minutes)...\")\n",
    "    \n",
    "    # Track deployment time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create the deployment\n",
    "    result = ml_client.online_deployments.begin_create_or_update(blue_deployment).result()\n",
    "    \n",
    "    # Log completion information\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"Deployment completed in {duration/60:.1f} minutes\")\n",
    "    \n",
    "    # Display deployment details\n",
    "    print(f\"\\nDeployment created:\")\n",
    "    print(f\"  Name: {result.name}\")\n",
    "    print(f\"  State: {result.provisioning_state}\")\n",
    "    print(f\"  Model: {result.model.name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Deployment failed: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Deployment creation failed: {str(e)}\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "endpoint.traffic"
   },
   "outputs": [],
   "source": [
    "# Update endpoint traffic to use the blue deployment\n",
    "try:\n",
    "    log_section(\"Updating Traffic Allocation\")\n",
    "    logger.info(\"Setting traffic allocation to blue deployment\")\n",
    "    \n",
    "    # Update the traffic allocation\n",
    "    endpoint.traffic = {\"blue\": 100}\n",
    "    ml_client.begin_create_or_update(endpoint).result()\n",
    "    \n",
    "    logger.info(\"Traffic successfully allocated to blue deployment\")\n",
    "    print(\"All traffic is now directed to the 'blue' deployment\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to update traffic allocation: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Traffic allocation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test the deployment\n",
    "\n",
    "Using the `MLClient` created earlier, we will get a handle to the endpoint. The endpoint can be invoked using the invoke command with the following parameters:\n",
    "\n",
    "- `endpoint_name` - Name of the endpoint\n",
    "- `request_file` - File with request data\n",
    "- `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We will send a sample request using a [sample-request-sklearn-custom.json](sample-request-sklearn-custom.json) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment\n",
    "try:\n",
    "    log_section(\"Testing Deployment\")\n",
    "    logger.info(\"Sending test request to the endpoint\")\n",
    "    \n",
    "    # Check if request file exists\n",
    "    request_file = \"sample-request-sklearn.json\"\n",
    "    if not os.path.exists(request_file):\n",
    "        logger.error(f\"Request file not found: {request_file}\")\n",
    "        raise FileNotFoundError(f\"Request file not found: {request_file}\")\n",
    "    \n",
    "    # Show request data for reference\n",
    "    with open(request_file, \"r\") as f:\n",
    "        request_data = json.load(f)\n",
    "        print(\"Request data:\")\n",
    "        display(JSON(request_data))\n",
    "    \n",
    "    # Invoke the endpoint\n",
    "    start_time = time.time()\n",
    "    response = ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=endpoint_name,\n",
    "        deployment_name=\"blue\",\n",
    "        request_file=request_file,\n",
    "    )\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # Log and display the response\n",
    "    logger.info(f\"Request completed in {duration:.2f} seconds\")\n",
    "    print(\"\\nResponse:\")\n",
    "    \n",
    "    # Try to parse and display as JSON\n",
    "    try:\n",
    "        response_json = json.loads(response)\n",
    "        display(JSON(response_json))\n",
    "        logger.info(\"Response received successfully\")\n",
    "    except json.JSONDecodeError:\n",
    "        # Fall back to plain text display\n",
    "        print(response)\n",
    "        logger.info(\"Response is not JSON format\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error testing endpoint: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Endpoint testing failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Get endpoint details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the details for online endpoint\n",
    "try:\n",
    "    log_section(\"Getting Endpoint Details\")\n",
    "    logger.info(f\"Retrieving endpoint details for {endpoint_name}\")\n",
    "    \n",
    "    # Get endpoint details\n",
    "    endpoint = ml_client.online_endpoints.get(name=endpoint_name)\n",
    "    \n",
    "    # Log key information\n",
    "    logger.info(f\"Endpoint state: {endpoint.provisioning_state}\")\n",
    "    logger.info(f\"Scoring URI: {endpoint.scoring_uri}\")\n",
    "    \n",
    "    # Display details for user\n",
    "    print(f\"Endpoint: {endpoint.name}\")\n",
    "    print(f\"Scoring URI: {endpoint.scoring_uri}\")\n",
    "    print(f\"Auth mode: {endpoint.auth_mode}\")\n",
    "    \n",
    "    # Get and display deployment details\n",
    "    deployment = ml_client.online_deployments.get(name=\"blue\", endpoint_name=endpoint_name)\n",
    "    print(f\"\\nDeployment: {deployment.name}\")\n",
    "    print(f\"Model: {deployment.model.name}\")\n",
    "    print(f\"VM Size: {deployment.instance_type}\")\n",
    "    print(f\"Instance count: {deployment.instance_count}\")\n",
    "    \n",
    "    # Output variables for CI/CD pipelines\n",
    "    if is_ci:\n",
    "        print(f\"::set-output name=endpoint_name::{endpoint.name}\")\n",
    "        print(f\"::set-output name=scoring_uri::{endpoint.scoring_uri}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving endpoint details: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Getting endpoint details failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Delete the deployment and endopoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "try:\n",
    "    log_section(\"Cleaning Up Resources\")\n",
    "    \n",
    "    # Determine if deletion should happen\n",
    "    should_delete = False\n",
    "    if is_ci:\n",
    "        logger.info(\"CI environment detected - will clean up automatically\")\n",
    "        should_delete = True\n",
    "    else:\n",
    "        # Ask for confirmation in interactive mode\n",
    "        confirm = input(f\"Do you want to delete the endpoint '{endpoint_name}'? (yes/no): \")\n",
    "        should_delete = confirm.lower() in ['y', 'yes']\n",
    "        if not should_delete:\n",
    "            logger.info(\"Deletion cancelled by user\")\n",
    "            print(f\"⚠️  Remember to delete the endpoint '{endpoint_name}' when no longer needed\")\n",
    "    \n",
    "    # Perform deletion if confirmed\n",
    "    if should_delete:\n",
    "        logger.info(f\"Deleting endpoint {endpoint_name}\")\n",
    "        print(\"Deleting endpoint...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        ml_client.online_endpoints.begin_delete(name=endpoint_name).result()\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"Endpoint deleted in {duration:.1f} seconds\")\n",
    "        print(f\"Endpoint '{endpoint_name}' deleted successfully\")\n",
    "        \n",
    "        # Set output for CI pipelines\n",
    "        if is_ci:\n",
    "            print(\"::set-output name=cleanup_status::success\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error deleting endpoint: {str(e)}\")\n",
    "    if is_ci:\n",
    "        print(f\"::error::Endpoint deletion failed: {str(e)}\")\n",
    "        print(\"::set-output name=cleanup_status::failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of notebook execution\n",
    "log_section(\"Notebook Summary\")\n",
    "logger.info(\"Notebook execution completed\")\n",
    "\n",
    "print(\"\\nNotebook Execution Summary:\")\n",
    "print(f\"  Endpoint: {endpoint_name}\")\n",
    "print(f\"  Deployment: blue (Standard_F4s_v2)\")\n",
    "print(f\"  Model: MLflow diabetes model\")\n",
    "\n",
    "# Verify resources were cleaned up\n",
    "try:\n",
    "    # Try to get the endpoint - this should fail if it was deleted\n",
    "    ml_client.online_endpoints.get(name=endpoint_name)\n",
    "    print(f\"\\n⚠️  Warning: Endpoint '{endpoint_name}' still exists and may incur costs\")\n",
    "except:\n",
    "    print(\"\\n✅ All resources have been cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Deploy an mlflow model to an online endpoint. This will be a no-code-deployment. It doesn't require scoring script and environment."
  },
  "interpreter": {
   "hash": "f6c16bbccc10ca03d07b4bd30ccd59ce8e25aee703e3c8d834bba9b524ce7685"
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
