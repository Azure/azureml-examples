{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Evaluating Distilled Models with AzureML\n",
    "\n",
    "In this notebook, you will learn how to run evaluations of opensource distilled model using the AzureML SDK. Along with this notebook, we've included a preconfigured set of 5 tasks using well-known public datasets.\n",
    "\n",
    "*Disclaimer: This notebook has been tested against MaaS endpoints for Llama 3.1. Other deployments or model versions are not guaranteed to work with the evaluation pipelines distributed with this notebook.*  \n",
    "\n",
    "## Prerequistes\n",
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace - [Configure workspace](../../configuration.ipynb)\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section\n",
    "- A python environment with [mlflow](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow-configure-tracking?view=azureml-api-2&tabs=python%2Cmlflow) for retrieving eval metrics\n",
    "- Distilled model endpoint url and connection name for workspace connection\n",
    "\n",
    "## Supported tasks\n",
    "- Summarization\n",
    "- Math\n",
    "- NLI\n",
    "\n",
    "Note that evaluation pipelines automatically download relevant datasets from public sources.\n",
    "\n",
    "You can also set the sample ratio, the fraction of the selected dataset to run for the eval.\n",
    "\n",
    "**Warning**: Many datasets contain thousands of examples which can lead to high endpoint usage costs. We advise starting with a small sample ratio (e.g., 1%) to verify the pipeline and then increasing the ratio if desired. Note that benchmark metrics obtained with small sample ratios may not be comparable between different models. Please use sample_ratio=1 for model comparisons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AzureML settings - please fill in your values\n",
    "subscription_id = \"<Azure subscription ID>\"\n",
    "resource_group = \"<Resource group>\"\n",
    "workspace_name = \"<Workspace name\"\n",
    "experiment_name = \"<Experiment name>\"\n",
    "\n",
    "# Eval to run - you can change this to any of the 5 supported task names\n",
    "# Supported evals: text-summarization\n",
    "task_name = \"text-summarization\"\n",
    "eval_name = \"dialogsum\"\n",
    "\n",
    "# Distilled model settings\n",
    "endpoint_url = \"endpoint_url\"\n",
    "\n",
    "# Name of the connection in your Workspace storing access keys\n",
    "connection_name = \"<Connection name>\"\n",
    "\n",
    "# Sample ratio - what fraction of the dataset to run for the eval?\n",
    "# **WARNING** be aware of endpoint costs!\n",
    "sample_ratio = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to get an `MLClient` for communicating with your Workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# client for AzureML Workspace actions\n",
    "ml_client = MLClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group_name=resource_group,\n",
    "    workspace_name=workspace_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell launches the evaluation pipeline job using [serverless compute](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-serverless-compute) by default. You can optionally [create your own compute cluster](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster) and use it to execute the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_job\n",
    "\n",
    "# load the pipeline from the yaml def\n",
    "pipeline_job = load_job(f\"./pipelines/summarization/{eval_name}.yaml\")\n",
    "\n",
    "# Set pipeline job inputs\n",
    "pipeline_job.inputs.endpoint_url = endpoint_url\n",
    "pipeline_job.inputs.connection_name = connection_name\n",
    "pipeline_job.inputs.sample_ratio = sample_ratio\n",
    "\n",
    "# Optionally use your own compute cluster\n",
    "# pipeline_job.settings.default_compute = \"<Your compute cluster name>\"\n",
    "\n",
    "# Start the job in the Workspace\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=experiment_name\n",
    ")\n",
    "returned_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to stream the job. Notebook execution will be paused until the job finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve metrics from the run\n",
    "When the pipeline finishes, you can retrieve evaluation metrics from the run via mlflow. The primary measure of accuracy for the evals is task dependant eg. `summarization` performance is usually measured in `rouge` scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "accuracy_metric_name = \"rouge1\"\n",
    "\n",
    "mlflow_tracking_uri = ml_client.workspaces.get(\n",
    "    ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "run = mlflow.get_run(run_id=returned_job.name)\n",
    "metric_val = run.data.metrics[accuracy_metric_name]\n",
    "\n",
    "if sample_ratio < 1.0:\n",
    "    print(\n",
    "        f\"**Warning** sample_ratio is {sample_ratio}. Use sample_ratio=1.0 when comparing metrics between models.\"\n",
    "    )\n",
    "\n",
    "print(f\"Eval: {eval_name}\")\n",
    "print(f\"Sample ratio: {sample_ratio}\")\n",
    "print(f\"Accuracy metric name: {accuracy_metric_name}\")\n",
    "print(f\"Accuracy metric value: {metric_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
