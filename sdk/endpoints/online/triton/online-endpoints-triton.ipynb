{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a model to online endpoints using Triton\n",
    "Learn how to deploy a model using Triton as an online endpoint in Azure Machine Learning.\n",
    "\n",
    "Triton is multi-framework, open-source software that is optimized for inference. It supports popular machine learning frameworks like TensorFlow, ONNX Runtime, PyTorch, NVIDIA TensorRT, and more. It can be used for your CPU or GPU workloads.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n",
    "\n",
    "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
    "\n",
    "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
    "\n",
    "* You must have an Azure Machine Learning workspace. \n",
    "\n",
    "### Please note, for Triton no-code-deployment, testing via local endpoints is currently not supported, so this tutorial will only show how to set up on online endpoint.\n",
    "\n",
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-ml in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (0.1.0b6)\n",
      "Requirement already satisfied: docker in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (6.0.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake<12.8.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (12.7.0)\n",
      "Requirement already satisfied: isodate in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (0.6.1)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (1.1.28)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (6.0)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (0.6.21)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (1.3.0)\n",
      "Requirement already satisfied: pathspec==0.9.* in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (0.9.0)\n",
      "Requirement already satisfied: azure-storage-blob<12.13.0,>=12.10.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (12.12.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (4.14.0)\n",
      "Requirement already satisfied: azure-storage-file-share<12.9.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (12.8.0)\n",
      "Requirement already satisfied: azure-core!=1.22.0,<2.0.0,>=1.8.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (1.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (4.2.0)\n",
      "Requirement already satisfied: tqdm<=4.63.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (4.63.0)\n",
      "Requirement already satisfied: azure-identity in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (1.10.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (2.4.0)\n",
      "Requirement already satisfied: pydash<=4.9.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (4.9.0)\n",
      "Requirement already satisfied: applicationinsights<=0.11.10 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (0.11.10)\n",
      "Requirement already satisfied: colorama<=0.4.4 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (0.4.4)\n",
      "Requirement already satisfied: strictyaml<=1.6.1 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (1.6.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.5 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-ai-ml) (3.17.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from docker->azure-ai-ml) (2.27.1)\n",
      "Requirement already satisfied: packaging>=14.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from docker->azure-ai-ml) (21.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from docker->azure-ai-ml) (1.26.9)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from docker->azure-ai-ml) (1.3.2)\n",
      "Requirement already satisfied: six in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from isodate->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (2022.5.18.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (1.3.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-storage-blob<12.13.0,>=12.10.0->azure-ai-ml) (37.0.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (22.1.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (5.9.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.18.1)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-identity->azure-ai-ml) (1.17.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from azure-identity->azure-ai-ml) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from strictyaml<=1.6.1->azure-ai-ml) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from requests>=2.26.0->docker->azure-ai-ml) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from requests>=2.26.0->docker->azure-ai-ml) (2.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from packaging>=14.0->docker->azure-ai-ml) (3.0.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-blob<12.13.0,>=12.10.0->azure-ai-ml) (1.15.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (3.8.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->azure-ai-ml) (2.4.0)\n",
      "Requirement already satisfied: pycparser in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<12.13.0,>=12.10.0->azure-ai-ml) (2.21)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements (Remove)\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, AzureCliCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your AML workspace\n",
    "subscription_id = \"f57ce3c6-5c6f-4f1e-8cba-b782d8974590\" # Remove\n",
    "resource_group = \"rg-azureml-pg\" # Remove\n",
    "workspace = \"aml-pg-01\" # Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FDRCQDFXU to authenticate.\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!az login --use-device-code\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    AzureCliCredential(), subscription_id, resource_group, workspace # Restore to default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Install Additional Requirements\n",
    "\n",
    "Install Python requirements using the following command. These will be used for scoring\n",
    "\n",
    "pip install numpy tritonclient[http] pillow gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (1.22.4)\n",
      "Collecting tritonclient[http]\n",
      "  Downloading tritonclient-2.25.0-py3-none-manylinux1_x86_64.whl (11.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.4 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (9.2.0)\n",
      "Collecting gevent\n",
      "  Downloading gevent-21.12.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 68.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 68.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting geventhttpclient>=1.4.4; extra == \"http\"\n",
      "  Downloading geventhttpclient-2.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 12.0 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp>=3.8.1; extra == \"http\"\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 69.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from gevent) (65.3.0)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.4.0-cp38-cp38-manylinux2010_x86_64.whl (259 kB)\n",
      "\u001b[K     |████████████████████████████████| 259 kB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zope.event\n",
      "  Using cached zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting greenlet<2.0,>=1.1.0; platform_python_implementation == \"CPython\"\n",
      "  Downloading greenlet-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from geventhttpclient>=1.4.4; extra == \"http\"->tritonclient[http]) (2022.5.18.1)\n",
      "Requirement already satisfied: six in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from geventhttpclient>=1.4.4; extra == \"http\"->tritonclient[http]) (1.16.0)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[K     |████████████████████████████████| 357 kB 65.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from aiohttp>=3.8.1; extra == \"http\"->tritonclient[http]) (22.1.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 70.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[K     |████████████████████████████████| 262 kB 69.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from aiohttp>=3.8.1; extra == \"http\"->tritonclient[http]) (2.0.12)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 69.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.0 in /data/workspace/dalabrun/venv/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1; extra == \"http\"->tritonclient[http]) (3.3)\n",
      "Installing collected packages: python-rapidjson, zope.interface, zope.event, greenlet, gevent, brotli, geventhttpclient, async-timeout, frozenlist, aiosignal, multidict, yarl, aiohttp, tritonclient\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 brotli-1.0.9 frozenlist-1.3.1 gevent-21.12.0 geventhttpclient-2.0 greenlet-1.1.3 multidict-6.0.2 python-rapidjson-1.8 tritonclient-2.25.0 yarl-1.8.1 zope.event-4.5.0 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !BASE_PATH=endpoints/online/triton/single-model # Sets base bath env variable\n",
    "# !export ENDPOINT_NAME=triton-single-endpt-`echo $RANDOM` # Sets endpoint name\n",
    "# !{sys.executable} -m pip install numpy tritonclient[http] pillow gevent # Install python requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deploy your online endpoint to Azure\n",
    "Next, deploy your online endpoint to Azure.\n",
    "\n",
    "## 3.1 Configure online endpoint\n",
    "`endpoint_name`: The name of the endpoint. It must be unique in the Azure region. Naming rules are defined under [managed online endpoint limits](https://docs.microsoft.com/azure/machine-learning/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints-preview).\n",
    "\n",
    "`auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` does not expire, but `aml_token` does expire. \n",
    "\n",
    "Optionally, you can add description, tags to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"foo\": \"bar\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create the endpoint\n",
    "Using the `MLClient` created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-08261710825185.westus2.inference.ml.azure.com/score', 'swagger_uri': 'https://endpoint-08261710825185.westus2.inference.ml.azure.com/swagger.json', 'name': 'endpoint-08261710825185', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourcegroups/rg-azureml-pg/providers/microsoft.machinelearningservices/workspaces/aml-pg-01/onlineendpoints/endpoint-08261710825185', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/providers/Microsoft.MachineLearningServices/locations/westus2/mfeOperationsStatus/oe:fb0892ae-a828-4c36-a8cb-ce6ad3c8a0fd:e1496590-f7e6-4fd1-ae7a-f35ec177d58e?api-version=2022-02-01-preview'}, 'id': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourceGroups/rg-azureml-pg/providers/Microsoft.MachineLearningServices/workspaces/aml-pg-01/onlineEndpoints/endpoint-08261710825185', 'Resource__source_path': None, 'base_path': '/data/workspace/dalabrun/azureml-examples/sdk/endpoints/online/triton', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f1e5460b550>, 'auth_mode': 'key', 'location': 'westus2', 'identity': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedServiceIdentity object at 0x7f1e556fdb80>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Configure online deployment\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. We will create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n",
    "\n",
    "### Key aspects of deployment \n",
    "- `name` - Name of the deployment.\n",
    "- `endpoint_name` - Name of the endpoint to create the deployment under.\n",
    "- `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
    "- `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.\n",
    "- `code_configuration` - the configuration for the source code and scoring script\n",
    "    - `path`- Path to the source code directory for scoring the model\n",
    "    - `scoring_script` - Relative path to the scoring file in the source code directory\n",
    "- `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](https://docs.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
    "- `instance_count` - The number of instances to use for the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a blue deployment\n",
    "model = Model(name=\"sample-densenet-onnx-model\", version=\"1\", path=\"./models\", type=\"triton_model\")\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_NC6s_v3\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readiness route vs. liveness route\n",
    "An HTTP server defines paths for both liveness and readiness. A liveness route is used to check whether the server is running. A readiness route is used to check whether the server is ready to do work. In machine learning inference, a server could respond 200 OK to a liveness request before loading a model. The server could respond 200 OK to a readiness request only after the model has been loaded into memory.\n",
    "\n",
    "Review the [Kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) for more information about liveness and readiness probes.\n",
    "\n",
    "Notice that this deployment uses the same path for both liveness and readiness, since TF Serving only defines a liveness route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create the deployment\n",
    "Using the `MLClient` created earlier, we will now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint endpoint-08261710825185 exists\n",
      "\u001b[32mUploading models (32.72 MBs): 100%|██████████| 32719461/32719461 [00:00<00:00, 46065399.42it/s]\n",
      "\u001b[39m\n",
      "\n",
      "Creating/updating online deployment blue "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done (9m 51s)\n"
     ]
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-08261710825185.westus2.inference.ml.azure.com/', 'swagger_uri': 'https://endpoint-08261710825185.westus2.inference.ml.azure.com/swagger.json', 'name': 'endpoint-08261710825185', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourcegroups/rg-azureml-pg/providers/microsoft.machinelearningservices/workspaces/aml-pg-01/onlineendpoints/endpoint-08261710825185', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/providers/Microsoft.MachineLearningServices/locations/westus2/mfeOperationsStatus/oe:fb0892ae-a828-4c36-a8cb-ce6ad3c8a0fd:cbb71e63-ba82-41b8-889f-44b1bfa5effb?api-version=2022-02-01-preview'}, 'id': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourceGroups/rg-azureml-pg/providers/Microsoft.MachineLearningServices/workspaces/aml-pg-01/onlineEndpoints/endpoint-08261710825185', 'Resource__source_path': None, 'base_path': '/data/workspace/dalabrun/azureml-examples/sdk/endpoints/online/triton', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f1e556fd430>, 'auth_mode': 'key', 'location': 'westus2', 'identity': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedServiceIdentity object at 0x7f1e556fd0d0>, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test the endpoint with sample data\n",
    "Using the `MLClient` created earlier, we will get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n",
    "- `endpoint_name` - Name of the endpoint\n",
    "- `request_file` - File with request data\n",
    "- `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We will send a sample request using a [json](./model-1/sample-request.json) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue': 100}\n",
      "https://endpoint-08261710825185.westus2.inference.ml.azure.com/\n"
     ]
    }
   ],
   "source": [
    "# Get the details for online endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the scoring_url and auth token for your AML deployment\n",
    "\n",
    "scoring_uri = \"https://endpoint-08261710825185.westus2.inference.ml.azure.com/\"\n",
    "auth_token = \"idgctxWJZCrI7qxdrCu9ZEbMnhs5cIYl\"\n",
    "img_url = \"https://aka.ms/peacock-pic\" # This is a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is server ready - True\n",
      "Is model ready - True\n",
      "/data/workspace/dalabrun/azureml-examples/sdk/endpoints/online/triton/scoring_utils/densenet_labels.txt\n",
      "84 : PEACOCK\n"
     ]
    }
   ],
   "source": [
    "# test the blue deployment with some sample data\n",
    "from scoring_utils import prepost\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import gevent.ssl\n",
    "import tritonclient.http as tritonhttpclient\n",
    "\n",
    "# We remove the scheme from the url\n",
    "scoring_uri = scoring_uri[8:]\n",
    "\n",
    "# Initialize client handler \n",
    "triton_client = tritonhttpclient.InferenceServerClient(\n",
    "        url=scoring_uri,\n",
    "        ssl=True,\n",
    "        ssl_context_factory=gevent.ssl._create_default_https_context,\n",
    "    )\n",
    "\n",
    "# Create headers\n",
    "headers = {}\n",
    "headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n",
    "\n",
    "# Check status of triton server\n",
    "health_ctx = triton_client.is_server_ready(headers=headers)\n",
    "print(\"Is server ready - {}\".format(health_ctx))\n",
    "\n",
    "# Check status of model\n",
    "model_name = \"model_1\"\n",
    "status_ctx = triton_client.is_model_ready(model_name, \"1\", headers)\n",
    "print(\"Is model ready - {}\".format(status_ctx))\n",
    "\n",
    "img_content = requests.get(img_url).content\n",
    "img_data = prepost.preprocess(img_content)\n",
    "\n",
    "# Populate inputs and outputs\n",
    "input = tritonhttpclient.InferInput(\"data_0\", img_data.shape, \"FP32\")\n",
    "input.set_data_from_numpy(img_data)\n",
    "inputs = [input]\n",
    "output = tritonhttpclient.InferRequestedOutput(\"fc6_1\")\n",
    "outputs = [output]\n",
    "\n",
    "result = triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)\n",
    "max_label = np.argmax(result.as_numpy(\"fc6_1\"))\n",
    "label_name = prepost.postprocess(max_label)\n",
    "print(label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Managing endpoints and deployments\n",
    "\n",
    "## 5.1 Get the logs for the new deployment\n",
    "Get the logs for the green deployment and verify as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Instance status:\\nSystemSetup: Succeeded\\nUserContainerImagePull: Succeeded\\nModelDownload: Succeeded\\nUserContainerStart: Succeeded\\n\\nContainer events:\\nKind: Pod, Name: Downloading, Type: Normal, Time: 2022-08-26T17:17:43.099773Z, Message: Start downloading models\\nKind: Pod, Name: Pulling, Type: Normal, Time: 2022-08-26T17:17:48.443422Z, Message: Start pulling container image\\nKind: Pod, Name: Pulled, Type: Normal, Time: 2022-08-26T17:22:00.421567Z, Message: Container image is pulled successfully\\nKind: Pod, Name: Downloaded, Type: Normal, Time: 2022-08-26T17:22:00.421567Z, Message: Models are downloaded successfully\\nKind: Pod, Name: Created, Type: Normal, Time: 2022-08-26T17:22:00.457065Z, Message: Created container inference-server\\nKind: Pod, Name: Started, Type: Normal, Time: 2022-08-26T17:22:00.628379Z, Message: Started container inference-server\\nKind: Pod, Name: ContainerReady, Type: Normal, Time: 2022-08-26T17:22:19.82993125Z, Message: Container is ready\\n\\nContainer logs:\\nI0826 17:22:02.105092 1 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f23a4000000' with size 268435456\\nI0826 17:22:02.105450 1 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\\nI0826 17:22:02.106683 1 model_repository_manager.cc:1045] loading: model_1:1\\nI0826 17:22:02.207046 1 onnxruntime.cc:2029] TRITONBACKEND_ModelInitialize: model_1 (version 1)\\nI0826 17:22:02.436376 1 onnxruntime.cc:2072] TRITONBACKEND_ModelInstanceInitialize: model_1 (GPU device 0)\\nI0826 17:22:08.254173 1 model_repository_manager.cc:1212] successfully loaded 'model_1' version 1\\nI0826 17:22:08.254276 1 server.cc:504] \\n+------------------+------+\\n| Repository Agent | Path |\\n+------------------+------+\\n+------------------+------+\\n\\nI0826 17:22:08.254333 1 server.cc:543] \\n+-------------+-----------------------------------------------------------------+--------+\\n| Backend     | Path                                                            | Config |\\n+-------------+-----------------------------------------------------------------+--------+\\n| tensorrt    | <built-in>                                                      | {}     |\\n| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so         | {}     |\\n| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so | {}     |\\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}     |\\n| openvino    | /opt/tritonserver/backends/openvino/libtriton_openvino.so       | {}     |\\n+-------------+-----------------------------------------------------------------+--------+\\n\\nI0826 17:22:08.254372 1 server.cc:586] \\n+---------+---------+--------+\\n| Model   | Version | Status |\\n+---------+---------+--------+\\n| model_1 | 1       | READY  |\\n+---------+---------+--------+\\n\\nI0826 17:22:08.254452 1 tritonserver.cc:1718] \\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| Option                           | Value                                                                                                                                                                                  |\\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| server_id                        | triton                                                                                                                                                                                 |\\n| server_version                   | 2.13.0                                                                                                                                                                                 |\\n| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\\n| model_repository_path[0]         | /var/azureml-app/azureml-models/sample-densenet-onnx-model/1/models                                                                                                                    |\\n| model_control_mode               | MODE_NONE                                                                                                                                                                              |\\n| strict_model_config              | 0                                                                                                                                                                                      |\\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\\n| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                               |\\n| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\\n| strict_readiness                 | 1                                                                                                                                                                                      |\\n| exit_timeout                     | 30                                                                                                                                                                                     |\\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n\\nI0826 17:22:08.256618 1 grpc_server.cc:4111] Started GRPCInferenceService at 0.0.0.0:8001\\nI0826 17:22:08.256845 1 http_server.cc:2803] Started HTTPService at 0.0.0.0:8000\\nI0826 17:22:08.299830 1 http_server.cc:162] Started Metrics Service at 0.0.0.0:8002\\n\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Deploy a custom container as an online endpoint. Use web servers other than the default Python Flask server used by Azure ML without losing the benefits of Azure ML's built-in monitoring, scaling, alerting, and authentication."
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "908744cbfb0aedc61cfa3e1692f27c65572b0691d24c46e79632128c1237a7f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
