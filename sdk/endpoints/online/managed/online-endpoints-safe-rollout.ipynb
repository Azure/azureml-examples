{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe rollout for managed online endpoints\n",
    "\n",
    "You've an existing model deployed in production and you want to deploy a new version of the model. How do you roll out your new machine learning model without causing any disruption? A good answer is blue-green deployment, an approach in which a new version of a web service is introduced to production by rolling out the change to a small subset of users/requests before rolling it out completely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements - In order to benefit from this tutorial, you will need:\n",
    "- This sample notebook assumes you're using online endpoints; for more information, see [What are Azure Machine Learning endpoints (preview)?](https://docs.microsoft.com/azure/machine-learning/concept-endpoints).\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with computer cluster - [Configure workspace](../../jobs/configuration.ipynb)\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../README.md) - check the getting started section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this sample, you'll learn to:\n",
    "\n",
    "1. Deploy a new online endpoint called \"blue\" that serves version 1 of the model\n",
    "1. Scale this deployment so that it can handle more requests\n",
    "1. Deploy version 2 of the model to an endpoint called \"green\" that accepts no live traffic\n",
    "1. Test the green deployment in isolation\n",
    "1. Send 10% of live traffic to the green deployment\n",
    "1. Fully cut-over all live traffic to the green deployment\n",
    "1. Delete the now-unused v1 blue deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "The [workspace](https://docs.microsoft.com/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your AML workspace\n",
    "subscription_id = \"f57ce3c6-5c6f-4f1e-8cba-b782d8974590\"\n",
    "resource_group = \"rg-aml\"\n",
    "workspace = \"ml-lab\"\n",
    "\n",
    "# # enter details of your AML workspace\n",
    "# subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "# resource_group = \"<RESOURCE_GROUP>\"\n",
    "# workspace = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Online Endpoint\n",
    "\n",
    "Online endpoints are endpoints that are used for online (real-time) inferencing. Online endpoints contain deployments that are ready to receive data from clients and can send responses back in real time.\n",
    "\n",
    "To create an online endpoint we will use `ManagedOnlineEndpoint`. This class allows user to configure the following key aspects:\n",
    "\n",
    "- `name` - Name of the endpoint. Needs to be unique at the Azure region level\n",
    "- `auth_mode` - The authentication method for the endpoint. Key-based authentication and Azure ML token-based authentication are supported. Key-based authentication doesn't expire but Azure ML token-based authentication does. Possible values are `key` or `aml_token`.\n",
    "- `identity`- The managed identity configuration for accessing Azure resources for endpoint provisioning and inference.\n",
    "    - `type`- The type of managed identity. Azure Machine Learning supports `system_assigned` or `user_assigned identity`.\n",
    "    - `user_assigned_identities` - List (array) of fully qualified resource IDs of the user-assigned identities. This property is required is `identity.type` is user_assigned.\n",
    "- `description`- Description of the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Configure the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"foo\": \"bar\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create the endpoint\n",
    "Using the `MLClient` created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-07272210408433.westus2.inference.ml.azure.com/score', 'swagger_uri': 'https://endpoint-07272210408433.westus2.inference.ml.azure.com/swagger.json', 'name': 'endpoint-07272210408433', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourcegroups/rg-aml/providers/microsoft.machinelearningservices/workspaces/ml-lab/onlineendpoints/endpoint-07272210408433', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/providers/Microsoft.MachineLearningServices/locations/westus2/mfeOperationsStatus/oe:15029b6a-f55e-4ad2-bcf2-6721c2de9839:1aed2d67-7593-444f-bba5-2bc576fcf47f?api-version=2022-02-01-preview'}, 'id': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourceGroups/rg-aml/providers/Microsoft.MachineLearningServices/workspaces/ml-lab/onlineEndpoints/endpoint-07272210408433', 'source_path': None, 'base_path': '/home/shohei/projects/shohei1029/azureml-examples/sdk/endpoints/online/managed', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fd2de7c9ee0>, 'auth_mode': 'key', 'location': 'westus2', 'identity': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedServiceIdentity object at 0x7fd2de7cc220>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a blue deployment\n",
    "\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. We will create a deployment for our endpoint using the `ManagedOnlineDeployment` class. This class allows user to configure the following key aspects.\n",
    "\n",
    "- `name` - Name of the deployment.\n",
    "- `endpoint_name` - Name of the endpoint to create the deployment under.\n",
    "- `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
    "- `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.\n",
    "- `code_configuration` - the configuration for the source code and scoring script\n",
    "    - `path`- Path to the source code directory for scoring the model\n",
    "    - `scoring_script` - Relative path to the scoring file in the source code directory\n",
    "- `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](https://docs.microsoft.com/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
    "- `instance_count` - The number of instances to use for the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configure blue deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blue deployment\n",
    "model = Model(path=\"../model-1/model/sklearn_regression_model.pkl\")\n",
    "env = Environment(\n",
    "    conda_file=\"../model-1/environment/conda.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n",
    ")\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../model-1/onlinescoring\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_F2s_v2\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create the deployment\n",
    "\n",
    "Using the `MLClient` created earlier, we will now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint endpoint-07272210408433 exists\n",
      "\u001b[32mUploading onlinescoring (0.0 MBs): 100%|██████████| 3945/3945 [00:00<00:00, 5463.94it/s] \n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading sklearn_regression_model.pkl\u001b[32m (< 1 MB): 100%|██████████| 756/756 [00:00<00:00, 6.85kB/s]\n",
      "\u001b[39m\n",
      "\n",
      "Creating/updating online deployment blue Done (10m 11s)\n"
     ]
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-07272210408433.westus2.inference.ml.azure.com/score', 'swagger_uri': 'https://endpoint-07272210408433.westus2.inference.ml.azure.com/swagger.json', 'name': 'endpoint-07272210408433', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourcegroups/rg-aml/providers/microsoft.machinelearningservices/workspaces/ml-lab/onlineendpoints/endpoint-07272210408433', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/providers/Microsoft.MachineLearningServices/locations/westus2/mfeOperationsStatus/oe:15029b6a-f55e-4ad2-bcf2-6721c2de9839:9e7cebee-2c66-4ed9-8df9-ea7f889e1e31?api-version=2022-02-01-preview'}, 'id': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourceGroups/rg-aml/providers/Microsoft.MachineLearningServices/workspaces/ml-lab/onlineEndpoints/endpoint-07272210408433', 'source_path': None, 'base_path': '/home/shohei/projects/shohei1029/azureml-examples/sdk/endpoints/online/managed', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fd2de7cce80>, 'auth_mode': 'key', 'location': 'westus2', 'identity': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedServiceIdentity object at 0x7fd2de7cc640>, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test the endpoint with sample data\n",
    "\n",
    "Using the `MLClient` created earlier, we will get a handle to the endpoint. The endpoint can be invoked using the invoke command with the following parameters:\n",
    "\n",
    "- `endpoint_name` - Name of the endpoint\n",
    "- `request_file` - File with request data\n",
    "- `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We will send a sample request using a [json](./model-1/sample-request.json) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[11055.977245525679, 4503.079536107787]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"../model-1/sample-request.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scale the deployment\n",
    "\n",
    "Using the `MLClient` created earlier, we will get a handle to the deployment. The deployment can be scaled by increasing or decreasing the `instance count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint endpoint-07272210408433 exists\n",
      "Creating/updating online deployment blue Done (3m 57s)\n"
     ]
    }
   ],
   "source": [
    "# scale the deployment\n",
    "blue_deployment = ml_client.online_deployments.get(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name\n",
    ")\n",
    "blue_deployment.instance_count = 2\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Get endpoint details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue': 100, 'green': 0}\n",
      "https://endpoint-07272210408433.westus2.inference.ml.azure.com/score\n"
     ]
    }
   ],
   "source": [
    "# Get the details for online endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Deploy a new model, but send no traffic yet\n",
    "Create a new deployment named green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create green deployment\n",
    "model2 = Model(path=\"../model-2/model/sklearn_regression_model.pkl\")\n",
    "env2 = Environment(\n",
    "    conda_file=\"../model-2/environment/conda.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n",
    ")\n",
    "\n",
    "green_deployment = ManagedOnlineDeployment(\n",
    "    name=\"green\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model2,\n",
    "    environment=env2,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../model-2/onlinescoring\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_F2s_v2\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint endpoint-07272210408433 exists\n",
      "\u001b[32mUploading onlinescoring (0.0 MBs): 100%|██████████| 1209/1209 [00:00<00:00, 11228.77it/s]\n",
      "\u001b[39m\n",
      "\n",
      "Creating/updating online deployment green Done (5m 59s)\n"
     ]
    }
   ],
   "source": [
    "# use MLClient to create green deployment\n",
    "ml_client.begin_create_or_update(green_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Test the new deployment \n",
    "Though green has 0% of traffic allocated, you can still invoke the endpoint and deployment with [json](./model-2/sample-request.json) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[11055.977245525679, 4503.079536107787]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"green\",\n",
    "    request_file=\"../model-2/sample-request.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Test the deployment with mirrored traffic (preview)\n",
    "Once you've tested your `green` deployment, you can copy (or 'mirror') a percentage of the live traffic to it. Mirroring traffic doesn't change results returned to clients. Requests still flow 100% to the blue deployment. The mirrored percentage of the traffic is copied and submitted to the `green` deployment so you can gather metrics and logging without impacting your clients. Mirroring is useful when you want to validate a new deployment without impacting clients. For example, to check if latency is within acceptable bounds and that there are no HTTP errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command mirrors 10% of the traffic to the `green` deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-07272210408433.westus2.inference.ml.azure.com/score', 'swagger_uri': 'https://endpoint-07272210408433.westus2.inference.ml.azure.com/swagger.json', 'name': 'endpoint-07272210408433', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourcegroups/rg-aml/providers/microsoft.machinelearningservices/workspaces/ml-lab/onlineendpoints/endpoint-07272210408433', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/providers/Microsoft.MachineLearningServices/locations/westus2/mfeOperationsStatus/oe:15029b6a-f55e-4ad2-bcf2-6721c2de9839:6b9c29c6-70ef-4927-b025-5cd7cdf9530a?api-version=2022-02-01-preview'}, 'id': '/subscriptions/f57ce3c6-5c6f-4f1e-8cba-b782d8974590/resourceGroups/rg-aml/providers/Microsoft.MachineLearningServices/workspaces/ml-lab/onlineEndpoints/endpoint-07272210408433', 'source_path': None, 'base_path': '/home/shohei/projects/shohei1029/azureml-examples/sdk/endpoints/online/managed', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fd2de7c9370>, 'auth_mode': 'key', 'location': 'westus2', 'identity': <azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedServiceIdentity object at 0x7fd2de7c9730>, 'traffic': {'blue': 100}, 'mirror_traffic': {'green': 50}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.mirror_traffic = {\"green\": 50}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'green': 50}\n"
     ]
    }
   ],
   "source": [
    "# Get mirror traffic settings\n",
    "print(endpoint.mirror_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary cell to test mirror traffic\n",
    "for i in range(10):\n",
    "    ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        request_file=\"../model-1/sample-request.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, you can set the mirror traffic to zero to disable mirroring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.mirror_traffic = {\"green\": 0}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Test the new deployment with a small percentage of live traffic\n",
    "Once you've tested your `green` deployment, allocate a small percentage of traffic to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 90, \"green\": 10}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your green deployment will receive 10% of requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Send all traffic to your new deployment\n",
    "Once you're satisfied that your green deployment is fully satisfactory, switch all traffic to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 0, \"green\": 100}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Remove the old deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.delete(name=\"blue\", endpoint_name=online_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Safely rollout a new version of a web service to production by rolling out the change to a small subset of users/requests before rolling it out completely"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "731e0bd12406af7e55e8a77a0250c93b84b4489edec11884b66a66fffa187d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
