{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLTable\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "\n",
    "1. Read data in a job\n",
    "1. Read *and* write data in a job\n",
    "1. Register data as an asset in Azure Machine Learning\n",
    "1. Read registered data assets from Azure Machine Learning in a job\n",
    "\n",
    "## Connect to Azure Machine Learning Workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [interactive authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.interactivebrowsercredential?view=azure-python) for this tutorial. More advanced connection methods can be found [here](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml import MLClient\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "#enter details of your AML workspace\n",
    "subscription_id = ''\n",
    "resource_group = ''\n",
    "workspace = ''\n",
    "\n",
    "#get a handle to the workspace\n",
    "ml_client = MLClient(InteractiveBrowserCredential(), subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLTable file\n",
    "\n",
    "The ML table file looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths: \n",
      "  - file: ./titanic.csv\n",
      "transformations: \n",
      "  - read_delimited: \n",
      "      delimiter: ',' \n",
      "      encoding: 'ascii' \n",
      "      empty_as_string: false "
     ]
    }
   ],
   "source": [
    "!cat ./sample_data/MLTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "      <th>Column4</th>\n",
       "      <th>Column5</th>\n",
       "      <th>Column6</th>\n",
       "      <th>Column7</th>\n",
       "      <th>Column8</th>\n",
       "      <th>Column9</th>\n",
       "      <th>Column10</th>\n",
       "      <th>Column11</th>\n",
       "      <th>Column12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PassengerId</td>\n",
       "      <td>Survived</td>\n",
       "      <td>Pclass</td>\n",
       "      <td>Name</td>\n",
       "      <td>Sex</td>\n",
       "      <td>Age</td>\n",
       "      <td>SibSp</td>\n",
       "      <td>Parch</td>\n",
       "      <td>Ticket</td>\n",
       "      <td>Fare</td>\n",
       "      <td>Cabin</td>\n",
       "      <td>Embarked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.25</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.925</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Column1   Column2 Column3  \\\n",
       "0  PassengerId  Survived  Pclass   \n",
       "1            1         0       3   \n",
       "2            2         1       1   \n",
       "3            3         1       3   \n",
       "4            4         1       1   \n",
       "\n",
       "                                             Column4 Column5 Column6 Column7  \\\n",
       "0                                               Name     Sex     Age   SibSp   \n",
       "1                            Braund, Mr. Owen Harris    male      22       1   \n",
       "2  Cumings, Mrs. John Bradley (Florence Briggs Th...  female      38       1   \n",
       "3                             Heikkinen, Miss. Laina  female      26       0   \n",
       "4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female      35       1   \n",
       "\n",
       "  Column8           Column9 Column10 Column11  Column12  \n",
       "0   Parch            Ticket     Fare    Cabin  Embarked  \n",
       "1       0         A/5 21171     7.25     None         S  \n",
       "2       0          PC 17599  71.2833      C85         C  \n",
       "3       0  STON/O2. 3101282    7.925     None         S  \n",
       "4       0            113803     53.1     C123         S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mltable as mlt\n",
    "\n",
    "tbl = mlt.load(\"./sample_data\")\n",
    "df = tbl.to_pandas_dataframe()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data in a job\n",
    "\n",
    "Similar to URI but here we use MLTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Environment\n",
    "\n",
    "env = Environment(image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\", \n",
    "    conda_file=\"env/mltable_dep.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/amusing_corn_gxs3xymwyw?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/kemp-testing-rg/workspaces/mldev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ml.entities import Data, JobInput, CommandJob\n",
    "from azure.ml._constants import AssetTypes\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"input_data\": JobInput(\n",
    "        type=AssetTypes.MLTABLE, \n",
    "        path='./sample_data'\n",
    "    )\n",
    "}\n",
    "\n",
    "job = CommandJob(\n",
    "    code=\"./src\", #local path where the code is stored\n",
    "    command='python read_mltable.py --input_data ${{inputs.input_data}}',\n",
    "    inputs=my_job_inputs,\n",
    "    environment=env,\n",
    "    compute=\"cpu-cluster\"\n",
    ")\n",
    "\n",
    "#submit the command job\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "#get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the code\n",
    "\n",
    "When the job has executed, you will see in the log files a print out of the first 10 records of the titanic sample data. The cell above you can see the inputs to the job were defined using a `dict`:\n",
    "\n",
    "```python\n",
    "my_job_inputs = {\n",
    "    \"input_data\": JobInput(\n",
    "        type=AssetTypes.URI_FILE, \n",
    "        path='./sample_data/titanic.csv'\n",
    "    )\n",
    "}\n",
    "```\n",
    "\n",
    "The `JobInput` class allow you to define data inputs where:\n",
    "\n",
    "- `type` can be a `uri_file` (a specific file) or `uri_folder` (a folder location)\n",
    "- `path` can be a local path or a cloud path. Azure Machine Learning supports `https://`, `abfss://`, `wasbs://` and `azureml://` URIs. As you saw above, if the path is local but your compute is defined to be in the cloud, Azure Machine Learning will automatically upload the data to cloud storage for you.\n",
    "\n",
    "The `JobInput` defaults the `mode` - how the input will be exposed during job runtime - to `InputOutputModes.RO_MOUNT` (read-only mount). Put another way, Azure Machine Learning will mount the file or folder to the compute and set the file/folder to read-only. By design, you cannot *write* to `JobInputs` only `JobOutputs` - we will cover this later in the notebook.\n",
    "\n",
    "#### Accessing data already in the cloud\n",
    "\n",
    "As mentioned above, the `path` in JobInput supports `https://`, `abfss://`, `wasbs://` and `azureml://` protocols. Therefore, you can simply change the `path` in the above cell to a cloud-based URI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading *and* writing data in a job\n",
    "\n",
    "By design, you cannot *write* to `JobInputs` only `JobOutputs`. Say, you want to read in some data, do some processing and then write the processed data back to the cloud. In the example below you get the URI of the default Azure ML datastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dstor = ml_client.datastores.get_default()\n",
    "output_path = default_dstor.protocol + '://' + default_dstor.account_name + '.blob.' + default_dstor.endpoint + '/' + default_dstor.container_name\n",
    "\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a `JobOutput` that will mount your Azure Machine Learning default storage (Azure Blob) in Read-*Write* mode. The python code simply takes the CSV as import and exports it as a parquet file, i.e.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(args.input_data)\n",
    "output_path = os.path.join(args.output_folder, \"my_output.parquet\")\n",
    "df.to_parquet(output_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Data, UriReference, JobInput, CommandJob, JobOutput\n",
    "from azure.ml._constants import AssetTypes\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"input_data\": JobInput(\n",
    "        type=AssetTypes.URI_FILE, \n",
    "        path='./sample_data/titanic.csv'\n",
    "    )\n",
    "}\n",
    "\n",
    "my_job_outputs = {\n",
    "    \"output_folder\": JobOutput(\n",
    "        type=AssetTypes.URI_FOLDER, \n",
    "        path=output_path\n",
    "    )\n",
    "}\n",
    "\n",
    "job = CommandJob(\n",
    "    code=\"./src\", #local path where the code is stored\n",
    "    command='python read_write_data.py --input_data ${{inputs.input_data}} --output_folder ${{outputs.output_folder}}',\n",
    "    inputs=my_job_inputs,\n",
    "    outputs=my_job_outputs,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
    "    compute=\"cpu-cluster\"\n",
    ")\n",
    "\n",
    "#submit the command job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "#get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering data as an asset in Azure Machine Learning\n",
    "\n",
    "You can register data as an asset in Azure Machine Learning. The benefits of registering data are:\n",
    "\n",
    "- Easy to share with other members of the team (no need to remember file locations)\n",
    "- Versioning of the metadata (location, description, etc)\n",
    "- Lineage tracking\n",
    "\n",
    "Below we show an example of versioning the sample data in this repo. The data is uploaded to cloud storage and registered as an asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Data\n",
    "from azure.ml._constants import AssetTypes\n",
    "\n",
    "my_data = Data(\n",
    "    path=\"./sample_data/titanic.csv\",\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Titanic Data\",\n",
    "    name=\"titanic\",\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Whilst the above example shows a local file. Remember that `path` supports cloud storage (`https`, `abfss`, `wasbs` protocols). Therefore, if you want to register data in a cloud location just specify the path with any of the supported protocols.\n",
    "\n",
    "### Consume data assets in an Azure Machine Learning Job\n",
    "\n",
    "Below we use the previously registered data asset in the job by refering to the long-form ID in the `path`:\n",
    "\n",
    "```txt\n",
    "/subscriptions/XXXXX/resourceGroups/XXXXX/providers/Microsoft.MachineLearningServices/workspaces/XXXXX/datasets/titanic/versions/1\n",
    "```\n",
    "\n",
    "This long-form URI is accessed using:\n",
    "\n",
    "```python\n",
    "registered_data_asset = ml_client.data.get(name='titanic', version='1')\n",
    "registered_data_asset.id\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Data, UriReference, JobInput, CommandJob\n",
    "from azure.ml._constants import AssetTypes\n",
    "\n",
    "registered_data_asset = ml_client.data.get(name='titanic', version='1')\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"input_data\": JobInput(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=registered_data_asset.id\n",
    "    )\n",
    "}\n",
    "\n",
    "job = CommandJob(\n",
    "    code=\"./src\", \n",
    "    command='python read_data_asset.py --input_data ${{inputs.input_data}}',\n",
    "    inputs=my_job_inputs,\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\n",
    "    compute=\"cpu-cluster\"\n",
    ")\n",
    "\n",
    "#submit the command job\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "#get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Read, write and register a data asset"
  },
  "interpreter": {
   "hash": "914e05ec6db097c6058fb02e940550a6dcea163440b917ff82e4b46d48329739"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
