{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "Please note that the this Triton Private Preview is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (PREVIEW) Deploying a web service hosted on NVIDIA Triton to Azure Kubernetes Service (AKS)\n",
    "This notebook shows the steps for deploying a service with [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server): registering a model, creating an image, provisioning a cluster (one-time action), and deploying a service to it. \n",
    "\n",
    "In this case, we use a Densenet image classification model running with ONNX Runtime, but Triton also supports TensorFlow, PyTorch, and Caffe models.\n",
    " \n",
    "We then test and delete the service, image and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get workspace\n",
    "Load existing workspace from the config file info. If you are running this notebook in a Compute Instance, a configuration file has already been created for you. If you are running this notebook somewhere else, please follow the steps to [create a configuration file](https://docs.microsoft.com/azure/machine-learning/how-to-configure-environment#workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create compute cluster\n",
    "\n",
    "This script creates a compute cluster. Uncomment the first line in the code cell below to see what is happening in that file.\n",
    "\n",
    "Note that the setup script assumes you have quota for [NCSv3 machines](https://docs.microsoft.com/azure/virtual-machines/ncv3-series) in the South Central US region. If you need to request additional quota, please create a support request. You can also pass in the --vm_size parameter to setup.py and specify a different VM size.\n",
    "\n",
    "Note that this cell can take 10-15 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the below line to see the contents of setup.py\n",
    "# %cat ../scripts/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!python setup.py --compute_loc='westus2' --vm_size='Standard_NC6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the model\n",
    "Register an existing trained model, add description and tags.\n",
    "\n",
    "** Note: ** Under `model_path` there must be a sub-directory named `triton`, which has the structure of a Triton [Model Repository](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#repository-layout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "import os\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=os.path.join(\"..\", \"..\", \"models\", \"triton\"),\n",
    "    model_name=\"densenet_onnx\",\n",
    "    tags={'area': \"Image classification\", 'type': \"classification\"},\n",
    "    description=\"Image classification trained on Imagenet Dataset\",\n",
    "    workspace=ws\n",
    ")\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model as a web service\n",
    "First create a scoring script. You can see the one we created for you in the `scripts` directory. Then, create an InferenceConfig and a DeploymentConfig and call Model.deploy().\n",
    "\n",
    "Note that this step may take 10-15 minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import Model, InferenceConfig\n",
    "\n",
    "\n",
    "aks_service_name = \"triton-densenet-onnx\"\n",
    "env = Environment.get(ws, \"AzureML-Triton\").clone(\"My-Triton\")\n",
    "\n",
    "for pip_package in [\"pillow\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    # This entry script is where we dispatch a call to the Triton server\n",
    "    entry_script=\"score_densenet.py\", \n",
    "    source_directory=os.path.join(\"source_dir\"),\n",
    "    environment=env\n",
    ")\n",
    "\n",
    "aks_config = AksWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=4,\n",
    "    gpu_cores=1,\n",
    "    compute_target_name='aks-gpu-deploy'\n",
    ")\n",
    "\n",
    "aks_service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=aks_service_name,\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=aks_config)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aks_service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the web service\n",
    "We test the web sevice by passing the test images content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!python test_service.py --endpoint_name=$aks_service_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete resources\n",
    "Delete the webservice and compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!python delete_resources.py\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "gopalv"
   }
  ],
  "index": {
   "compute": "AKS-GPU",
   "frameworks": "ONNX",
   "other": "Triton Inference Server",
   "scenario": "deployment"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('azureml': conda)",
   "language": "python",
   "name": "python_defaultSpec_1600393001476"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
