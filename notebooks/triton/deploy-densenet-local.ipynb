{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy an image classification model to the Triton Inference Server on local compute\n",
    "\n",
    "This notebook shows you how to deploy a Densenet image classification model to the high-performance [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) on local compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import tempfile\n",
    "import urllib\n",
    "from azure.storage.blob import BlobClient\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "url = \"https://aka.ms/aml-triton-sample-model\"\n",
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "blob_client = BlobClient.from_blob_url(response.url)\n",
    "\n",
    "# Download the model\n",
    "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
    "\n",
    "\n",
    "folder_path = prefix.joinpath(\"models\", \"triton\", \"densenet_onnx\", \"1\")\n",
    "model_file_path = prefix.joinpath(folder_path, \"model.onnx\")\n",
    "\n",
    "# Save the model if it does not already exist\n",
    "if not os.path.exists(model_file_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    with open(model_file_path, \"wb\") as my_blob:\n",
    "        download_stream = blob_client.download_blob()\n",
    "        my_blob.write(download_stream.readall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = prefix.joinpath(\"models\")\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"densenet_onnx\",\n",
    "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
    "    description=\"Image classification trained on Imagenet Dataset\",\n",
    "    workspace=ws,\n",
    ")\n",
    "\n",
    "print(model.name, model.description, model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "\n",
    "service_name = \"triton-densenet-onnx-local\"\n",
    "env = Environment.get(ws, \"AzureML-Triton\").clone(\"My-Triton\")\n",
    "\n",
    "for pip_package in [\"pillow\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    # This entry script is where we dispatch a call to the Triton server\n",
    "    entry_script=\"score_densenet.py\",\n",
    "    source_directory=prefix.joinpath(\"code\", \"deployment\", \"triton\"),\n",
    "    environment=env,\n",
    ")\n",
    "\n",
    "config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
    "\n",
    "data_file = prefix.joinpath(\"data\", \"raw\", \"images\", \"peacock.jpg\")\n",
    "test_sample = open(data_file, \"rb\").read()\n",
    "resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service.delete()\n",
    "os.remove(model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scale."
   ]
  }
 ],
 "metadata": {
  "index": {
   "compute": "Local",
   "frameworks": "onnx",
   "other": "triton",
   "scenario": "deployment"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "deploy-densenet-local",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
