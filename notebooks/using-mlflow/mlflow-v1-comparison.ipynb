{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e438ace",
   "metadata": {},
   "source": [
    "# Migrating from `azureml-core` Tracking to `MLflow` Tracking APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad54789",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goals of this notebook are:\n",
    "1. To provide a comparison of logging APIs in `azureml-core` and `MLflow`\n",
    "2. To provide examples that can be executed in real-time.\n",
    "3. Provide a way to migrate from `azureml-core`'s run management and tracking APIs to `MLflow` run management and tracking APIs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before proceeding, please make sure you have the following pip packages installed:\n",
    "1. `azureml-core`\n",
    "2. `mlflow`\n",
    "3. `azureml-mlflow`\n",
    "\n",
    "Please make sure you also have an Azure Machine Learning workspace. You can create one using the following steps: https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources\n",
    "\n",
    "## Relevant Documentation\n",
    "\n",
    "If you would like a more detailed view of the logging APIs available in `MLflow`, please see [here](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact). For a general overview of `MLflow` tracking in general, please see [here](https://www.mlflow.org/docs/latest/tracking.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8843ec",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the following cells to retrieve your workspace and set the `MLflow` tracking URI to point at the AzureML backend. This step is required for `MLflow` metrics and artifacts to get logged properly to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import mlflow\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06e913",
   "metadata": {},
   "source": [
    "## Creating and Managing Experiments with `MlFlow`\n",
    "\n",
    "You can create and manage experiments using `Mlflow` just as you can with `azureml-core`. You can find more information on `mlflow.create_experiment()` [here](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.create_experiment), `mlflow.get_experiment_by_name()` [here](\"https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.get_experiment\"), and `mlflow.list_experiments()` [here](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.list_experiments). You can also set the default experiment to use for all runs with `mlflow.set_experiment()`; more information can be found [here](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355887b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.create_experiment(\"create-experiment-mlflow\")\n",
    "print(mlflow.get_experiment_by_name(\"create-experiment-mlflow\"))\n",
    "print(mlflow.list_experiments())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2478e8",
   "metadata": {},
   "source": [
    "## Creating and Managing Runs with `MLflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d3ba5",
   "metadata": {},
   "source": [
    "### Interactive Runs\n",
    "\n",
    "In `azureml-core`, interactive runs are started using `experiment.start_logging()`. You can start interactive runs using `MLflow`and an `AzureML` backend by doing the following:\n",
    "\n",
    "1. Follow the setup instructions above\n",
    "1. If you want to set an experiment for the run, you can call `mlflow.set_experiment(\"<name of experiment>\")`, which will automatically create any new runs under that experiment.\n",
    "1. Start the run by either:\n",
    "    1. Using `mlflow.start_run()`. This method returns an [`mlflow.ActiveRun`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.ActiveRun) (clickable link).\n",
    "        2. You can call this method using the context manager paradigm: `with mlflow.start_run() as run`.\n",
    "    1. Calling any of `MLflow`'s logging APIs. A new run will be started if one does not exist. You can retrieve the current active run by calling `mlflow.active_run()`\n",
    "1. Complete the run. \n",
    "    1. If you are using the run as a context manager, the run will automatically complete when the context manager exits.\n",
    "    1. Otherwise, you can end the currently active run by calling `mlflow.end_run()`\n",
    "\n",
    "\n",
    "**Note**: The `mlflow.ActiveRun` object returned by `mlflow.active_run()` **will not** contain items like parameters, metrics, etc. You may find more information [here](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.active_run). See the section `Viewing Run Metrics` to see the steps retrieving metrics for a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b324741-33bb-4cb2-b0c6-1de7c8124214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting an MLflow run using a context manager\n",
    "mlflow.set_experiment(\"create-experiment-mlflow-context-manager\")\n",
    "with mlflow.start_run() as run:\n",
    "    # run is started when context manager is entered and ended when context manager exits\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e9d14-d237-4fc7-b940-96c8c504ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting an MLflow\n",
    "mlflow.set_experiment(\"create-experiment-mlflow-manual\")\n",
    "interactive_mlflow_run = mlflow.start_run()\n",
    "# End the MLflow run manually\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7db141-c2ea-4966-9acc-03168d52e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, an MLflow run can be started by calling one of the logging APIs\n",
    "mlflow.set_experiment(\"create-experiment-mlflow-logging\")\n",
    "mlflow.log_metric(\"sample_metric\", 1)\n",
    "# Note that the run object returned by `mlflow.active_run()` will not have data like metrics and parameters\n",
    "run = mlflow.active_run()\n",
    "print(run)\n",
    "# End the MLflow run manually\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19faea",
   "metadata": {},
   "source": [
    "### Remote Runs\n",
    "\n",
    "For remote training runs, the tracking URI and experiment for the run are already set by the AzureML backend. Instead of calling `azureml-core`'s `Run.get_context()` to retrieve the run inside of a training script, you can either:\n",
    "1. Call `mlflow.start_run()` to get the `MLflow` run for that particular training job. As with an interactive local run, you can use this method with the context manager paradigm.\n",
    "1. Or, use one of the `MLflow` logging APIs to start logging metrics and artifacts. Afterwards, you can call `mlflow.active_run()` to retrieve the current `MLflow` run.\n",
    "\n",
    "It is important to note that unless you do one of the above in your training script, `mlflow.active_run()` will return `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0335216",
   "metadata": {},
   "source": [
    "## Logging API Comparison\n",
    "\n",
    "The following section outlines each of the logging APIs available in the `azureml-core` and presents how to use `MLflow` to either:\n",
    "1. Reproduce the same behavior\n",
    "2. Provide functionality similar to that of `azureml-core`\n",
    "\n",
    "The structure of the document will be as follows:\n",
    "\n",
    "## AzureML Tracking API\n",
    "\n",
    "< Table describing AzureML API, accepted parameters, and available `MLflow` alternatives with notes. The `MLflow` alternatives will have clickable links to the official `MLflow` documentation for that method >\n",
    "\n",
    "< Code examples >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824285d",
   "metadata": {},
   "source": [
    "### Start the interactive `AzureML` Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ca145-f857-4c42-baee-71d11bf711c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# create an AzureML experiment and start a run\n",
    "experiment = Experiment(ws, \"logging-with-mlflow-and-azureml-core\")\n",
    "azureml_run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f2c8c",
   "metadata": {},
   "source": [
    "### Start the interactive `MLflow` Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d553cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MLflow experiment and start a run\n",
    "mlflow.set_experiment(\"logging-with-mlflow-and-azureml-core\")\n",
    "mlflow_run = mlflow.start_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9732cd8",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb57320",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log()`\n",
    "\n",
    "| azureml.core API                   | MLFlow API                                | Notes |\n",
    "|-------------------------------|-------------------------------------------|-----------------|\n",
    "| `Run.log(name: str, value: float, description=\"\", step=None)` | [`mlflow.log_metric(key: str, value: float, step: Optional[int] = None)`](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric)                                |    |\n",
    "| `Run.log(self, name: str, value: bool, description=\"\", step=None)` |  [`mlflow.log_metric(key: str, value: float, step: Optional[int] = None)`](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric), where `value` is 0 or 1 |  |\n",
    "| `Run.log(self, name: str, value: str, description=\"\", step=None)` | `mlflow.log_text(text: str, artifact_file: str) ` | `mlflow.log_text()` logs the string as an artifact and will not be considered a metric. The result of `log_text()` will be displayed in the `Outputs + logs` tab of the AzureML Studio UI  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53da00",
   "metadata": {},
   "source": [
    "### Code example: logging an integer or float metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12456a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AzureML\n",
    "azureml_run.log(\"sample_int_metric\", 1)\n",
    "\n",
    "# Using MLflow\n",
    "mlflow.log_metric(\"sample_int_metric\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c643a",
   "metadata": {},
   "source": [
    "### Code example: logging a boolean metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AzureML\n",
    "azureml_run.log(\"sample_boolean_metric\", True)\n",
    "\n",
    "# Using MLflow\n",
    "mlflow.log_metric(\"sample_boolean_metric\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc9236",
   "metadata": {},
   "source": [
    "### Code example: logging a string metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AzureML\n",
    "azureml_run.log(\"sample_string_metric\", \"a_metric\")\n",
    "\n",
    "# Using MLflow. Note that the string will get logged only as an artifact and will not be logged as a metric\n",
    "mlflow.log_text(\"sample_string_text\", \"string.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5094b50",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_image()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc50622",
   "metadata": {},
   "source": [
    "| azureml.core API                    | MLFlow equivalent or proposed alternative | Notes |\n",
    "|-------------------------------|-------------------------------------------|-------|\n",
    "| `Run.log_image(name: str, plot: Optional[matplotlib.pyplot] = None)`  | [`mlflow.log_figure(figure: Union[matplotlib.figure.Figure, plotly.graph_objects.Figure], artifact_file: str)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_figure)  |  <ul><li/>Appears in Images tab.<li/>Logged as an artifact<li/>Both appear in Studio UI<li/>`mlflow.log_figure` is **experimental**</ul>                          \n",
    "| `Run.log_image(name: str, path: Optional[Union[str, os.PathLike, pathlib.Path] = None)`  | [`mlflow.log_artifact(local_path: str, artifact_path: Optional[str] = None)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact) |<ul><li/>Image logged with `MLflow` appears in the Images tab.<li/>`MLflow` logs image as an artifact<li/>Works with png and jpeg.</ul>       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b2900",
   "metadata": {},
   "source": [
    "### Logging an image saved to a png or jpeg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bee883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AzureML\n",
    "azureml_run.log_image(\"sample_image\", path=\"Azure.png\")\n",
    "\n",
    "# Using MLflow\n",
    "mlflow.log_artifact(\"Azure.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d2057",
   "metadata": {},
   "source": [
    "### Logging a matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae464109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([1, 2, 3])\n",
    "\n",
    "# Using AzureML\n",
    "azureml_run.log_image(\"sample_pyplot\", plot=plt)\n",
    "\n",
    "# Using MLflow\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [2, 3])\n",
    "mlflow.log_figure(fig, \"sample_pyplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d483d",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_list()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479ec82",
   "metadata": {},
   "source": [
    "| azureml.core API              | MLFlow equivalent or proposed alternative |       Notes     |\n",
    "|-------------------------------|-------------------------------------------|-----------------|\n",
    "| `Run.log_list(name, value, description=\"\")`   | [`MlFlowClient().log_batch(run_id: str, metrics: Sequence[Metric] = (), params: Sequence[Param] = (), tags: Sequence[RunTag] = ())`](https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.log_batch) | <ul><li/>Metrics logged with `MLflow` in metrics tab.<li/>`MLflow` does not support logging text metrics</ul>       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_log = [1, 2, 3, 2, 1, 2, 3, 2, 1]\n",
    "\n",
    "# Using AzureML\n",
    "azureml_run.log_list(\"sample_list\", list_to_log)\n",
    "\n",
    "# Using MLflow\n",
    "from mlflow.entities import Metric\n",
    "from mlflow.tracking import MlflowClient\n",
    "import time\n",
    "\n",
    "metrics = [\n",
    "    Metric(key=\"sample_list\", value=val, timestamp=int(time.time() * 1000), step=0)\n",
    "    for val in list_to_log\n",
    "]\n",
    "MlflowClient().log_batch(mlflow_run.info.run_id, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477e6f9",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_row()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7094405",
   "metadata": {},
   "source": [
    "| azureml.core API                    | MLFlow equivalent or proposed alternative | Notes |\n",
    "|-------------------------------|-------------------------------------------|-------|\n",
    "| `Run.log_row(self, name, description=None, **kwargs)` | [`mlflow.log_metrics(metrics: Dict[str, float], step: Optional[int] = None)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics) | <ul><li/>Does not render in UI as a table.<li/>Does not support logging text values</li></ul> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 SDK\n",
    "azureml_run.log_row(\"sample_table\", col1=5, col2=10)\n",
    "\n",
    "# MLFlow\n",
    "metrics = {\"sample_table.col1\": 5, \"sample_table.col2\": 10}\n",
    "mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86d710",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_table()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af981581",
   "metadata": {},
   "source": [
    "| azureml.core API                    | MLFlow equivalent or proposed alternative | Notes           |\n",
    "|-------------------------------|-------------------------------------------|-----------------|\n",
    "| `Run.log_table(name, value, description=\"\")` | [`mlflow.log_metrics(metrics: Dict[str, float], step: Optional[int] = None)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics)                               | <ul><li/>`MLflow`logs metrics as different metrics for each column<li/>Metrics logged with `MLflow` appear in the metrics tab but not as a table<li/>`MLflow` does not support logging text values.</ul>        |\n",
    "| `Run.log_table(name, value, description=\"\")` | [`mlflow.log_artifact(local_path: str, artifact_path: Optional[str] = None)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact) | <ul><li/>Stored as an artifact<li>Does not appear in the metrics.</ul>        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AzureML\n",
    "table = {\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]}\n",
    "azureml_run.log_table(\"table\", table)\n",
    "\n",
    "# Using mlflow.log_metrics\n",
    "# Add a metric for each column prefixed by metric name. Similar to log_row\n",
    "row1 = {\"table.col1\": 5, \"table.col2\": 10}\n",
    "# To be done for each row in the table\n",
    "mlflow.log_metrics(row1)\n",
    "\n",
    "# Using mlflow.log_artifact\n",
    "import json\n",
    "\n",
    "with open(\"table.json\", \"w\") as f:\n",
    "    json.dump(table, f)\n",
    "mlflow.log_artifact(\"table.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b3928",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_accuracy_table()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890937a4",
   "metadata": {},
   "source": [
    "| azureml.core API              | MLFlow equivalent or proposed alternative |  Notes          |\n",
    "|-------------------------------|-------------------------------------------|-----------------|\n",
    "| `Run.log_accuracy_table(name, value, description=\"\")`   | [`mlflow.log_dict(dictionary: Any, artifact_file: str)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_dict)| <ul><li/>Dict logged with `MLflow` does not render in the Studio UI as accuracy table.<li/>Dict logged with `MLflow` does not appear in the metrics tab<li/>`MLflow` logs dict as an artifact<li/>`log_dict` is **experimental**</ul>        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d20ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_TABLE = (\n",
    "    '{\"schema_type\": \"accuracy_table\", \"schema_version\": \"v1\", \"data\": {\"probability_tables\": '\n",
    "    + \"[[[114311, 385689, 0, 0], [0, 0, 385689, 114311]], [[67998, 432002, 0, 0], [0, 0, \"\n",
    "    + '432002, 67998]]], \"percentile_tables\": [[[114311, 385689, 0, 0], [1, 0, 385689, '\n",
    "    + '114310]], [[67998, 432002, 0, 0], [1, 0, 432002, 67997]]], \"class_labels\": [\"0\", \"1\"], '\n",
    "    + '\"probability_thresholds\": [0.52], \"percentile_thresholds\": [0.09]}}'\n",
    ")\n",
    "\n",
    "# Using AzureML\n",
    "azureml_run.log_accuracy_table(\"v1_accuracy_table\", ACCURACY_TABLE)\n",
    "\n",
    "# Using MLflow\n",
    "mlflow.log_dict(ACCURACY_TABLE, \"mlflow_accuracy_table.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc87aa",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_confusion_matrix()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5a42b",
   "metadata": {},
   "source": [
    "| azureml.core API              | MLFlow equivalent or proposed alternative |  Notes          |\n",
    "|-------------------------------|-------------------------------------------|-----------------|\n",
    "| `Run.log_confusion_matrix(name, value, description=\"\")`   | [`mlflow.log_dict(dictionary: Any, artifact_file: str)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_dict)| <ul><li/>Dict logged with `MLflow` does not render in the Studio UI as confusion matrix.<li/>Dict logged with `MLflow` does not appear in the metrics tab<li/>`MLflow` logs dict as an artifact<li/>`log_dict` is **experimental**</ul>        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72180a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_MATRIX = (\n",
    "    '{\"schema_type\": \"confusion_matrix\", \"schema_version\": \"v1\", \"data\": {\"class_labels\": '\n",
    "    + '[\"0\", \"1\", \"2\", \"3\"], \"matrix\": [[3, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]]}}'\n",
    ")\n",
    "\n",
    "# v1 SDK\n",
    "azureml_run.log_confusion_matrix(\"v1_confusion_matrix\", json.loads(CONF_MATRIX))\n",
    "\n",
    "# MLFlow\n",
    "mlflow.log_dict(CONF_MATRIX, \"mlflow_confusion_matrix.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf98e4d",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_predictions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc640d9a",
   "metadata": {},
   "source": [
    "| azureml.core API              | MLFlow equivalent or proposed alternative |  Notes          |\n",
    "|-------------------------------|-------------------------------------------|-----------------|\n",
    "| `Run.log_predictions(name, value, description=\"\")`   | [`mlflow.log_dict(dictionary: Any, artifact_file: str)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_dict)| <ul><li/>Dict logged with `MLflow` does not render in the Studio UI as predictions.<li/>Dict logged with `MLflow` does not appear in the metrics tab<li/>`MLflow` logs dict as an artifact<li/>`log_dict` is **experimental**</ul>        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS = (\n",
    "    '{\"schema_type\": \"predictions\", \"schema_version\": \"v1\", \"data\": {\"bin_averages\": [0.25,'\n",
    "    + ' 0.75], \"bin_errors\": [0.013, 0.042], \"bin_counts\": [56, 34], \"bin_edges\": [0.0, 0.5, 1.0]}}'\n",
    ")\n",
    "\n",
    "# v1 SDK\n",
    "azureml_run.log_predictions(\"test_predictions\", json.loads(PREDICTIONS))\n",
    "\n",
    "# MLFlow\n",
    "mlflow.log_dict(PREDICTIONS, \"mlflow_predictions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42021d30",
   "metadata": {},
   "source": [
    "## `azureml.core.Run.log_residuals()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76031a88",
   "metadata": {},
   "source": [
    "| azureml.core API              | MLFlow equivalent or proposed alternative |  Notes          |\n",
    "|-------------------------------|-------------------------------------------|-----------------|\n",
    "| `Run.log_residuals(name, value, description=\"\")`   | [`mlflow.log_dict(dictionary: Any, artifact_file: str)`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_dict)| <ul><li/>Dict logged with `MLflow` does not render in the Studio UI as residuals.<li/>Dict logged with `MLflow` does not appear in the metrics tab<li/>`MLflow` logs dict as an artifact<li/>`log_dict` is **experimental**</ul>        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41665e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIDUALS = (\n",
    "    '{\"schema_type\": \"residuals\", \"schema_version\": \"v1\", \"data\": {\"bin_edges\": [100, 200, 300], '\n",
    "    + '\"bin_counts\": [0.88, 20, 30, 50.99]}}'\n",
    ")\n",
    "\n",
    "# v1 SDK\n",
    "azureml_run.log_residuals(\"test_residuals\", json.loads(RESIDUALS))\n",
    "\n",
    "# MLFlow\n",
    "mlflow.log_dict(RESIDUALS, \"mlflow_residuals.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the AzureML and MLflow runs\n",
    "azureml_run.complete()\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc36e26",
   "metadata": {},
   "source": [
    "## Retreiving Run Info and Data with `MlFlow`\n",
    "\n",
    "You can access run information using `MLflow` through the run object's `data` and `info` properties. See [here](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.Run) for more information on the `MLflow.entities.Run` object and the information it exposes.\n",
    "\n",
    "After run has completed, you can retrieve it using the [`MlFlowClient()`](https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient) (clickable link). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Use MlFlow to retrieve the run that was just completed\n",
    "client = MlflowClient()\n",
    "finished_mlflow_run = MlflowClient().get_run(mlflow_run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff044d",
   "metadata": {},
   "source": [
    "You can view the metrics, parameters, and tags for the run in the `data` field of the run object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = finished_mlflow_run.data.metrics\n",
    "tags = finished_mlflow_run.data.tags\n",
    "params = finished_mlflow_run.data.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d635c1",
   "metadata": {},
   "source": [
    "**Note:** The metrics dictionary under `mlflow.entities.Run.data.metrics` will only have the **most recently logged value** for a given metric name. For example, if you log, in order, 1, then 2, then 3, then 4 to a metric called `sample_metric`, only 4 will be present in the metrics dictionary for the key `sample_metric`.\n",
    "\n",
    "To get all metrics logged for a particular metric name, you can use `MlFlowClient.get_metric_history()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28199be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as multiple_metrics_run:\n",
    "    mlflow.log_metric(\"sample_metric\", 1)\n",
    "    mlflow.log_metric(\"sample_metric\", 2)\n",
    "    mlflow.log_metric(\"sample_metric\", 3)\n",
    "    mlflow.log_metric(\"sample_metric\", 4)\n",
    "\n",
    "print(client.get_run(multiple_metrics_run.info.run_id).data.metrics)\n",
    "print(client.get_metric_history(multiple_metrics_run.info.run_id, \"sample_metric\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0a25c",
   "metadata": {},
   "source": [
    "You can view general information about the run, such as start time, run id, experiment id, etc. through the `info` field of the run object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33117cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_start_time = finished_mlflow_run.info.start_time\n",
    "run_experiment_id = finished_mlflow_run.info.experiment_id\n",
    "run_id = finished_mlflow_run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bcc62-b9cd-47c2-b5c0-58e1868765c2",
   "metadata": {},
   "source": [
    "## Retrieving Run Artifacts with `MLflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349388cc",
   "metadata": {},
   "source": [
    "To view the artifacts of a run, you can use [`MlFlowClient.list_artifacts()`](https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.list_artifacts) (clickable link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_artifacts(finished_mlflow_run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea839317-116e-4aef-a208-bff4f9a9c6cb",
   "metadata": {},
   "source": [
    "To download an artifact, you can use [`MLFlowClient.download_artifacts()`](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts) (clickable link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997239ea-2038-4d06-a8ef-bc57b76bf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.download_artifacts(finished_mlflow_run.info.run_id, \"Azure.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd4a12-f31f-4415-8f81-6960c562ff23",
   "metadata": {},
   "source": [
    "## Searching Runs with `MLflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecde2c3-8c73-41db-a4e6-ad46b8881add",
   "metadata": {},
   "source": [
    "You can use [`mlflow.search_runs()`](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_runs) (clickable link) to query for runs programatically. The search API is a simplified\n",
    "version of the SQL WHERE clause and supports the following functionality:\n",
    "1. query on metrics, params\n",
    "1. query on tags\n",
    "1. query on run metadata (for example, status)\n",
    "1. query on runs of single experiment or multiple experiments\n",
    "\n",
    "The `search_runs()` call returns either:\n",
    "1. By default, a `pandas.DataFrame`.\n",
    "1. Optionally, a list. You can specify the list option by passing `\"list\"` as the value for the keyword argument `output_format`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057cde7-03df-4732-95e7-92c802bb43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "## example: get list of runs in order of descending accuracy and return as a list, use:\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=\"0\",\n",
    "    filter_string=\"\",\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=1,\n",
    "    order_by=[\"metrics.acc DESC\"],\n",
    "    output_format=\"list\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617aa115-e65c-4535-9aa4-d0daf92da7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example: get all active runs from experiments IDs 3, 4, and 17 that used a CNN model with 10 layers\n",
    "query = \"params.model = 'CNN' and params.layers = '10'\"\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[\"3\", \"4\", \"17\"],\n",
    "    filter_string=query,\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
