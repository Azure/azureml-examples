{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter optimization using HyperOpt and nested runs in MLflow\n",
    "\n",
    "This notebook demonstrates how to use MLflow for tracking experiment using MLflow in Azure ML when you want to do hyper-parameter optimization using child runs. In MLflow a run can start child runs which will get logged inside of it. This is useful when doing hyper-parameters cause you can get one dedicated run for each trial, while keeping all the runs controlled by a parent run which orchestrates the parameters search. \n",
    "\n",
    "In this example we will show also how you can get artifacts from the child run and log them inside of the parent. We are going to use the library `HyperOpt` for doing the optimization.\n",
    "\n",
    "> **Important:** You may also want to explore using the parameter sweep component from Azure ML for doing hyper-parameter optimization in job-based runs. Check [CLI (v2) sweep job](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-sweephttps://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-sweep) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the dependencies for this notebook\n",
    "%pip install -r xgboost_nested_runs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. It's always a good idea to start by configuring the name of the experiment we are working with in MLflow. Experiments allows you to organize runs in a comprehensive way so you can compare different experiment's runs with different parameters and configuration. MLflow configures the default experiment named \"Default\" but you can change this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='', experiment_id='368f3886-e4e3-449a-bd1e-593c5b0e01ff', lifecycle_stage='active', name='heart-condition-classifier', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"heart-condition-classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>132</td>\n",
       "      <td>341</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>135</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>407</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   1       145   233    1        2      150      0      2.3   \n",
       "1     67    1   4       160   286    0        2      108      1      1.5   \n",
       "2     67    1   4       120   229    0        2      129      1      2.6   \n",
       "3     37    1   3       130   250    0        0      187      0      3.5   \n",
       "4     41    0   2       130   204    0        2      172      0      1.4   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   52    1   1       118   186    0        2      190      0      0.0   \n",
       "299   43    0   4       132   341    1        2      136      1      3.0   \n",
       "300   65    1   4       135   254    0        2      127      0      2.8   \n",
       "301   48    1   4       130   256    1        2      150      1      0.0   \n",
       "302   63    0   4       150   407    0        2      154      0      4.0   \n",
       "\n",
       "     slope  ca        thal  target  \n",
       "0        3   0       fixed       0  \n",
       "1        2   3      normal       1  \n",
       "2        2   2  reversible       0  \n",
       "3        3   0      normal       0  \n",
       "4        1   0      normal       0  \n",
       "..     ...  ..         ...     ...  \n",
       "298      2   0       fixed       0  \n",
       "299      2   0  reversible       1  \n",
       "300      2   1  reversible       1  \n",
       "301      1   2  reversible       1  \n",
       "302      2   3  reversible       1  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of the variables are categorical. To make it simpler for our model to handle these values, let's use their encoded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"thal\"] = df[\"thal\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoded values looks then as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 0, 1], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"thal\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our dataset in train and test, so we can assess the performance of the model without overfitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"target\", axis=1), df[\"target\"], test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the search space\n",
    "\n",
    "Since we are going to do hyper-parameter optimization, we need to define the search space we are going to use for each parameter. In this simple example, we are optimizing the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 1e-5, 1e-1),\n",
    "    \"max_leaves\": hp.choice(\"max_leaves \", options=[2, 5, 9, 10, 20, 30]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to place all the training code inside of a function so we can use for searching the best parameters configuration. HyperOpt requires us to have **the first positional parameter of this function to receive the parameters of each trial**. Here we are placing them in an argument called `params` which is a dictionary. The rest of the parameters needs to be named parameters. You need to comply with this convention.\n",
    "\n",
    "The function needs to return either a float value which correspond to the loss of the trial, or a dictionary which specific keys. Read the documentation of HyperOpt for other options and details. We are using the dictionary option here cause we want to return some extra information about each trial which is the run ID in MLflow corresponding to the trial. Pay attention how this is indicated in a key `attachments`. This will be handy later to download assets for each run.\n",
    "\n",
    "It's worth noticing how the MLflow run is started with the parameter `nested=True`. This indicates to MLflow that the current run is a child run of a higher level one. The parent run controls the parameter optimization while each child focus on training one particular model. Also, notice how we are using `autolog` to log metrics, parameters and models automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_curve, auc\n",
    "from typing import Any, Dict\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "\n",
    "def train(\n",
    "    params: Dict[str, Any], X_train: pd.DataFrame, X_test, y_train: pd.DataFrame, y_test\n",
    ") -> Dict[str, Any]:\n",
    "    with mlflow.start_run(nested=True) as child_run:\n",
    "        # Using autolog for xgboost\n",
    "        mlflow.xgboost.autolog(silent=True)\n",
    "\n",
    "        model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", **params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate some metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=2)\n",
    "        auc_metric = auc(fpr, tpr)\n",
    "\n",
    "        # Return a dictionary HyperOpt can use to better inform the search\n",
    "        # We are also returning the run_id of each iteration so we can use it\n",
    "        # later to access the best run from Mlflow.\n",
    "        return {\n",
    "            \"status\": STATUS_OK,\n",
    "            \"loss\": -recall,\n",
    "            \"attachments\": {\"run_id\": child_run.info.run_id},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a search function\n",
    "\n",
    "We are going to create a search function for doing the hyper-parameter optimization. This is not required -  you can place all the code directly on notebook's cells but it is a good choice. The function will:\n",
    "\n",
    "1. Start a new MLflow run to do the optimization this parent run is the one in charge of doing the overall optimization. Notice that if you are running inside of an AML job, you are should not start the run manually cause it is already done for you.\n",
    "2. Creates a `fmin` object which will do the optimization using HyperOpt.\n",
    "3. Gets the best trial and best run ID from MLflow.\n",
    "4. Downloads the best model from the child run which was better and register it in the parent run.\n",
    "5. Logs the best child run in the parent run along with the best hyper-parameters configuration we found.\n",
    "6. Logs some extra artifacts (see details later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt.plotting\n",
    "from hyperopt import fmin, tpe, rand, Trials\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def search(\n",
    "    space: Dict[str, Any], X_train: pd.DataFrame, X_test, y_train: pd.DataFrame, y_test\n",
    "):\n",
    "    with mlflow.start_run() as run:\n",
    "        trials = Trials()\n",
    "        best_params = fmin(\n",
    "            fn=partial(\n",
    "                train, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test\n",
    "            ),\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=5,\n",
    "            trials=trials,\n",
    "        )\n",
    "        best_run_id = trials.trial_attachments(trials.best_trial)[\"run_id\"]\n",
    "        best_run = mlflow.get_run(best_run_id)\n",
    "\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        best_model = client.download_artifacts(best_run_id, path=\"model\")\n",
    "\n",
    "        mlflow.log_param(\"best_run_id\", best_run_id)\n",
    "        mlflow.log_params({f\"best_{p}\": v for p, v in best_params.items()})\n",
    "        mlflow.log_metric(\"best_loss\", trials.best_trial[\"result\"][\"loss\"])\n",
    "        mlflow.log_artifacts(local_dir=best_model, artifact_path=\"model\")\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot()\n",
    "        hyperopt.plotting.main_plot_histogram(trials, do_show=False)\n",
    "        mlflow.log_figure(fig, \"loss_histogram\")\n",
    "\n",
    "        return trials, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to notice about `fmin`:\n",
    "- `fmin` is used to minimize the loss according to the objective indicated in the function `train`. If you pay closer look at how it is indicated, we wrapped the function with `partial`. This is because the function receives extra parameters we are not looking to optimize, but they are directly passed to the function. We are passing here the train and test splits for the features and labels.\n",
    "- `algo` is indicated with `tpe.suggest`, meaning that we are relying on HyperOpt to guess which is the best algorithm to use based on the space we are using.\n",
    "- `max_evals` indicates how many trials will be executed. We are not using early termination in this example, but you are welcome to explore it.\n",
    "- `trials` is passed to the `fmin` function. This is not required but when the argument is present, all the debug information about the trials is stored there. It's a best practice to always indicate it.\n",
    "\n",
    "Some things to notice in the `search` function:\n",
    "- The function `fmin` returns the best parameters combination, but it doesn't provide information about which is the best run. For that, we use `trials.best_trial`. We also added `run_id` as an attachment, so we can use it to get the best run inside of MLflow. This is convenient because we don't have to query all the child runs to guess which is the best one.\n",
    "- We are downloading the best model so we can log it in the parent run. This is done using the function `client.download_artifacts` for the child run and then `mlflow.log_artifacts` for the parent run. So the best model is logged twice, inside of the parent run and also inside of the child run that generated it.\n",
    "- For further reference, the best run ID is also logged as a parameters with the method `mlflow.log_param`.\n",
    "- HyperOpt also has some features for plotting the results. We are logging this plots using `mlflow.log_figure` which will store these plots as `png` files automatically for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:59<00:00, 11.82s/trial, best loss: -0.48148148148148145]\n"
     ]
    }
   ],
   "source": [
    "trials, best_params = search(space, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (heart)",
   "language": "python",
   "name": "heart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
