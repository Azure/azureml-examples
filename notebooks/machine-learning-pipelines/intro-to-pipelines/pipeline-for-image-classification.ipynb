{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build a simple ML pipeline for image classification\n",
        "\n",
        "## Introduction\n",
        "This tutorial shows how to train a simple deep neural network using the [Fashion MNIST dataset and Keras on Azure Machine Learning. Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up your development environment\n",
        "\n",
        "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
        "\n",
        "### Import packages\n",
        "\n",
        "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Dataset, Datastore, ComputeTarget, Experiment, ScriptRunConfig\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core import Pipeline\n",
        "# check core SDK version number\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857515040
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to workspace\n",
        "\n",
        "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# load workspace\n",
        "workspace = Workspace.from_config()\n",
        "print('Workspace name: ' + workspace.name, \n",
        "      'Azure region: ' + workspace.location, \n",
        "      'Subscription id: ' + workspace.subscription_id, \n",
        "      'Resource group: ' + workspace.resource_group, sep='\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857518344
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create experiment and a directory\n",
        "\n",
        "Create an experiment to track the runs in your workspace and a directory to deliver the necessary code from your computer to the remote resource."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# create an ML experiment\n",
        "exp = Experiment(workspace=workspace, name='keras-mnist-fashion')\n",
        "\n",
        "# create a directory\n",
        "script_folder = './keras-mnist-fashion'\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857519697
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create or Attach existing compute resource\n",
        "\n",
        "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# choose a name for your cluster\r\n",
        "cluster_name = \"gpu-cluster\"\r\n",
        "\r\n",
        "found = False\r\n",
        "# Check if this compute target already exists in the workspace.\r\n",
        "cts = workspace.compute_targets\r\n",
        "if cluster_name in cts and cts[cluster_name].type == 'AmlCompute':\r\n",
        "    found = True\r\n",
        "    print('Found existing compute target.')\r\n",
        "    compute_target = cts[cluster_name]\r\n",
        "    \r\n",
        "if not found:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\", # for GPU, use \"STANDARD_NC6\"\r\n",
        "                                                                #vm_priority = 'lowpriority', # optional\r\n",
        "                                                                max_nodes = 4)\r\n",
        "\r\n",
        "    # Create the cluster.\r\n",
        "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\r\n",
        "    \r\n",
        "    # Can poll for a minimum number of nodes and for a specific timeout.\r\n",
        "    # If no min_node_count is provided, it will use the scale settings for the cluster.\r\n",
        "    compute_target.wait_for_completion(show_output = True, min_node_count=None, timeout_in_minutes = 10)\r\n",
        "    \r\n",
        "# For a more detailed view of current AmlCompute status, use get_status().print(compute_target.get_status().serialize())\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618857535988
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Fashion MNIST dataset\n",
        "\n",
        "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_urls = ['https://data4mldemo6150520719.blob.core.windows.net/demo/mnist-fashion']\n",
        "fashion_ds = Dataset.File.from_files(data_urls)\n",
        "\n",
        "# list the files referenced by fashion_ds\n",
        "fashion_ds.to_path()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857745715
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build 2-step ML pipeline\n",
        "\n",
        "### Step 1: data preparation\n",
        "\n",
        "In step one, we will load the image and labels from Fashion MNIST dataset into mnist_train.csv and mnist_test.csv"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermediate data (or output of a step) is represented by a `OutputFileDatasetConfig` object. preprared_fashion_ds is produced as the output of step 1, and used as the input of step 2. `OutputFileDatasetConfig` introduces a data dependency between steps, and creates an implicit execution order in the pipeline. You can register a `OutputFileDatasetConfig` as a dataset and version the output data automatically."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\n",
        "\n",
        "# learn more about the output config\n",
        "help(OutputFileDatasetConfig)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857745896
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write output to datastore under folder `outputdataset` and register it as a dataset after the experiment completes\n",
        "# make sure the service principal in your datastore has blob data contributor role in order to write data back\n",
        "datastore=workspace.get_default_datastore()\n",
        "prepared_fashion_ds = OutputFileDatasetConfig(destination=(datastore, 'outputdataset/{run-id}')).register_on_complete(name='prepared_fashion_ds')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857751426
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "prep_step = PythonScriptStep(name='prepare step',\n",
        "                             script_name=\"prepare.py\",\n",
        "                             # mount fashion_ds dataset to the compute_target\n",
        "                             arguments=[fashion_ds.as_named_input('fashion_ds').as_mount(), prepared_fashion_ds],\n",
        "                             source_directory=script_folder,\n",
        "                             compute_target=compute_target,\n",
        "                             allow_reuse=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857756352
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: train CNN with Keras\n",
        "\n",
        "Next, construct a ScriptRunConfig to configure the training run that trains a CNN model using Keras. It takes a dataset as the input."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies.yml\n",
        "\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- pip:\n",
        "  - azureml-defaults\n",
        "  - keras\n",
        "  - tensorflow\n",
        "  - numpy\n",
        "  - scikit-learn\n",
        "  - pandas\n",
        "  - matplotlib"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "keras_env = Environment.from_conda_specification(name = 'keras-env', file_path = './conda_dependencies.yml')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857796171
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_src = ScriptRunConfig(source_directory=script_folder,\n",
        "                            script='train.py',\n",
        "                            compute_target=compute_target,\n",
        "                            environment=keras_env)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857800650
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the run configuration details into the PythonScriptStep."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_step = PythonScriptStep(name='train step',\n",
        "                              arguments=[prepared_fashion_ds.read_delimited_files().as_input(name='prepared_fashion_ds')],\n",
        "                              source_directory=train_src.source_directory,\n",
        "                              script_name=train_src.script,\n",
        "                              runconfig=train_src.run_config)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857803377
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the pipeline\n",
        "Once we have the steps (or steps collection), we can build the pipeline.\n",
        "\n",
        "A pipeline is created with a list of steps and a workspace. Submit a pipeline using `submit`. When submit is called, a PipelineRun is created which in turn creates StepRun objects for each step in the workflow."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# build pipeline & run experiment\n",
        "pipeline = Pipeline(workspace, steps=[prep_step, train_step])\n",
        "run = exp.submit(pipeline)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618857811244
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor the PipelineRun"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "gather": {
          "logged": 1618858385645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.find_step_run('train step')[0].get_metrics()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618858387247
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the input dataset and the output model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Azure Machine Learning dataset makes it easy to trace how your data is used in ML.\n",
        "For each Machine Learning experiment, you can easily trace the datasets used as the input through `Run` object."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get input datasets\n",
        "prep_step = run.find_step_run('prepare step')[0]\n",
        "inputs = prep_step.get_details()['inputDatasets']\n",
        "input_dataset = inputs[0]['dataset']\n",
        "\n",
        "# list the files referenced by input_dataset\n",
        "input_dataset.to_path()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618858389559
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register the input Fashion MNIST dataset with the workspace so that you can reuse it in other experiments or share it with your colleagues who have access to your workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_ds = input_dataset.register(workspace = workspace,\n",
        "                                    name = 'fashion_ds',\n",
        "                                    description = 'image and label files from fashion mnist',\n",
        "                                    create_new_version = True)\n",
        "fashion_ds"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618858389826
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register the output model with dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.find_step_run('train step')[0].register_model(model_name = 'keras-model', model_path = 'outputs/model/', \n",
        "                                                  datasets =[('train test data',fashion_ds)])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618858392541
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Train",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sihhu"
      }
    ],
    "star_tag": [
      "featured"
    ],
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "Remote"
    ],
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python36"
    },
    "tags": [
      "Dataset",
      "Pipeline",
      "Estimator",
      "ScriptRun"
    ],
    "datasets": [
      "Fashion MNIST"
    ],
    "category": "tutorial",
    "framework": [
      "Azure ML"
    ],
    "friendly_name": "Datasets with ML Pipeline",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}