{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer-parallel-run.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural style transfer on video\n",
        "Using modified code from `pytorch`'s neural style [example](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html), we show how to setup a pipeline for doing style transfer on video. The pipeline has following steps:\n",
        "1. Split a video into images\n",
        "2. Run neural style on each image using one of the provided models (from `pytorch` pretrained models for this example).\n",
        "3. Stitch the image back into a video.\n",
        "\n",
        "> **Tip**\n",
        "If your system requires low-latency processing (to process a single document or small set of documents quickly), use [real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service) instead of batch prediction."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.20.0\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1619115029718
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workspace name: opendatasetspmworkspace2\n",
            "Azure region: eastus2\n",
            "Subscription id: 21d8f407-c4c4-452e-87a4-e609bfb86248\n",
            "Resource group: opendatasetspmrg\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1619115031179
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import AmlCompute, ComputeTarget\n",
        "from azureml.core import Datastore, Dataset\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.data import OutputFileDatasetConfig"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1619115031401
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create directory for model\n",
        "model_dir = 'models'\n",
        "if not os.path.isdir(model_dir):\n",
        "    os.mkdir(model_dir)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1619115031612
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "def download_model(model_name):\n",
        "    # downloaded models from https://pytorch.org/tutorials/advanced/neural_style_tutorial.html are kept here\n",
        "    url = \"https://pipelinedata.blob.core.windows.net/styletransfer/saved_models/\" + model_name\n",
        "    local_path = os.path.join(model_dir, model_name)\n",
        "    urllib.request.urlretrieve(url, local_path)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1619115031783
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Register all Models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import Model\n",
        "mosaic_model = None\n",
        "candy_model = None\n",
        "\n",
        "models = Model.list(workspace=ws, tags=['scenario'])\n",
        "for m in models:\n",
        "    print(\"Name:\", m.name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)\n",
        "    if m.name == 'mosaic' and mosaic_model is None:\n",
        "        mosaic_model = m\n",
        "    elif m.name == 'candy' and candy_model is None:\n",
        "        candy_model = m\n",
        "\n",
        "if mosaic_model is None:\n",
        "    print('Mosaic model does not exist, registering it')\n",
        "    download_model('mosaic.pth')\n",
        "    mosaic_model = Model.register(model_path = os.path.join(model_dir, \"mosaic.pth\"),\n",
        "                       model_name = \"mosaic\",\n",
        "                       tags = {'type': \"mosaic\", 'scenario': \"Style transfer using batch inference\"},\n",
        "                       description = \"Style transfer - Mosaic\",\n",
        "                       workspace = ws)\n",
        "else:\n",
        "    print('Reusing existing mosaic model')\n",
        "    \n",
        "\n",
        "if candy_model is None:\n",
        "    print('Candy model does not exist, registering it')\n",
        "    download_model('candy.pth')\n",
        "    candy_model = Model.register(model_path = os.path.join(model_dir, \"candy.pth\"),\n",
        "                       model_name = \"candy\",\n",
        "                       tags = {'type': \"candy\", 'scenario': \"Style transfer using batch inference\"},\n",
        "                       description = \"Style transfer - Candy\",\n",
        "                       workspace = ws)\n",
        "else:\n",
        "    print('Reusing existing candy model')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: candy \tVersion: 1 \tDescription: Style transfer - Candy {'type': 'candy', 'scenario': 'Style transfer using batch inference'}\n",
            "Name: mosaic \tVersion: 1 \tDescription: Style transfer - Mosaic {'type': 'mosaic', 'scenario': 'Style transfer using batch inference'}\n",
            "Reusing existing mosaic model\n",
            "Reusing existing candy model\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1619115131169
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create or use existing compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a name for your cluster\r\n",
        "cpu_cluster_name = \"cpu-cluster\"\r\n",
        "\r\n",
        "found = False\r\n",
        "# Check if this compute target already exists in the workspace.\r\n",
        "cts = ws.compute_targets\r\n",
        "if cpu_cluster_name in cts and cts[cpu_cluster_name].type == 'AmlCompute':\r\n",
        "    found = True\r\n",
        "    print('Found existing compute target.')\r\n",
        "    compute_target = cts[cpu_cluster_name]\r\n",
        "    \r\n",
        "if not found:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_v2\", max_nodes = 1)\r\n",
        "\r\n",
        "    # Create the cluster.\r\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
        "    cpu_cluster.wait_for_completion(show_output = True, min_node_count=None, timeout_in_minutes = 10)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# AmlCompute\r\n",
        "gpu_cluster_name = \"gpu-cluster\"\r\n",
        "\r\n",
        "found = False\r\n",
        "# Check if this compute target already exists in the workspace.\r\n",
        "cts = ws.compute_targets\r\n",
        "if gpu_cluster_name in cts and cts[gpu_cluster_name].type == 'AmlCompute':\r\n",
        "    found = True\r\n",
        "    print('Found existing compute target.')\r\n",
        "    compute_target = cts[gpu_cluster_name]\r\n",
        "    \r\n",
        "if not found:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\", max_nodes = 3)\r\n",
        "\r\n",
        "    # Create the cluster.\r\n",
        "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\r\n",
        "    gpu_cluster.wait_for_completion(show_output = True, min_node_count=None, timeout_in_minutes = 10)\r\n",
        "    \r\n",
        "# For a more detailed view of current AmlCompute status, use get_status().print(compute_target.get_status().serialize())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing compute target.\n",
            "Found existing compute target.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1619115298942
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Scripts\n",
        "We use an edited version of `neural_style_mpi.py` (original is [here](https://github.com/pytorch/examples/blob/master/fast_neural_style/neural_style/neural_style.py)). Scripts to split and stitch the video are thin wrappers to calls to `ffmpeg`. \n",
        "\n",
        "We install `ffmpeg` through conda dependencies."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scripts_folder = \"scripts\""
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1619115871187
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_video_script_file = \"process_video.py\"\n",
        "\n",
        "# peek at contents\n",
        "with open(os.path.join(scripts_folder, process_video_script_file)) as process_video_file:\n",
        "    print(process_video_file.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import argparse\n",
            "import glob\n",
            "import os\n",
            "import subprocess\n",
            "\n",
            "parser = argparse.ArgumentParser(description=\"Process input video\")\n",
            "parser.add_argument('--input_video', required=True)\n",
            "parser.add_argument('--output_audio', required=True)\n",
            "parser.add_argument('--output_images', required=True)\n",
            "\n",
            "args = parser.parse_args()\n",
            "\n",
            "os.makedirs(args.output_audio, exist_ok=True)\n",
            "os.makedirs(args.output_images, exist_ok=True)\n",
            "\n",
            "subprocess.run(\"ffmpeg -i {} {}/video.aac\".format(args.input_video, args.output_audio),\n",
            "               shell=True,\n",
            "               check=True)\n",
            "\n",
            "subprocess.run(\"ffmpeg -i {} {}/%05d_video.jpg -hide_banner\".format(args.input_video, args.output_images),\n",
            "               shell=True,\n",
            "               check=True)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1619115872606
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stitch_video_script_file = \"stitch_video.py\"\n",
        "\n",
        "# peek at contents\n",
        "with open(os.path.join(scripts_folder, stitch_video_script_file)) as stitch_video_file:\n",
        "    print(stitch_video_file.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import argparse\n",
            "import os\n",
            "import subprocess\n",
            "\n",
            "parser = argparse.ArgumentParser(description=\"Process input video\")\n",
            "parser.add_argument('--images_dir', required=True)\n",
            "parser.add_argument('--input_audio', required=True)\n",
            "parser.add_argument('--output_dir', required=True)\n",
            "\n",
            "args = parser.parse_args()\n",
            "\n",
            "os.makedirs(args.output_dir, exist_ok=True)\n",
            "\n",
            "subprocess.run(\"ffmpeg -framerate 30 -i {}/%05d_video.jpg -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p \"\n",
            "               \"-y {}/video_without_audio.mp4\"\n",
            "               .format(args.images_dir, args.output_dir),\n",
            "               shell=True, check=True)\n",
            "\n",
            "subprocess.run(\"ffmpeg -i {}/video_without_audio.mp4 -i {}/video.aac -map 0:0 -map 1:0 -vcodec \"\n",
            "               \"copy -acodec copy -y {}/video_with_audio.mp4\"\n",
            "               .format(args.output_dir, args.input_audio, args.output_dir),\n",
            "               shell=True, check=True)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1619115874595
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample video **organutan.mp4** is stored at a publicly shared datastore. We are registering the datastore below. If you want to take a look at the original video, click here. (https://pipelinedata.blob.core.windows.net/sample-videos/orangutan.mp4)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# datastore for input video\n",
        "account_name = \"pipelinedata\"\n",
        "video_ds = Datastore.register_azure_blob_container(ws, \"videos\", \"sample-videos\",\n",
        "                                            account_name=account_name, overwrite=True)\n",
        "\n",
        "# the default blob store attached to a workspace\n",
        "default_datastore = ws.get_default_datastore()"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1619115885185
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample video"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "video_name=os.getenv(\"STYLE_TRANSFER_VIDEO_NAME\", \"orangutan.mp4\") \n",
        "orangutan_video = Dataset.File.from_files((video_ds,video_name))"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1619115892215
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd = CondaDependencies()\n",
        "\n",
        "cd.add_channel(\"conda-forge\")\n",
        "cd.add_conda_package(\"ffmpeg==4.0.2\")\n",
        "\n",
        "# Runconfig\n",
        "amlcompute_run_config = RunConfiguration(conda_dependencies=cd)\n",
        "amlcompute_run_config.environment.docker.base_image = \"pytorch/pytorch\"\n",
        "amlcompute_run_config.environment.spark.precache_packages = False"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1619115892415
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffmpeg_audio = OutputFileDatasetConfig(name=\"ffmpeg_audio\")\n",
        "processed_images = OutputFileDatasetConfig(name=\"processed_images\")\n",
        "output_video = OutputFileDatasetConfig(name=\"output_video\")\n",
        "\n",
        "ffmpeg_images = OutputFileDatasetConfig(name=\"ffmpeg_images\")"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1619115894441
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define tweakable parameters to pipeline\n",
        "These parameters can be changed when the pipeline is published and rerun from a REST call.\n",
        "As part of ParallelRunStep following 2 pipeline parameters will be created which can be used to override values.\n",
        "    node_count\n",
        "    process_count_per_node"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.graph import PipelineParameter\n",
        "# create a parameter for style (one of \"candy\", \"mosaic\") to transfer the images to\n",
        "style_param = PipelineParameter(name=\"style\", default_value=\"mosaic\")\n",
        "# create a parameter for the number of nodes to use in step no. 2 (style transfer)\n",
        "nodecount_param = PipelineParameter(name=\"nodecount\", default_value=2)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1619115902385
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_video_step = PythonScriptStep(\n",
        "    name=\"split video\",\n",
        "    script_name=\"process_video.py\",\n",
        "    arguments=[\"--input_video\", orangutan_video.as_mount(),\n",
        "               \"--output_audio\", ffmpeg_audio,\n",
        "               \"--output_images\", ffmpeg_images],\n",
        "    compute_target=cpu_cluster_name,\n",
        "    runconfig=amlcompute_run_config,\n",
        "    source_directory=scripts_folder\n",
        ")\n",
        "\n",
        "stitch_video_step = PythonScriptStep(\n",
        "    name=\"stitch\",\n",
        "    script_name=\"stitch_video.py\",\n",
        "    arguments=[\"--images_dir\", processed_images.as_input(), \n",
        "               \"--input_audio\", ffmpeg_audio.as_input(), \n",
        "               \"--output_dir\", output_video],\n",
        "    compute_target=cpu_cluster_name,\n",
        "    runconfig=amlcompute_run_config,\n",
        "    source_directory=scripts_folder\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1619115987903
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create environment, parallel step run config and parallel run step"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
        "\n",
        "parallel_cd = CondaDependencies()\n",
        "\n",
        "parallel_cd.add_channel(\"pytorch\")\n",
        "parallel_cd.add_conda_package(\"pytorch\")\n",
        "parallel_cd.add_conda_package(\"torchvision\")\n",
        "parallel_cd.add_conda_package(\"pillow<7\") # needed for torchvision==0.4.0\n",
        "parallel_cd.add_pip_package(\"azureml-core\")\n",
        "\n",
        "styleenvironment = Environment(name=\"styleenvironment\")\n",
        "styleenvironment.python.conda_dependencies=parallel_cd\n",
        "styleenvironment.docker.base_image = DEFAULT_GPU_IMAGE"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1619115992660
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AmlCompute\r\n",
        "gpu_cluster_name = \"gpu-cluster\"\r\n",
        "\r\n",
        "found = False\r\n",
        "# Check if this compute target already exists in the workspace.\r\n",
        "cts = ws.compute_targets\r\n",
        "if gpu_cluster_name in cts and cts[gpu_cluster_name].type == 'AmlCompute':\r\n",
        "    found = True\r\n",
        "    print('Found existing compute target.')\r\n",
        "    compute_target = cts[gpu_cluster_name]\r\n",
        "    \r\n",
        "if not found:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\", max_nodes = 3)\r\n",
        "\r\n",
        "    # Create the cluster.\r\n",
        "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\r\n",
        "    gpu_cluster.wait_for_completion(show_output = True, min_node_count=None, timeout_in_minutes = 10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing compute target.\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1619115996963
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.steps import ParallelRunConfig\n",
        "\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    environment=styleenvironment,\n",
        "    entry_script='transform.py',\n",
        "    output_action='summary_only',\n",
        "    mini_batch_size=\"1\",\n",
        "    error_threshold=1,\n",
        "    source_directory=scripts_folder,\n",
        "    compute_target=gpu_cluster_name, \n",
        "    node_count=nodecount_param,\n",
        "    process_count_per_node=2\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1619116006210
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunStep\n",
        "from datetime import datetime\n",
        "\n",
        "parallel_step_name = 'styletransfer-' + datetime.now().strftime('%Y%m%d%H%M')\n",
        "\n",
        "distributed_style_transfer_step = ParallelRunStep(\n",
        "    name=parallel_step_name,\n",
        "    inputs=[ffmpeg_images], # Input file share/blob container/file dataset\n",
        "    output=processed_images,  # Output file share/blob container\n",
        "    arguments=[\"--style\", style_param],\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    allow_reuse=False #[optional - default value True]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1619116009413
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(workspace=ws, steps=[stitch_video_step])\n",
        "\n",
        "pipeline.validate()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step stitch is ready to be created [0297f789]\n",
            "Step styletransfer-202104221826 is ready to be created [f8baa156]\n",
            "Step split video is ready to be created [469e1855]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1619116014815
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline and provide values for the PipelineParameters used in the pipeline\n",
        "pipeline_run = Experiment(ws, 'styletransfer_parallel_mosaic').submit(pipeline)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created step stitch [0297f789][1998c925-33f7-41df-96e3-94cd7f5287f3], (This step will run and generate new outputs)\n",
            "Created step styletransfer-202104221826 [f8baa156][1d73e971-51a6-41b6-af95-f2917450b981], (This step will run and generate new outputs)\n",
            "Created step split video [469e1855][6e194585-7607-4eb1-978b-136569457bf8], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun e677b35c-b4fa-4b23-b390-3f3cd2d14ab8\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/styletransfer_parallel_mosaic/runs/e677b35c-b4fa-4b23-b390-3f3cd2d14ab8?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1619116023453
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monitor pipeline run\n",
        "\n",
        "The pipeline run status could be checked in Azure Machine Learning portal (https://ml.azure.com). The link to the pipeline run could be retrieved by inspecting the `pipeline_run` object.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# This will output information of the pipeline run, including the link to the details page of portal.\n",
        "pipeline_run"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "Run(Experiment: styletransfer_parallel_mosaic,\nId: e677b35c-b4fa-4b23-b390-3f3cd2d14ab8,\nType: azureml.PipelineRun,\nStatus: Preparing)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>styletransfer_parallel_mosaic</td><td>e677b35c-b4fa-4b23-b390-3f3cd2d14ab8</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/experiments/styletransfer_parallel_mosaic/runs/e677b35c-b4fa-4b23-b390-3f3cd2d14ab8?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1619116023659
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: View detailed logs (streaming) "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait the run for completion and show output log to console\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineRunId: e677b35c-b4fa-4b23-b390-3f3cd2d14ab8\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/styletransfer_parallel_mosaic/runs/e677b35c-b4fa-4b23-b390-3f3cd2d14ab8?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\n",
            "PipelineRun Status: NotStarted\n",
            "PipelineRun Status: Running\n",
            "\n",
            "\n",
            "StepRunId: 7212993f-ac20-4dbb-8767-fb103e2161d6\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/styletransfer_parallel_mosaic/runs/7212993f-ac20-4dbb-8767-fb103e2161d6?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\n",
            "StepRun( split video ) Status: NotStarted\n",
            "\n",
            "Streaming azureml-logs/20_image_build_log.txt\n",
            "=============================================\n",
            "2021/04/22 18:27:27 Downloading source code...\n",
            "2021/04/22 18:27:29 Finished downloading source code\n",
            "2021/04/22 18:27:30 Creating Docker network: acb_default_network, driver: 'bridge'\n",
            "2021/04/22 18:27:30 Successfully set up Docker network: acb_default_network\n",
            "2021/04/22 18:27:30 Setting up Docker configuration...\n",
            "2021/04/22 18:27:31 Successfully set up Docker configuration\n",
            "2021/04/22 18:27:31 Logging in to registry: 8095dee4090c43e39dba374a3a5f3c08.azurecr.io\n",
            "2021/04/22 18:27:32 Successfully logged into 8095dee4090c43e39dba374a3a5f3c08.azurecr.io\n",
            "2021/04/22 18:27:32 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
            "2021/04/22 18:27:32 Scanning for dependencies...\n",
            "2021/04/22 18:27:33 Successfully scanned dependencies\n",
            "2021/04/22 18:27:33 Launching container with name: acb_step_0\n",
            "Sending build context to Docker daemon  66.56kB\n",
            "\n",
            "Step 1/17 : FROM pytorch/pytorch@sha256:9ebb176339b25a2d155e6f127c5948968b3f61e5f720c4598ef79cf450db8bfe\n",
            "sha256:9ebb176339b25a2d155e6f127c5948968b3f61e5f720c4598ef79cf450db8bfe: Pulling from pytorch/pytorch\n",
            "92dc2a97ff99: Pulling fs layer\n",
            "be13a9d27eb8: Pulling fs layer\n",
            "c8299583700a: Pulling fs layer\n",
            "70a80b9c7100: Pulling fs layer\n",
            "9dda6e51b6e4: Pulling fs layer\n",
            "ab1504e75c6c: Pulling fs layer\n",
            "70a80b9c7100: Waiting\n",
            "ab1504e75c6c: Waiting\n",
            "c8299583700a: Download complete\n",
            "be13a9d27eb8: Verifying Checksum\n",
            "be13a9d27eb8: Download complete\n",
            "70a80b9c7100: Verifying Checksum\n",
            "70a80b9c7100: Download complete\n",
            "92dc2a97ff99: Verifying Checksum\n",
            "92dc2a97ff99: Download complete\n",
            "ab1504e75c6c: Verifying Checksum\n",
            "ab1504e75c6c: Download complete\n",
            "92dc2a97ff99: Pull complete\n",
            "StepRun( split video ) Status: Running\n",
            "be13a9d27eb8: Pull complete\n",
            "c8299583700a: Pull complete\n",
            "70a80b9c7100: Pull complete\n",
            "\n",
            "9dda6e51b6e4: Verifying Checksum\n",
            "9dda6e51b6e4: Download complete\n",
            "9dda6e51b6e4: Pull complete\n",
            "ab1504e75c6c: Pull complete\n",
            "Digest: sha256:9ebb176339b25a2d155e6f127c5948968b3f61e5f720c4598ef79cf450db8bfe\n",
            "Status: Downloaded newer image for pytorch/pytorch@sha256:9ebb176339b25a2d155e6f127c5948968b3f61e5f720c4598ef79cf450db8bfe\n",
            " ---> 5ffed6c83695\n",
            "Step 2/17 : USER root\n",
            " ---> Running in 12a08d802403\n",
            "Removing intermediate container 12a08d802403\n",
            " ---> 008ff82b7535\n",
            "Step 3/17 : RUN mkdir -p $HOME/.cache\n",
            " ---> Running in 95df33cb117e\n",
            "Removing intermediate container 95df33cb117e\n",
            " ---> beb7a941a343\n",
            "Step 4/17 : WORKDIR /\n",
            " ---> Running in 9d70a5a68bcb\n",
            "Removing intermediate container 9d70a5a68bcb\n",
            " ---> b40c4ea16b22\n",
            "Step 5/17 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
            " ---> 938df3c092eb\n",
            "Step 6/17 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
            " ---> Running in c73b29f2435a\n",
            "Removing intermediate container c73b29f2435a\n",
            " ---> 4875659bfcaa\n",
            "Step 7/17 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
            " ---> 1117f787d178\n",
            "Step 8/17 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2 -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
            " ---> Running in 2ab077bbd107\n",
            "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
            "Collecting package metadata (repodata.json): ...working... \n",
            "done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "gnutls-3.5.19        | 1.9 MB    |            |   0% \n",
            "gnutls-3.5.19        | 1.9 MB    | 9          |  10% \n",
            "gnutls-3.5.19        | 1.9 MB    | ########## | 100% \n",
            "gnutls-3.5.19        | 1.9 MB    | ########## | 100% \n",
            "\n",
            "libpng-1.6.37        | 364 KB    |            |   0% \n",
            "libpng-1.6.37        | 364 KB    | ########## | 100% \n",
            "libpng-1.6.37        | 364 KB    | ########## | 100% \n",
            "\n",
            "sqlite-3.23.1        | 1.5 MB    |            |   0% \n",
            "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
            "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
            "\n",
            "openssl-1.0.2u       | 3.1 MB    |            |   0% \n",
            "openssl-1.0.2u       | 3.1 MB    | ####6      |  46% \n",
            "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
            "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
            "\n",
            "x264-1!152.20180806  | 1.4 MB    |            |   0% \n",
            "x264-1!152.20180806  | 1.4 MB    | ########## | 100% \n",
            "x264-1!152.20180806  | 1.4 MB    | ########## | 100% \n",
            "\n",
            "readline-7.0         | 387 KB    |            |   0% \n",
            "readline-7.0         | 387 KB    | ########## | 100% \n",
            "readline-7.0         | 387 KB    | ########## | 100% \n",
            "\n",
            "setuptools-50.3.0    | 891 KB    |            |   0% \n",
            "setuptools-50.3.0    | 891 KB    | ########## | 100% \n",
            "setuptools-50.3.0    | 891 KB    | ########## | 100% \n",
            "\n",
            "ffmpeg-4.0.2         | 62.1 MB   |            |   0% \n",
            "ffmpeg-4.0.2         | 62.1 MB   | 9          |  10% \n",
            "ffmpeg-4.0.2         | 62.1 MB   | ##3        |  24% \n",
            "ffmpeg-4.0.2         | 62.1 MB   | ####       |  40% \n",
            "ffmpeg-4.0.2         | 62.1 MB   | #####      |  50% \n",
            "ffmpeg-4.0.2         | 62.1 MB   | ######3    |  63% \n",
            "ffmpeg-4.0.2         | 62.1 MB   | #######8   |  78% \n",
            "ffmpeg-4.0.2         | 62.1 MB   | #########  |  90% \n",
            "\n",
            "ffmpeg-4.0.2         | 62.1 MB   | ########## | 100% \n",
            "\n",
            "libstdcxx-ng-9.1.0   | 4.0 MB    |            |   0% \n",
            "libstdcxx-ng-9.1.0   | 4.0 MB    | 5          |   5% \n",
            "libstdcxx-ng-9.1.0   | 4.0 MB    | ###2       |  32% \n",
            "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
            "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
            "\n",
            "python-3.6.2         | 27.0 MB   |            |   0% \n",
            "python-3.6.2         | 27.0 MB   | 1          |   2% \n",
            "python-3.6.2         | 27.0 MB   | ###1       |  31% \n",
            "python-3.6.2         | 27.0 MB   | #####6     |  57% \n",
            "python-3.6.2         | 27.0 MB   | ########1  |  81% \n",
            "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
            "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
            "\n",
            "ca-certificates-2020 | 128 KB    |            |   0% \n",
            "ca-certificates-2020 | 128 KB    | ########## | 100% \n",
            "\n",
            "openh264-1.8.0       | 1.4 MB    |            |   0% \n",
            "openh264-1.8.0       | 1.4 MB    | ########## | 100% \n",
            "openh264-1.8.0       | 1.4 MB    | ########## | 100% \n",
            "\n",
            "gmp-6.1.2            | 744 KB    |            |   0% \n",
            "gmp-6.1.2            | 744 KB    | ########## | 100% \n",
            "gmp-6.1.2            | 744 KB    | ########## | 100% \n",
            "\n",
            "libiconv-1.15        | 2.0 MB    |            |   0% \n",
            "libiconv-1.15        | 2.0 MB    | ########## | 100% \n",
            "libiconv-1.15        | 2.0 MB    | ########## | 100% \n",
            "\n",
            "pip-20.2.4           | 2.0 MB    |            |   0% \n",
            "pip-20.2.4           | 2.0 MB    | ########## | 100% \n",
            "pip-20.2.4           | 2.0 MB    | ########## | 100% \n",
            "\n",
            "wheel-0.35.1         | 36 KB     |            |   0% \n",
            "wheel-0.35.1         | 36 KB     | ########## | 100% \n",
            "\n",
            "zlib-1.2.11          | 120 KB    |            |   0% \n",
            "zlib-1.2.11          | 120 KB    | ########## | 100% \n",
            "\n",
            "libgcc-ng-9.1.0      | 8.1 MB    |            |   0% \n",
            "libgcc-ng-9.1.0      | 8.1 MB    | #######4   |  75% \n",
            "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n",
            "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n",
            "\n",
            "libffi-3.2.1         | 52 KB     |            |   0% \n",
            "libffi-3.2.1         | 52 KB     | ########## | 100% \n",
            "\n",
            "freetype-2.10.4      | 901 KB    |            |   0% \n",
            "freetype-2.10.4      | 901 KB    | ########## | 100% \n",
            "freetype-2.10.4      | 901 KB    | ########## | 100% \n",
            "\n",
            "bzip2-1.0.8          | 105 KB    |            |   0% \n",
            "bzip2-1.0.8          | 105 KB    | ########## | 100% \n",
            "\n",
            "certifi-2020.6.20    | 160 KB    |            |   0% \n",
            "certifi-2020.6.20    | 160 KB    | ########## | 100% \n",
            "\n",
            "tk-8.6.10            | 3.2 MB    |            |   0% \n",
            "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
            "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
            "\n",
            "nettle-3.3           | 3.1 MB    |            |   0% \n",
            "nettle-3.3           | 3.1 MB    | ########## | 100% \n",
            "nettle-3.3           | 3.1 MB    | ########## | 100% \n",
            "\n",
            "libedit-3.1          | 171 KB    |            |   0% \n",
            "libedit-3.1          | 171 KB    | ########## | 100% \n",
            "\n",
            "xz-5.2.5             | 438 KB    |            |   0% \n",
            "xz-5.2.5             | 438 KB    | ########## | 100% \n",
            "xz-5.2.5             | 438 KB    | ########## | 100% \n",
            "\n",
            "ncurses-6.0          | 907 KB    |            |   0% \n",
            "ncurses-6.0          | 907 KB    | ########## | 100% \n",
            "ncurses-6.0          | 907 KB    | ########## | 100% \n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Installing pip dependencies: ...working... \n",
            "Ran pip subprocess with arguments:\n",
            "['/azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.hpdweh04.requirements.txt']\n",
            "Pip subprocess output:\n",
            "Collecting azureml-defaults\n",
            "  Downloading azureml_defaults-1.27.0-py3-none-any.whl (3.1 kB)\n",
            "Collecting applicationinsights>=0.11.7\n",
            "  Downloading applicationinsights-0.11.9-py2.py3-none-any.whl (58 kB)\n",
            "Collecting flask==1.0.3\n",
            "  Downloading Flask-1.0.3-py2.py3-none-any.whl (92 kB)\n",
            "Collecting json-logging-py==0.2\n",
            "  Downloading json-logging-py-0.2.tar.gz (3.6 kB)\n",
            "Collecting azureml-model-management-sdk==1.0.1b6.post1\n",
            "  Downloading azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130 kB)\n",
            "Collecting configparser==3.7.4\n",
            "  Downloading configparser-3.7.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting azureml-dataset-runtime[fuse]~=1.27.0\n",
            "  Downloading azureml_dataset_runtime-1.27.0-py3-none-any.whl (3.4 kB)\n",
            "Collecting azureml-core~=1.27.0\n",
            "  Downloading azureml_core-1.27.0-py3-none-any.whl (2.2 MB)\n",
            "Collecting werkzeug<=1.0.1,>=0.16.1\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "Collecting gunicorn==19.9.0\n",
            "  Downloading gunicorn-19.9.0-py2.py3-none-any.whl (112 kB)\n",
            "Collecting click>=5.1\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "Collecting itsdangerous>=0.24\n",
            "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Jinja2>=2.10\n",
            "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
            "Collecting dill>=0.2.7.1\n",
            "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "Collecting liac-arff>=2.1.1\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "Collecting python-dateutil>=2.5.3\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "Collecting pandas>=0.20.2\n",
            "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
            "Collecting numpy>=1.13.0\n",
            "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "Collecting pytz>=2017.2\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "Collecting adal>=0.4.5\n",
            "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
            "Collecting six>=1.10\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting requests>=2.17.3\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "Collecting azureml-dataprep<2.15.0a,>=2.14.0a\n",
            "  Downloading azureml_dataprep-2.14.2-py3-none-any.whl (39.4 MB)\n",
            "Collecting pyarrow<2.0.0,>=0.17.0\n",
            "  Downloading pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting fusepy<4.0.0,>=3.0.1; extra == \"fuse\"\n",
            "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
            "Collecting msrest<1.0.0,>=0.5.1\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "Collecting azure-mgmt-containerregistry>=2.0.0\n",
            "  Downloading azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718 kB)\n",
            "Collecting msrestazure>=0.4.33\n",
            "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
            "Collecting PyJWT<3.0.0\n",
            "  Downloading PyJWT-2.0.1-py3-none-any.whl (15 kB)\n",
            "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0\n",
            "  Downloading cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2 MB)\n",
            "Collecting jsonpickle<3.0.0\n",
            "  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting pathspec<1.0.0\n",
            "  Downloading pathspec-0.8.1-py2.py3-none-any.whl (28 kB)\n",
            "Collecting urllib3>=1.23\n",
            "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
            "Collecting docker<5.0.0\n",
            "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
            "Collecting azure-mgmt-keyvault<7.0.0,>=0.40.0\n",
            "  Downloading azure_mgmt_keyvault-2.2.0-py2.py3-none-any.whl (89 kB)\n",
            "Collecting backports.tempfile\n",
            "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Collecting pyopenssl<21.0.0\n",
            "  Downloading pyOpenSSL-20.0.1-py2.py3-none-any.whl (54 kB)\n",
            "Collecting azure-graphrbac<1.0.0,>=0.40.0\n",
            "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
            "Collecting ndg-httpsclient\n",
            "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
            "Collecting azure-common<2.0.0,>=1.1.12\n",
            "  Downloading azure_common-1.1.27-py2.py3-none-any.whl (12 kB)\n",
            "Collecting jmespath<1.0.0\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting azure-mgmt-authorization<1.0.0,>=0.40.0\n",
            "  Downloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\n",
            "Collecting azure-mgmt-resource<15.0.0,>=1.2.1\n",
            "  Downloading azure_mgmt_resource-12.1.0-py2.py3-none-any.whl (1.1 MB)\n",
            "Collecting contextlib2<1.0.0\n",
            "  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting SecretStorage<4.0.0\n",
            "  Downloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\n",
            "Collecting ruamel.yaml<1.0.0,>=0.15.35\n",
            "  Downloading ruamel.yaml-0.17.4-py3-none-any.whl (101 kB)\n",
            "Collecting azure-mgmt-storage<16.0.0,>=1.5.0\n",
            "  Downloading azure_mgmt_storage-11.2.0-py2.py3-none-any.whl (547 kB)\n",
            "Collecting MarkupSafe>=0.23\n",
            "  Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux2010_x86_64.whl (32 kB)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2/lib/python3.6/site-packages (from requests>=2.17.3->azureml-model-management-sdk==1.0.1b6.post1->azureml-defaults->-r /azureml-environment-setup/condaenv.hpdweh04.requirements.txt (line 1)) (2020.6.20)\n",
            "Collecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "Collecting chardet<5,>=3.0.2\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "Collecting azureml-dataprep-native<34.0.0,>=33.0.0\n",
            "  Downloading azureml_dataprep_native-33.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
            "Collecting dotnetcore2<3.0.0,>=2.1.14\n",
            "  Downloading dotnetcore2-2.1.20-py3-none-manylinux1_x86_64.whl (28.7 MB)\n",
            "Collecting azureml-dataprep-rslex<1.13.0a,>=1.12.0dev0\n",
            "  Downloading azureml_dataprep_rslex-1.12.1-cp36-cp36m-manylinux1_x86_64.whl (9.6 MB)\n",
            "Collecting cloudpickle<2.0.0,>=1.1.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting azure-identity<1.5.0,>=1.2.0\n",
            "  Downloading azure_identity-1.4.1-py2.py3-none-any.whl (86 kB)\n",
            "Collecting requests-oauthlib>=0.5.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "Collecting cffi>=1.12\n",
            "  Downloading cffi-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (401 kB)\n",
            "Collecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-0.58.0-py2.py3-none-any.whl (61 kB)\n",
            "Collecting backports.weakref\n",
            "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
            "Collecting pyasn1>=0.1.1\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting jeepney>=0.6\n",
            "  Downloading jeepney-0.6.0-py3-none-any.whl (45 kB)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\"\n",
            "  Downloading ruamel.yaml.clib-0.2.2-cp36-cp36m-manylinux1_x86_64.whl (549 kB)\n",
            "Collecting distro>=1.2.0\n",
            "  Downloading distro-1.5.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting msal<2.0.0,>=1.3.0\n",
            "  Downloading msal-1.11.0-py2.py3-none-any.whl (63 kB)\n",
            "Collecting azure-core<2.0.0,>=1.0.0\n",
            "  Downloading azure_core-1.13.0-py2.py3-none-any.whl (133 kB)\n",
            "Collecting msal-extensions~=0.2.2\n",
            "  Downloading msal_extensions-0.2.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Collecting portalocker~=1.0; platform_system != \"Windows\"\n",
            "  Downloading portalocker-1.7.1-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: json-logging-py, liac-arff, fusepy\n",
            "  Building wheel for json-logging-py (setup.py): started\n",
            "  Building wheel for json-logging-py (setup.py): finished with status 'done'\n",
            "  Created wheel for json-logging-py: filename=json_logging_py-0.2-py3-none-any.whl size=3924 sha256=c4903108be104013eecaaca9092d1eb36b96b1cea76251582037557fad7de498\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/1d/52/535a274b9c2ce7d4064838f2bdb62013801281ef7d7f21e2ee\n",
            "  Building wheel for liac-arff (setup.py): started\n",
            "  Building wheel for liac-arff (setup.py): finished with status 'done'\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11730 sha256=9ec5dc536c58165be2791f21a4fd59bf9ada399bff8b671a486d5090500416b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/ba/da/8562a6a6dbb428fd1ecc21053106df3948645cd991958f669b\n",
            "  Building wheel for fusepy (setup.py): started\n",
            "  Building wheel for fusepy (setup.py): finished with status 'done'\n",
            "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10504 sha256=cf5ba6158596569483597a451139236452b55aa837c2aef025e8e7b4ce5306a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/5c/83/1dd7e8a232d12227e5410120f4374b33adeb4037473105b079\n",
            "Successfully built json-logging-py liac-arff fusepy\n",
            "Installing collected packages: applicationinsights, werkzeug, click, itsdangerous, MarkupSafe, Jinja2, flask, json-logging-py, dill, liac-arff, six, python-dateutil, numpy, pytz, pandas, PyJWT, idna, chardet, urllib3, requests, pycparser, cffi, cryptography, adal, azureml-model-management-sdk, configparser, azureml-dataprep-native, distro, dotnetcore2, azureml-dataprep-rslex, cloudpickle, msal, azure-core, portalocker, msal-extensions, azure-identity, azureml-dataprep, pyarrow, fusepy, azureml-dataset-runtime, oauthlib, requests-oauthlib, isodate, msrest, azure-common, msrestazure, azure-mgmt-containerregistry, typing-extensions, zipp, importlib-metadata, jsonpickle, pathspec, websocket-client, docker, azure-mgmt-keyvault, backports.weakref, backports.tempfile, pyopenssl, azure-graphrbac, pyasn1, ndg-httpsclient, jmespath, azure-mgmt-authorization, azure-mgmt-resource, contextlib2, jeepney, SecretStorage, ruamel.yaml.clib, ruamel.yaml, azure-mgmt-storage, azureml-core, gunicorn, azureml-defaults\n",
            "Successfully installed Jinja2-2.11.3 MarkupSafe-1.1.1 PyJWT-2.0.1 SecretStorage-3.3.1 adal-1.2.7 applicationinsights-0.11.9 azure-common-1.1.27 azure-core-1.13.0 azure-graphrbac-0.61.1 azure-identity-1.4.1 azure-mgmt-authorization-0.61.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-2.2.0 azure-mgmt-resource-12.1.0 azure-mgmt-storage-11.2.0 azureml-core-1.27.0 azureml-dataprep-2.14.2 azureml-dataprep-native-33.0.0 azureml-dataprep-rslex-1.12.1 azureml-dataset-runtime-1.27.0 azureml-defaults-1.27.0 azureml-model-management-sdk-1.0.1b6.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 cffi-1.14.5 chardet-4.0.0 click-7.1.2 cloudpickle-1.6.0 configparser-3.7.4 contextlib2-0.6.0.post1 cryptography-3.4.7 dill-0.3.3 distro-1.5.0 docker-4.4.4 dotnetcore2-2.1.20 flask-1.0.3 fusepy-3.0.1 gunicorn-19.9.0 idna-2.10 importlib-metadata-4.0.1 isodate-0.6.0 itsdangerous-1.1.0 jeepney-0.6.0 jmespath-0.10.0 json-logging-py-0.2 jsonpickle-2.0.0 liac-arff-2.5.0 msal-1.11.0 msal-extensions-0.2.2 msrest-0.6.21 msrestazure-0.6.4 ndg-httpsclient-0.5.1 numpy-1.19.5 oauthlib-3.1.0 pandas-1.1.5 pathspec-0.8.1 portalocker-1.7.1 pyarrow-1.0.1 pyasn1-0.4.8 pycparser-2.20 pyopenssl-20.0.1 python-dateutil-2.8.1 pytz-2021.1 requests-2.25.1 requests-oauthlib-1.3.0 ruamel.yaml-0.17.4 ruamel.yaml.clib-0.2.2 six-1.15.0 typing-extensions-3.7.4.3 urllib3-1.26.4 websocket-client-0.58.0 werkzeug-1.0.1 zipp-3.4.1\n",
            "\n",
            "done\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate /azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "\u001b[91m\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.9.2\n",
            "  latest version: 4.10.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c defaults conda\n",
            "\n",
            "\n",
            "\u001b[0mWARNING: /root/.conda/pkgs does not exist\n",
            "Removing intermediate container 2ab077bbd107\n",
            " ---> f4bac31fdbdc\n",
            "Step 9/17 : ENV PATH /azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2/bin:$PATH\n",
            " ---> Running in ee23d176d574\n",
            "Removing intermediate container ee23d176d574\n",
            " ---> f447c547a031\n",
            "Step 10/17 : COPY azureml-environment-setup/send_conda_dependencies.py azureml-environment-setup/send_conda_dependencies.py\n",
            " ---> 0f9ccade93f1\n",
            "Step 11/17 : COPY azureml-environment-setup/environment_context.json azureml-environment-setup/environment_context.json\n",
            " ---> 39fa7c63b05d\n",
            "Step 12/17 : RUN python /azureml-environment-setup/send_conda_dependencies.py -p /azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2\n",
            " ---> Running in e5d1dde99746\n",
            "Report materialized dependencies for the environment\n",
            "Reading environment context\n",
            "Exporting conda environment\n",
            "Sending request with materialized conda environment details\n",
            "Successfully sent materialized environment details\n",
            "Removing intermediate container e5d1dde99746\n",
            " ---> daa893d39e5a\n",
            "Step 13/17 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2\n",
            " ---> Running in 0673d9dcb30b\n",
            "Removing intermediate container 0673d9dcb30b\n",
            " ---> 874cd42f35ad\n",
            "Step 14/17 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_049f7d1e2434805bd38c937efe4f28e2/lib:$LD_LIBRARY_PATH\n",
            " ---> Running in 8aff64b11545\n",
            "Removing intermediate container 8aff64b11545\n",
            " ---> 092c0f2dd0f0\n",
            "Step 15/17 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
            " ---> 52e3708e802a\n",
            "Step 16/17 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
            " ---> Running in 409a0061e2f5\n",
            "Removing intermediate container 409a0061e2f5\n",
            " ---> 89c04e4a897e\n",
            "Step 17/17 : CMD [\"bash\"]\n",
            " ---> Running in f723ce09fbd6\n",
            "Removing intermediate container f723ce09fbd6\n",
            " ---> a0b9562bcf9f\n",
            "Successfully built a0b9562bcf9f\n",
            "Successfully tagged 8095dee4090c43e39dba374a3a5f3c08.azurecr.io/azureml/azureml_33c08d71c3ff72566deacddac6bf36fc:latest\n",
            "Successfully tagged 8095dee4090c43e39dba374a3a5f3c08.azurecr.io/azureml/azureml_33c08d71c3ff72566deacddac6bf36fc:1\n",
            "2021/04/22 18:33:44 Successfully executed container: acb_step_0\n",
            "2021/04/22 18:33:44 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
            "2021/04/22 18:33:44 Pushing image: 8095dee4090c43e39dba374a3a5f3c08.azurecr.io/azureml/azureml_33c08d71c3ff72566deacddac6bf36fc:1, attempt 1\n",
            "The push refers to repository [8095dee4090c43e39dba374a3a5f3c08.azurecr.io/azureml/azureml_33c08d71c3ff72566deacddac6bf36fc]\n",
            "e9ce672106da: Preparing\n",
            "75f72e6edbd6: Preparing\n",
            "76c6a0fde909: Preparing\n",
            "9fa5d759e613: Preparing\n",
            "5f07858b072a: Preparing\n",
            "debd7fa69038: Preparing\n",
            "1ba7f0c0afc3: Preparing\n",
            "e210b9b95e3d: Preparing\n",
            "a8e262e73fa9: Preparing\n",
            "3216491659cb: Preparing\n",
            "a0addc413928: Preparing\n",
            "07adad8f2cae: Preparing\n",
            "0dd1581dbc3c: Preparing\n",
            "837d6facb613: Preparing\n",
            "debd7fa69038: Waiting\n",
            "1ba7f0c0afc3: Waiting\n",
            "e210b9b95e3d: Waiting\n",
            "a8e262e73fa9: Waiting\n",
            "3216491659cb: Waiting\n",
            "a0addc413928: Waiting\n",
            "07adad8f2cae: Waiting\n",
            "0dd1581dbc3c: Waiting\n",
            "837d6facb613: Waiting\n",
            "76c6a0fde909: Pushed\n",
            "9fa5d759e613: Pushed\n",
            "75f72e6edbd6: Pushed\n",
            "e9ce672106da: Pushed\n",
            "1ba7f0c0afc3: Pushed\n",
            "e210b9b95e3d: Pushed\n",
            "debd7fa69038: Pushed\n",
            "a8e262e73fa9: Pushed\n",
            "07adad8f2cae: Pushed\n",
            "0dd1581dbc3c: Pushed\n",
            "a0addc413928: Pushed\n",
            "837d6facb613: Pushed\n",
            "5f07858b072a: Pushed\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download output video"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloads the video in `output_video` folder"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video(run, target_dir=None):\n",
        "    stitch_run = run.find_step_run(stitch_video_step.name)[0]\n",
        "    port_data = stitch_run.get_details()['outputDatasets'][0]['dataset']\n",
        "    port_data.download(target_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run.wait_for_completion()\n",
        "download_video(pipeline_run, \"output_video_mosaic\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Publish pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = \"style-transfer-batch-inference\"\n",
        "print(pipeline_name)\n",
        "\n",
        "published_pipeline = pipeline.publish(\n",
        "    name=pipeline_name, \n",
        "    description=pipeline_name)\n",
        "print(\"Newly published pipeline id: {}\".format(published_pipeline.id))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get published pipeline\n",
        "This is another way to get the published pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PublishedPipeline\n",
        "\n",
        "# You could retrieve all pipelines that are published, or \n",
        "# just get the published pipeline object that you have the ID for.\n",
        "\n",
        "# Get all published pipeline objects in the workspace\n",
        "all_pub_pipelines = PublishedPipeline.list(ws)\n",
        "\n",
        "# We will iterate through the list of published pipelines and \n",
        "# use the last ID in the list for Schelue operations: \n",
        "print(\"Published pipelines found in the workspace:\")\n",
        "for pub_pipeline in all_pub_pipelines:\n",
        "    print(\"Name:\", pub_pipeline.name,\"\\tDescription:\", pub_pipeline.description, \"\\tId:\", pub_pipeline.id, \"\\tStatus:\", pub_pipeline.status)\n",
        "    if(pub_pipeline.name == pipeline_name):\n",
        "        published_pipeline = pub_pipeline\n",
        "\n",
        "print(\"Published pipeline id: {}\".format(published_pipeline.id))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run pipeline through REST calls for other styles\n",
        "\n",
        "# Get AAD token"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import requests\n",
        "\n",
        "auth = InteractiveLoginAuthentication()\n",
        "aad_token = auth.get_authentication_header()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get endpoint URL"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rest_endpoint = published_pipeline.endpoint\n",
        "print(\"Pipeline REST endpoing: {}\".format(rest_endpoint))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send request and monitor"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'styletransfer_parallel_candy'\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=aad_token,\n",
        "                         json={\"ExperimentName\": experiment_name,\n",
        "                               \"ParameterAssignments\": {\"style\": \"candy\", \"NodeCount\": 3}})\n",
        "\n",
        "run_id = response.json()[\"Id\"]\n",
        "\n",
        "from azureml.pipeline.core.run import PipelineRun\n",
        "published_pipeline_run_candy = PipelineRun(ws.experiments[experiment_name], run_id)\n",
        "\n",
        "# Show detail information of run\n",
        "published_pipeline_run_candy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download output from re-run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline_run_candy.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "download_video(published_pipeline_run_candy, target_dir=\"output_video_candy\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": true,
    "task": "Style transfer",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil joringer asraniwa pansav tracych"
      }
    ],
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "Batch Inferencing",
      "Pipeline"
    ],
    "datasets": [],
    "category": "Other notebooks",
    "framework": [
      "None"
    ],
    "friendly_name": "Style transfer using ParallelRunStep",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}