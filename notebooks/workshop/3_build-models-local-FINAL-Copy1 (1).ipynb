{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to *actually* do machine learning? \n",
    "## Part 2: Build and train models\n",
    "\n",
    "*Made for Microsoft Build 2019*\n",
    "\n",
    "This is the second in a series that walks through how Azure Machine Learning service can speed up your machine learning modelling workflow so you can focus on the interesting tasks that matter. \n",
    "\n",
    "**Goal:**\n",
    "In this notebook, we'll build and train regression models using the data we've prepared in Part 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate py36 && pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "device\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "tqdm.pandas(desc='Progress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_y(df):\n",
    "    y = np.log(df.cost)\n",
    "    df.drop(['cost'], axis=1, inplace=True)\n",
    "    return df, y\n",
    "\n",
    "def split_features(df):\n",
    "    catf = ['pickup_year', 'pickup_month', 'pickup_monthday', 'pickup_weekday', \n",
    "            'pickup_hour', 'pickup_minute', 'pickup_second', 'vendor', 'passengers']\n",
    "\n",
    "    numf = [col for col in df.columns if col not in catf]\n",
    "    for c in catf: \n",
    "        df[c] = df[c].astype('category').cat.as_ordered()\n",
    "        df[c] = df[c].cat.codes+1\n",
    "    \n",
    "    return catf, numf\n",
    "\n",
    "def numericalize(df):\n",
    "    df[name] = col.cat.codes+1\n",
    "\n",
    "def split_dataset(df, y): return train_test_split(df, y, test_size=0.25, random_state=42)\n",
    "\n",
    "def inv_y(y): return np.exp(y)\n",
    "\n",
    "def get_numf_scaler(train): return preprocessing.StandardScaler().fit(train)\n",
    "\n",
    "def scale_numf(df, num, scaler):\n",
    "    cols = numf\n",
    "    index = df.index\n",
    "    scaled = scaler.transform(df[numf])\n",
    "    scaled = pd.DataFrame(scaled, columns=cols, index=index)\n",
    "    return pd.concat([scaled, df.drop(numf, axis=1)], axis=1)\n",
    "\n",
    "class RegressionColumnarDataset(data.Dataset):\n",
    "    def __init__(self, df, cats, y):\n",
    "        self.dfcats = df[cats]\n",
    "        self.dfconts = df.drop(cats, axis=1)\n",
    "        \n",
    "        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n",
    "        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n",
    "        self.y = y.values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "    \n",
    "def rmse(targ, y_pred):\n",
    "    return np.sqrt(mean_squared_error(inv_y(y_pred), inv_y(targ))) #.detach().numpy()\n",
    "\n",
    "def emb_init(x):\n",
    "    x = x.weight.data\n",
    "    sc = 2/(x.size(1)+1)\n",
    "    x.uniform_(-sc,sc)\n",
    "\n",
    "class MixedInputModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn=True):\n",
    "        super().__init__()\n",
    "        for i,(c,s) in enumerate(emb_szs): print(emb_szs[i],c,s)\n",
    "        for i,(c,s) in enumerate(emb_szs): assert c >= 1,'cardinality must be >=2, got emb_szs[{i}]: ({c},{s})'\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont=n_emb, n_cont\n",
    "        \n",
    "        szs = [n_emb+n_cont] + szs\n",
    "        self.lins = nn.ModuleList([nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(sz) for sz in szs[1:]])\n",
    "        for o in self.lins: nn.init.kaiming_normal_(o.weight.data)\n",
    "        self.outp = nn.Linear(szs[-1], out_sz)\n",
    "        nn.init.kaiming_normal_(self.outp.weight.data)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.use_bn,self.y_range = use_bn,y_range\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "        for l,d,b in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(l(x))\n",
    "            if self.use_bn: x = b(x)\n",
    "            x = d(x)\n",
    "        x = self.outp(x)\n",
    "        if self.y_range:\n",
    "            x = torch.sigmoid(x)\n",
    "            x = x*(self.y_range[1] - self.y_range[0])\n",
    "            x = x+self.y_range[0]\n",
    "        return x.squeeze()\n",
    "\n",
    "def fit(model, train_dl, val_dl, loss_fn, opt, scheduler, epochs=3):\n",
    "    num_batch = len(train_dl)\n",
    "    for epoch in tnrange(epochs):      \n",
    "        y_true_train = list()\n",
    "        y_pred_train = list()\n",
    "        total_loss_train = 0          \n",
    "        \n",
    "        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n",
    "        for cat, cont, y in t:\n",
    "            cat = cat\n",
    "            cont = cont\n",
    "            y = y\n",
    "            \n",
    "            t.set_description('Epoch {epoch}')\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            pred = model(cat, cont)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            lr[epoch].append(opt.param_groups[0]['lr'])\n",
    "            tloss[epoch].append(loss.item())\n",
    "            scheduler.step()\n",
    "            opt.step()\n",
    "            \n",
    "            t.set_postfix(loss=loss.item())\n",
    "            \n",
    "            y_true_train += list(y.cpu().data.numpy())\n",
    "            y_pred_train += list(pred.cpu().data.numpy())\n",
    "            total_loss_train += loss.item()\n",
    "            \n",
    "        train_acc = rmse(y_true_train, y_pred_train)\n",
    "        train_loss = total_loss_train/len(train_dl)\n",
    "        \n",
    "        if val_dl:\n",
    "            y_true_val = list()\n",
    "            y_pred_val = list()\n",
    "            total_loss_val = 0\n",
    "            for cat, cont, y in tqdm_notebook(val_dl, leave=False):\n",
    "                cat = cat\n",
    "                cont = cont\n",
    "                y = y\n",
    "                pred = model(cat, cont)\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                y_true_val += list(y.cpu().data.numpy())\n",
    "                y_pred_val += list(pred.cpu().data.numpy())\n",
    "                total_loss_val += loss.item()\n",
    "                vloss[epoch].append(loss.item())\n",
    "            valacc = rmse(y_true_val, y_pred_val)\n",
    "            valloss = total_loss_val/len(valdl)\n",
    "            print('Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f} | val_loss: {valloss:.4f} val_rmse: {valacc:.4f}')\n",
    "        else:\n",
    "            print('Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f}')\n",
    "    \n",
    "    return lr, tloss, vloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tha data is a half-million random sample from the 55M original training set from the [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data) Kaggle's challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "dataset = Dataset.get(workspace = ws, name='nyc_taxi_clean')\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df\n",
    "df_new[\"pickup_datetime\"] = pd.to_numeric(df_new[\"pickup_datetime\"])\n",
    "df_new[\"dropoff_datetime\"] = pd.to_numeric(df_new[\"dropoff_datetime\"])\n",
    "df_new[\"pickup_weekday\"] = pd.to_datetime(df_new[\"pickup_weekday\"], errors = 'coerce')\n",
    "df_new['pickup_weekday'] = df_new['pickup_weekday'].dt.weekday\n",
    "df_new[\"dropoff_weekday\"] = pd.to_datetime(df_new[\"dropoff_weekday\"], errors = 'coerce')\n",
    "df_new['dropoff_weekday'] = df_new['dropoff_weekday'].dt.weekday\n",
    "df_new=df_new.fillna(\"1\")\n",
    "df=df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passengers.describe()\n",
    "\n",
    "df.passengers.quantile([.85, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cost.describe()\n",
    "\n",
    "df.cost.quantile([.85, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[(df.cost > 0) & (df.passengers < 6) & (df.cost < 53),:]\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y = separate_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.hist(bins=20, figsize=(8,6))\n",
    "_ = ax.set_xlabel(\"Ride Value (EUR)\")\n",
    "_ = ax.set_ylabel(\"# Rides\")\n",
    "_ = ax.set_title('Ditribution of Ride Values (USD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catf, numf = split_features(df)\n",
    "\n",
    "len(catf)\n",
    "catf\n",
    "\n",
    "len(numf)\n",
    "numf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(df, y)\n",
    "\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = get_numf_scaler(X_train[numf])\n",
    "\n",
    "X_train_sc = scale_numf(X_train, numf, scaler)\n",
    "X_train_sc.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sc = scale_numf(X_test, numf, scaler)\n",
    "\n",
    "X_train_sc.shape\n",
    "X_test_sc.shape\n",
    "X_test_sc.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pytorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = RegressionColumnarDataset(X_train_sc, catf, y_train)\n",
    "valds = RegressionColumnarDataset(X_test_sc, catf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 128,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "traindl = data.DataLoader(trainds, **params)\n",
    "valdl = data.DataLoader(valds, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model and related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = (0, y_train.max()*1.2)\n",
    "y_range\n",
    "\n",
    "cat_sz = [(c, df[c].max()+1) for c in catf]\n",
    "cat_sz\n",
    "\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MixedInputModel(emb_szs=emb_szs, \n",
    "                    n_cont=len(df.columns)-len(catf), \n",
    "                    emb_drop=0.04, \n",
    "                    out_sz=1, \n",
    "                    szs=[1000,500,250], \n",
    "                    drops=[0.001,0.01,0.01], \n",
    "                    y_range=y_range).to(device)\n",
    "\n",
    "opt = optim.Adam(m.parameters(), 1e-2)\n",
    "lr_cosine = lr_scheduler.CosineAnnealingLR(opt, 1000)\n",
    "\n",
    "lr = defaultdict(list)\n",
    "tloss = defaultdict(list)\n",
    "vloss = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, tloss, vloss = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, scheduler=lr_cosine, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [np.mean(tloss[el]) for el in tloss]\n",
    "v = [np.mean(vloss[el]) for el in vloss]\n",
    "p = pd.DataFrame({'Train Loss': t, 'Validation Loss': v, 'Epochs': range(10)})\n",
    "\n",
    "_ = p.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n",
    "           title='Train and Validation Loss over Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Model to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('./modelfiles/', 'model.pkl')\n",
    "torch.save(m, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model with AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.register(model_path = model_path,\n",
    "                     model_name = \"pytorch_regression\",\n",
    "                     tags = {\"onnx\": \"demo\"},\n",
    "                    description = \"Taxi PyTorch Regression\",\n",
    "                    workspace = ws)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
