{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to *actually* do machine learning? \n",
    "## Part 3: Deploy models\n",
    "\n",
    "*Made for Microsoft Build 2019*\n",
    "\n",
    "This is the third in a series that walks through how Azure Machine Learning service can speed up your machine learning modelling workflow so you can focus on the interesting tasks that matter. \n",
    "\n",
    "**Goal:**\n",
    "In this notebook, we'll prepare deploy our pytorch model on gpu and cpu cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!source activate py36\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a VM with your PyTorch model in the Cloud\n",
    "\n",
    "### Load Azure ML workspace\n",
    "\n",
    "We begin by instantiating a workspace object from the existing workspace created earlier in the configuration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "import azureml.dataprep as dprep\n",
    "from azureml.core.model import Model\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builddemossus\n",
      "southcentralus\n",
      "build2019\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# read existing workspace from config.json\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "print(ws.name, ws.location, ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your registered model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(name=\"pytorch_regression_remote\", workspace=ws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Displaying your registered models\n",
    "\n",
    "This step is not required, so feel free to skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pytorch_regression_remote \tVersion: 3 \tDescription: None {}\n",
      "Name: pytorch_regression \tVersion: 3 \tDescription: Taxi PyTorch Regression {'onnx': 'demo'}\n"
     ]
    }
   ],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify our Score and Environment Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to deploy our PyTorch Model on AML with inference. We begin by writing a score.py file, which will help us run the model in our Azure ML virtual machine (VM), and then specify our environment by writing a yml file. \n",
    "\n",
    "### Write Score File\n",
    "\n",
    "A score file is what tells our Azure cloud service what to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def haversine_distance(df, start_lat, end_lat, start_lng, end_lng, prefix):\n",
    "    \"\"\"\n",
    "    calculates haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    R = 6371  #radius of earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[start_lat])\n",
    "    phi2 = np.radians(df[end_lat])\n",
    "    \n",
    "    delta_phi = np.radians(df[end_lat]-df[start_lat])\n",
    "    delta_lambda = np.radians(df[end_lng]-df[start_lng])\n",
    "    \n",
    "        \n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (R * c) #in kilometers\n",
    "    df[prefix+'distance_km'] = d\n",
    "\n",
    "def add_datepart(df, col, prefix):\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[prefix + n] = getattr(df[col].dt, n.lower())\n",
    "    df[prefix + 'Elapsed'] = df[col].astype(np.int64) // 10 ** 9\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "def reject_outliers(data, m = 2.):\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d/(mdev if mdev else 1.)\n",
    "    return s<m\n",
    "\n",
    "def parse_gps(df, prefix):\n",
    "    lat = prefix + '_latitude'\n",
    "    lon = prefix + '_longitude'\n",
    "    df[prefix + '_x'] = np.cos(df[lat]) * np.cos(df[lon])\n",
    "    df[prefix + '_y'] = np.cos(df[lat]) * np.sin(df[lon]) \n",
    "    df[prefix + '_z'] = np.sin(df[lat])\n",
    "    df.drop([lat, lon], axis=1, inplace=True)\n",
    "    \n",
    "def prepare_dataset(df):\n",
    "    df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime, infer_datetime_format=True)\n",
    "    add_datepart(df, 'pickup_datetime', 'pickup')\n",
    "    haversine_distance(df, 'pickup_latitude', 'dropoff_latitude', 'pickup_longitude', 'dropoff_longitude', '')\n",
    "    parse_gps(df, 'pickup')\n",
    "    parse_gps(df, 'dropoff')\n",
    "    df.dropna(inplace=True)\n",
    "    y = np.log(df.fare_amount)\n",
    "    df.drop(['key', 'fare_amount'], axis=1, inplace=True)\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "def split_features(df):\n",
    "    catf = ['pickupYear', 'pickupMonth', 'pickupWeek', 'pickupDay', 'pickupDayofweek', \n",
    "            'pickupDayofyear', 'pickupHour', 'pickupMinute', 'pickupSecond', 'pickupIs_month_end',\n",
    "            'pickupIs_month_start', 'pickupIs_quarter_end', 'pickupIs_quarter_start',\n",
    "            'pickupIs_year_end', 'pickupIs_year_start']\n",
    "\n",
    "    numf = [col for col in df.columns if col not in catf]\n",
    "    for c in catf: \n",
    "        df[c] = df[c].astype('category').cat.as_ordered()\n",
    "        df[c] = df[c].cat.codes+1\n",
    "    \n",
    "    return catf, numf\n",
    "\n",
    "def numericalize(df):\n",
    "    df[name] = col.cat.codes+1\n",
    "\n",
    "def split_dataset(df, y): return train_test_split(df, y, test_size=0.25, random_state=42)\n",
    "\n",
    "def inv_y(y): return np.exp(y)\n",
    "\n",
    "def get_numf_scaler(train): return preprocessing.StandardScaler().fit(train)\n",
    "\n",
    "def scale_numf(df, num, scaler):\n",
    "    cols = numf\n",
    "    index = df.index\n",
    "    scaled = scaler.transform(df[numf])\n",
    "    scaled = pd.DataFrame(scaled, columns=cols, index=index)\n",
    "    return pd.concat([scaled, df.drop(numf, axis=1)], axis=1)\n",
    "\n",
    "class RegressionColumnarDataset(data.Dataset):\n",
    "    def __init__(self, df, cats, y):\n",
    "        self.dfcats = df[cats]\n",
    "        self.dfconts = df.drop(cats, axis=1)\n",
    "        \n",
    "        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n",
    "        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n",
    "        self.y = y.values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "    \n",
    "def rmse(targ, y_pred):\n",
    "    return np.sqrt(mean_squared_error(inv_y(y_pred), inv_y(targ))) #.detach().numpy()\n",
    "\n",
    "def emb_init(x):\n",
    "    x = x.weight.data\n",
    "    sc = 2/(x.size(1)+1)\n",
    "    x.uniform_(-sc,sc)\n",
    "\n",
    "class MixedInputModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn=True):\n",
    "        super().__init__()\n",
    "        for i,(c,s) in enumerate(emb_szs): assert c > 1,'cardinality must be >=2, got emb_szs[{i}]: ({c},{s})'\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont=n_emb, n_cont\n",
    "        \n",
    "        szs = [n_emb+n_cont] + szs\n",
    "        self.lins = nn.ModuleList([nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(sz) for sz in szs[1:]])\n",
    "        for o in self.lins: nn.init.kaiming_normal_(o.weight.data)\n",
    "        self.outp = nn.Linear(szs[-1], out_sz)\n",
    "        nn.init.kaiming_normal_(self.outp.weight.data)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.use_bn,self.y_range = use_bn,y_range\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "        #x = torch.ones(128,204)\n",
    "        #print(x.shape)\n",
    "        for l,d,b in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(l(x))\n",
    "            if self.use_bn: x = b(x)\n",
    "            x = d(x)\n",
    "        x = self.outp(x)\n",
    "        if self.y_range:\n",
    "            x = torch.sigmoid(x)\n",
    "            x = x*(self.y_range[1] - self.y_range[0])\n",
    "            x = x+self.y_range[0]\n",
    "        return x.squeeze()\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('python_regression_remote')\n",
    "    model = torch.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "\n",
    "def run(input_data):\n",
    "    my_tuple=tuple([tuple(x) for x in json.loads(input_data)])\n",
    "    input_data=(torch.tensor(my_tuple[0]), torch.tensor(my_tuple[1]))\n",
    "\n",
    "\n",
    "    result = model(torch.tensor(my_tuple[0]), torch.tensor(my_tuple[1]))\n",
    "    result= result.detach()\n",
    "    result = result.numpy()\n",
    "    result = float(result)\n",
    "    return json.dumps(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Environment File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"torch\",\"numpy\", \"azureml-core\", \"azureml-dataprep\",\"scikit-learn\",\"seaborn\",\"tqdm\"])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Container Image\n",
    "\n",
    "This step will likely take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Running.....................................................\n",
      "SucceededImage creation operation finished for image pytorchimage.cpu:11, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# added flag to include CUDA execution provider\n",
    "cpu_image_config = ContainerImage.image_configuration(execution_script = \"score.py\",\n",
    "                                                  runtime = \"python\",\n",
    "                                                  conda_file = \"myenv.yml\",\n",
    "                                                  description = \"Taxi Regression ONNX Runtime container\",\n",
    "                                                  tags = {\"demo\": \"pytorch\"})\n",
    "\n",
    "# use the ONNX Runtime CPU base image\n",
    "#cpu_image_config.base_image = \"mcr.microsoft.com/azureml/onnxruntime:v0.4.0\"\n",
    "\n",
    "cpu_image = ContainerImage.create(name = \"pytorchimage.cpu\",\n",
    "                              # this is the model object\n",
    "                              models = [model],\n",
    "                              image_config = cpu_image_config,\n",
    "                              workspace = ws)\n",
    "\n",
    "cpu_image.wait_for_creation(show_output = True)\n",
    "\n",
    "# OR use an existing image from your image registry\n",
    "# cpu_image = ContainerImage(name = \"onnximage.cpu\", workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you need to debug your code, the next line of code accesses the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://builddemossus1844419833.blob.core.windows.net/azureml/ImageLogs/21d5c7ca-f432-4365-9ff9-8b9538b12a34/build.log?sv=2018-03-28&sr=b&sig=tOwFTHrOw84Sh%2FHKDhgxHgXaLvxWMXeqdhawesYAI6M%3D&st=2019-05-08T14%3A26%3A47Z&se=2019-06-07T14%3A31%3A47Z&sp=rl\n"
     ]
    }
   ],
   "source": [
    "print(cpu_image.image_build_log_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're all done specifying what we want our virtual machine to do. Let's configure and deploy our container image.\n",
    "\n",
    "### Deploy the container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "# create the AKS service with CPU nodes\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.image import Image\n",
    "from azureml.core.model import Model\n",
    "\n",
    "cpu_aks_name = 'AKS-CPU'\n",
    "\n",
    "cpu_aks_target = ComputeTarget(workspace = ws, name=cpu_aks_name)\n",
    "\n",
    "try:\n",
    "    cpu_aks_target = ComputeTarget(workspace = ws, name=cpu_aks_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "     # Use the  configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration(vm_size='Standard_D3', location='East US2' )\n",
    "     # Create the cluster\n",
    "    cpu_aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                   name = cpu_aks_name, \n",
    "                                   provisioning_configuration = prov_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long running operation information not known, unable to poll. Current state is Succeeded\n",
      "Succeeded\n",
      "None\n",
      "CPU times: user 13.1 ms, sys: 122 µs, total: 13.2 ms\n",
      "Wall time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpu_aks_target.wait_for_completion(show_output = True)\n",
    "print(cpu_aks_target.provisioning_state)\n",
    "print(cpu_aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_aks_config = AksWebservice.deploy_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating service\n",
      "Running........................\n",
      "FailedAKS service creation operation finished, operation \"Failed\"\n",
      "Service creation polling reached terminal state, current service state: Transitioning\n",
      "{\n",
      "  \"code\": \"KubernetesDeploymentFailed\",\n",
      "  \"statusCode\": 400,\n",
      "  \"message\": \"Kubernetes Deployment failed\",\n",
      "  \"details\": [\n",
      "    {\n",
      "      \"code\": \"CrashLoopBackOff\",\n",
      "      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance cpu-aks-service.\\nYou can also try to run image builddemossu40880fa1.azurecr.io/pytorchimage.cpu:11 locally. Please refer to http://aka.ms/debugimage for more information.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Transitioning\n",
      "2019-05-08T14:33:57,763305853+00:00 - nginx/run \n",
      "2019-05-08T14:33:57,764145256+00:00 - rsyslog/run \n",
      "2019-05-08T14:33:57,765378160+00:00 - iot-server/run \n",
      "2019-05-08T14:33:57,768913772+00:00 - gunicorn/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2019-05-08T14:33:57,838730506+00:00 - iot-server/finish 1 0\n",
      "2019-05-08T14:33:57,839775409+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.6.0\n",
      "Listening at: http://127.0.0.1:9090 (13)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 45\n",
      "Initializing logger\n",
      "Starting up app insights client\n",
      "Starting up request id generator\n",
      "Starting up app insight hooks\n",
      "Invoking user's init function\n",
      "2019-05-08 14:34:03,408 | azureml.core.run | DEBUG | Could not load run context Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run., switching offline: False\n",
      "2019-05-08 14:34:03,408 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n",
      "2019-05-08 14:34:03,408 | azureml.core.model | DEBUG | RunEnvironmentException: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "2019-05-08 14:34:03,408 | azureml.core.model | DEBUG | Checking root for python_regression_remote because candidate dir azureml-models had 1 nodes: azureml-models/pytorch_regression_remote/3/model_remote.pkl\n",
      "User's init function failed\n",
      "Encountered Exception Traceback (most recent call last):\n",
      "  File \"/var/azureml-app/aml_blueprint.py\", line 158, in register\n",
      "    main.init()\n",
      "  File \"/var/azureml-app/main.py\", line 87, in init\n",
      "    driver_module.init()\n",
      "  File \"score.py\", line 184, in init\n",
      "    model_path = Model.get_model_path('python_regression_remote')\n",
      "  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 411, in get_model_path\n",
      "    return Model._get_model_path_local(model_name, version)\n",
      "  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 432, in _get_model_path_local\n",
      "    return Model._get_model_path_local_from_root(model_name)\n",
      "  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 475, in _get_model_path_local_from_root\n",
      "    \"set logging level to DEBUG.\".format(candidate_model_path))\n",
      "azureml.exceptions._azureml_exception.ModelNotFoundException: Model not found in cache or in root at ./python_regression_remote. For more info,set logging level to DEBUG.\n",
      "\n",
      "Worker exiting (pid: 45)\n",
      "Shutting down: Master\n",
      "Reason: Worker failed to boot.\n",
      "2019-05-08T14:34:03,707118252+00:00 - gunicorn/finish 3 0\n",
      "2019-05-08T14:34:03,708185155+00:00 - Exit code 3 is not normal. Killing image.\n",
      "\n",
      "CPU times: user 510 ms, sys: 34.1 ms, total: 544 ms\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpu_aks_service_name ='cpu-aks-service'\n",
    "\n",
    "cpu_aks_service = Webservice.deploy_from_image(workspace = ws, \n",
    "                                           name = cpu_aks_service_name,\n",
    "                                           image = cpu_image,\n",
    "                                           deployment_config = cpu_aks_config,\n",
    "                                           deployment_target = cpu_aks_target)\n",
    "cpu_aks_service.wait_for_deployment(show_output = True)\n",
    "print(cpu_aks_service.state)\n",
    "print(cpu_aks_service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-08T14:33:57,763305853+00:00 - nginx/run \n",
      "2019-05-08T14:33:57,764145256+00:00 - rsyslog/run \n",
      "2019-05-08T14:33:57,765378160+00:00 - iot-server/run \n",
      "2019-05-08T14:33:57,768913772+00:00 - gunicorn/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2019-05-08T14:33:57,838730506+00:00 - iot-server/finish 1 0\n",
      "2019-05-08T14:33:57,839775409+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.6.0\n",
      "Listening at: http://127.0.0.1:9090 (13)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 45\n",
      "Initializing logger\n",
      "Starting up app insights client\n",
      "Starting up request id generator\n",
      "Starting up app insight hooks\n",
      "Invoking user's init function\n",
      "2019-05-08 14:34:03,408 | azureml.core.run | DEBUG | Could not load run context Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run., switching offline: False\n",
      "2019-05-08 14:34:03,408 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n",
      "2019-05-08 14:34:03,408 | azureml.core.model | DEBUG | RunEnvironmentException: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n",
      "2019-05-08 14:34:03,408 | azureml.core.model | DEBUG | Checking root for python_regression_remote because candidate dir azureml-models had 1 nodes: azureml-models/pytorch_regression_remote/3/model_remote.pkl\n",
      "User's init function failed\n",
      "Encountered Exception Traceback (most recent call last):\n",
      "  File \"/var/azureml-app/aml_blueprint.py\", line 158, in register\n",
      "    main.init()\n",
      "  File \"/var/azureml-app/main.py\", line 87, in init\n",
      "    driver_module.init()\n",
      "  File \"score.py\", line 184, in init\n",
      "    model_path = Model.get_model_path('python_regression_remote')\n",
      "  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 411, in get_model_path\n",
      "    return Model._get_model_path_local(model_name, version)\n",
      "  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 432, in _get_model_path_local\n",
      "    return Model._get_model_path_local_from_root(model_name)\n",
      "  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 475, in _get_model_path_local_from_root\n",
      "    \"set logging level to DEBUG.\".format(candidate_model_path))\n",
      "azureml.exceptions._azureml_exception.ModelNotFoundException: Model not found in cache or in root at ./python_regression_remote. For more info,set logging level to DEBUG.\n",
      "\n",
      "Worker exiting (pid: 45)\n",
      "Shutting down: Master\n",
      "Reason: Worker failed to boot.\n",
      "2019-05-08T14:34:03,707118252+00:00 - gunicorn/finish 3 0\n",
      "2019-05-08T14:34:03,708185155+00:00 - Exit code 3 is not normal. Killing image.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if cpu_aks_service.state != 'Healthy':\n",
    "    # run this command for debugging.\n",
    "    print(cpu_aks_service.get_logs())\n",
    "    # If your deployment fails, make sure to delete your aks_service before trying again!\n",
    "    cpu_aks_service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success!\n",
    "\n",
    "If you've made it this far, you've deployed a working VM with a PyTorch regression model running in the cloud using Azure ML. Congratulations!\n",
    "\n",
    "Let's see how well our model deals with our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try predicting your own trips!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ([1, 1, 5, 1, 20, 31, 21, 1, 2],\n",
    " [-0.20771729946136475,\n",
    "  -0.20766863226890564,\n",
    "  0.0,\n",
    "  0.0,\n",
    "  -0.25793421268463135,\n",
    "  0.0,\n",
    "  0.9385162591934204,\n",
    "  1.1996960639953613,\n",
    "  0.6472086310386658,\n",
    "  -0.20173433423042297,\n",
    "  -0.8676630854606628,\n",
    "  -0.35962310433387756,\n",
    "  -0.5872217416763306,\n",
    "  -0.9175888895988464]) // THIS is your input data\n",
    "\n",
    "# input_data=json.dumps([([[ 1,  1,  5,  1, 20, 31, 21,  1,  2]]),\n",
    "# ([[-0.2077173 , -0.20766863,  0.        ,  0.        , -0.2579342 ,\n",
    "#           0.        ,  0.93851626,  1.1996961 ,  0.64720863, -0.20173433,\n",
    "#          -0.8676631 , -0.3596231 , -0.58722174, -0.9175889 ]])])\n",
    "\n",
    "result = service.run(input_data=json.dumps(input_data))\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANUP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to delete your service after you are done using it!\n",
    "cpu_aks_service.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "viswamy"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "msauthor": "vinitra.swamy"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
